{
	"name": "02A_105_RptTrackLevel_BranchVolumesReport",
	"properties": {
		"folder": {
			"name": "TRANSFORM/02 STRUCTURED/Reports/LMS"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "DevSparkPool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "70591a63-3b85-42f0-b077-ef053a44873c"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace/bigDataPools/DevSparkPool",
				"name": "DevSparkPool",
				"type": "Spark",
				"endpoint": "https://citylogistics-synapseanalytics-workspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/DevSparkPool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.2",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
					"# if 'prod' in Environment:\r\n",
					"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
					"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
					"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
					"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
					"# else:\r\n",
					"#     StorageAccount = 'citylogisticsstorage'\r\n",
					"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
					"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
					"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
					"\r\n",
					"# # ' + StorageAccount + '"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# #Create DataFrame for the stlmstrack LMS Table\r\n",
					"# stlmstrack = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stlmstrack.parquet', format='parquet')\r\n",
					"# stlmstrack.createOrReplaceTempView(\"stlmstrack\")\r\n",
					"\r\n",
					"# #Create DataFrame for the stparcel LMS Table\r\n",
					"# stparcel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stparcel.parquet', format='parquet')\r\n",
					"# stparcel.createOrReplaceTempView(\"stparcel\")\r\n",
					"\r\n",
					"# #Create DataFrame for the stdeliverypickupcustomer LMS Table\r\n",
					"# stdeliverypickupcustomer = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stdeliverypickupcustomer.parquet', format='parquet')\r\n",
					"# stdeliverypickupcustomer.createOrReplaceTempView(\"stdeliverypickupcustomer\")\r\n",
					"\r\n",
					"# #Create DataFrame for the stbillcustomer LMS Table\r\n",
					"# stbillcustomer = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stbillcustomer.parquet', format='parquet')\r\n",
					"# stbillcustomer.createOrReplaceTempView(\"stbillcustomer\")\r\n",
					"\r\n",
					"# #Create DataFrame for the stlocation LMS Table\r\n",
					"# stlocation = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stlocation.parquet', format='parquet')\r\n",
					"# stlocation.createOrReplaceTempView(\"stlocation\")\r\n",
					"\r\n",
					"# #create dataframe for the stsroute lms table\r\n",
					"# stsroute = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stsroute.parquet', format='parquet')\r\n",
					"# stsroute.createOrReplaceTempView(\"stsroute\")\r\n",
					"\r\n",
					"# #Create DataFrame for the stzone LMS Table\r\n",
					"# stzone = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stzone.parquet', format='parquet')\r\n",
					"# stzone.createOrReplaceTempView(\"stzone\")\r\n",
					"\r\n",
					"# #Create DataFrame for the dbodcorder LMS Table\r\n",
					"# dbodcorder = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodc_order.parquet', format='parquet')\r\n",
					"# dbodcorder.createOrReplaceTempView(\"dbodcorder\")\r\n",
					"\r\n",
					"# #Create DataFrame for the dbodcprepack LMS Table\r\n",
					"# dbodcprepack = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodc_prepack.parquet', format='parquet')\r\n",
					"# dbodcprepack.createOrReplaceTempView(\"dbodcprepack\")\r\n",
					"\r\n",
					"# #Create DataFrame for the dbodctransfer LMS Table\r\n",
					"# dbodctransfer = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodc_transfer.parquet', format='parquet')\r\n",
					"# dbodctransfer.createOrReplaceTempView(\"dbodctransfer\")\r\n",
					""
				],
				"execution_count": 2
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"<mark>**rpttracklevel_branchvolumes**</mark>"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"CREATE OR REPLACE TEMP VIEW\r\n",
					"rpttracklevel_in\r\n",
					"AS\r\n",
					"SELECT\r\n",
					"\r\n",
					"lt.lms_location_id as locationid,\r\n",
					"lt.lms_location_description as location,\r\n",
					"t.lms_track_closedt as date,\r\n",
					"bc.lms_customer_id as  billingcustomerid,\r\n",
					"bc.lms_customer_name as  billingcustomer,\r\n",
					"dz.lms_zone_description as deliverycustcustomerzonedesc,\r\n",
					"ds.lms_sroute_code as deliverysroutecode,\r\n",
					"dc.lms_customer_cref as deliverycustomercref,\r\n",
					"dc.lms_customer_id as  deliverycustomerid,\r\n",
					"dc.lms_customer_name as  deliverycustomer,\r\n",
					"ld.lms_location_id as deliverycustomerzonelocationid,\r\n",
					"ld.lms_location_description as deliverycustomerzonelocationdescription,\r\n",
					"1 as pid,\r\n",
					"t.lms_track_parcelweight as parcelweight,\r\n",
					"COALESCE\r\n",
					"(\r\n",
					"\tCASE\t\r\n",
					"\t\tWHEN COALESCE(lt.lms_location_description,'') <> COALESCE(ld.lms_location_description,'') THEN 1\r\n",
					"\t\tELSE 0\r\n",
					"\tEND,0\r\n",
					") as crossdock,\r\n",
					"0 as dcr,\r\n",
					"0 as pidd,\r\n",
					"0 as parcelweightd,\r\n",
					"0 as pidl,\r\n",
					"0 as parcelweightl\r\n",
					"\r\n",
					"FROM stlmstrack t\r\n",
					"LEFT JOIN stlocation lt on lt.lms_location_id = t.lms_track_tolocid\r\n",
					"LEFT JOIN stbillcustomer bc on bc.lms_customer_id =  t.lms_track_orderbillcustid\r\n",
					"LEFT JOIN stdeliverypickupcustomer dc on dc.lms_customer_id =  t.lms_track_orderdelivercustid\r\n",
					"--LEFT JOIN stdeliverypickupcustomer pc on pc.lms_customer_id =  t.lms_track_orderpickupcustid\r\n",
					"LEFT JOIN stsroute ds on ds.lms_sroute_id = dc.lms_customer_srouteid\r\n",
					"LEFT JOIN stzone dz on dz.lms_zone_id = ds.lms_sroute_zoneid\r\n",
					"LEFT JOIN stlocation ld on ld.lms_location_id = dz.lms_zone_locid\r\n",
					"\r\n",
					"WHERE \r\n",
					"t.lms_track_closedt >= (to_timestamp(year(current_date())-1||'-'||(month(current_date()))||'-'||'01'))\r\n",
					"AND lms_track_closedt IS NOT NULL\r\n",
					"AND t.lms_track_tracktypeid = 2     \r\n",
					"\r\n",
					""
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"rpttracklevel_in = spark.sql(\"SELECT * FROM rpttracklevel_in\")"
				],
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"CREATE OR REPLACE TEMP VIEW\r\n",
					"rpttracklevel_outdn\r\n",
					"AS\r\n",
					"SELECT\r\n",
					"\r\n",
					"lf.lms_location_id as locationid,\r\n",
					"lf.lms_location_description as location,\r\n",
					"t.lms_track_opendt as date,\r\n",
					"bc.lms_customer_id as  billingcustomerid,\r\n",
					"bc.lms_customer_name as  billingcustomer,\r\n",
					"dz.lms_zone_description as deliverycustcustomerzonedesc,\r\n",
					"ds.lms_sroute_code as deliverysroutecode,\r\n",
					"dc.lms_customer_cref as deliverycustomercref,\r\n",
					"dc.lms_customer_id as  deliverycustomerid,\r\n",
					"dc.lms_customer_name as  deliverycustomer,\r\n",
					"ld.lms_location_id as deliverycustomerzonelocationid,\r\n",
					"ld.lms_location_description as deliverycustomerzonelocationdescription,\r\n",
					"0 as pid,\r\n",
					"0 as parcelweight,\r\n",
					"0 as crossdock,\r\n",
					"0 as dcr,\r\n",
					"1 as pidd,\r\n",
					"t.lms_track_parcelweight as parcelweightd,\r\n",
					"0 as pidl,\r\n",
					"0 as parcelweightl\r\n",
					"\r\n",
					"FROM stlmstrack t\r\n",
					"LEFT JOIN stlocation lf on lf.lms_location_id = t.lms_track_fromlocid\r\n",
					"LEFT JOIN stbillcustomer bc on bc.lms_customer_id =  t.lms_track_orderbillcustid\r\n",
					"LEFT JOIN stdeliverypickupcustomer dc on dc.lms_customer_id =  t.lms_track_orderdelivercustid\r\n",
					"--LEFT JOIN stdeliverypickupcustomer pc on pc.lms_customer_id =  t.lms_track_orderpickupcustid\r\n",
					"LEFT JOIN stsroute ds on ds.lms_sroute_id = dc.lms_customer_srouteid\r\n",
					"LEFT JOIN stzone dz on dz.lms_zone_id = ds.lms_sroute_zoneid\r\n",
					"LEFT JOIN stlocation ld on ld.lms_location_id = dz.lms_zone_locid\r\n",
					"\r\n",
					"WHERE \r\n",
					"t.lms_track_opendt >= (to_timestamp(year(current_date())-1||'-'||(month(current_date()))||'-'||'01'))\r\n",
					"AND t.lms_track_tracktypeid = 6     "
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"rpttracklevel_outdn = spark.sql(\"SELECT * FROM rpttracklevel_outdn\")"
				],
				"execution_count": 6
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"CREATE OR REPLACE TEMP VIEW\r\n",
					"rpttracklevel_outlh\r\n",
					"AS\r\n",
					"SELECT\r\n",
					"\r\n",
					"lf.lms_location_id as locationid,\r\n",
					"lf.lms_location_description as location,\r\n",
					"t.lms_track_opendt as date,\r\n",
					"bc.lms_customer_id as  billingcustomerid,\r\n",
					"bc.lms_customer_name as  billingcustomer,\r\n",
					"dz.lms_zone_description as deliverycustcustomerzonedesc,\r\n",
					"ds.lms_sroute_code as deliverysroutecode,\r\n",
					"dc.lms_customer_cref as deliverycustomercref,\r\n",
					"dc.lms_customer_id as  deliverycustomerid,\r\n",
					"dc.lms_customer_name as  deliverycustomer,\r\n",
					"ld.lms_location_id as deliverycustomerzonelocationid,\r\n",
					"ld.lms_location_description as deliverycustomerzonelocationdescription,\r\n",
					"0 as pid,\r\n",
					"0 as parcelweight,\r\n",
					"0 as crossdock,\r\n",
					"0 as dcr,\r\n",
					"0 as pidd,\r\n",
					"0 parcelweightd,\r\n",
					"0 as pidl,\r\n",
					"t.lms_track_parcelweight as parcelweightl\r\n",
					"\r\n",
					"FROM stlmstrack t\r\n",
					"LEFT JOIN stlocation lf on lf.lms_location_id = t.lms_track_fromlocid\r\n",
					"LEFT JOIN stbillcustomer bc on bc.lms_customer_id =  t.lms_track_orderbillcustid\r\n",
					"LEFT JOIN stdeliverypickupcustomer dc on dc.lms_customer_id =  t.lms_track_orderdelivercustid\r\n",
					"--LEFT JOIN stdeliverypickupcustomer pc on pc.lms_customer_id =  t.lms_track_orderpickupcustid\r\n",
					"LEFT JOIN stsroute ds on ds.lms_sroute_id = dc.lms_customer_srouteid\r\n",
					"LEFT JOIN stzone dz on dz.lms_zone_id = ds.lms_sroute_zoneid\r\n",
					"LEFT JOIN stlocation ld on ld.lms_location_id = dz.lms_zone_locid\r\n",
					"\r\n",
					"WHERE \r\n",
					"t.lms_track_opendt >= (to_timestamp(year(current_date())-1||'-'||(month(current_date()))||'-'||'01'))\r\n",
					"AND t.lms_track_tracktypeid <> 6     "
				],
				"execution_count": 7
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"rpttracklevel_outlh = spark.sql(\"SELECT * FROM rpttracklevel_outlh\")"
				],
				"execution_count": 8
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"CREATE OR REPLACE TEMP VIEW\r\n",
					"rpttracklevel_dcrec\r\n",
					"AS\r\n",
					"SELECT\r\n",
					"\r\n",
					"lt.lms_location_id as locationid,\r\n",
					"lt.lms_location_description as location,\r\n",
					"dct.acceptancedate as date,\r\n",
					"bc.lms_customer_id as  billingcustomerid,\r\n",
					"bc.lms_customer_name as  billingcustomer,\r\n",
					"dz.lms_zone_description as deliverycustcustomerzonedesc,\r\n",
					"ds.lms_sroute_code as deliverysroutecode,\r\n",
					"dc.lms_customer_cref as deliverycustomercref,\r\n",
					"dc.lms_customer_id as  deliverycustomerid,\r\n",
					"dc.lms_customer_name as  deliverycustomer,\r\n",
					"ld.lms_location_id as deliverycustomerzonelocationid,\r\n",
					"ld.lms_location_description as deliverycustomerzonelocationdescription,\r\n",
					"1 as pid,\r\n",
					"p.lms_parcel_weight as parcelweight,\r\n",
					"0 as crossdock,\r\n",
					"1 as dcr,\r\n",
					"0 as pidd,\r\n",
					"0 parcelweightd,\r\n",
					"0 as pidl,\r\n",
					"0 as parcelweightl\r\n",
					"\r\n",
					"FROM stparcel p\r\n",
					"LEFT JOIN dbodctransfer dct on dct.transfercode = p.lms_parcel_barcode\r\n",
					"LEFT JOIN dbodcprepack dcp  on dcp.ID = dct.dc_prepackid\r\n",
					"LEFT JOIN dbodcorder dco on dco.id = dcp.dc_orderid\r\n",
					"LEFT JOIN stlocation lt on lt.lms_location_id = \r\n",
					"CASE dco.dccode \r\n",
					"    WHEN 'JHB' then 62\r\n",
					"    WHEN 'CT' then 7\r\n",
					"    WHEN 'CC_DUR' then 10\r\n",
					"    WHEN 'RHDC' then 10\r\n",
					"    WHEN 'BLM' then 8\r\n",
					"    WHEN 'PE' then 11\r\n",
					"END\r\n",
					"LEFT JOIN stbillcustomer bc on bc.lms_customer_id =  p.lms_parcel_orderbillcustid\r\n",
					"LEFT JOIN stdeliverypickupcustomer dc on dc.lms_customer_id =  p.lms_parcel_orderdelivercustid\r\n",
					"--LEFT JOIN stdeliverypickupcustomer pc on pc.lms_customer_id =  p.lms_parcel_orderpickupcustid\r\n",
					"LEFT JOIN stsroute ds on ds.lms_sroute_id = dc.lms_customer_srouteid\r\n",
					"LEFT JOIN stzone dz on dz.lms_zone_id = ds.lms_sroute_zoneid\r\n",
					"LEFT JOIN stlocation ld on ld.lms_location_id = dz.lms_zone_locid\r\n",
					"\r\n",
					"WHERE \r\n",
					"dct.acceptancedate >= (to_timestamp(year(current_date())-1||'-'||(month(current_date()))||'-'||'01'))\r\n",
					""
				],
				"execution_count": 9
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"rpttracklevel_dcrec = spark.sql(\"SELECT * FROM rpttracklevel_dcrec\")"
				],
				"execution_count": 10
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"CREATE OR REPLACE TEMP VIEW\r\n",
					"rpttracklevel_rec\r\n",
					"AS\r\n",
					"SELECT\r\n",
					"\r\n",
					"lf.lms_location_id as locationid,\r\n",
					"lf.lms_location_description as location,\r\n",
					"p.lms_parcel_acceptancedate as date,\r\n",
					"bc.lms_customer_id as  billingcustomerid,\r\n",
					"bc.lms_customer_name as  billingcustomer,\r\n",
					"dz.lms_zone_description as deliverycustcustomerzonedesc,\r\n",
					"ds.lms_sroute_code as deliverysroutecode,\r\n",
					"dc.lms_customer_cref as deliverycustomercref,\r\n",
					"dc.lms_customer_id as  deliverycustomerid,\r\n",
					"dc.lms_customer_name as  deliverycustomer,\r\n",
					"ld.lms_location_id as deliverycustomerzonelocationid,\r\n",
					"ld.lms_location_description as deliverycustomerzonelocationdescription,\r\n",
					"1 as pid,\r\n",
					"p.lms_parcel_weight as parcelweight,\r\n",
					"0 as crossdock,\r\n",
					"1 as dcr,\r\n",
					"0 as pidd,\r\n",
					"0 parcelweightd,\r\n",
					"0 as pidl,\r\n",
					"0 as parcelweightl\r\n",
					"\r\n",
					"FROM stparcel p\r\n",
					"LEFT JOIN stbillcustomer bc on bc.lms_customer_id =  p.lms_parcel_orderbillcustid\r\n",
					"LEFT JOIN stdeliverypickupcustomer dc on dc.lms_customer_id =  p.lms_parcel_orderdelivercustid\r\n",
					"LEFT JOIN stdeliverypickupcustomer pc on pc.lms_customer_id =  p.lms_parcel_orderpickupcustid\r\n",
					"LEFT JOIN stsroute ds on ds.lms_sroute_id = dc.lms_customer_srouteid\r\n",
					"LEFT JOIN stzone dz on dz.lms_zone_id = ds.lms_sroute_zoneid\r\n",
					"LEFT JOIN stsroute ps on ps.lms_sroute_id = pc.lms_customer_srouteid\r\n",
					"LEFT JOIN stzone pz on pz.lms_zone_id = ps.lms_sroute_zoneid\r\n",
					"LEFT JOIN stlocation ld on ld.lms_location_id = dz.lms_zone_locid\r\n",
					"LEFT JOIN stlocation lf on lf.lms_location_id = pz.lms_zone_locid\r\n",
					"\r\n",
					"WHERE \r\n",
					"p.lms_parcel_acceptancedate >= (to_timestamp(year(current_date())-1||'-'||(month(current_date()))||'-'||'01'))\r\n",
					"AND  ((bc.lms_customer_id = 344917 AND lf.lms_location_id = 7)\tOR (bc.lms_customer_id = 383489 AND lf.lms_location_id = 10) OR (bc.lms_customer_id = 579600 AND lf.lms_location_id = 31))"
				],
				"execution_count": 11
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"rpttracklevel_rec = spark.sql(\"SELECT * FROM rpttracklevel_rec\")"
				],
				"execution_count": 12
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"rpttracklevel_hourlydailyvolumesunion = rpttracklevel_in.unionByName(rpttracklevel_outdn, allowMissingColumns = True).unionByName(rpttracklevel_outlh, allowMissingColumns = True).unionByName(rpttracklevel_dcrec, allowMissingColumns = True).unionByName(rpttracklevel_rec, allowMissingColumns = True)\r\n",
					"rpttracklevel_hourlydailyvolumesunion.createOrReplaceTempView(\"rpttracklevel_hourlydailyvolumesunion\")"
				],
				"execution_count": 13
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"CREATE OR REPLACE TEMP VIEW\r\n",
					"branchvolumesreport\r\n",
					"AS\r\n",
					"SELECT\r\n",
					"\r\n",
					"    locationid,\r\n",
					"    location,\r\n",
					"    date,\r\n",
					"    hour(date) as thour,\r\n",
					"    sum(pid) as aggrpid,\r\n",
					"    sum(parcelweight) as aggrparcelweightrecived,\r\n",
					"    sum(pidl) as aggrpidl,\r\n",
					"    sum(parcelweightl) as aggrparcelweightlinehaul,\r\n",
					"    sum(pidd) as aggrpidd,\r\n",
					"    sum(parcelweightd) as aggrparcelweightdelivery\r\n",
					"\r\n",
					"FROM rpttracklevel_hourlydailyvolumesunion\r\n",
					"\r\n",
					"GROUP BY \r\n",
					"locationid,\r\n",
					"Location,\r\n",
					"date,\r\n",
					"hour(date)\r\n",
					""
				],
				"execution_count": 14
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"branchvolumesreport = spark.sql(\"SELECT * FROM branchvolumesreport\")"
				],
				"execution_count": 15
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# branchvolumesreport.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/RPT/branchvolumesreport.parquet', mode = \"overwrite\")"
				],
				"execution_count": 16
			}
		]
	}
}