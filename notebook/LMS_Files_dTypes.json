{
	"name": "LMS_Files_dTypes",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "TESTSparkPool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "1a36fbe3-0e05-400a-a24f-fae5d68df0fa"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse_Prod/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace-prod/bigDataPools/TESTSparkPool",
				"name": "TESTSparkPool",
				"type": "Spark",
				"endpoint": "https://citylogistics-synapseanalytics-workspace-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/TESTSparkPool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import functions as F\r\n",
					"from pyspark.sql.types import StructType, StructField, StringType, TimestampType,ArrayType,LongType,BooleanType\r\n",
					"from datetime import datetime\r\n",
					"import os\r\n",
					"import py4j\r\n",
					"from pyspark.sql import Row\r\n",
					"from pyspark.sql import functions as F\r\n",
					"import json\r\n",
					"from pyspark.sql.functions import col, explode_outer\r\n",
					"from io import StringIO\r\n",
					"import csv"
				],
				"execution_count": 18
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import os\r\n",
					"from pyspark.sql import SparkSession\r\n",
					"from pyspark.sql import Row\r\n",
					"\r\n",
					"# Initialize Spark session\r\n",
					"spark = SparkSession.builder \\\r\n",
					"    .appName(\"Get Data Types\") \\\r\n",
					"    .getOrCreate()\r\n",
					"\r\n",
					"# Define the path (keep spaces as is)\r\n",
					"path_with_spaces = \"abfss://synapse@citylogisticsstorageprod.dfs.core.windows.net/Unstructured Data/LMS/\"\r\n",
					"\r\n",
					"# List to hold file paths\r\n",
					"file_list = []\r\n",
					"\r\n",
					"# Get the file system\r\n",
					"URI = spark._jvm.java.net.URI\r\n",
					"Path = spark._jvm.org.apache.hadoop.fs.Path\r\n",
					"FileSystem = spark._jvm.org.apache.hadoop.fs.FileSystem\r\n",
					"\r\n",
					"try:\r\n",
					"    # Create a Path object with the given path\r\n",
					"    hadoop_path = Path(path_with_spaces)\r\n",
					"    \r\n",
					"    # Get the file system\r\n",
					"    fs = FileSystem.get(hadoop_path.toUri(), spark._jsc.hadoopConfiguration())\r\n",
					"    \r\n",
					"    # Check if the path exists\r\n",
					"    if not fs.exists(hadoop_path):\r\n",
					"        raise Exception(f\"The specified path does not exist: {path_with_spaces}\")\r\n",
					"    \r\n",
					"    # List status of the path\r\n",
					"    status = fs.listStatus(hadoop_path)\r\n",
					"\r\n",
					"    for file_status in status:\r\n",
					"        file_path = file_status.getPath().toString()\r\n",
					"        file_list.append(file_path)\r\n",
					"\r\n",
					"    #print(\"Files found:\")\r\n",
					"    #for file in file_list:\r\n",
					"    #    print(file)\r\n",
					"    \r\n",
					"    # List to hold schema information as rows\r\n",
					"    schema_info = []\r\n",
					"\r\n",
					"    # Read each file and get its schema\r\n",
					"    for file_path in file_list:\r\n",
					"        try:\r\n",
					"            # Read the Parquet file into a DataFrame\r\n",
					"            df = spark.read.parquet(file_path)\r\n",
					"            \r\n",
					"            # Collect the schema\r\n",
					"            schema = df.dtypes\r\n",
					"            for column_name, data_type in schema:\r\n",
					"                file_name = os.path.basename(file_path)  # Extract file name from path\r\n",
					"                schema_info.append(Row(file_name=file_name, column_name=column_name, data_type=data_type))\r\n",
					"        \r\n",
					"        except Exception as e:\r\n",
					"            print(f\"Error reading {file_path}: {e}\")\r\n",
					"\r\n",
					"    # Create a DataFrame from the schema information\r\n",
					"    schema_df = spark.createDataFrame(schema_info)\r\n",
					"    schema_df.show(truncate=False)\r\n",
					"    print(schema_df)\r\n",
					"except py4j.protocol.Py4JJavaError as e:\r\n",
					"    print(\"Error occurred while listing files: \", e)\r\n",
					"except Exception as e:\r\n",
					"    print(f\"Error: {e}\")\r\n",
					""
				],
				"execution_count": 28
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				],
				"execution_count": null
			}
		]
	}
}