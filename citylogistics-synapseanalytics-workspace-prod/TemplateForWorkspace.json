{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "citylogistics-synapseanalytics-workspace-prod"
		},
		"AzureSqlDatabase_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'AzureSqlDatabase'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=@{linkedService().ASAEndpoint};Initial Catalog=@{linkedService().ASADataBase}"
		},
		"PostgreSQL_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'PostgreSQL'",
			"defaultValue": "Host=@{linkedService().ServerName};Port=@{linkedService().Port};Database=@{linkedService().DatabaseName};UID=@{linkedService().UserName};EncryptionMethod=0"
		},
		"PostgreSqlTestServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'PostgreSqlTestServer'",
			"defaultValue": "Host=orvdb.citylogistics.co.za;Port=5432;Database=orv;UID=synapse;EncryptionMethod=0"
		},
		"SQLTestServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'SQLTestServer'",
			"defaultValue": "Integrated Security=True;Data Source=CCSQL01;Initial Catalog=\"SBK_CityLogistics \""
		},
		"SqlServer_DataBaseAuth_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'SqlServer_DataBaseAuth'",
			"defaultValue": "Integrated Security=False;Data Source=@{linkedService().ServerName};Initial Catalog=@{linkedService().DatabaseName};User ID=@{linkedService().UserName}"
		},
		"SqlServer_WindowsAuth_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'SqlServer_WindowsAuth'",
			"defaultValue": "Integrated Security=True;Data Source=@{linkedService().ServerName};Initial Catalog=@{linkedService().DatabaseName}"
		},
		"citylogistics-synapseanalytics-workspace-prod-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'citylogistics-synapseanalytics-workspace-prod-WorkspaceDefaultSqlServer'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=tcp:citylogistics-synapseanalytics-workspace-prod.sql.azuresynapse.net,1433;Initial Catalog=@{linkedService().DBName}"
		},
		"AzureKeyVaultCity_properties_typeProperties_baseUrl": {
			"type": "string",
			"defaultValue": "https://CityLogisticsKeyVault.vault.azure.net/"
		},
		"OnPremFileServerLinkedServer_properties_typeProperties_host": {
			"type": "string",
			"defaultValue": "@{linkedService().ServerName}"
		},
		"OnPremFileServerLinkedServer_properties_typeProperties_userId": {
			"type": "string",
			"defaultValue": "@{linkedService().UserName}"
		},
		"SQLTestServer_properties_typeProperties_userName": {
			"type": "string",
			"defaultValue": "Synapse@citylogistics.co.za"
		},
		"SharePointHTTPLS_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "@{concat('https://',linkedService().BaseUrl, '/', linkedService().SiteURL, '_api/web/GetFileByServerRelativeUrl (''', linkedService().FileName,''')/$value')}"
		},
		"SqlServer_WindowsAuth_properties_typeProperties_userName": {
			"type": "string",
			"defaultValue": "@{linkedService().UserName}"
		},
		"citylogistics-synapseanalytics-workspace-prod-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://citylogisticsstorageprod.dfs.core.windows.net"
		},
		"citylogistics_synapseanalytics_workspace_WorkspaceSecondaryStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "@{linkedService().DataLake}"
		},
		"citylogisticsstoragedevdatalake_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://citylogisticsstorage.dfs.core.windows.net/"
		},
		"Full Take-on PROD_Daily Trigger Midnight_properties_CLMasterData LOAD PROD_parameters_cw_items": {
			"type": "array",
			"defaultValue": [
				{
					"Source": {
						"TableName": "citybranch",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbocitybranch"
					}
				},
				{
					"Source": {
						"TableName": "deporequester",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbodeporequester"
					}
				},
				{
					"Source": {
						"TableName": "depot",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbodepot"
					}
				},
				{
					"Source": {
						"TableName": "ipaddress",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dboipaddress"
					}
				},
				{
					"Source": {
						"TableName": "maillistaddress",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbomaillistaddress"
					}
				},
				{
					"Source": {
						"TableName": "maillistcontact",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbomaillistcontact"
					}
				},
				{
					"Source": {
						"TableName": "maillistcustomer",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbomaillistcustomer"
					}
				},
				{
					"Source": {
						"TableName": "peoplebasic",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbopeoplebasic"
					}
				},
				{
					"Source": {
						"TableName": "peopledriver",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbopeopledriver"
					}
				},
				{
					"Source": {
						"TableName": "peoplepersonal",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbopeoplepersonal"
					}
				},
				{
					"Source": {
						"TableName": "selist",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dboselist"
					}
				},
				{
					"Source": {
						"TableName": "selistitem",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dboselistitem"
					}
				},
				{
					"Source": {
						"TableName": "vehicleadmin",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbovehicleadmin"
					}
				},
				{
					"Source": {
						"TableName": "vehiclebasic",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbovehiclebasic"
					}
				},
				{
					"Source": {
						"TableName": "vehicleequipment",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbovehicleequipment"
					}
				},
				{
					"Source": {
						"TableName": "vehiclefinance",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbovehiclefinance"
					}
				},
				{
					"Source": {
						"TableName": "vehiclemovement",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbovehiclemovement"
					}
				},
				{
					"Source": {
						"TableName": "vehicleoperations",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbovehicleoperations"
					}
				},
				{
					"Source": {
						"TableName": "vehicletracking",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbovehicletracking"
					}
				},
				{
					"Source": {
						"TableName": "vehicletype",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbovehicletype"
					}
				},
				{
					"Source": {
						"TableName": "wfapproval",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbowfapproval"
					}
				},
				{
					"Source": {
						"TableName": "wfmail",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbowfmail"
					}
				},
				{
					"Source": {
						"TableName": "wftargets",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbowftargets"
					}
				},
				{
					"Source": {
						"TableName": "manifest",
						"SchemaName": "v2"
					},
					"Increment": {
						"IncField": "lastupdate",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "v2manifest"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "customerorder",
						"SchemaName": "v2"
					},
					"Increment": {
						"IncField": "lastupdate",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "v2customerorder"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "consignment",
						"SchemaName": "v2"
					},
					"Increment": {
						"IncField": "lastupdate",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "v2consignment"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "deliverynote",
						"SchemaName": "v2"
					},
					"Increment": {
						"IncField": "lastupdate",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "v2deliverynote"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				}
			]
		},
		"Full Take-on PROD_Daily Trigger Midnight_properties_CLMasterData LOAD PROD_parameters_TakeOnPeriod": {
			"type": "string",
			"defaultValue": " "
		},
		"Full Take-on PROD_Daily Trigger Midnight_properties_CLMasterData LOAD PROD_parameters_Full_Load": {
			"type": "bool",
			"defaultValue": true
		},
		"Full Take-on PROD_Daily Trigger Midnight_properties_CLMasterData LOAD PROD_parameters_ColumnsNamesBlankSpaces": {
			"type": "bool",
			"defaultValue": false
		},
		"Full Take-on PROD_Daily Trigger Midnight_properties_CLMasterData LOAD PROD_parameters_LimitedLoadTopX": {
			"type": "string",
			"defaultValue": " "
		},
		"Full Take-on PROD_Daily Trigger Midnight_properties_LMS LOAD PROD_parameters_CW_Items": {
			"type": "array",
			"defaultValue": [
				{
					"Source": {
						"TableName": "Bill_BillCustGroups",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbobill_billcustgroups"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Bill_BillTypesR",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbobill_billtypesr"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Bill_BTYPE",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbobill_btype"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Bill_CalcTypes",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbobill_calctypes"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Bill_CollectStatus",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbobill_collectstatus"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Bill_CustomerGroups",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbobill_customergroups"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Bill_CustomerGroupTypes",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbobill_customergrouptypes"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Bill_CustTypeR",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbobill_custtyper"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Bill_DeliverType",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbobill_delivertype"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Bill_MasterEcommZones",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbobill_masterecommzones"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Bill_MonthsR",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbobill_monthsr"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Bill_ParcelPackaging",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbobill_parcelpackaging"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Bill_ParcelType",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbobill_parceltype"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Bill_Periods",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbobill_periods"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Bill_PostalCodesR",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbobill_postalcodesr"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Bill_ProductCategory",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbobill_productcategory"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Bill_RateCardR",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbobill_ratecardr"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Bill_RateGroup",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbobill_rategroup"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Bill_RouteType",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbobill_routetype"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Bill_ServiceType",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbobill_servicetype"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Bill_SurchargeTypes",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbobill_surchargetypes"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Bill_UnitOfMeasureR",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbobill_unitofmeasurer"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Bill_UOM",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbobill_uom"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Bill_VehicleTypeRates",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbobill_vehicletyperates"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "VehicleCapacity",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbovehiclecapacity"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "HiltiGroupCode",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbohiltigroupcode"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "HiltiGroup",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbohiltigroup"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Hiltirouteratetype",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbohiltirouteratetype"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Bill_WeightCalcType",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbobill_weightcalctype"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Bill_Wtype",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbobill_wtype"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Bill_Zone",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbobill_zone"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Bill_ZoneRoute",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbobill_zoneroute"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Broutemaster",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbobroutemaster"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "CountryMaster",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbocountrymaster"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "courier",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbocourier"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "CustAcc",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbocustacc"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "db_CollectStatus",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbodb_collectstatus"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "db_DelayReason",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbodb_delayreason"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "db_DeliveryStatus",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbodb_deliverystatus"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "db_Endorsement",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbodb_endorsement"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "db_InServiceReason",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbodb_inservicereason"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "db_MobileFeedbackReason",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbodb_mobilefeedbackreason"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "db_NotDbReason",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbodb_notdbreason"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "db_ReasonDetail",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbodb_reasondetail"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "db_ReasonGroup",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbodb_reasongroup"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Dstatus",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbodstatus"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "RouteRateType",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dborouteratetype"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "SRoute",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbosroute"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "SRouteGroup",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbosroutegroup"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Stations",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbostations"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "TrackType",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbotracktype"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Users",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbousers"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Volumiser",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbovolumiser"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "WBEndorsement",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbowbendorsement"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Zone",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbozone"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "PLC_Status",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dboplc_status"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "PLC_ScanPoint",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dboplc_scanpoint"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "PLC_DivertMapping",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dboplc_divertmapping"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Damaged_Parcel_Audit_TapeType",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbodamaged_parcel_audit_tapetype"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Damaged_Parcel_Audit_Flute_Profiles",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbodamaged_parcel_audit_flute_profiles"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "VolumiserLocation",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbovolumiserlocation"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				}
			]
		},
		"Full Take-on PROD_Daily Trigger Midnight_properties_LMS LOAD PROD_parameters_TakeOnPeriod": {
			"type": "string",
			"defaultValue": " "
		},
		"Full Take-on PROD_Daily Trigger Midnight_properties_LMS LOAD PROD_parameters_Full_Load": {
			"type": "bool",
			"defaultValue": true
		},
		"Full Take-on PROD_Daily Trigger Midnight_properties_LMS LOAD PROD_parameters_ColumnsNamesBlankSpaces": {
			"type": "bool",
			"defaultValue": false
		},
		"Full Take-on PROD_Daily Trigger Midnight_properties_LMS LOAD PROD_parameters_LimitedLoadTopX": {
			"type": "string",
			"defaultValue": " "
		},
		"Full Take-on PROD_Daily Trigger Midnight_properties_SAP LOAD PROD_parameters_CW_Items": {
			"type": "array",
			"defaultValue": [
				{
					"Source": {
						"TableName": "OITM"
					},
					"Increment": {
						"IncField": "itemcode",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dbooitm"
					}
				},
				{
					"Source": {
						"TableName": "OACT"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbooact"
					}
				},
				{
					"Source": {
						"TableName": "OASC"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbooasc"
					}
				},
				{
					"Source": {
						"TableName": "OCRD"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dboocrd"
					}
				},
				{
					"Source": {
						"TableName": "OCTG"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbooctg"
					}
				},
				{
					"Source": {
						"TableName": "OFPR"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dboofpr"
					}
				},
				{
					"Source": {
						"TableName": "OITB"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbooitb"
					}
				},
				{
					"Source": {
						"TableName": "OPRJ"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dbooprj"
					}
				},
				{
					"Source": {
						"TableName": "OUSR"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dboousr"
					}
				},
				{
					"Source": {
						"TableName": "OWHS"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "dboowhs"
					}
				}
			]
		},
		"Full Take-on PROD_Daily Trigger Midnight_properties_SAP LOAD PROD_parameters_Full_Load": {
			"type": "bool",
			"defaultValue": true
		},
		"Full Take-on PROD_Daily Trigger Midnight_properties_SAP LOAD PROD_parameters_ColumnsNamesBlankSpaces": {
			"type": "bool",
			"defaultValue": true
		},
		"Full Take-on PROD_Daily Trigger Midnight_properties_Sharepoint Full Take-on_parameters_CW_Items": {
			"type": "array",
			"defaultValue": [
				{
					"Source": {
						"TableName": "Shared Documents/General/ASA Excel Data Sources/Cost To Serve Mapping Sources"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"destination": {
						"FileName": "Unstructured Data/CTS Mappings"
					}
				}
			]
		},
		"Full Take-on PROD_Daily Trigger Midnight_properties_Sharepoint Full Take-on_parameters_SharepointSiteName": {
			"type": "string",
			"defaultValue": "DataWarehouse"
		},
		"Full Take-on PROD_Daily Trigger Midnight_properties_ORV LOAD PROD_parameters_cw_items": {
			"type": "array",
			"defaultValue": [
				{
					"Source": {
						"TableName": "costmodel"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "publiccostmodel"
					}
				},
				{
					"Source": {
						"TableName": "courier"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "publiccourier"
					}
				},
				{
					"Source": {
						"TableName": "depo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "publicdepo"
					}
				},
				{
					"Source": {
						"TableName": "device"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "publicdevice"
					}
				},
				{
					"Source": {
						"TableName": "dispatchcrew"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "publicdispatchcrew"
					}
				},
				{
					"Source": {
						"TableName": "geofence"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "publicgeofence"
					}
				},
				{
					"Source": {
						"TableName": "geofence"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "publicgeofence"
					}
				},
				{
					"Source": {
						"TableName": "geofenceattribute"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "publicgeofenceattribute"
					}
				},
				{
					"Source": {
						"TableName": "geofenceattribute"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "publicgeofenceattribute"
					}
				},
				{
					"Source": {
						"TableName": "geopoint"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "publicgeopoint"
					}
				},
				{
					"Source": {
						"TableName": "geopoint"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "publicgeopoint"
					}
				},
				{
					"Source": {
						"TableName": "hvcategory"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "publichvcategory"
					}
				},
				{
					"Source": {
						"TableName": "hvcategory"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "publichvcategory"
					}
				},
				{
					"Source": {
						"TableName": "hvcustomersurvey"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "publichvcustomersurvey"
					}
				},
				{
					"Source": {
						"TableName": "hvsla"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "publichvsla"
					}
				},
				{
					"Source": {
						"TableName": "hvtag"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "publichvtag"
					}
				},
				{
					"Source": {
						"TableName": "hvtype"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "publichvtype"
					}
				},
				{
					"Source": {
						"TableName": "lhcustomer"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "publiclhcustomer"
					}
				},
				{
					"Source": {
						"TableName": "list"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "publiclist"
					}
				},
				{
					"Source": {
						"TableName": "listitem"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "publiclistitem"
					}
				},
				{
					"Source": {
						"TableName": "mall"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "publicmall"
					}
				},
				{
					"Source": {
						"TableName": "manualdebriefreason"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "publicmanualdebriefreason"
					}
				},
				{
					"Source": {
						"TableName": "orvrole"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "publicorvrole"
					}
				},
				{
					"Source": {
						"TableName": "orvuserhvtype"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "publicorvuserhvtype"
					}
				},
				{
					"Source": {
						"TableName": "precheckoption"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "publicprecheckoption"
					}
				},
				{
					"Source": {
						"TableName": "precheckquestion"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "publicprecheckquestion"
					}
				},
				{
					"Source": {
						"TableName": "reasons"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "publicreasons"
					}
				},
				{
					"Source": {
						"TableName": "roletofeature"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "publicroletofeature"
					}
				},
				{
					"Source": {
						"TableName": "routetocourier"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "publicroutetocourier"
					}
				},
				{
					"Source": {
						"TableName": "systemfeature"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "publicsystemfeature"
					}
				},
				{
					"Source": {
						"TableName": "userdepo"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "publicuserdepo"
					}
				},
				{
					"Source": {
						"TableName": "userdevice"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "publicuserdevice"
					}
				},
				{
					"Source": {
						"TableName": "userrole"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "publicuserrole"
					}
				},
				{
					"Source": {
						"TableName": "usertorole"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "publicusertorole"
					}
				},
				{
					"Source": {
						"TableName": "vanassistant"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "publicvanassistant"
					}
				},
				{
					"Source": {
						"TableName": "vehicle"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "publicvehicle"
					}
				},
				{
					"Source": {
						"TableName": "zoneroute"
					},
					"Increment": {
						"IncField": "0",
						"IdField": "0"
					},
					"Destination": {
						"FileName": "publiczoneroute"
					}
				}
			]
		},
		"Full Take-on PROD_Daily Trigger Midnight_properties_ORV LOAD PROD_parameters_TakeOnPeriod": {
			"type": "string",
			"defaultValue": " "
		},
		"Full Take-on PROD_Daily Trigger Midnight_properties_ORV LOAD PROD_parameters_Full_Load": {
			"type": "bool",
			"defaultValue": true
		},
		"Full Take-on PROD_Daily Trigger Midnight_properties_ORV LOAD PROD_parameters_ColumnsNamesBlankSpaces": {
			"type": "bool",
			"defaultValue": false
		},
		"Full Take-on PROD_Daily Trigger Midnight_properties_ORV LOAD PROD_parameters_LimitedLoadTopX": {
			"type": "string",
			"defaultValue": " "
		},
		"Full Take-on PROD_Daily Trigger Midnight_properties_TMS LOAD PROD_parameters_cw_items": {
			"type": "array",
			"defaultValue": [
				{
					"Source": {
						"TableName": "driverpayitem"
					},
					"Increment": {
						"IncField": "moddate",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publicdriverpayitem"
					}
				}
			]
		},
		"Full Take-on PROD_Daily Trigger Midnight_properties_TMS LOAD PROD_parameters_TakeOnPeriod": {
			"type": "string",
			"defaultValue": " "
		},
		"Full Take-on PROD_Daily Trigger Midnight_properties_TMS LOAD PROD_parameters_Full_Load": {
			"type": "bool",
			"defaultValue": true
		},
		"Full Take-on PROD_Daily Trigger Midnight_properties_TMS LOAD PROD_parameters_ColumnsNamesBlankSpaces": {
			"type": "bool",
			"defaultValue": false
		},
		"Full Take-on PROD_Daily Trigger Midnight_properties_TMS LOAD PROD_parameters_LimitedLoadTopX": {
			"type": "string",
			"defaultValue": " "
		},
		"Incre PROD_Daily Trigger Midnight_properties_LMS LOAD PROD_parameters_CW_Items": {
			"type": "array",
			"defaultValue": [
				{
					"Source": {
						"TableName": "Bill_ConsignmentR",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "updatedate",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dbobill_consignmentr"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Bill_ExceptionR",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "updatedate",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dbobill_exceptionr"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Bill_ConsignRouteChargeR",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "updatedate",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dbobill_consignroutecharger"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Bill_ConsignSurChargeR",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "updatedate",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dbobill_consignsurcharger"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Bill_BillCustomersR",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dbobill_billcustomersr"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Collect",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "lastupdate",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dbocollect"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Consignment",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "lastupdate",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dboconsignment"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Customer",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "lastupdate",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dbocustomer"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "DC_Transfer",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dbodc_transfer"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "DC_Prepack",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dbodc_prepack"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "DC_Order",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dbodc_order"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Driver",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "lastupdate",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dbodriver"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "LeadTime",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dboleadtime"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "LeadTimeGroup",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dboleadtimegroup"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "LeadTimeGroupDC",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dboleadtimegroupdc"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "LeadTimeSection",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dboleadtimesection"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "LeadTimeStructure",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dboleadtimestructure"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Loads",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "lastupdate",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dboloads"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "order",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "lastupdate",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dboorder"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Parcel",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "lastupdate",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dboparcel"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "ParcelDetail",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dboparceldetail"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "ParcelStatus",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dboparcelstatus"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Track",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "lastupdate",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dbotrack"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Vehicle",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "lastupdate",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dbovehicle"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Waybill",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "lastupdate",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dbowaybill"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "WaybillsPerParcel",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dbowaybillsperparcel"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "PLC_LPNRouting",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dboplc_lpnrouting"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "plc_lpnrouting_history_centurion",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dboplc_lpnrouting_history_centurion"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Damaged_Parcel_Audit",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dbodamaged_parcel_audit"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "CSD_Message",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dbocsd_message"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "CSD_MessageParcel",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dbocsd_messageparcel"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "VolumiserLog",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dbovolumiserlog"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Bill_AuditRouteRatesR",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dbobill_auditrouteratesr"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Bill_RouteRate",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dbobill_routerate"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "WeekendHoliday",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dboweekendholiday"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "Location",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dbolocation"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "PNPHUDetail",
						"SchemaName": "dbo"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dbopnphudetail"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				}
			]
		},
		"Incre PROD_Daily Trigger Midnight_properties_LMS LOAD PROD_parameters_TakeOnPeriod": {
			"type": "string",
			"defaultValue": " "
		},
		"Incre PROD_Daily Trigger Midnight_properties_LMS LOAD PROD_parameters_Full_Load": {
			"type": "bool",
			"defaultValue": false
		},
		"Incre PROD_Daily Trigger Midnight_properties_LMS LOAD PROD_parameters_ColumnsNamesBlankSpaces": {
			"type": "bool",
			"defaultValue": false
		},
		"Incre PROD_Daily Trigger Midnight_properties_LMS LOAD PROD_parameters_LimitedLoadTopX": {
			"type": "string",
			"defaultValue": " "
		},
		"Incre PROD_Daily Trigger Midnight_properties_SAP LOAD PROD_parameters_CW_Items": {
			"type": "array",
			"defaultValue": [
				{
					"Source": {
						"TableName": "IGE1"
					},
					"Increment": {
						"IncField": "u_doctime",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dboige1"
					}
				},
				{
					"Source": {
						"TableName": "IGN1"
					},
					"Increment": {
						"IncField": "u_doctime",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dboign1"
					}
				},
				{
					"Source": {
						"TableName": "INV1"
					},
					"Increment": {
						"IncField": "u_doctime",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dboinv1"
					}
				},
				{
					"Source": {
						"TableName": "JDT1"
					},
					"Increment": {
						"IncField": "u_doctime",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dbojdt1"
					}
				},
				{
					"Source": {
						"TableName": "MFRHistoryDaily"
					},
					"Increment": {
						"IncField": "changedateto",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dbomfrhistorydaily"
					}
				},
				{
					"Source": {
						"TableName": "MRV1"
					},
					"Increment": {
						"IncField": "u_doctime",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dbomrv1"
					}
				},
				{
					"Source": {
						"TableName": "MRV2"
					},
					"Increment": {
						"IncField": "u_doctime",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dbomrv2"
					}
				},
				{
					"Source": {
						"TableName": "OIGE"
					},
					"Increment": {
						"IncField": "u_doctime",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dbooige"
					}
				},
				{
					"Source": {
						"TableName": "OIGN"
					},
					"Increment": {
						"IncField": "u_doctime",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dbooign"
					}
				},
				{
					"Source": {
						"TableName": "OINM"
					},
					"Increment": {
						"IncField": "transnum",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dbooinm"
					}
				},
				{
					"Source": {
						"TableName": "OINV"
					},
					"Increment": {
						"IncField": "u_doctime",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dbooinv"
					}
				},
				{
					"Source": {
						"TableName": "OITW"
					},
					"Increment": {
						"IncField": "updatedate",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dbooitw"
					}
				},
				{
					"Source": {
						"TableName": "OJDT"
					},
					"Increment": {
						"IncField": "u_doctime",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dboojdt"
					}
				},
				{
					"Source": {
						"TableName": "OMRV"
					},
					"Increment": {
						"IncField": "u_doctime",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dboomrv"
					}
				},
				{
					"Source": {
						"TableName": "OPCH"
					},
					"Increment": {
						"IncField": "u_doctime",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dboopch"
					}
				},
				{
					"Source": {
						"TableName": "OPDN"
					},
					"Increment": {
						"IncField": "u_doctime",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dboopdn"
					}
				},
				{
					"Source": {
						"TableName": "OPOR"
					},
					"Increment": {
						"IncField": "u_doctime",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dboopor"
					}
				},
				{
					"Source": {
						"TableName": "ORIN"
					},
					"Increment": {
						"IncField": "u_doctime",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dboorin"
					}
				},
				{
					"Source": {
						"TableName": "ORPC"
					},
					"Increment": {
						"IncField": "u_doctime",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dboorpc"
					}
				},
				{
					"Source": {
						"TableName": "ORPD"
					},
					"Increment": {
						"IncField": "u_doctime",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dboorpd"
					}
				},
				{
					"Source": {
						"TableName": "PCH1"
					},
					"Increment": {
						"IncField": "u_doctime",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dbopch1"
					}
				},
				{
					"Source": {
						"TableName": "PDN1"
					},
					"Increment": {
						"IncField": "u_doctime",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dbopdn1"
					}
				},
				{
					"Source": {
						"TableName": "POR1"
					},
					"Increment": {
						"IncField": "u_doctime",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dbopor1"
					}
				},
				{
					"Source": {
						"TableName": "RIN1"
					},
					"Increment": {
						"IncField": "u_doctime",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dborin1"
					}
				},
				{
					"Source": {
						"TableName": "RPC1"
					},
					"Increment": {
						"IncField": "u_doctime",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dborpc1"
					}
				},
				{
					"Source": {
						"TableName": "RPD1"
					},
					"Increment": {
						"IncField": "u_doctime",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "dborpd1"
					}
				}
			]
		},
		"Incre PROD_Daily Trigger Midnight_properties_SAP LOAD PROD_parameters_TakeOnPeriod": {
			"type": "string",
			"defaultValue": " "
		},
		"Incre PROD_Daily Trigger Midnight_properties_SAP LOAD PROD_parameters_Full_Load": {
			"type": "bool",
			"defaultValue": true
		},
		"Incre PROD_Daily Trigger Midnight_properties_SAP LOAD PROD_parameters_ColumnsNamesBlankSpaces": {
			"type": "bool",
			"defaultValue": true
		},
		"Incre PROD_Daily Trigger Midnight_properties_SAP LOAD PROD_parameters_LimitedLoadTopX": {
			"type": "string",
			"defaultValue": " "
		},
		"Incre PROD_Daily Trigger Midnight_properties_ORV LOAD PROD_parameters_cw_items": {
			"type": "array",
			"defaultValue": [
				{
					"Source": {
						"TableName": "chatmsg"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publicchatmsg"
					}
				},
				{
					"Source": {
						"TableName": "claim"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publicclaim"
					}
				},
				{
					"Source": {
						"TableName": "customer"
					},
					"Increment": {
						"IncField": "lastupdate",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publiccustomer"
					}
				},
				{
					"Source": {
						"TableName": "customeraccess"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publiccustomeraccess"
					}
				},
				{
					"Source": {
						"TableName": "customeraccesschild"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publiccustomeraccesschild"
					}
				},
				{
					"Source": {
						"TableName": "cutomerdeliverytracking"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publiccutomerdeliverytracking"
					}
				},
				{
					"Source": {
						"TableName": "delivery"
					},
					"Increment": {
						"IncField": "moddate",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publicdelivery"
					}
				},
				{
					"Source": {
						"TableName": "dispatch"
					},
					"Increment": {
						"IncField": "moddate",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publicdispatch"
					}
				},
				{
					"Source": {
						"TableName": "dispatchactionlog"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publicdispatchactionlog"
					}
				},
				{
					"Source": {
						"TableName": "dispatchdevice"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publicdispatchdevice"
					}
				},
				{
					"Source": {
						"TableName": "dispatchsegment"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publicdispatchsegment"
					}
				},
				{
					"Source": {
						"TableName": "dispatchstart"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publicdispatchstart"
					}
				},
				{
					"Source": {
						"TableName": "draftcollection"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publicdraftcollection"
					}
				},
				{
					"Source": {
						"TableName": "emaildeliverystatus"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publicemaildeliverystatus"
					}
				},
				{
					"Source": {
						"TableName": "event"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publicevent"
					}
				},
				{
					"Source": {
						"TableName": "expense"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publicexpense"
					}
				},
				{
					"Source": {
						"TableName": "hvcomment"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publichvcomment"
					}
				},
				{
					"Source": {
						"TableName": "hvcustomersurvey"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publichvcustomersurvey"
					}
				},
				{
					"Source": {
						"TableName": "hvlog"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publichvlog"
					}
				},
				{
					"Source": {
						"TableName": "hvnotification"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publichvnotification"
					}
				},
				{
					"Source": {
						"TableName": "hvticket"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publichvticket"
					}
				},
				{
					"Source": {
						"TableName": "lhchangerequest"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publiclhchangerequest"
					}
				},
				{
					"Source": {
						"TableName": "lhdelivery"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publiclhdelivery"
					}
				},
				{
					"Source": {
						"TableName": "lhdispatchsegment"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publiclhdispatchsegment"
					}
				},
				{
					"Source": {
						"TableName": "lhtmsdata"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publiclhtmsdata"
					}
				},
				{
					"Source": {
						"TableName": "orvconnect"
					},
					"Increment": {
						"IncField": "lastupdate",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publicorvconnect"
					}
				},
				{
					"Source": {
						"TableName": "orvconnectemailampm"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publicorvconnectemailampm"
					}
				},
				{
					"Source": {
						"TableName": "orvconnectnotes"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publicorvconnectnotes"
					}
				},
				{
					"Source": {
						"TableName": "orvconnectparcel"
					},
					"Increment": {
						"IncField": "lastupdate",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publicorvconnectparcel"
					}
				},
				{
					"Source": {
						"TableName": "parcel"
					},
					"Increment": {
						"IncField": "moddate",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publicparcel"
					}
				},
				{
					"Source": {
						"TableName": "pod"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publicpod"
					}
				},
				{
					"Source": {
						"TableName": "precheckanswer"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publicprecheckanswer"
					}
				},
				{
					"Source": {
						"TableName": "supervisorticket"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publicsupervisorticket"
					}
				},
				{
					"Source": {
						"TableName": "tmstrailerslog"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publictmstrailerslog"
					}
				},
				{
					"Source": {
						"TableName": "whatsappmsg"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publicwhatsappmsg"
					}
				},
				{
					"Source": {
						"TableName": "reportdistotmgntdispatchdt"
					},
					"Increment": {
						"IncField": "dispatchid",
						"IdField": "dispatchid"
					},
					"Destination": {
						"FileName": "publicreportdistotmgntdispatchdt"
					}
				},
				{
					"Source": {
						"TableName": "orvuser"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publicorvuser"
					}
				}
			]
		},
		"Incre PROD_Daily Trigger Midnight_properties_ORV LOAD PROD_parameters_TakeOnPeriod": {
			"type": "string",
			"defaultValue": " "
		},
		"Incre PROD_Daily Trigger Midnight_properties_ORV LOAD PROD_parameters_Full_Load": {
			"type": "bool",
			"defaultValue": false
		},
		"Incre PROD_Daily Trigger Midnight_properties_ORV LOAD PROD_parameters_ColumnsNamesBlankSpaces": {
			"type": "bool",
			"defaultValue": false
		},
		"Incre PROD_Daily Trigger Midnight_properties_ORV LOAD PROD_parameters_LimitedLoadTopX": {
			"type": "string",
			"defaultValue": " "
		},
		"Incre PROD_Daily Trigger Midnight_properties_TMS LOAD PROD_parameters_cw_items": {
			"type": "array",
			"defaultValue": [
				{
					"Source": {
						"TableName": "address"
					},
					"Increment": {
						"IncField": "moddate",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publicaddress"
					}
				},
				{
					"Source": {
						"TableName": "booking"
					},
					"Increment": {
						"IncField": "moddate",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publicbooking"
					}
				},
				{
					"Source": {
						"TableName": "customer"
					},
					"Increment": {
						"IncField": "moddate",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publiccustomer"
					}
				},
				{
					"Source": {
						"TableName": "driverpay"
					},
					"Increment": {
						"IncField": "moddate",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publicdriverpay"
					}
				},
				{
					"Source": {
						"TableName": "financedata"
					},
					"Increment": {
						"IncField": "moddate",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publicfinancedata"
					}
				},
				{
					"Source": {
						"TableName": "fuelcard"
					},
					"Increment": {
						"IncField": "moddate",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publicfuelcard"
					}
				},
				{
					"Source": {
						"TableName": "fuelzone"
					},
					"Increment": {
						"IncField": "moddate",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publicfuelzone"
					}
				},
				{
					"Source": {
						"TableName": "fuelzonehistory"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publicfuelzonehistory"
					}
				},
				{
					"Source": {
						"TableName": "instruction"
					},
					"Increment": {
						"IncField": "moddate",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publicinstruction"
					}
				},
				{
					"Source": {
						"TableName": "invoice"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publicinvoice"
					}
				},
				{
					"Source": {
						"TableName": "invoiceitem"
					},
					"Increment": {
						"IncField": "moddate",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publicinvoiceitem"
					}
				},
				{
					"Source": {
						"TableName": "lmsdata"
					},
					"Increment": {
						"IncField": "moddate",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publiclmsdata"
					}
				},
				{
					"Source": {
						"TableName": "nonbooking"
					},
					"Increment": {
						"IncField": "moddate",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publicnonbooking"
					}
				},
				{
					"Source": {
						"TableName": "ratecomponent"
					},
					"Increment": {
						"IncField": "moddate",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publicratecomponent"
					}
				},
				{
					"Source": {
						"TableName": "rateprofile"
					},
					"Increment": {
						"IncField": "moddate",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publicrateprofile"
					}
				},
				{
					"Source": {
						"TableName": "route"
					},
					"Increment": {
						"IncField": "moddate",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publicroute"
					}
				},
				{
					"Source": {
						"TableName": "thirdparty"
					},
					"Increment": {
						"IncField": "moddate",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publicthirdparty"
					}
				},
				{
					"Source": {
						"TableName": "track"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publictrack"
					}
				},
				{
					"Source": {
						"TableName": "trip"
					},
					"Increment": {
						"IncField": "moddate",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publictrip"
					}
				},
				{
					"Source": {
						"TableName": "triptrackinghistory"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publictriptrackinghistory"
					}
				},
				{
					"Source": {
						"TableName": "selist"
					},
					"Increment": {
						"IncField": "moddate",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publicselist"
					}
				},
				{
					"Source": {
						"TableName": "selistitem"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publicselistitem"
					}
				},
				{
					"Source": {
						"TableName": "routepoint"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "publicroutepoint"
					}
				}
			]
		},
		"Incre PROD_Daily Trigger Midnight_properties_TMS LOAD PROD_parameters_TakeOnPeriod": {
			"type": "string",
			"defaultValue": " "
		},
		"Incre PROD_Daily Trigger Midnight_properties_TMS LOAD PROD_parameters_Full_Load": {
			"type": "bool",
			"defaultValue": false
		},
		"Incre PROD_Daily Trigger Midnight_properties_TMS LOAD PROD_parameters_ColumnsNamesBlankSpaces": {
			"type": "bool",
			"defaultValue": false
		},
		"Incre PROD_Daily Trigger Midnight_properties_TMS LOAD PROD_parameters_LimitedLoadTopX": {
			"type": "string",
			"defaultValue": " "
		},
		"Incre PROD_Daily Trigger Midnight_properties_CLMasterData LOAD PROD_parameters_cw_items": {
			"type": "array",
			"defaultValue": [
				{
					"Source": {
						"TableName": "clockinghoursdetail",
						"SchemaName": "prp"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "prpclockinghoursdetail"
					}
				},
				{
					"Source": {
						"TableName": "manifest",
						"SchemaName": "v2"
					},
					"Increment": {
						"IncField": "lastupdate",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "v2manifest"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "customerorder",
						"SchemaName": "v2"
					},
					"Increment": {
						"IncField": "lastupdate",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "v2customerorder"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "consignment",
						"SchemaName": "v2"
					},
					"Increment": {
						"IncField": "lastupdate",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "v2consignment"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				},
				{
					"Source": {
						"TableName": "deliverynote",
						"SchemaName": "v2"
					},
					"Increment": {
						"IncField": "lastupdate",
						"IdField": "id"
					},
					"Destination": {
						"FileName": "v2deliverynote"
					},
					"SparkConfig": {
						"NodeSize": "Small",
						"MinExecutors": "1",
						"MaxExecutors": "3"
					}
				}
			]
		},
		"Incre PROD_Daily Trigger Midnight_properties_CLMasterData LOAD PROD_parameters_TakeOnPeriod": {
			"type": "string",
			"defaultValue": " "
		},
		"Incre PROD_Daily Trigger Midnight_properties_CLMasterData LOAD PROD_parameters_Full_Load": {
			"type": "bool",
			"defaultValue": false
		},
		"Incre PROD_Daily Trigger Midnight_properties_CLMasterData LOAD PROD_parameters_ColumnsNamesBlankSpaces": {
			"type": "bool",
			"defaultValue": false
		},
		"Incre PROD_Daily Trigger Midnight_properties_CLMasterData LOAD PROD_parameters_LimitedLoadTopX": {
			"type": "string",
			"defaultValue": " "
		},
		"Rolling PROD_Weekly Trigger_properties_LMS LOAD PROD_parameters_CW_Items": {
			"type": "array",
			"defaultValue": [
				{
					"Source": {
						"TableName": "Bill_ConsignmentR"
					},
					"Increment": {
						"IncField": "updatedate",
						"IdField": "id"
					},
					"destination": {
						"FileName": "dbobill_consignmentr"
					}
				},
				{
					"Source": {
						"TableName": "Bill_ExceptionR"
					},
					"Increment": {
						"IncField": "updatedate",
						"IdField": "id"
					},
					"destination": {
						"FileName": "dbobill_exceptionr"
					}
				},
				{
					"Source": {
						"TableName": "Bill_ConsignRouteChargeR"
					},
					"Increment": {
						"IncField": "updatedate",
						"IdField": "id"
					},
					"destination": {
						"FileName": "dbobill_consignroutecharger"
					}
				},
				{
					"Source": {
						"TableName": "Bill_ConsignSurChargeR"
					},
					"Increment": {
						"IncField": "updatedate",
						"IdField": "id"
					},
					"destination": {
						"FileName": "dbobill_consignsurcharger"
					}
				},
				{
					"Source": {
						"TableName": "Bill_BillCustomersR"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"destination": {
						"FileName": "dbobill_billcustomersr"
					}
				},
				{
					"Source": {
						"TableName": "Collect"
					},
					"Increment": {
						"IncField": "lastupdate",
						"IdField": "id"
					},
					"destination": {
						"FileName": "dbocollect"
					}
				},
				{
					"Source": {
						"TableName": "Consignment"
					},
					"Increment": {
						"IncField": "lastupdate",
						"IdField": "id"
					},
					"destination": {
						"FileName": "dboconsignment"
					}
				},
				{
					"Source": {
						"TableName": "Customer"
					},
					"Increment": {
						"IncField": "lastupdate",
						"IdField": "id"
					},
					"destination": {
						"FileName": "dbocustomer"
					}
				},
				{
					"Source": {
						"TableName": "DC_Transfer"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"destination": {
						"FileName": "dbodc_transfer"
					}
				},
				{
					"Source": {
						"TableName": "Driver"
					},
					"Increment": {
						"IncField": "lastupdate",
						"IdField": "id"
					},
					"destination": {
						"FileName": "dbodriver"
					}
				},
				{
					"Source": {
						"TableName": "LeadTime"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"destination": {
						"FileName": "dboleadtime"
					}
				},
				{
					"Source": {
						"TableName": "LeadTimeGroup"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"destination": {
						"FileName": "dboleadtimegroup"
					}
				},
				{
					"Source": {
						"TableName": "LeadTimeGroupDC"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"destination": {
						"FileName": "dboleadtimegroupdc"
					}
				},
				{
					"Source": {
						"TableName": "LeadTimeSection"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"destination": {
						"FileName": "dboleadtimesection"
					}
				},
				{
					"Source": {
						"TableName": "LeadTimeStructure"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"destination": {
						"FileName": "dboleadtimestructure"
					}
				},
				{
					"Source": {
						"TableName": "Loads"
					},
					"Increment": {
						"IncField": "lastupdate",
						"IdField": "id"
					},
					"destination": {
						"FileName": "dboloads"
					}
				},
				{
					"Source": {
						"TableName": "order"
					},
					"Increment": {
						"IncField": "lastupdate",
						"IdField": "id"
					},
					"destination": {
						"FileName": "dboorder"
					}
				},
				{
					"Source": {
						"TableName": "Parcel"
					},
					"Increment": {
						"IncField": "lastupdate",
						"IdField": "id"
					},
					"destination": {
						"FileName": "dboparcel"
					}
				},
				{
					"Source": {
						"TableName": "ParcelDetail"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"destination": {
						"FileName": "dboparceldetail"
					}
				},
				{
					"Source": {
						"TableName": "ParcelStatus"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"destination": {
						"FileName": "dboparcelstatus"
					}
				},
				{
					"Source": {
						"TableName": "Track"
					},
					"Increment": {
						"IncField": "lastupdate",
						"IdField": "id"
					},
					"destination": {
						"FileName": "dbotrack"
					}
				},
				{
					"Source": {
						"TableName": "Vehicle"
					},
					"Increment": {
						"IncField": "lastupdate",
						"IdField": "id"
					},
					"destination": {
						"FileName": "dbovehicle"
					}
				},
				{
					"Source": {
						"TableName": "Waybill"
					},
					"Increment": {
						"IncField": "lastupdate",
						"IdField": "id"
					},
					"destination": {
						"FileName": "dbowaybill"
					}
				},
				{
					"Source": {
						"TableName": "WaybillsPerParcel"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"destination": {
						"FileName": "dbowaybillsperparcel"
					}
				},
				{
					"Source": {
						"TableName": "PLC_LPNRouting"
					},
					"Increment": {
						"IncField": "id",
						"IdField": "id"
					},
					"destination": {
						"FileName": "dboplc_lpnrouting"
					}
				}
			]
		},
		"Rolling PROD_Weekly Trigger_properties_LMS LOAD PROD_parameters_TakeOnPeriod": {
			"type": "string",
			"defaultValue": "SELECT DATEADD(DAY,1,EOMONTH(DATEADD(MONTH,-3,GETDATE()),-1))"
		},
		"Rolling PROD_Weekly Trigger_properties_LMS LOAD PROD_parameters_Full_Load": {
			"type": "bool",
			"defaultValue": false
		},
		"Rolling PROD_Weekly Trigger_properties_LMS LOAD PROD_parameters_ColumnsNamesBlankSpaces": {
			"type": "bool",
			"defaultValue": false
		},
		"Rolling PROD_Weekly Trigger_properties_SAP LOAD PROD_parameters_CW_Items": {
			"type": "array",
			"defaultValue": [
				{
					"Source": {
						"TableName": "IGE1"
					},
					"Increment": {
						"IncField": "u_doctime",
						"IdField": "id"
					},
					"destination": {
						"FileName": "dboige1"
					}
				},
				{
					"Source": {
						"TableName": "IGN1"
					},
					"Increment": {
						"IncField": "u_doctime",
						"IdField": "id"
					},
					"destination": {
						"FileName": "dboign1"
					}
				},
				{
					"Source": {
						"TableName": "INV1"
					},
					"Increment": {
						"IncField": "u_doctime",
						"IdField": "id"
					},
					"destination": {
						"FileName": "dboinv1"
					}
				},
				{
					"Source": {
						"TableName": "JDT1"
					},
					"Increment": {
						"IncField": "u_doctime",
						"IdField": "id"
					},
					"destination": {
						"FileName": "dbojdt1"
					}
				},
				{
					"Source": {
						"TableName": "MFRHistoryDaily"
					},
					"Increment": {
						"IncField": "changedateto",
						"IdField": "id"
					},
					"destination": {
						"FileName": "dbomfrhistorydaily"
					}
				},
				{
					"Source": {
						"TableName": "MRV1"
					},
					"Increment": {
						"IncField": "u_doctime",
						"IdField": "id"
					},
					"destination": {
						"FileName": "dbomrv1"
					}
				},
				{
					"Source": {
						"TableName": "MRV2"
					},
					"Increment": {
						"IncField": "u_doctime",
						"IdField": "id"
					},
					"destination": {
						"FileName": "dbomrv2"
					}
				},
				{
					"Source": {
						"TableName": "OIGE"
					},
					"Increment": {
						"IncField": "u_doctime",
						"IdField": "id"
					},
					"destination": {
						"FileName": "dbooige"
					}
				},
				{
					"Source": {
						"TableName": "OIGN"
					},
					"Increment": {
						"IncField": "u_doctime",
						"IdField": "id"
					},
					"destination": {
						"FileName": "dbooign"
					}
				},
				{
					"Source": {
						"TableName": "OINM"
					},
					"Increment": {
						"IncField": "transnum",
						"IdField": "id"
					},
					"destination": {
						"FileName": "dbooinm"
					}
				},
				{
					"Source": {
						"TableName": "OINV"
					},
					"Increment": {
						"IncField": "u_doctime",
						"IdField": "id"
					},
					"destination": {
						"FileName": "dbooinv"
					}
				},
				{
					"Source": {
						"TableName": "OITW"
					},
					"Increment": {
						"IncField": "updatedate",
						"IdField": "id"
					},
					"destination": {
						"FileName": "dbooitw"
					}
				},
				{
					"Source": {
						"TableName": "OJDT"
					},
					"Increment": {
						"IncField": "u_doctime",
						"IdField": "id"
					},
					"destination": {
						"FileName": "dboojdt"
					}
				},
				{
					"Source": {
						"TableName": "OMRV"
					},
					"Increment": {
						"IncField": "u_doctime",
						"IdField": "id"
					},
					"destination": {
						"FileName": "dboomrv"
					}
				},
				{
					"Source": {
						"TableName": "OPCH"
					},
					"Increment": {
						"IncField": "u_doctime",
						"IdField": "id"
					},
					"destination": {
						"FileName": "dboopch"
					}
				},
				{
					"Source": {
						"TableName": "OPDN"
					},
					"Increment": {
						"IncField": "u_doctime",
						"IdField": "id"
					},
					"destination": {
						"FileName": "dboopdn"
					}
				},
				{
					"Source": {
						"TableName": "OPOR"
					},
					"Increment": {
						"IncField": "u_doctime",
						"IdField": "id"
					},
					"destination": {
						"FileName": "dboopor"
					}
				},
				{
					"Source": {
						"TableName": "ORIN"
					},
					"Increment": {
						"IncField": "u_doctime",
						"IdField": "id"
					},
					"destination": {
						"FileName": "dboorin"
					}
				},
				{
					"Source": {
						"TableName": "ORPC"
					},
					"Increment": {
						"IncField": "u_doctime",
						"IdField": "id"
					},
					"destination": {
						"FileName": "dboorpc"
					}
				},
				{
					"Source": {
						"TableName": "ORPD"
					},
					"Increment": {
						"IncField": "u_doctime",
						"IdField": "id"
					},
					"destination": {
						"FileName": "dboorpd"
					}
				},
				{
					"Source": {
						"TableName": "PCH1"
					},
					"Increment": {
						"IncField": "u_doctime",
						"IdField": "id"
					},
					"destination": {
						"FileName": "dbopch1"
					}
				},
				{
					"Source": {
						"TableName": "PDN1"
					},
					"Increment": {
						"IncField": "u_doctime",
						"IdField": "id"
					},
					"destination": {
						"FileName": "dbopdn1"
					}
				},
				{
					"Source": {
						"TableName": "POR1"
					},
					"Increment": {
						"IncField": "u_doctime",
						"IdField": "id"
					},
					"destination": {
						"FileName": "dbopor1"
					}
				},
				{
					"Source": {
						"TableName": "RIN1"
					},
					"Increment": {
						"IncField": "u_doctime",
						"IdField": "id"
					},
					"destination": {
						"FileName": "dborin1"
					}
				},
				{
					"Source": {
						"TableName": "RPC1"
					},
					"Increment": {
						"IncField": "u_doctime",
						"IdField": "id"
					},
					"destination": {
						"FileName": "dborpc1"
					}
				},
				{
					"Source": {
						"TableName": "RPD1"
					},
					"Increment": {
						"IncField": "u_doctime",
						"IdField": "id"
					},
					"destination": {
						"FileName": "dborpd1"
					}
				}
			]
		},
		"Rolling PROD_Weekly Trigger_properties_SAP LOAD PROD_parameters_TakeOnPeriod": {
			"type": "string",
			"defaultValue": "SELECT DATEADD(DAY,1,EOMONTH(DATEADD(MONTH,-3,GETDATE()),-1))"
		},
		"Rolling PROD_Weekly Trigger_properties_SAP LOAD PROD_parameters_Full_Load": {
			"type": "bool",
			"defaultValue": false
		},
		"Rolling PROD_Weekly Trigger_properties_SAP LOAD PROD_parameters_ColumnsNamesBlankSpaces": {
			"type": "bool",
			"defaultValue": true
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/Avis Full Take-on')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Copy Avis data",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 1,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "BinarySource",
								"storeSettings": {
									"type": "FileServerReadSettings",
									"recursive": true
								},
								"formatSettings": {
									"type": "BinaryReadSettings"
								}
							},
							"sink": {
								"type": "BinarySink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								}
							},
							"enableStaging": false
						},
						"inputs": [
							{
								"referenceName": "OnPremSambaBinary",
								"type": "DatasetReference",
								"parameters": {
									"ServerName": {
										"value": "@variables('ServerName')",
										"type": "Expression"
									},
									"UserName": {
										"value": "@variables('UserName')",
										"type": "Expression"
									},
									"SecretName": {
										"value": "@variables('SecretName')",
										"type": "Expression"
									},
									"DirectoryName": {
										"value": "@variables('SourceDirectoryName')",
										"type": "Expression"
									},
									"FileName": {
										"value": "@variables('FileName')",
										"type": "Expression"
									}
								}
							}
						],
						"outputs": [
							{
								"referenceName": "DataLakeBinary",
								"type": "DatasetReference",
								"parameters": {
									"ContainerName": {
										"value": "@variables('ContainerName')",
										"type": "Expression"
									},
									"DestFolderName": {
										"value": "@variables('DestFolderName')",
										"type": "Expression"
									},
									"FileName": {
										"value": "@variables('FileName')",
										"type": "Expression"
									},
									"DataLake": {
										"value": "@variables('DataLake')",
										"type": "Expression"
									}
								}
							}
						]
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"variables": {
					"ServerName": {
						"type": "String",
						"defaultValue": "\\\\10.1.8.34"
					},
					"SourceDirectoryName": {
						"type": "String",
						"defaultValue": "Avis"
					},
					"FileName": {
						"type": "String"
					},
					"UserName": {
						"type": "String",
						"defaultValue": "Synapse@citylogistics.co.za"
					},
					"SecretName": {
						"type": "String",
						"defaultValue": "OnPremSambaSecret"
					},
					"ContainerName": {
						"type": "String",
						"defaultValue": "synapse"
					},
					"DataLake": {
						"type": "String",
						"defaultValue": "https://citylogisticsstorageprod.dfs.core.windows.net/"
					},
					"DestFolderName": {
						"type": "String",
						"defaultValue": "\\Unstructured Data\\Avis_Files"
					}
				},
				"folder": {
					"name": "PROD/BackEnd/Extract/SharePoint"
				},
				"annotations": [],
				"lastPublishTime": "2022-04-05T14:54:15Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/OnPremSambaBinary')]",
				"[concat(variables('workspaceId'), '/datasets/DataLakeBinary')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CLMasterData LOAD PROD')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Iterate Data",
						"type": "ForEach",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@pipeline().parameters.CW_Items",
								"type": "Expression"
							},
							"batchCount": 4,
							"activities": [
								{
									"name": "If ColumnsNamesBlankSpacesOrLimitedLoad",
									"type": "IfCondition",
									"dependsOn": [
										{
											"activity": "If FullLoad",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"expression": {
											"value": "@or(pipeline().parameters.ColumnsNamesBlankSpaces,greater(length(pipeline().parameters.LimitedLoadTopX),1))",
											"type": "Expression"
										},
										"ifTrueActivities": [
											{
												"name": "SelectStatementBuilder",
												"type": "Lookup",
												"dependsOn": [],
												"policy": {
													"timeout": "0.03:00:00",
													"retry": 3,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [],
												"typeProperties": {
													"source": {
														"type": "SqlServerSource",
														"sqlReaderQuery": {
															"value": "@concat(' \nDECLARE @s VARCHAR(MAX)\nDECLARE @tablename VARCHAR(MAX) = ''', item().Source.SchemaName,'.',item().Source.TableName, '''\nSELECT @s = ISNULL(@s + '', '','''') + ''['' + c.name + '']'' + '' as [''+replace(c.name,'' '',''_'')+''] ''\nFROM sys.all_columns c \njoin sys.objects t ON c.object_id = t.object_id \nWHERE t.name =  @tablename \nSELECT ''SELECT ', if(greater(length(pipeline().parameters.LimitedLoadTopX),0),concat('TOP ', pipeline().parameters.LimitedLoadTopX),''), ' '' + @s + '' FROM ['' +  @tablename + '']', if(greater(length(item().Increment.IdField),1),concat(' ORDER BY ', item().Increment.IdField, ' DESC'),''),''' as Query')",
															"type": "Expression"
														},
														"queryTimeout": "02:00:00",
														"partitionOption": "None"
													},
													"dataset": {
														"referenceName": "SqlServerDataSet",
														"type": "DatasetReference",
														"parameters": {
															"ServerName": {
																"value": "@variables('ServerName')",
																"type": "Expression"
															},
															"DatabaseName": {
																"value": "@variables('DatabaseName')",
																"type": "Expression"
															},
															"UserName": {
																"value": "@variables('UserName')",
																"type": "Expression"
															},
															"CW_SchemaName": {
																"value": "@variables('SchemaName')",
																"type": "Expression"
															},
															"SecretName": {
																"value": "@variables('SecretName')",
																"type": "Expression"
															},
															"CW_TableName": {
																"value": "@item().Source.TableName",
																"type": "Expression"
															}
														}
													}
												}
											}
										]
									}
								},
								{
									"name": "If FullLoad",
									"type": "IfCondition",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"expression": {
											"value": "@pipeline().parameters.Full_Load",
											"type": "Expression"
										},
										"ifFalseActivities": [
											{
												"name": "Lookup Last Max Field",
												"type": "Lookup",
												"dependsOn": [],
												"policy": {
													"timeout": "0.03:00:00",
													"retry": 0,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [],
												"typeProperties": {
													"source": {
														"type": "AzureSqlSource",
														"sqlReaderQuery": {
															"value": "@concat('SELECT MAX(',item().Increment.IncField,')  as Field \n FROM\n    OPENROWSET(\n        BULK ','''',variables('DataLake'),variables('ContainerName'),'/',variables('FolderName'),'/',item().destination.FileName,'.parquet','''',\n        ',FORMAT = ','''PARQUET''','\n    ) AS [result]')",
															"type": "Expression"
														},
														"queryTimeout": "02:00:00",
														"partitionOption": "None"
													},
													"dataset": {
														"referenceName": "AzureSqlConnection",
														"type": "DatasetReference",
														"parameters": {
															"ASAEndpoint": {
																"value": "@variables('ASAEndpoint')",
																"type": "Expression"
															},
															"ASADataBase": {
																"value": "@variables('ASADataBase')",
																"type": "Expression"
															}
														}
													}
												}
											},
											{
												"name": "Remove Old Data Incremental Data_SingleFile",
												"type": "Delete",
												"dependsOn": [
													{
														"activity": "Lookup Last Max Field",
														"dependencyConditions": [
															"Succeeded"
														]
													}
												],
												"policy": {
													"timeout": "0.03:00:00",
													"retry": 3,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [],
												"typeProperties": {
													"dataset": {
														"referenceName": "DataLakeParquet",
														"type": "DatasetReference",
														"parameters": {
															"CW_FolderName": {
																"value": "@concat(variables('FolderName'),'_Incremental_Data')",
																"type": "Expression"
															},
															"CW_FileName": {
																"value": "@concat(item().Destination.FileName,'.parquet')",
																"type": "Expression"
															},
															"ContainerName": {
																"value": "@variables('TempContainerName')",
																"type": "Expression"
															},
															"DataLake": {
																"value": "@variables('DataLake')",
																"type": "Expression"
															}
														}
													},
													"enableLogging": false,
													"storeSettings": {
														"type": "AzureBlobFSReadSettings",
														"recursive": true,
														"enablePartitionDiscovery": false
													}
												}
											}
										],
										"ifTrueActivities": [
											{
												"name": "Remove Old Data Data_Folder",
												"type": "Delete",
												"dependsOn": [
													{
														"activity": "Remove Old Data Data_SingleFile",
														"dependencyConditions": [
															"Failed"
														]
													}
												],
												"policy": {
													"timeout": "0.03:00:00",
													"retry": 0,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [],
												"typeProperties": {
													"dataset": {
														"referenceName": "DataLakeParquet",
														"type": "DatasetReference",
														"parameters": {
															"CW_FolderName": {
																"value": "@variables('FolderName')",
																"type": "Expression"
															},
															"CW_FileName": {
																"value": "@concat(item().Destination.FileName,'.parquet')",
																"type": "Expression"
															},
															"ContainerName": {
																"value": "@variables('TempContainerName')",
																"type": "Expression"
															},
															"DataLake": {
																"value": "@variables('DataLake')",
																"type": "Expression"
															}
														}
													},
													"enableLogging": false,
													"storeSettings": {
														"type": "AzureBlobFSReadSettings",
														"recursive": true,
														"wildcardFileName": {
															"value": "@{concat(variables('TempContainerName'),'/',variables('FolderName'),'/',item().Destination.FileName,'.parquet','/')}",
															"type": "Expression"
														},
														"enablePartitionDiscovery": false
													}
												}
											},
											{
												"name": "Remove Old Data Data_SingleFile",
												"type": "Delete",
												"dependsOn": [],
												"policy": {
													"timeout": "0.03:00:00",
													"retry": 0,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [],
												"typeProperties": {
													"dataset": {
														"referenceName": "DataLakeParquet",
														"type": "DatasetReference",
														"parameters": {
															"CW_FolderName": {
																"value": "@variables('FolderName')",
																"type": "Expression"
															},
															"CW_FileName": {
																"value": "@concat(item().Destination.FileName,'.parquet')",
																"type": "Expression"
															},
															"ContainerName": {
																"value": "@variables('TempContainerName')",
																"type": "Expression"
															},
															"DataLake": {
																"value": "@variables('DataLake')",
																"type": "Expression"
															}
														}
													},
													"enableLogging": false,
													"storeSettings": {
														"type": "AzureBlobFSReadSettings",
														"recursive": true,
														"enablePartitionDiscovery": false
													}
												}
											},
											{
												"name": "WaitBecause of Error",
												"type": "Wait",
												"dependsOn": [
													{
														"activity": "Remove Old Data Data_Folder",
														"dependencyConditions": [
															"Failed"
														]
													}
												],
												"userProperties": [],
												"typeProperties": {
													"waitTimeInSeconds": 1
												}
											}
										]
									}
								},
								{
									"name": "Switch1",
									"type": "Switch",
									"dependsOn": [
										{
											"activity": "If ColumnsNamesBlankSpacesOrLimitedLoad",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"on": {
											"value": "@if(pipeline().parameters.Full_Load,'FullTakeOn'\n   ,if(equals(item().Increment.IncField,item().Increment.IdField),'IdLoad'\n      ,'DateLoad'\n   )\n)",
											"type": "Expression"
										},
										"cases": [
											{
												"value": "DateLoad",
												"activities": [
													{
														"name": "Copy New incremental data",
														"type": "Copy",
														"dependsOn": [],
														"policy": {
															"timeout": "0.03:00:00",
															"retry": 3,
															"retryIntervalInSeconds": 30,
															"secureOutput": false,
															"secureInput": false
														},
														"userProperties": [],
														"typeProperties": {
															"source": {
																"type": "SqlServerSource",
																"sqlReaderQuery": {
																	"value": "@concat(if(or(pipeline().parameters.ColumnsNamesBlankSpaces, greater(length(pipeline().parameters.LimitedLoadTopX),1)),\n        activity('SelectStatementBuilder').output.firstRow.Query,concat('SELECT * FROM [', item().Source.SchemaName,'].[', item().Source.TableName,']')\n        )\n    ,' WHERE ', item().Increment.IncField, ' > ',\n        if(greater(length(pipeline().parameters.TakeOnPeriod),1)\n        ,pipeline().parameters.TakeOnPeriod\n        ,concat('''',activity('Lookup Last Max Field').output.firstRow.Field,'''')\n        )\n    )",
																	"type": "Expression"
																},
																"queryTimeout": "02:00:00",
																"partitionOption": "None"
															},
															"sink": {
																"type": "ParquetSink",
																"storeSettings": {
																	"type": "AzureBlobFSWriteSettings"
																},
																"formatSettings": {
																	"type": "ParquetWriteSettings"
																}
															},
															"enableStaging": false,
															"validateDataConsistency": false,
															"translator": {
																"type": "TabularTranslator",
																"typeConversion": true,
																"typeConversionSettings": {
																	"allowDataTruncation": true,
																	"treatBooleanAsNumber": false
																}
															}
														},
														"inputs": [
															{
																"referenceName": "SqlServerDataSet",
																"type": "DatasetReference",
																"parameters": {
																	"ServerName": {
																		"value": "@variables('ServerName')",
																		"type": "Expression"
																	},
																	"DatabaseName": {
																		"value": "@variables('DatabaseName')",
																		"type": "Expression"
																	},
																	"UserName": {
																		"value": "@variables('UserName')",
																		"type": "Expression"
																	},
																	"CW_SchemaName": {
																		"value": "@variables('SchemaName')",
																		"type": "Expression"
																	},
																	"SecretName": {
																		"value": "@variables('SecretName')",
																		"type": "Expression"
																	},
																	"CW_TableName": {
																		"value": "@item().Source.TableName",
																		"type": "Expression"
																	}
																}
															}
														],
														"outputs": [
															{
																"referenceName": "DataLakeParquet",
																"type": "DatasetReference",
																"parameters": {
																	"CW_FolderName": {
																		"value": "@variables('FolderName')",
																		"type": "Expression"
																	},
																	"CW_FileName": {
																		"value": "@concat(item().Destination.FileName,'.parquet')",
																		"type": "Expression"
																	},
																	"ContainerName": {
																		"value": "@variables('TempContainerName')",
																		"type": "Expression"
																	},
																	"DataLake": {
																		"value": "@variables('DataLake')",
																		"type": "Expression"
																	}
																}
															}
														]
													}
												]
											},
											{
												"value": "IdLoad",
												"activities": [
													{
														"name": "Copy New incremental data_IDLoad",
														"type": "Copy",
														"dependsOn": [],
														"policy": {
															"timeout": "0.03:00:00",
															"retry": 3,
															"retryIntervalInSeconds": 30,
															"secureOutput": false,
															"secureInput": false
														},
														"userProperties": [],
														"typeProperties": {
															"source": {
																"type": "SqlServerSource",
																"sqlReaderQuery": {
																	"value": "@concat(if(or(pipeline().parameters.ColumnsNamesBlankSpaces,    greater(length(pipeline().parameters.LimitedLoadTopX),1)),\n        activity('SelectStatementBuilder').output.firstRow.Query, concat('SELECT * FROM [', item().Source.SchemaName,'].[', item().Source.TableName,']')\n        )   \n    ,' WHERE ', item().Increment.IncField, ' > ',\n        if(greater(length(pipeline().parameters.TakeOnPeriod),1)\n        ,pipeline().parameters.TakeOnPeriod\n        ,activity('Lookup Last Max Field').output.firstRow.Field\n        )\n    )",
																	"type": "Expression"
																},
																"queryTimeout": "02:00:00",
																"partitionOption": "None"
															},
															"sink": {
																"type": "ParquetSink",
																"storeSettings": {
																	"type": "AzureBlobFSWriteSettings"
																},
																"formatSettings": {
																	"type": "ParquetWriteSettings"
																}
															},
															"enableStaging": false,
															"validateDataConsistency": false,
															"translator": {
																"type": "TabularTranslator",
																"typeConversion": true,
																"typeConversionSettings": {
																	"allowDataTruncation": true,
																	"treatBooleanAsNumber": false
																}
															}
														},
														"inputs": [
															{
																"referenceName": "SqlServerDataSet",
																"type": "DatasetReference",
																"parameters": {
																	"ServerName": {
																		"value": "@variables('ServerName')",
																		"type": "Expression"
																	},
																	"DatabaseName": {
																		"value": "@variables('DatabaseName')",
																		"type": "Expression"
																	},
																	"UserName": {
																		"value": "@variables('UserName')",
																		"type": "Expression"
																	},
																	"CW_SchemaName": {
																		"value": "@variables('SchemaName')",
																		"type": "Expression"
																	},
																	"SecretName": {
																		"value": "@variables('SecretName')",
																		"type": "Expression"
																	},
																	"CW_TableName": {
																		"value": "@item().Source.TableName",
																		"type": "Expression"
																	}
																}
															}
														],
														"outputs": [
															{
																"referenceName": "DataLakeParquet",
																"type": "DatasetReference",
																"parameters": {
																	"CW_FolderName": {
																		"value": "@variables('FolderName')",
																		"type": "Expression"
																	},
																	"CW_FileName": {
																		"value": "@concat(item().Destination.FileName,'.parquet')",
																		"type": "Expression"
																	},
																	"ContainerName": {
																		"value": "@variables('TempContainerName')",
																		"type": "Expression"
																	},
																	"DataLake": {
																		"value": "@variables('DataLake')",
																		"type": "Expression"
																	}
																}
															}
														]
													}
												]
											},
											{
												"value": "FullTakeOn",
												"activities": [
													{
														"name": "Copy_Data_Full_Take_On",
														"type": "Copy",
														"dependsOn": [],
														"policy": {
															"timeout": "7.00:00:00",
															"retry": 3,
															"retryIntervalInSeconds": 30,
															"secureOutput": false,
															"secureInput": false
														},
														"userProperties": [],
														"typeProperties": {
															"source": {
																"type": "SqlServerSource",
																"sqlReaderQuery": {
																	"value": "@if(or(pipeline().parameters.ColumnsNamesBlankSpaces, greater(length(pipeline().parameters.LimitedLoadTopX),1)),\n    activity('SelectStatementBuilder').output.firstRow.Query,\n    concat('SELECT * FROM [', item().Source.SchemaName,'].[', item().Source.TableName,']')\n    )",
																	"type": "Expression"
																},
																"partitionOption": "None"
															},
															"sink": {
																"type": "ParquetSink",
																"storeSettings": {
																	"type": "AzureBlobFSWriteSettings"
																},
																"formatSettings": {
																	"type": "ParquetWriteSettings"
																}
															},
															"enableStaging": false,
															"validateDataConsistency": false,
															"translator": {
																"type": "TabularTranslator",
																"typeConversion": true,
																"typeConversionSettings": {
																	"allowDataTruncation": true,
																	"treatBooleanAsNumber": false
																}
															}
														},
														"inputs": [
															{
																"referenceName": "SqlServerDataSet",
																"type": "DatasetReference",
																"parameters": {
																	"ServerName": {
																		"value": "@variables('ServerName')",
																		"type": "Expression"
																	},
																	"DatabaseName": {
																		"value": "@variables('DatabaseName')",
																		"type": "Expression"
																	},
																	"UserName": {
																		"value": "@variables('UserName')",
																		"type": "Expression"
																	},
																	"CW_SchemaName": {
																		"value": "@variables('SchemaName')",
																		"type": "Expression"
																	},
																	"SecretName": {
																		"value": "@variables('SecretName')",
																		"type": "Expression"
																	},
																	"CW_TableName": {
																		"value": "@item().Source.TableName",
																		"type": "Expression"
																	}
																}
															}
														],
														"outputs": [
															{
																"referenceName": "DataLakeParquet",
																"type": "DatasetReference",
																"parameters": {
																	"CW_FolderName": {
																		"value": "@variables('FolderName')",
																		"type": "Expression"
																	},
																	"CW_FileName": {
																		"value": "@concat(item().Destination.FileName,'.parquet')",
																		"type": "Expression"
																	},
																	"ContainerName": {
																		"value": "@variables('TempContainerName')",
																		"type": "Expression"
																	},
																	"DataLake": {
																		"value": "@variables('DataLake')",
																		"type": "Expression"
																	}
																}
															}
														]
													}
												]
											}
										],
										"defaultActivities": [
											{
												"name": "Fail1",
												"type": "Fail",
												"dependsOn": [],
												"userProperties": [],
												"typeProperties": {
													"message": "ERROR",
													"errorCode": "500"
												}
											}
										]
									}
								}
							]
						}
					},
					{
						"name": "ProdNotebook",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "Iterate Data",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "IncrementalProdScript",
								"type": "NotebookReference"
							},
							"parameters": {
								"cw_FolderName": {
									"value": {
										"value": "@variables('FolderName')",
										"type": "Expression"
									},
									"type": "string"
								},
								"ContainerName": {
									"value": {
										"value": "@variables('ContainerName')",
										"type": "Expression"
									},
									"type": "string"
								},
								"DataLakeDF": {
									"value": {
										"value": "@replace(variables('DataLake'),'https://','')",
										"type": "Expression"
									},
									"type": "string"
								},
								"Items": {
									"value": {
										"value": "@{pipeline().parameters.cw_items}",
										"type": "Expression"
									},
									"type": "string"
								},
								"FullLoad": {
									"value": {
										"value": "@pipeline().parameters.Full_Load",
										"type": "Expression"
									},
									"type": "string"
								},
								"TempContainerName": {
									"value": {
										"value": "@variables('TempContainerName')",
										"type": "Expression"
									},
									"type": "string"
								}
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "TESTSparkPool",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": true,
								"spark.dynamicAllocation.minExecutors": 1,
								"spark.dynamicAllocation.maxExecutors": 4
							},
							"driverSize": "Small",
							"numExecutors": 1
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"parameters": {
					"cw_items": {
						"type": "Array",
						"defaultValue": []
					},
					"TakeOnPeriod": {
						"type": "string",
						"defaultValue": " "
					},
					"Full_Load": {
						"type": "bool",
						"defaultValue": false
					},
					"ColumnsNamesBlankSpaces": {
						"type": "bool",
						"defaultValue": false
					},
					"LimitedLoadTopX": {
						"type": "string",
						"defaultValue": "1"
					}
				},
				"variables": {
					"FolderName": {
						"type": "String",
						"defaultValue": "Unstructured Data/CLMasterData"
					},
					"DataLake": {
						"type": "String",
						"defaultValue": "https://citylogisticsstorageprod.dfs.core.windows.net/"
					},
					"ServerName": {
						"type": "String",
						"defaultValue": "CCLMSPROD"
					},
					"DataBaseName": {
						"type": "String",
						"defaultValue": "CLMasterData"
					},
					"UserName": {
						"type": "String",
						"defaultValue": "Synapse@citylogistics.co.za"
					},
					"SecretName": {
						"type": "String",
						"defaultValue": "CLMastSecret"
					},
					"SchemaName": {
						"type": "String",
						"defaultValue": "public"
					},
					"ContainerName": {
						"type": "String",
						"defaultValue": "synapse"
					},
					"ASAEndpoint": {
						"type": "String",
						"defaultValue": "citylogistics-synapseanalytics-workspace-prod-ondemand.sql.azuresynapse.net"
					},
					"ASADataBase": {
						"type": "String",
						"defaultValue": "master"
					},
					"TempContainerName": {
						"type": "String",
						"defaultValue": "temp"
					}
				},
				"folder": {
					"name": "PROD/BackEnd/Extract/CLMasterData"
				},
				"annotations": [],
				"lastPublishTime": "2022-04-22T08:33:31Z",
				"type": "Microsoft.Synapse/workspaces/pipelines"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/IncrementalProdScript')]",
				"[concat(variables('workspaceId'), '/bigDataPools/TESTSparkPool')]",
				"[concat(variables('workspaceId'), '/datasets/SqlServerDataSet')]",
				"[concat(variables('workspaceId'), '/datasets/AzureSqlConnection')]",
				"[concat(variables('workspaceId'), '/datasets/DataLakeParquet')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Finance End-to-End')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Sap Source Incremental load",
						"type": "ExecutePipeline",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "SAP LOAD PROD",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {
								"CW_Items": {
									"value": "@pipeline().parameters.SAPSource1CW_Items",
									"type": "Expression"
								},
								"TakeOnPeriod": " ",
								"Full_Load": true,
								"ColumnsNamesBlankSpaces": false,
								"LimitedLoadTopX": " "
							}
						}
					},
					{
						"name": "Sap Source Full load",
						"type": "ExecutePipeline",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "SAP LOAD PROD",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {
								"CW_Items": {
									"value": "@pipeline().parameters.SAPSource2CW_Items",
									"type": "Expression"
								},
								"TakeOnPeriod": " ",
								"Full_Load": true,
								"ColumnsNamesBlankSpaces": false,
								"LimitedLoadTopX": " "
							}
						}
					},
					{
						"name": "SAP_STRunOrder",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "Sap Source Incremental load",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "Sap Source Full load",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "00_STSAP_RunOrder",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "PRDSparkPool",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": true,
								"spark.dynamicAllocation.minExecutors": 2,
								"spark.dynamicAllocation.maxExecutors": 4
							},
							"driverSize": "Small",
							"numExecutors": 2
						}
					},
					{
						"name": "Finance Power BI Refresh",
						"type": "ExecutePipeline",
						"dependsOn": [
							{
								"activity": "SAP_STRunOrder",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "PBI_Dataflows",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {
								"URL_Data": {
									"value": "@pipeline().parameters.SAPPowerBIRefresh",
									"type": "Expression"
								}
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"parameters": {
					"SAPSource1CW_Items": {
						"type": "array",
						"defaultValue": [
							{
								"Source": {
									"TableName": "IGE1"
								},
								"Increment": {
									"IncField": "u_doctime",
									"IdField": "DocEntry"
								},
								"destination": {
									"FileName": "dboige1"
								}
							},
							{
								"Source": {
									"TableName": "IGN1"
								},
								"Increment": {
									"IncField": "u_doctime",
									"IdField": "DocEntry"
								},
								"destination": {
									"FileName": "dboign1"
								}
							},
							{
								"Source": {
									"TableName": "INV1"
								},
								"Increment": {
									"IncField": "u_doctime",
									"IdField": "DocEntry"
								},
								"destination": {
									"FileName": "dboinv1"
								}
							},
							{
								"Source": {
									"TableName": "JDT1"
								},
								"Increment": {
									"IncField": "u_doctime",
									"IdField": "TransId"
								},
								"destination": {
									"FileName": "dbojdt1"
								}
							},
							{
								"Source": {
									"TableName": "MRV1"
								},
								"Increment": {
									"IncField": "u_doctime",
									"IdField": "DocEntry"
								},
								"destination": {
									"FileName": "dbomrv1"
								}
							},
							{
								"Source": {
									"TableName": "MRV2"
								},
								"Increment": {
									"IncField": "u_doctime",
									"IdField": "DocEntry"
								},
								"destination": {
									"FileName": "dbomrv2"
								}
							},
							{
								"Source": {
									"TableName": "OIGE"
								},
								"Increment": {
									"IncField": "u_doctime",
									"IdField": "DocEntry"
								},
								"destination": {
									"FileName": "dbooige"
								}
							},
							{
								"Source": {
									"TableName": "OIGN"
								},
								"Increment": {
									"IncField": "u_doctime",
									"IdField": "DocEntry"
								},
								"destination": {
									"FileName": "dbooign"
								}
							},
							{
								"Source": {
									"TableName": "OINM"
								},
								"Increment": {
									"IncField": "transnum",
									"IdField": "TransNum"
								},
								"destination": {
									"FileName": "dbooinm"
								}
							},
							{
								"Source": {
									"TableName": "OINV"
								},
								"Increment": {
									"IncField": "u_doctime",
									"IdField": "DocEntry"
								},
								"destination": {
									"FileName": "dbooinv"
								}
							},
							{
								"Source": {
									"TableName": "OJDT"
								},
								"Increment": {
									"IncField": "u_doctime",
									"IdField": "TransId"
								},
								"destination": {
									"FileName": "dboojdt"
								}
							},
							{
								"Source": {
									"TableName": "OMRV"
								},
								"Increment": {
									"IncField": "u_doctime",
									"IdField": "DocEntry"
								},
								"destination": {
									"FileName": "dboomrv"
								}
							},
							{
								"Source": {
									"TableName": "OPCH"
								},
								"Increment": {
									"IncField": "u_doctime",
									"IdField": "DocEntry"
								},
								"destination": {
									"FileName": "dboopch"
								}
							},
							{
								"Source": {
									"TableName": "OPDN"
								},
								"Increment": {
									"IncField": "u_doctime",
									"IdField": "DocEntry"
								},
								"destination": {
									"FileName": "dboopdn"
								}
							},
							{
								"Source": {
									"TableName": "OPOR"
								},
								"Increment": {
									"IncField": "u_doctime",
									"IdField": "DocEntry"
								},
								"destination": {
									"FileName": "dboopor"
								}
							},
							{
								"Source": {
									"TableName": "ORIN"
								},
								"Increment": {
									"IncField": "u_doctime",
									"IdField": "DocEntry"
								},
								"destination": {
									"FileName": "dboorin"
								}
							},
							{
								"Source": {
									"TableName": "ORPC"
								},
								"Increment": {
									"IncField": "u_doctime",
									"IdField": "DocEntry"
								},
								"destination": {
									"FileName": "dboorpc"
								}
							},
							{
								"Source": {
									"TableName": "PCH1"
								},
								"Increment": {
									"IncField": "u_doctime",
									"IdField": "DocEntry"
								},
								"destination": {
									"FileName": "dbopch1"
								}
							},
							{
								"Source": {
									"TableName": "PDN1"
								},
								"Increment": {
									"IncField": "u_doctime",
									"IdField": "DocEntry"
								},
								"destination": {
									"FileName": "dbopdn1"
								}
							},
							{
								"Source": {
									"TableName": "POR1"
								},
								"Increment": {
									"IncField": "u_doctime",
									"IdField": "DocEntry"
								},
								"destination": {
									"FileName": "dbopor1"
								}
							},
							{
								"Source": {
									"TableName": "RIN1"
								},
								"Increment": {
									"IncField": "u_doctime",
									"IdField": "DocEntry"
								},
								"destination": {
									"FileName": "dborin1"
								}
							},
							{
								"Source": {
									"TableName": "RPC1"
								},
								"Increment": {
									"IncField": "u_doctime",
									"IdField": "DocEntry"
								},
								"destination": {
									"FileName": "dborpc1"
								}
							},
							{
								"Source": {
									"TableName": "RPD1"
								},
								"Increment": {
									"IncField": "u_doctime",
									"IdField": "DocEntry"
								},
								"destination": {
									"FileName": "dborpd1"
								}
							},
							{
								"Source": {
									"TableName": "OITM"
								},
								"Increment": {
									"IncField": "itemcode",
									"IdField": "ItemCode"
								},
								"destination": {
									"FileName": "dbooitm"
								}
							}
						]
					},
					"SAPSource2CW_Items": {
						"type": "array",
						"defaultValue": [
							{
								"Source": {
									"TableName": "OACT"
								},
								"Increment": {
									"IncField": "0",
									"IdField": "0"
								},
								"destination": {
									"FileName": "dbooact"
								}
							},
							{
								"Source": {
									"TableName": "OCRD"
								},
								"Increment": {
									"IncField": "0",
									"IdField": "0"
								},
								"destination": {
									"FileName": "dboocrd"
								}
							},
							{
								"Source": {
									"TableName": "OITB"
								},
								"Increment": {
									"IncField": "0",
									"IdField": "0"
								},
								"destination": {
									"FileName": "dbooitb"
								}
							}
						]
					},
					"SAPPowerBIRefresh": {
						"type": "array",
						"defaultValue": [
							{
								"GroupID": "2a5062fe-1c10-4c36-91df-40ffcbc2b229",
								"dataflowID": "ecb69492-9c6f-4922-ab63-6d52b6d2209d"
							}
						]
					}
				},
				"folder": {
					"name": "End-to-End"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/SAP LOAD PROD')]",
				"[concat(variables('workspaceId'), '/notebooks/00_STSAP_RunOrder')]",
				"[concat(variables('workspaceId'), '/bigDataPools/PRDSparkPool')]",
				"[concat(variables('workspaceId'), '/pipelines/PBI_Dataflows')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LMS LOAD PROD')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Iterate Data",
						"type": "ForEach",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@pipeline().parameters.CW_Items",
								"type": "Expression"
							},
							"batchCount": 4,
							"activities": [
								{
									"name": "If ColumnsNamesBlankSpacesOrLimitedLoad",
									"type": "IfCondition",
									"dependsOn": [
										{
											"activity": "If FullLoad",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"expression": {
											"value": "@or(pipeline().parameters.ColumnsNamesBlankSpaces,greater(length(pipeline().parameters.LimitedLoadTopX),1))",
											"type": "Expression"
										},
										"ifTrueActivities": [
											{
												"name": "SelectStatementBuilder",
												"type": "Lookup",
												"dependsOn": [],
												"policy": {
													"timeout": "0.03:00:00",
													"retry": 3,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [],
												"typeProperties": {
													"source": {
														"type": "SqlServerSource",
														"sqlReaderQuery": {
															"value": "@concat(' \nDECLARE @s VARCHAR(MAX)\nDECLARE @tablename VARCHAR(MAX) = ''', item().Source.TableName,'''\nSELECT @s = ISNULL(@s + '', '','''') + ''['' + c.name + '']'' + '' as [''+replace(c.name,'' '',''_'')+''] ''\nFROM sys.all_columns c \njoin sys.objects t ON c.object_id = t.object_id \nWHERE t.name =  @tablename \nSELECT ''SELECT ', if(greater(length(pipeline().parameters.LimitedLoadTopX),0),concat('TOP ', pipeline().parameters.LimitedLoadTopX),''), ' '' + @s + '' FROM ['' +  @tablename + '']', if(greater(length(item().Increment.IdField),1),concat(' ORDER BY ', item().Increment.IdField, ' DESC'),''),''' as Query')",
															"type": "Expression"
														},
														"queryTimeout": "02:00:00",
														"partitionOption": "None"
													},
													"dataset": {
														"referenceName": "SqlServerDataSet",
														"type": "DatasetReference",
														"parameters": {
															"ServerName": {
																"value": "@variables('ServerName')",
																"type": "Expression"
															},
															"DatabaseName": {
																"value": "@variables('DatabaseName')",
																"type": "Expression"
															},
															"UserName": {
																"value": "@variables('UserName')",
																"type": "Expression"
															},
															"CW_SchemaName": {
																"value": "@variables('SchemaName')",
																"type": "Expression"
															},
															"SecretName": {
																"value": "@variables('SecretName')",
																"type": "Expression"
															},
															"CW_TableName": {
																"value": "@item().Source.TableName",
																"type": "Expression"
															}
														}
													}
												}
											}
										]
									}
								},
								{
									"name": "If FullLoad",
									"type": "IfCondition",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"expression": {
											"value": "@pipeline().parameters.Full_Load",
											"type": "Expression"
										},
										"ifFalseActivities": [
											{
												"name": "Lookup Last Max Field",
												"type": "Lookup",
												"dependsOn": [],
												"policy": {
													"timeout": "0.03:00:00",
													"retry": 0,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [],
												"typeProperties": {
													"source": {
														"type": "AzureSqlSource",
														"sqlReaderQuery": {
															"value": "@concat('SELECT MAX(',item().Increment.IncField,')  as Field \n FROM\n    OPENROWSET(\n        BULK ','''',variables('DataLake'),variables('ContainerName'),'/',variables('FolderName'),'/',item().destination.FileName,'.parquet','''',\n        ',FORMAT = ','''PARQUET''','\n    ) AS [result]')",
															"type": "Expression"
														},
														"queryTimeout": "02:00:00",
														"partitionOption": "None"
													},
													"dataset": {
														"referenceName": "AzureSqlConnection",
														"type": "DatasetReference",
														"parameters": {
															"ASAEndpoint": {
																"value": "@variables('ASAEndpoint')",
																"type": "Expression"
															},
															"ASADataBase": {
																"value": "@variables('ASADataBase')",
																"type": "Expression"
															}
														}
													}
												}
											},
											{
												"name": "Remove Old Data Incremental Data_SingleFile",
												"type": "Delete",
												"dependsOn": [
													{
														"activity": "Lookup Last Max Field",
														"dependencyConditions": [
															"Succeeded"
														]
													}
												],
												"policy": {
													"timeout": "0.03:00:00",
													"retry": 3,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [],
												"typeProperties": {
													"dataset": {
														"referenceName": "DataLakeParquet",
														"type": "DatasetReference",
														"parameters": {
															"CW_FolderName": {
																"value": "@concat(variables('FolderName'),'_Incremental_Data')",
																"type": "Expression"
															},
															"CW_FileName": {
																"value": "@concat(item().Destination.FileName,'.parquet')",
																"type": "Expression"
															},
															"ContainerName": {
																"value": "@variables('TempContainerName')",
																"type": "Expression"
															},
															"DataLake": {
																"value": "@variables('DataLake')",
																"type": "Expression"
															}
														}
													},
													"enableLogging": false,
													"storeSettings": {
														"type": "AzureBlobFSReadSettings",
														"recursive": true,
														"enablePartitionDiscovery": false
													}
												}
											}
										],
										"ifTrueActivities": [
											{
												"name": "Remove Old Data Data_Folder",
												"type": "Delete",
												"dependsOn": [
													{
														"activity": "Remove Old Data Data_SingleFile",
														"dependencyConditions": [
															"Failed"
														]
													}
												],
												"policy": {
													"timeout": "0.03:00:00",
													"retry": 0,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [],
												"typeProperties": {
													"dataset": {
														"referenceName": "DataLakeParquet",
														"type": "DatasetReference",
														"parameters": {
															"CW_FolderName": {
																"value": "@variables('FolderName')",
																"type": "Expression"
															},
															"CW_FileName": {
																"value": "@concat(item().Destination.FileName,'.parquet')",
																"type": "Expression"
															},
															"ContainerName": {
																"value": "@variables('TempContainerName')",
																"type": "Expression"
															},
															"DataLake": {
																"value": "@variables('DataLake')",
																"type": "Expression"
															}
														}
													},
													"enableLogging": false,
													"storeSettings": {
														"type": "AzureBlobFSReadSettings",
														"recursive": true,
														"wildcardFileName": {
															"value": "@{concat(variables('TempContainerName'),'/',variables('FolderName'),'/',item().Destination.FileName,'.parquet','/')}",
															"type": "Expression"
														},
														"enablePartitionDiscovery": false
													}
												}
											},
											{
												"name": "Remove Old Data Data_SingleFile",
												"type": "Delete",
												"dependsOn": [],
												"policy": {
													"timeout": "0.03:00:00",
													"retry": 0,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [],
												"typeProperties": {
													"dataset": {
														"referenceName": "DataLakeParquet",
														"type": "DatasetReference",
														"parameters": {
															"CW_FolderName": {
																"value": "@variables('FolderName')",
																"type": "Expression"
															},
															"CW_FileName": {
																"value": "@concat(item().Destination.FileName,'.parquet')",
																"type": "Expression"
															},
															"ContainerName": {
																"value": "@variables('TempContainerName')",
																"type": "Expression"
															},
															"DataLake": {
																"value": "@variables('DataLake')",
																"type": "Expression"
															}
														}
													},
													"enableLogging": false,
													"storeSettings": {
														"type": "AzureBlobFSReadSettings",
														"recursive": true,
														"enablePartitionDiscovery": false
													}
												}
											},
											{
												"name": "WaitBecause of Error",
												"type": "Wait",
												"dependsOn": [
													{
														"activity": "Remove Old Data Data_Folder",
														"dependencyConditions": [
															"Failed"
														]
													}
												],
												"userProperties": [],
												"typeProperties": {
													"waitTimeInSeconds": 1
												}
											}
										]
									}
								},
								{
									"name": "Switch1",
									"type": "Switch",
									"dependsOn": [
										{
											"activity": "If ColumnsNamesBlankSpacesOrLimitedLoad",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"on": {
											"value": "@if(pipeline().parameters.Full_Load,'FullTakeOn'\n   ,if(equals(item().Increment.IncField,item().Increment.IdField),'IdLoad'\n      ,'DateLoad'\n   )\n)",
											"type": "Expression"
										},
										"cases": [
											{
												"value": "DateLoad",
												"activities": [
													{
														"name": "Copy New incremental data",
														"type": "Copy",
														"dependsOn": [],
														"policy": {
															"timeout": "0.03:00:00",
															"retry": 3,
															"retryIntervalInSeconds": 30,
															"secureOutput": false,
															"secureInput": false
														},
														"userProperties": [],
														"typeProperties": {
															"source": {
																"type": "SqlServerSource",
																"sqlReaderQuery": {
																	"value": "@concat(if(or(pipeline().parameters.ColumnsNamesBlankSpaces, greater(length(pipeline().parameters.LimitedLoadTopX),1)),\n        activity('SelectStatementBuilder').output.firstRow.Query,concat('SELECT * FROM [', item().Source.TableName,']')\n        )\n    ,' WHERE ', item().Increment.IncField, ' > ',\n        if(greater(length(pipeline().parameters.TakeOnPeriod),1)\n        ,pipeline().parameters.TakeOnPeriod\n        ,concat('''',activity('Lookup Last Max Field').output.firstRow.Field,'''')\n        )\n    )",
																	"type": "Expression"
																},
																"queryTimeout": "02:00:00",
																"partitionOption": "None"
															},
															"sink": {
																"type": "ParquetSink",
																"storeSettings": {
																	"type": "AzureBlobFSWriteSettings"
																},
																"formatSettings": {
																	"type": "ParquetWriteSettings"
																}
															},
															"enableStaging": false,
															"validateDataConsistency": false,
															"translator": {
																"type": "TabularTranslator",
																"typeConversion": true,
																"typeConversionSettings": {
																	"allowDataTruncation": true,
																	"treatBooleanAsNumber": false
																}
															}
														},
														"inputs": [
															{
																"referenceName": "SqlServerDataSet",
																"type": "DatasetReference",
																"parameters": {
																	"ServerName": {
																		"value": "@variables('ServerName')",
																		"type": "Expression"
																	},
																	"DatabaseName": {
																		"value": "@variables('DatabaseName')",
																		"type": "Expression"
																	},
																	"UserName": {
																		"value": "@variables('UserName')",
																		"type": "Expression"
																	},
																	"CW_SchemaName": {
																		"value": "@variables('SchemaName')",
																		"type": "Expression"
																	},
																	"SecretName": {
																		"value": "@variables('SecretName')",
																		"type": "Expression"
																	},
																	"CW_TableName": {
																		"value": "@item().Source.TableName",
																		"type": "Expression"
																	}
																}
															}
														],
														"outputs": [
															{
																"referenceName": "DataLakeParquet",
																"type": "DatasetReference",
																"parameters": {
																	"CW_FolderName": {
																		"value": "@variables('FolderName')",
																		"type": "Expression"
																	},
																	"CW_FileName": {
																		"value": "@concat(item().Destination.FileName,'.parquet')",
																		"type": "Expression"
																	},
																	"ContainerName": {
																		"value": "@variables('TempContainerName')",
																		"type": "Expression"
																	},
																	"DataLake": {
																		"value": "@variables('DataLake')",
																		"type": "Expression"
																	}
																}
															}
														]
													}
												]
											},
											{
												"value": "IdLoad",
												"activities": [
													{
														"name": "Copy New incremental data_IDLoad",
														"type": "Copy",
														"dependsOn": [],
														"policy": {
															"timeout": "0.03:00:00",
															"retry": 3,
															"retryIntervalInSeconds": 30,
															"secureOutput": false,
															"secureInput": false
														},
														"userProperties": [],
														"typeProperties": {
															"source": {
																"type": "SqlServerSource",
																"sqlReaderQuery": {
																	"value": "@concat(if(or(pipeline().parameters.ColumnsNamesBlankSpaces,    greater(length(pipeline().parameters.LimitedLoadTopX),1)),\n        activity('SelectStatementBuilder').output.firstRow.Query, concat('SELECT * FROM [', item().Source.TableName,']')\n        )   \n    ,' WHERE ', item().Increment.IncField, ' > ',\n        if(greater(length(pipeline().parameters.TakeOnPeriod),1)\n        ,pipeline().parameters.TakeOnPeriod\n        ,activity('Lookup Last Max Field').output.firstRow.Field\n        )\n    )",
																	"type": "Expression"
																},
																"queryTimeout": "02:00:00",
																"partitionOption": "None"
															},
															"sink": {
																"type": "ParquetSink",
																"storeSettings": {
																	"type": "AzureBlobFSWriteSettings"
																},
																"formatSettings": {
																	"type": "ParquetWriteSettings"
																}
															},
															"enableStaging": false,
															"validateDataConsistency": false,
															"translator": {
																"type": "TabularTranslator",
																"typeConversion": true,
																"typeConversionSettings": {
																	"allowDataTruncation": true,
																	"treatBooleanAsNumber": false
																}
															}
														},
														"inputs": [
															{
																"referenceName": "SqlServerDataSet",
																"type": "DatasetReference",
																"parameters": {
																	"ServerName": {
																		"value": "@variables('ServerName')",
																		"type": "Expression"
																	},
																	"DatabaseName": {
																		"value": "@variables('DatabaseName')",
																		"type": "Expression"
																	},
																	"UserName": {
																		"value": "@variables('UserName')",
																		"type": "Expression"
																	},
																	"CW_SchemaName": {
																		"value": "@variables('SchemaName')",
																		"type": "Expression"
																	},
																	"SecretName": {
																		"value": "@variables('SecretName')",
																		"type": "Expression"
																	},
																	"CW_TableName": {
																		"value": "@item().Source.TableName",
																		"type": "Expression"
																	}
																}
															}
														],
														"outputs": [
															{
																"referenceName": "DataLakeParquet",
																"type": "DatasetReference",
																"parameters": {
																	"CW_FolderName": {
																		"value": "@variables('FolderName')",
																		"type": "Expression"
																	},
																	"CW_FileName": {
																		"value": "@concat(item().Destination.FileName,'.parquet')",
																		"type": "Expression"
																	},
																	"ContainerName": {
																		"value": "@variables('TempContainerName')",
																		"type": "Expression"
																	},
																	"DataLake": {
																		"value": "@variables('DataLake')",
																		"type": "Expression"
																	}
																}
															}
														]
													}
												]
											},
											{
												"value": "FullTakeOn",
												"activities": [
													{
														"name": "Copy_Data_Full_Take_On",
														"type": "Copy",
														"dependsOn": [],
														"policy": {
															"timeout": "7.00:00:00",
															"retry": 3,
															"retryIntervalInSeconds": 30,
															"secureOutput": false,
															"secureInput": false
														},
														"userProperties": [],
														"typeProperties": {
															"source": {
																"type": "SqlServerSource",
																"sqlReaderQuery": {
																	"value": "@if(or(pipeline().parameters.ColumnsNamesBlankSpaces, greater(length(pipeline().parameters.LimitedLoadTopX),1)),\n    activity('SelectStatementBuilder').output.firstRow.Query,\n    concat('SELECT * FROM [', item().Source.TableName,']')\n    )",
																	"type": "Expression"
																},
																"partitionOption": "None"
															},
															"sink": {
																"type": "ParquetSink",
																"storeSettings": {
																	"type": "AzureBlobFSWriteSettings"
																},
																"formatSettings": {
																	"type": "ParquetWriteSettings"
																}
															},
															"enableStaging": false,
															"validateDataConsistency": false,
															"translator": {
																"type": "TabularTranslator",
																"typeConversion": true,
																"typeConversionSettings": {
																	"allowDataTruncation": true,
																	"treatBooleanAsNumber": false
																}
															}
														},
														"inputs": [
															{
																"referenceName": "SqlServerDataSet",
																"type": "DatasetReference",
																"parameters": {
																	"ServerName": {
																		"value": "@variables('ServerName')",
																		"type": "Expression"
																	},
																	"DatabaseName": {
																		"value": "@variables('DatabaseName')",
																		"type": "Expression"
																	},
																	"UserName": {
																		"value": "@variables('UserName')",
																		"type": "Expression"
																	},
																	"CW_SchemaName": {
																		"value": "@variables('SchemaName')",
																		"type": "Expression"
																	},
																	"SecretName": {
																		"value": "@variables('SecretName')",
																		"type": "Expression"
																	},
																	"CW_TableName": {
																		"value": "@item().Source.TableName",
																		"type": "Expression"
																	}
																}
															}
														],
														"outputs": [
															{
																"referenceName": "DataLakeParquet",
																"type": "DatasetReference",
																"parameters": {
																	"CW_FolderName": {
																		"value": "@variables('FolderName')",
																		"type": "Expression"
																	},
																	"CW_FileName": {
																		"value": "@concat(item().Destination.FileName,'.parquet')",
																		"type": "Expression"
																	},
																	"ContainerName": {
																		"value": "@variables('TempContainerName')",
																		"type": "Expression"
																	},
																	"DataLake": {
																		"value": "@variables('DataLake')",
																		"type": "Expression"
																	}
																}
															}
														]
													}
												]
											}
										],
										"defaultActivities": [
											{
												"name": "Fail1",
												"type": "Fail",
												"dependsOn": [],
												"userProperties": [],
												"typeProperties": {
													"message": "ERROR",
													"errorCode": "500"
												}
											}
										]
									}
								}
							]
						}
					},
					{
						"name": "ProdNotebook",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "Iterate Data",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "IncrementalProdScript",
								"type": "NotebookReference"
							},
							"parameters": {
								"cw_FolderName": {
									"value": {
										"value": "@variables('FolderName')",
										"type": "Expression"
									},
									"type": "string"
								},
								"ContainerName": {
									"value": {
										"value": "@variables('ContainerName')",
										"type": "Expression"
									},
									"type": "string"
								},
								"DataLakeDF": {
									"value": {
										"value": "@replace(variables('DataLake'),'https://','')",
										"type": "Expression"
									},
									"type": "string"
								},
								"Items": {
									"value": {
										"value": "@{pipeline().parameters.cw_items}",
										"type": "Expression"
									},
									"type": "string"
								},
								"FullLoad": {
									"value": {
										"value": "@pipeline().parameters.Full_Load",
										"type": "Expression"
									},
									"type": "string"
								},
								"TempContainerName": {
									"value": {
										"value": "@variables('TempContainerName')",
										"type": "Expression"
									},
									"type": "string"
								}
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "TESTSparkPool",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": true,
								"spark.dynamicAllocation.minExecutors": 1,
								"spark.dynamicAllocation.maxExecutors": 4
							},
							"driverSize": "Small",
							"numExecutors": 1
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"parameters": {
					"CW_Items": {
						"type": "Array",
						"defaultValue": []
					},
					"TakeOnPeriod": {
						"type": "string",
						"defaultValue": " "
					},
					"Full_Load": {
						"type": "bool",
						"defaultValue": false
					},
					"ColumnsNamesBlankSpaces": {
						"type": "bool",
						"defaultValue": false
					},
					"LimitedLoadTopX": {
						"type": "string",
						"defaultValue": " "
					}
				},
				"variables": {
					"FolderName": {
						"type": "String",
						"defaultValue": "Unstructured Data/LMS"
					},
					"DataLake": {
						"type": "String",
						"defaultValue": "https://citylogisticsstorageprod.dfs.core.windows.net/"
					},
					"ServerName": {
						"type": "String",
						"defaultValue": "CCLMSPROD"
					},
					"DatabaseName": {
						"type": "String",
						"defaultValue": "LogidataCC"
					},
					"UserName": {
						"type": "String",
						"defaultValue": "Synapse@citylogistics.co.za"
					},
					"SecretName": {
						"type": "String",
						"defaultValue": "LMSSecret"
					},
					"SchemaName": {
						"type": "String",
						"defaultValue": "dbo"
					},
					"ContainerName": {
						"type": "String",
						"defaultValue": "synapse"
					},
					"ASAEndpoint": {
						"type": "String",
						"defaultValue": "citylogistics-synapseanalytics-workspace-prod-ondemand.sql.azuresynapse.net"
					},
					"ASADataBase": {
						"type": "String",
						"defaultValue": "master"
					},
					"TempContainerName": {
						"type": "String",
						"defaultValue": "temp"
					}
				},
				"folder": {
					"name": "PROD/BackEnd/Extract/LMS"
				},
				"annotations": [],
				"lastPublishTime": "2022-04-04T06:38:01Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/IncrementalProdScript')]",
				"[concat(variables('workspaceId'), '/bigDataPools/TESTSparkPool')]",
				"[concat(variables('workspaceId'), '/datasets/SqlServerDataSet')]",
				"[concat(variables('workspaceId'), '/datasets/AzureSqlConnection')]",
				"[concat(variables('workspaceId'), '/datasets/DataLakeParquet')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LMSv2 Adhoc Runs')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Execute CLMasterData for Ad-Hoc LMSv2 - Full Take On",
						"type": "ExecutePipeline",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "CLMasterData LOAD PROD",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {
								"cw_items": {
									"value": "@pipeline().parameters.cw_items",
									"type": "Expression"
								},
								"TakeOnPeriod": " ",
								"Full_Load": true,
								"ColumnsNamesBlankSpaces": false,
								"LimitedLoadTopX": " "
							}
						}
					},
					{
						"name": "Run the LMSv2Recon notebook",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "Execute CLMasterData for Ad-Hoc LMSv2 - Full Take On",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "00_LMSv1v2Recon_RunOrder",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "PRDSparkPool",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": true,
								"spark.dynamicAllocation.minExecutors": 2,
								"spark.dynamicAllocation.maxExecutors": 4
							},
							"driverSize": "Small",
							"numExecutors": 2
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"parameters": {
					"cw_items": {
						"type": "array",
						"defaultValue": [
							{
								"Source": {
									"TableName": "manifest",
									"SchemaName": "v2"
								},
								"Increment": {
									"IncField": "lastupdate",
									"IdField": "id"
								},
								"Destination": {
									"FileName": "v2manifest"
								},
								"SparkConfig": {
									"NodeSize": "Small",
									"MinExecutors": "1",
									"MaxExecutors": "3"
								}
							},
							{
								"Source": {
									"TableName": "customerorder",
									"SchemaName": "v2"
								},
								"Increment": {
									"IncField": "lastupdate",
									"IdField": "id"
								},
								"Destination": {
									"FileName": "v2customerorder"
								},
								"SparkConfig": {
									"NodeSize": "Small",
									"MinExecutors": "1",
									"MaxExecutors": "3"
								}
							},
							{
								"Source": {
									"TableName": "consignment",
									"SchemaName": "v2"
								},
								"Increment": {
									"IncField": "lastupdate",
									"IdField": "id"
								},
								"Destination": {
									"FileName": "v2consignment"
								},
								"SparkConfig": {
									"NodeSize": "Small",
									"MinExecutors": "1",
									"MaxExecutors": "3"
								}
							},
							{
								"Source": {
									"TableName": "movement",
									"SchemaName": "v2"
								},
								"Increment": {
									"IncField": "lastupdate",
									"IdField": "id"
								},
								"Destination": {
									"FileName": "v2movement"
								},
								"SparkConfig": {
									"NodeSize": "Small",
									"MinExecutors": "1",
									"MaxExecutors": "3"
								}
							},
							{
								"Source": {
									"TableName": "track",
									"SchemaName": "v2"
								},
								"Increment": {
									"IncField": "lastupdate",
									"IdField": "id"
								},
								"Destination": {
									"FileName": "v2track"
								},
								"SparkConfig": {
									"NodeSize": "Small",
									"MinExecutors": "1",
									"MaxExecutors": "3"
								}
							},
							{
								"Source": {
									"TableName": "parcel",
									"SchemaName": "v2"
								},
								"Increment": {
									"IncField": "lastupdate",
									"IdField": "id"
								},
								"Destination": {
									"FileName": "v2parcel"
								},
								"SparkConfig": {
									"NodeSize": "Small",
									"MinExecutors": "1",
									"MaxExecutors": "3"
								}
							}
						]
					}
				},
				"folder": {
					"name": "Ad-Hoc"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/CLMasterData LOAD PROD')]",
				"[concat(variables('workspaceId'), '/notebooks/00_LMSv1v2Recon_RunOrder')]",
				"[concat(variables('workspaceId'), '/bigDataPools/PRDSparkPool')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ORV LOAD PROD')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Iterate Data",
						"type": "ForEach",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@pipeline().parameters.CW_Items",
								"type": "Expression"
							},
							"batchCount": 4,
							"activities": [
								{
									"name": "If ColumnsNamesBlankSpacesOrLimitedLoad",
									"type": "IfCondition",
									"dependsOn": [
										{
											"activity": "If FullLoad",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"expression": {
											"value": "@or(pipeline().parameters.ColumnsNamesBlankSpaces,greater(length(pipeline().parameters.LimitedLoadTopX),1))",
											"type": "Expression"
										},
										"ifTrueActivities": [
											{
												"name": "SelectStatementBuilder",
												"type": "Lookup",
												"dependsOn": [],
												"policy": {
													"timeout": "0.03:00:00",
													"retry": 3,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [],
												"typeProperties": {
													"source": {
														"type": "SqlServerSource",
														"sqlReaderQuery": {
															"value": "@concat(' \nDECLARE @s VARCHAR(MAX)\nDECLARE @tablename VARCHAR(MAX) = ''', item().Source.TableName,'''\nSELECT @s = ISNULL(@s + '', '','''') + ''['' + c.name + '']'' + '' as [''+replace(c.name,'' '',''_'')+''] ''\nFROM sys.all_columns c \njoin sys.objects t ON c.object_id = t.object_id \nWHERE t.name =  @tablename \nSELECT ''SELECT ', if(greater(length(pipeline().parameters.LimitedLoadTopX),0),concat('TOP ', pipeline().parameters.LimitedLoadTopX),''), ' '' + @s + '' FROM ['' +  @tablename + '']', if(greater(length(item().Increment.IdField),1),concat(' ORDER BY ', item().Increment.IdField, ' DESC'),''),''' as Query')",
															"type": "Expression"
														},
														"queryTimeout": "02:00:00",
														"partitionOption": "None"
													},
													"dataset": {
														"referenceName": "SqlServerDataSet",
														"type": "DatasetReference",
														"parameters": {
															"ServerName": {
																"value": "@variables('ServerName')",
																"type": "Expression"
															},
															"DatabaseName": {
																"value": "@variables('DatabaseName')",
																"type": "Expression"
															},
															"UserName": {
																"value": "@variables('UserName')",
																"type": "Expression"
															},
															"CW_SchemaName": {
																"value": "@variables('SchemaName')",
																"type": "Expression"
															},
															"SecretName": {
																"value": "@variables('SecretName')",
																"type": "Expression"
															},
															"CW_TableName": {
																"value": "@item().Source.TableName",
																"type": "Expression"
															}
														}
													}
												}
											}
										]
									}
								},
								{
									"name": "If FullLoad",
									"type": "IfCondition",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"expression": {
											"value": "@pipeline().parameters.Full_Load",
											"type": "Expression"
										},
										"ifFalseActivities": [
											{
												"name": "Lookup Last Max Field",
												"type": "Lookup",
												"dependsOn": [],
												"policy": {
													"timeout": "0.03:00:00",
													"retry": 0,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [],
												"typeProperties": {
													"source": {
														"type": "AzureSqlSource",
														"sqlReaderQuery": {
															"value": "@concat('SELECT MAX(',item().Increment.IncField,')  as Field \n FROM\n    OPENROWSET(\n        BULK ','''',variables('DataLake'),variables('ContainerName'),'/',variables('FolderName'),'/',item().destination.FileName,'.parquet','''',\n        ',FORMAT = ','''PARQUET''','\n    ) AS [result]')",
															"type": "Expression"
														},
														"queryTimeout": "02:00:00",
														"partitionOption": "None"
													},
													"dataset": {
														"referenceName": "AzureSqlConnection",
														"type": "DatasetReference",
														"parameters": {
															"ASAEndpoint": {
																"value": "@variables('ASAEndpoint')",
																"type": "Expression"
															},
															"ASADataBase": {
																"value": "@variables('ASADataBase')",
																"type": "Expression"
															}
														}
													}
												}
											},
											{
												"name": "Remove Old Data Incremental Data_SingleFile",
												"type": "Delete",
												"dependsOn": [
													{
														"activity": "Lookup Last Max Field",
														"dependencyConditions": [
															"Succeeded"
														]
													}
												],
												"policy": {
													"timeout": "0.03:00:00",
													"retry": 3,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [],
												"typeProperties": {
													"dataset": {
														"referenceName": "DataLakeParquet",
														"type": "DatasetReference",
														"parameters": {
															"CW_FolderName": {
																"value": "@concat(variables('FolderName'),'_Incremental_Data')",
																"type": "Expression"
															},
															"CW_FileName": {
																"value": "@concat(item().Destination.FileName,'.parquet')",
																"type": "Expression"
															},
															"ContainerName": {
																"value": "@variables('TempContainerName')",
																"type": "Expression"
															},
															"DataLake": {
																"value": "@variables('DataLake')",
																"type": "Expression"
															}
														}
													},
													"enableLogging": false,
													"storeSettings": {
														"type": "AzureBlobFSReadSettings",
														"recursive": true,
														"enablePartitionDiscovery": false
													}
												}
											}
										],
										"ifTrueActivities": [
											{
												"name": "Remove Old Data Data_Folder",
												"type": "Delete",
												"dependsOn": [
													{
														"activity": "Remove Old Data Data_SingleFile",
														"dependencyConditions": [
															"Failed"
														]
													}
												],
												"policy": {
													"timeout": "0.03:00:00",
													"retry": 0,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [],
												"typeProperties": {
													"dataset": {
														"referenceName": "DataLakeParquet",
														"type": "DatasetReference",
														"parameters": {
															"CW_FolderName": {
																"value": "@variables('FolderName')",
																"type": "Expression"
															},
															"CW_FileName": {
																"value": "@concat(item().Destination.FileName,'.parquet')",
																"type": "Expression"
															},
															"ContainerName": {
																"value": "@variables('TempContainerName')",
																"type": "Expression"
															},
															"DataLake": {
																"value": "@variables('DataLake')",
																"type": "Expression"
															}
														}
													},
													"enableLogging": false,
													"storeSettings": {
														"type": "AzureBlobFSReadSettings",
														"recursive": true,
														"wildcardFileName": {
															"value": "@{concat(variables('TempContainerName'),'/',variables('FolderName'),'/',item().Destination.FileName,'.parquet','/')}",
															"type": "Expression"
														},
														"enablePartitionDiscovery": false
													}
												}
											},
											{
												"name": "Remove Old Data Data_SingleFile",
												"type": "Delete",
												"dependsOn": [],
												"policy": {
													"timeout": "0.03:00:00",
													"retry": 0,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [],
												"typeProperties": {
													"dataset": {
														"referenceName": "DataLakeParquet",
														"type": "DatasetReference",
														"parameters": {
															"CW_FolderName": {
																"value": "@variables('FolderName')",
																"type": "Expression"
															},
															"CW_FileName": {
																"value": "@concat(item().Destination.FileName,'.parquet')",
																"type": "Expression"
															},
															"ContainerName": {
																"value": "@variables('TempContainerName')",
																"type": "Expression"
															},
															"DataLake": {
																"value": "@variables('DataLake')",
																"type": "Expression"
															}
														}
													},
													"enableLogging": false,
													"storeSettings": {
														"type": "AzureBlobFSReadSettings",
														"recursive": true,
														"enablePartitionDiscovery": false
													}
												}
											},
											{
												"name": "WaitBecause of Error",
												"type": "Wait",
												"dependsOn": [
													{
														"activity": "Remove Old Data Data_Folder",
														"dependencyConditions": [
															"Failed"
														]
													}
												],
												"userProperties": [],
												"typeProperties": {
													"waitTimeInSeconds": 1
												}
											}
										]
									}
								},
								{
									"name": "Switch1",
									"type": "Switch",
									"dependsOn": [
										{
											"activity": "If ColumnsNamesBlankSpacesOrLimitedLoad",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"on": {
											"value": "@if(pipeline().parameters.Full_Load,'FullTakeOn'\n   ,if(equals(item().Increment.IncField,item().Increment.IdField),'IdLoad'\n      ,'DateLoad'\n   )\n)",
											"type": "Expression"
										},
										"cases": [
											{
												"value": "DateLoad",
												"activities": [
													{
														"name": "Copy New incremental data",
														"type": "Copy",
														"dependsOn": [],
														"policy": {
															"timeout": "0.03:00:00",
															"retry": 3,
															"retryIntervalInSeconds": 30,
															"secureOutput": false,
															"secureInput": false
														},
														"userProperties": [],
														"typeProperties": {
															"source": {
																"type": "PostgreSqlSource",
																"query": {
																	"value": "@concat(if(or(pipeline().parameters.ColumnsNamesBlankSpaces, greater(length(pipeline().parameters.LimitedLoadTopX),1)),\n        activity('SelectStatementBuilder').output.firstRow.Query,concat('SELECT * FROM ', item().Source.TableName)\n        )\n    ,' WHERE ', item().Increment.IncField, ' > ',\n        if(greater(length(pipeline().parameters.TakeOnPeriod),1)\n        ,pipeline().parameters.TakeOnPeriod\n        ,concat('''',activity('Lookup Last Max Field').output.firstRow.Field,'''')\n        )\n    )",
																	"type": "Expression"
																}
															},
															"sink": {
																"type": "ParquetSink",
																"storeSettings": {
																	"type": "AzureBlobFSWriteSettings"
																},
																"formatSettings": {
																	"type": "ParquetWriteSettings"
																}
															},
															"enableStaging": false,
															"validateDataConsistency": false,
															"translator": {
																"type": "TabularTranslator",
																"typeConversion": true,
																"typeConversionSettings": {
																	"allowDataTruncation": true,
																	"treatBooleanAsNumber": false
																}
															}
														},
														"inputs": [
															{
																"referenceName": "PostgresSQL_Dataset",
																"type": "DatasetReference",
																"parameters": {
																	"ServerName": {
																		"value": "@variables('ServerName')",
																		"type": "Expression"
																	},
																	"DatabaseName": {
																		"value": "@variables('DatabaseName')",
																		"type": "Expression"
																	},
																	"UserName": {
																		"value": "@variables('UserName')",
																		"type": "Expression"
																	},
																	"CW_SchemaName": {
																		"value": "@variables('SchemaName')",
																		"type": "Expression"
																	},
																	"SecretName": {
																		"value": "@variables('SecretName')",
																		"type": "Expression"
																	},
																	"CW_TableName": {
																		"value": "@item().Source.TableName",
																		"type": "Expression"
																	},
																	"Port": {
																		"value": "@variables('Port')",
																		"type": "Expression"
																	}
																}
															}
														],
														"outputs": [
															{
																"referenceName": "DataLakeParquet",
																"type": "DatasetReference",
																"parameters": {
																	"CW_FolderName": {
																		"value": "@variables('FolderName')",
																		"type": "Expression"
																	},
																	"CW_FileName": {
																		"value": "@concat(item().Destination.FileName,'.parquet')",
																		"type": "Expression"
																	},
																	"ContainerName": {
																		"value": "@variables('TempContainerName')",
																		"type": "Expression"
																	},
																	"DataLake": {
																		"value": "@variables('DataLake')",
																		"type": "Expression"
																	}
																}
															}
														]
													}
												]
											},
											{
												"value": "IdLoad",
												"activities": [
													{
														"name": "Copy New incremental data_IDLoad",
														"type": "Copy",
														"dependsOn": [],
														"policy": {
															"timeout": "0.03:00:00",
															"retry": 3,
															"retryIntervalInSeconds": 30,
															"secureOutput": false,
															"secureInput": false
														},
														"userProperties": [],
														"typeProperties": {
															"source": {
																"type": "PostgreSqlSource",
																"query": {
																	"value": "@concat(if(or(pipeline().parameters.ColumnsNamesBlankSpaces,    greater(length(pipeline().parameters.LimitedLoadTopX),1)),\n        activity('SelectStatementBuilder').output.firstRow.Query, concat('SELECT * FROM ', item().Source.TableName)\n        )   \n    ,' WHERE ', item().Increment.IncField, ' > ',\n        if(greater(length(pipeline().parameters.TakeOnPeriod),1)\n        ,pipeline().parameters.TakeOnPeriod\n        ,activity('Lookup Last Max Field').output.firstRow.Field\n        )\n    )",
																	"type": "Expression"
																}
															},
															"sink": {
																"type": "ParquetSink",
																"storeSettings": {
																	"type": "AzureBlobFSWriteSettings"
																},
																"formatSettings": {
																	"type": "ParquetWriteSettings"
																}
															},
															"enableStaging": false,
															"validateDataConsistency": false,
															"translator": {
																"type": "TabularTranslator",
																"typeConversion": true,
																"typeConversionSettings": {
																	"allowDataTruncation": true,
																	"treatBooleanAsNumber": false
																}
															}
														},
														"inputs": [
															{
																"referenceName": "PostgresSQL_Dataset",
																"type": "DatasetReference",
																"parameters": {
																	"ServerName": {
																		"value": "@variables('ServerName')",
																		"type": "Expression"
																	},
																	"DatabaseName": {
																		"value": "@variables('DatabaseName')",
																		"type": "Expression"
																	},
																	"UserName": {
																		"value": "@variables('UserName')",
																		"type": "Expression"
																	},
																	"CW_SchemaName": {
																		"value": "@variables('SchemaName')",
																		"type": "Expression"
																	},
																	"SecretName": {
																		"value": "@variables('SecretName')",
																		"type": "Expression"
																	},
																	"CW_TableName": {
																		"value": "@item().Source.TableName",
																		"type": "Expression"
																	},
																	"Port": {
																		"value": "@variables('Port')",
																		"type": "Expression"
																	}
																}
															}
														],
														"outputs": [
															{
																"referenceName": "DataLakeParquet",
																"type": "DatasetReference",
																"parameters": {
																	"CW_FolderName": {
																		"value": "@variables('FolderName')",
																		"type": "Expression"
																	},
																	"CW_FileName": {
																		"value": "@concat(item().Destination.FileName,'.parquet')",
																		"type": "Expression"
																	},
																	"ContainerName": {
																		"value": "@variables('TempContainerName')",
																		"type": "Expression"
																	},
																	"DataLake": {
																		"value": "@variables('DataLake')",
																		"type": "Expression"
																	}
																}
															}
														]
													}
												]
											},
											{
												"value": "FullTakeOn",
												"activities": [
													{
														"name": "Copy_Data_Full_Take_On",
														"type": "Copy",
														"dependsOn": [],
														"policy": {
															"timeout": "7.00:00:00",
															"retry": 3,
															"retryIntervalInSeconds": 30,
															"secureOutput": false,
															"secureInput": false
														},
														"userProperties": [],
														"typeProperties": {
															"source": {
																"type": "PostgreSqlSource",
																"query": {
																	"value": "@if(or(pipeline().parameters.ColumnsNamesBlankSpaces, greater(length(pipeline().parameters.LimitedLoadTopX),1)),\n    activity('SelectStatementBuilder').output.firstRow.Query,\n    concat('SELECT * FROM ', item().Source.TableName)\n    )",
																	"type": "Expression"
																}
															},
															"sink": {
																"type": "ParquetSink",
																"storeSettings": {
																	"type": "AzureBlobFSWriteSettings"
																},
																"formatSettings": {
																	"type": "ParquetWriteSettings"
																}
															},
															"enableStaging": false,
															"validateDataConsistency": false,
															"translator": {
																"type": "TabularTranslator",
																"typeConversion": true,
																"typeConversionSettings": {
																	"allowDataTruncation": true,
																	"treatBooleanAsNumber": false
																}
															}
														},
														"inputs": [
															{
																"referenceName": "PostgresSQL_Dataset",
																"type": "DatasetReference",
																"parameters": {
																	"ServerName": {
																		"value": "@variables('ServerName')",
																		"type": "Expression"
																	},
																	"DatabaseName": {
																		"value": "@variables('DatabaseName')",
																		"type": "Expression"
																	},
																	"UserName": {
																		"value": "@variables('UserName')",
																		"type": "Expression"
																	},
																	"CW_SchemaName": {
																		"value": "@variables('SchemaName')",
																		"type": "Expression"
																	},
																	"SecretName": {
																		"value": "@variables('SecretName')",
																		"type": "Expression"
																	},
																	"CW_TableName": {
																		"value": "@item().Source.TableName",
																		"type": "Expression"
																	},
																	"Port": {
																		"value": "@variables('Port')",
																		"type": "Expression"
																	}
																}
															}
														],
														"outputs": [
															{
																"referenceName": "DataLakeParquet",
																"type": "DatasetReference",
																"parameters": {
																	"CW_FolderName": {
																		"value": "@variables('FolderName')",
																		"type": "Expression"
																	},
																	"CW_FileName": {
																		"value": "@concat(item().Destination.FileName,'.parquet')",
																		"type": "Expression"
																	},
																	"ContainerName": {
																		"value": "@variables('TempContainerName')",
																		"type": "Expression"
																	},
																	"DataLake": {
																		"value": "@variables('DataLake')",
																		"type": "Expression"
																	}
																}
															}
														]
													}
												]
											}
										],
										"defaultActivities": [
											{
												"name": "Fail1",
												"type": "Fail",
												"dependsOn": [],
												"userProperties": [],
												"typeProperties": {
													"message": "ERROR",
													"errorCode": "500"
												}
											}
										]
									}
								}
							]
						}
					},
					{
						"name": "ProdNotebook",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "Iterate Data",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "IncrementalProdScript",
								"type": "NotebookReference"
							},
							"parameters": {
								"cw_FolderName": {
									"value": {
										"value": "@variables('FolderName')",
										"type": "Expression"
									},
									"type": "string"
								},
								"ContainerName": {
									"value": {
										"value": "@variables('ContainerName')",
										"type": "Expression"
									},
									"type": "string"
								},
								"DataLakeDF": {
									"value": {
										"value": "@replace(variables('DataLake'),'https://','')",
										"type": "Expression"
									},
									"type": "string"
								},
								"Items": {
									"value": {
										"value": "@{pipeline().parameters.cw_items}",
										"type": "Expression"
									},
									"type": "string"
								},
								"FullLoad": {
									"value": {
										"value": "@pipeline().parameters.Full_Load",
										"type": "Expression"
									},
									"type": "string"
								},
								"TempContainerName": {
									"value": {
										"value": "@variables('TempContainerName')",
										"type": "Expression"
									},
									"type": "string"
								}
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "TESTSparkPool",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": true,
								"spark.dynamicAllocation.minExecutors": 1,
								"spark.dynamicAllocation.maxExecutors": 4
							},
							"driverSize": "Small",
							"numExecutors": 1
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"parameters": {
					"cw_items": {
						"type": "Array",
						"defaultValue": []
					},
					"TakeOnPeriod": {
						"type": "string",
						"defaultValue": " "
					},
					"Full_Load": {
						"type": "bool",
						"defaultValue": false
					},
					"ColumnsNamesBlankSpaces": {
						"type": "bool",
						"defaultValue": false
					},
					"LimitedLoadTopX": {
						"type": "string",
						"defaultValue": " "
					}
				},
				"variables": {
					"FolderName": {
						"type": "String",
						"defaultValue": "Unstructured Data/Onroute"
					},
					"DataLake": {
						"type": "String",
						"defaultValue": "https://citylogisticsstorageprod.dfs.core.windows.net/"
					},
					"ServerName": {
						"type": "String",
						"defaultValue": "orvdb.citylogistics.co.za"
					},
					"DatabaseName": {
						"type": "String",
						"defaultValue": "orv"
					},
					"UserName": {
						"type": "String",
						"defaultValue": "synapse"
					},
					"SecretName": {
						"type": "String",
						"defaultValue": "ORVSecret"
					},
					"SchemaName": {
						"type": "String",
						"defaultValue": "public"
					},
					"ContainerName": {
						"type": "String",
						"defaultValue": "synapse"
					},
					"ASAEndpoint": {
						"type": "String",
						"defaultValue": "citylogistics-synapseanalytics-workspace-prod-ondemand.sql.azuresynapse.net"
					},
					"ASADataBase": {
						"type": "String",
						"defaultValue": "master"
					},
					"Integration Runtime": {
						"type": "String",
						"defaultValue": "DataFlowIR"
					},
					"Port": {
						"type": "String",
						"defaultValue": "5432"
					},
					"TempContainerName": {
						"type": "String",
						"defaultValue": "temp"
					}
				},
				"folder": {
					"name": "PROD/BackEnd/Extract/ORV"
				},
				"annotations": [],
				"lastPublishTime": "2022-04-04T06:22:12Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/IncrementalProdScript')]",
				"[concat(variables('workspaceId'), '/bigDataPools/TESTSparkPool')]",
				"[concat(variables('workspaceId'), '/datasets/SqlServerDataSet')]",
				"[concat(variables('workspaceId'), '/datasets/AzureSqlConnection')]",
				"[concat(variables('workspaceId'), '/datasets/DataLakeParquet')]",
				"[concat(variables('workspaceId'), '/datasets/PostgresSQL_Dataset')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/PBI_Dataflows')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "ForEach1",
						"type": "ForEach",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@pipeline().parameters.URL_Data",
								"type": "Expression"
							},
							"isSequential": false,
							"activities": [
								{
									"name": "Execute Pipeline1",
									"type": "ExecutePipeline",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"pipeline": {
											"referenceName": "Refresh_GetStatus",
											"type": "PipelineReference"
										},
										"waitOnCompletion": true,
										"parameters": {
											"GroupID": {
												"value": "@item().GroupID",
												"type": "Expression"
											},
											"DataflowID": {
												"value": "@item().DataflowID",
												"type": "Expression"
											}
										}
									}
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"parameters": {
					"URL_Data": {
						"type": "array",
						"defaultValue": [
							{
								"GroupID": "2a5062fe-1c10-4c36-91df-40ffcbc2b229",
								"DataflowID": "97c44a33-8f82-4fb6-a11f-5e6cea20d708"
							}
						]
					},
					"LMS_Refresh": {
						"type": "string",
						"defaultValue": [
							{
								"GroupID": "639fb4f4-960f-4f63-a713-5d578c6955ef",
								"DataflowID": "8a06d1fa-609f-4cc3-b0af-72e7964dded0"
							}
						]
					}
				},
				"folder": {
					"name": "PowerBIRefresh"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/Refresh_GetStatus')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Refresh_GetStatus')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Web1",
						"type": "WebActivity",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"url": {
								"value": "@concat('https://api.powerbi.com/v1.0/myorg/groups/', pipeline().parameters.GroupID,'/datasets/',pipeline().parameters.DataflowID,'/refreshes')",
								"type": "Expression"
							},
							"connectVia": {
								"referenceName": "AutoResolveIntegrationRuntime",
								"type": "IntegrationRuntimeReference"
							},
							"method": "POST",
							"headers": {},
							"body": "\"notifyOption\":\"False\"",
							"authentication": {
								"type": "MSI",
								"resource": "https://analysis.windows.net/powerbi/api"
							}
						}
					},
					{
						"name": "Until Refresh Completion DC",
						"type": "Until",
						"dependsOn": [
							{
								"activity": "Web1",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"expression": {
								"value": "@or(equals(activity('Get Power BI Dataflow status DC 1').output.value[0].status,'Completed'),\nequals(activity('Get Power BI Dataflow status DC 1').output.value[0].status,'Failed')\n)",
								"type": "Expression"
							},
							"activities": [
								{
									"name": "Wait 30s",
									"type": "Wait",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"waitTimeInSeconds": 30
									}
								},
								{
									"name": "Get Power BI Dataflow status DC 1",
									"type": "WebActivity",
									"dependsOn": [
										{
											"activity": "Wait 30s",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"policy": {
										"timeout": "0.3:00:00",
										"retry": 3,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"url": {
											"value": "@concat('https://api.powerbi.com/v1.0/myorg/groups/', pipeline().parameters.GroupID,'/datasets/',pipeline().parameters.DataflowID,'/refreshes?$top=1')",
											"type": "Expression"
										},
										"connectVia": {
											"referenceName": "AutoResolveIntegrationRuntime",
											"type": "IntegrationRuntimeReference"
										},
										"method": "GET",
										"headers": {},
										"authentication": {
											"type": "MSI",
											"resource": "https://analysis.windows.net/powerbi/api"
										}
									}
								}
							],
							"timeout": "0.12:00:00"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"parameters": {
					"GroupID": {
						"type": "string",
						"defaultValue": "@item().GroupID"
					},
					"DataflowID": {
						"type": "string",
						"defaultValue": "@item().DataflowID"
					}
				},
				"folder": {
					"name": "PowerBIRefresh"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/RunOrder')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "SST_RunOrder",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "DeDupe BillConsignmentR",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "SST_RunOrder",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "PRDSparkPool",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Medium",
							"conf": {
								"spark.dynamicAllocation.enabled": true,
								"spark.dynamicAllocation.minExecutors": 4,
								"spark.dynamicAllocation.maxExecutors": 6
							},
							"driverSize": "Medium",
							"numExecutors": 4
						}
					},
					{
						"name": "STMD_RunOrder",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "SST_RunOrder",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "STMD_RunOrder",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "PRDSparkPool",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Medium",
							"conf": {
								"spark.dynamicAllocation.enabled": true,
								"spark.dynamicAllocation.minExecutors": 2,
								"spark.dynamicAllocation.maxExecutors": 4
							},
							"driverSize": "Medium",
							"numExecutors": 2
						}
					},
					{
						"name": "STOPS_RunOrderDistribution",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "STMD_RunOrder",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "STOPS_RunOrderDistribution",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "TESTSparkPoolL",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Large",
							"conf": {
								"spark.dynamicAllocation.enabled": true,
								"spark.dynamicAllocation.minExecutors": 1,
								"spark.dynamicAllocation.maxExecutors": 6
							},
							"driverSize": "Large",
							"numExecutors": 1
						}
					},
					{
						"name": "STOPS_RunOrderLinehaul",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "STMD_RunOrder",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "STOPS_RunOrderLinehaul",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "PRDSparkPool",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Medium",
							"conf": {
								"spark.dynamicAllocation.enabled": true,
								"spark.dynamicAllocation.minExecutors": 2,
								"spark.dynamicAllocation.maxExecutors": 2
							},
							"driverSize": "Medium",
							"numExecutors": 2
						}
					},
					{
						"name": "STSAP_RunOrder",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "STOPS_RunOrderLinehaul",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "STOPS_RunOrderDistribution",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "00_STSAP_RunOrder",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "PRDSparkPool",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Medium",
							"conf": {
								"spark.dynamicAllocation.enabled": true,
								"spark.dynamicAllocation.minExecutors": 2,
								"spark.dynamicAllocation.maxExecutors": 3
							},
							"driverSize": "Medium",
							"numExecutors": 2
						}
					},
					{
						"name": "DeDupe BillConsignmentR",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "Dedupe on bill_consignmentr",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "PRDSparkPool",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": false,
								"spark.dynamicAllocation.minExecutors": 2,
								"spark.dynamicAllocation.maxExecutors": 2
							},
							"driverSize": "Small",
							"numExecutors": 2
						}
					},
					{
						"name": "STRPT_RunOrder",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "STOPS_RunOrderLinehaul",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "STOPS_RunOrderDistribution",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "00_ReportsRunOrder",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "PRDSparkPool",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Medium",
							"conf": {
								"spark.dynamicAllocation.enabled": true,
								"spark.dynamicAllocation.minExecutors": 2,
								"spark.dynamicAllocation.maxExecutors": 3
							},
							"driverSize": "Medium",
							"numExecutors": 2
						}
					},
					{
						"name": "LMSv1v2Recon_RunOrder",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "STOPS_RunOrderLinehaul",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "00_LMSv1v2Recon_RunOrder",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "PRDSparkPool",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Medium",
							"conf": {
								"spark.dynamicAllocation.enabled": true,
								"spark.dynamicAllocation.minExecutors": 2,
								"spark.dynamicAllocation.maxExecutors": 3
							},
							"driverSize": "Medium",
							"numExecutors": 2
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"folder": {
					"name": "PROD/SoRs"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/SST_RunOrder')]",
				"[concat(variables('workspaceId'), '/bigDataPools/PRDSparkPool')]",
				"[concat(variables('workspaceId'), '/notebooks/STMD_RunOrder')]",
				"[concat(variables('workspaceId'), '/notebooks/STOPS_RunOrderDistribution')]",
				"[concat(variables('workspaceId'), '/bigDataPools/TESTSparkPoolL')]",
				"[concat(variables('workspaceId'), '/notebooks/STOPS_RunOrderLinehaul')]",
				"[concat(variables('workspaceId'), '/notebooks/00_STSAP_RunOrder')]",
				"[concat(variables('workspaceId'), '/notebooks/Dedupe on bill_consignmentr')]",
				"[concat(variables('workspaceId'), '/notebooks/00_ReportsRunOrder')]",
				"[concat(variables('workspaceId'), '/notebooks/00_LMSv1v2Recon_RunOrder')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SAP LOAD PROD')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Iterate Data",
						"type": "ForEach",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@pipeline().parameters.CW_Items",
								"type": "Expression"
							},
							"batchCount": 10,
							"activities": [
								{
									"name": "If ColumnsNamesBlankSpacesOrLimitedLoad",
									"type": "IfCondition",
									"dependsOn": [
										{
											"activity": "If FullLoad",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"expression": {
											"value": "@or(pipeline().parameters.ColumnsNamesBlankSpaces,greater(length(pipeline().parameters.LimitedLoadTopX),1))",
											"type": "Expression"
										},
										"ifTrueActivities": [
											{
												"name": "SelectStatementBuilder",
												"type": "Lookup",
												"dependsOn": [],
												"policy": {
													"timeout": "0.03:00:00",
													"retry": 3,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [],
												"typeProperties": {
													"source": {
														"type": "SqlServerSource",
														"sqlReaderQuery": {
															"value": "@concat(' \nDECLARE @s VARCHAR(MAX)\nDECLARE @tablename VARCHAR(MAX) = ''', item().Source.TableName,'''\nSELECT @s = ISNULL(@s + '', '','''') + ''['' + c.name + '']'' + '' as [''+replace(c.name,'' '',''_'')+''] ''\nFROM sys.all_columns c \njoin sys.objects t ON c.object_id = t.object_id \nWHERE t.name =  @tablename \nSELECT ''SELECT ', if(greater(length(pipeline().parameters.LimitedLoadTopX),1),concat('TOP ', pipeline().parameters.LimitedLoadTopX),''), ' '' + @s + '' FROM ['' +  @tablename + '']', if(greater(length(pipeline().parameters.LimitedLoadTopX),1),if(greater(length(item().Increment.IdField),1),concat(' ORDER BY ', item().Increment.IdField, ' DESC'),''),''),''' as Query')",
															"type": "Expression"
														},
														"queryTimeout": "02:00:00",
														"partitionOption": "None"
													},
													"dataset": {
														"referenceName": "SqlServerDataSet",
														"type": "DatasetReference",
														"parameters": {
															"ServerName": {
																"value": "@variables('ServerName')",
																"type": "Expression"
															},
															"DatabaseName": {
																"value": "@variables('DatabaseName')",
																"type": "Expression"
															},
															"UserName": {
																"value": "@variables('UserName')",
																"type": "Expression"
															},
															"CW_SchemaName": {
																"value": "@variables('SchemaName')",
																"type": "Expression"
															},
															"SecretName": {
																"value": "@variables('SecretName')",
																"type": "Expression"
															},
															"CW_TableName": {
																"value": "@item().Source.TableName",
																"type": "Expression"
															}
														}
													}
												}
											}
										]
									}
								},
								{
									"name": "If FullLoad",
									"type": "IfCondition",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"expression": {
											"value": "@pipeline().parameters.Full_Load",
											"type": "Expression"
										},
										"ifFalseActivities": [
											{
												"name": "Lookup Last Max Field",
												"type": "Lookup",
												"dependsOn": [],
												"policy": {
													"timeout": "0.03:00:00",
													"retry": 0,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [],
												"typeProperties": {
													"source": {
														"type": "AzureSqlSource",
														"sqlReaderQuery": {
															"value": "@concat('SELECT MAX(',item().Increment.IncField,')  as Field \n FROM\n    OPENROWSET(\n        BULK ','''',variables('DataLake'),variables('ContainerName'),'/',variables('FolderName'),'/',item().destination.FileName,'.parquet','''',\n        ',FORMAT = ','''PARQUET''','\n    ) AS [result]')",
															"type": "Expression"
														},
														"queryTimeout": "02:00:00",
														"partitionOption": "None"
													},
													"dataset": {
														"referenceName": "AzureSqlConnection",
														"type": "DatasetReference",
														"parameters": {
															"ASAEndpoint": {
																"value": "@variables('ASAEndpoint')",
																"type": "Expression"
															},
															"ASADataBase": {
																"value": "@variables('ASADataBase')",
																"type": "Expression"
															}
														}
													}
												}
											},
											{
												"name": "Remove Old Data Incremental Data_SingleFile",
												"type": "Delete",
												"dependsOn": [
													{
														"activity": "Lookup Last Max Field",
														"dependencyConditions": [
															"Succeeded"
														]
													}
												],
												"policy": {
													"timeout": "0.03:00:00",
													"retry": 3,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [],
												"typeProperties": {
													"dataset": {
														"referenceName": "DataLakeParquet",
														"type": "DatasetReference",
														"parameters": {
															"CW_FolderName": {
																"value": "@concat(variables('FolderName'),'_Incremental_Data')",
																"type": "Expression"
															},
															"CW_FileName": {
																"value": "@concat(item().Destination.FileName,'.parquet')",
																"type": "Expression"
															},
															"ContainerName": {
																"value": "@variables('TempContainerName')",
																"type": "Expression"
															},
															"DataLake": {
																"value": "@variables('DataLake')",
																"type": "Expression"
															}
														}
													},
													"enableLogging": false,
													"storeSettings": {
														"type": "AzureBlobFSReadSettings",
														"recursive": true,
														"enablePartitionDiscovery": false
													}
												}
											}
										],
										"ifTrueActivities": [
											{
												"name": "Remove Old Data Data_Folder",
												"type": "Delete",
												"dependsOn": [
													{
														"activity": "Remove Old Data Data_SingleFile",
														"dependencyConditions": [
															"Failed"
														]
													}
												],
												"policy": {
													"timeout": "0.03:00:00",
													"retry": 0,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [],
												"typeProperties": {
													"dataset": {
														"referenceName": "DataLakeParquet",
														"type": "DatasetReference",
														"parameters": {
															"CW_FolderName": {
																"value": "@variables('FolderName')",
																"type": "Expression"
															},
															"CW_FileName": {
																"value": "@concat(item().Destination.FileName,'.parquet')",
																"type": "Expression"
															},
															"ContainerName": {
																"value": "@variables('TempContainerName')",
																"type": "Expression"
															},
															"DataLake": {
																"value": "@variables('DataLake')",
																"type": "Expression"
															}
														}
													},
													"enableLogging": false,
													"storeSettings": {
														"type": "AzureBlobFSReadSettings",
														"recursive": true,
														"wildcardFileName": {
															"value": "@{concat(variables('TempContainerName'),'/',variables('FolderName'),'/',item().Destination.FileName,'.parquet','/')}",
															"type": "Expression"
														},
														"enablePartitionDiscovery": false
													}
												}
											},
											{
												"name": "Remove Old Data Data_SingleFile",
												"type": "Delete",
												"dependsOn": [],
												"policy": {
													"timeout": "0.03:00:00",
													"retry": 0,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [],
												"typeProperties": {
													"dataset": {
														"referenceName": "DataLakeParquet",
														"type": "DatasetReference",
														"parameters": {
															"CW_FolderName": {
																"value": "@variables('FolderName')",
																"type": "Expression"
															},
															"CW_FileName": {
																"value": "@concat(item().Destination.FileName,'.parquet')",
																"type": "Expression"
															},
															"ContainerName": {
																"value": "@variables('TempContainerName')",
																"type": "Expression"
															},
															"DataLake": {
																"value": "@variables('DataLake')",
																"type": "Expression"
															}
														}
													},
													"enableLogging": false,
													"storeSettings": {
														"type": "AzureBlobFSReadSettings",
														"recursive": true,
														"enablePartitionDiscovery": false
													}
												}
											},
											{
												"name": "WaitBecause of Error",
												"type": "Wait",
												"dependsOn": [
													{
														"activity": "Remove Old Data Data_Folder",
														"dependencyConditions": [
															"Failed"
														]
													}
												],
												"userProperties": [],
												"typeProperties": {
													"waitTimeInSeconds": 1
												}
											}
										]
									}
								},
								{
									"name": "Switch1",
									"type": "Switch",
									"dependsOn": [
										{
											"activity": "If ColumnsNamesBlankSpacesOrLimitedLoad",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"on": {
											"value": "@if(pipeline().parameters.Full_Load,'FullTakeOn'\n   ,if(equals(item().Increment.IncField,item().Increment.IdField),'IdLoad'\n      ,'DateLoad'\n   )\n)",
											"type": "Expression"
										},
										"cases": [
											{
												"value": "DateLoad",
												"activities": [
													{
														"name": "Copy New incremental data",
														"type": "Copy",
														"dependsOn": [],
														"policy": {
															"timeout": "0.03:00:00",
															"retry": 3,
															"retryIntervalInSeconds": 30,
															"secureOutput": false,
															"secureInput": false
														},
														"userProperties": [],
														"typeProperties": {
															"source": {
																"type": "SqlServerSource",
																"sqlReaderQuery": {
																	"value": "@concat(if(or(pipeline().parameters.ColumnsNamesBlankSpaces, greater(length(pipeline().parameters.LimitedLoadTopX),1)),\n        activity('SelectStatementBuilder').output.firstRow.Query,concat('SELECT * FROM [', item().Source.TableName,']')\n        )\n    ,' WHERE ', item().Increment.IncField, ' > ',\n        if(greater(length(pipeline().parameters.TakeOnPeriod),1)\n        ,pipeline().parameters.TakeOnPeriod\n        ,concat('''',activity('Lookup Last Max Field').output.firstRow.Field,'''')\n        )\n    )",
																	"type": "Expression"
																},
																"queryTimeout": "02:00:00",
																"partitionOption": "None"
															},
															"sink": {
																"type": "ParquetSink",
																"storeSettings": {
																	"type": "AzureBlobFSWriteSettings"
																},
																"formatSettings": {
																	"type": "ParquetWriteSettings"
																}
															},
															"enableStaging": false,
															"validateDataConsistency": false,
															"translator": {
																"type": "TabularTranslator",
																"typeConversion": true,
																"typeConversionSettings": {
																	"allowDataTruncation": true,
																	"treatBooleanAsNumber": false
																}
															}
														},
														"inputs": [
															{
																"referenceName": "SqlServerDataSet",
																"type": "DatasetReference",
																"parameters": {
																	"ServerName": {
																		"value": "@variables('ServerName')",
																		"type": "Expression"
																	},
																	"DatabaseName": {
																		"value": "@variables('DatabaseName')",
																		"type": "Expression"
																	},
																	"UserName": {
																		"value": "@variables('UserName')",
																		"type": "Expression"
																	},
																	"CW_SchemaName": {
																		"value": "@variables('SchemaName')",
																		"type": "Expression"
																	},
																	"SecretName": {
																		"value": "@variables('SecretName')",
																		"type": "Expression"
																	},
																	"CW_TableName": {
																		"value": "@item().Source.TableName",
																		"type": "Expression"
																	}
																}
															}
														],
														"outputs": [
															{
																"referenceName": "DataLakeParquet",
																"type": "DatasetReference",
																"parameters": {
																	"CW_FolderName": {
																		"value": "@variables('FolderName')",
																		"type": "Expression"
																	},
																	"CW_FileName": {
																		"value": "@concat(item().Destination.FileName,'.parquet')",
																		"type": "Expression"
																	},
																	"ContainerName": {
																		"value": "@variables('TempContainerName')",
																		"type": "Expression"
																	},
																	"DataLake": {
																		"value": "@variables('DataLake')",
																		"type": "Expression"
																	}
																}
															}
														]
													}
												]
											},
											{
												"value": "IdLoad",
												"activities": [
													{
														"name": "Copy New incremental data_IDLoad",
														"type": "Copy",
														"dependsOn": [],
														"policy": {
															"timeout": "0.03:00:00",
															"retry": 3,
															"retryIntervalInSeconds": 30,
															"secureOutput": false,
															"secureInput": false
														},
														"userProperties": [],
														"typeProperties": {
															"source": {
																"type": "SqlServerSource",
																"sqlReaderQuery": {
																	"value": "@concat(if(or(pipeline().parameters.ColumnsNamesBlankSpaces,    greater(length(pipeline().parameters.LimitedLoadTopX),1)),\n        activity('SelectStatementBuilder').output.firstRow.Query, concat('SELECT * FROM [', item().Source.TableName,']')\n        )   \n    ,' WHERE ', item().Increment.IncField, ' > ',\n        if(greater(length(pipeline().parameters.TakeOnPeriod),1)\n        ,pipeline().parameters.TakeOnPeriod\n        ,activity('Lookup Last Max Field').output.firstRow.Field\n        )\n    )",
																	"type": "Expression"
																},
																"queryTimeout": "02:00:00",
																"partitionOption": "None"
															},
															"sink": {
																"type": "ParquetSink",
																"storeSettings": {
																	"type": "AzureBlobFSWriteSettings"
																},
																"formatSettings": {
																	"type": "ParquetWriteSettings"
																}
															},
															"enableStaging": false,
															"validateDataConsistency": false,
															"translator": {
																"type": "TabularTranslator",
																"typeConversion": true,
																"typeConversionSettings": {
																	"allowDataTruncation": true,
																	"treatBooleanAsNumber": false
																}
															}
														},
														"inputs": [
															{
																"referenceName": "SqlServerDataSet",
																"type": "DatasetReference",
																"parameters": {
																	"ServerName": {
																		"value": "@variables('ServerName')",
																		"type": "Expression"
																	},
																	"DatabaseName": {
																		"value": "@variables('DatabaseName')",
																		"type": "Expression"
																	},
																	"UserName": {
																		"value": "@variables('UserName')",
																		"type": "Expression"
																	},
																	"CW_SchemaName": {
																		"value": "@variables('SchemaName')",
																		"type": "Expression"
																	},
																	"SecretName": {
																		"value": "@variables('SecretName')",
																		"type": "Expression"
																	},
																	"CW_TableName": {
																		"value": "@item().Source.TableName",
																		"type": "Expression"
																	}
																}
															}
														],
														"outputs": [
															{
																"referenceName": "DataLakeParquet",
																"type": "DatasetReference",
																"parameters": {
																	"CW_FolderName": {
																		"value": "@variables('FolderName')",
																		"type": "Expression"
																	},
																	"CW_FileName": {
																		"value": "@concat(item().Destination.FileName,'.parquet')",
																		"type": "Expression"
																	},
																	"ContainerName": {
																		"value": "@variables('TempContainerName')",
																		"type": "Expression"
																	},
																	"DataLake": {
																		"value": "@variables('DataLake')",
																		"type": "Expression"
																	}
																}
															}
														]
													}
												]
											},
											{
												"value": "FullTakeOn",
												"activities": [
													{
														"name": "Copy_Data_Full_Take_On",
														"type": "Copy",
														"dependsOn": [],
														"policy": {
															"timeout": "7.00:00:00",
															"retry": 3,
															"retryIntervalInSeconds": 30,
															"secureOutput": false,
															"secureInput": false
														},
														"userProperties": [],
														"typeProperties": {
															"source": {
																"type": "SqlServerSource",
																"sqlReaderQuery": {
																	"value": "@if(or(pipeline().parameters.ColumnsNamesBlankSpaces, greater(length(pipeline().parameters.LimitedLoadTopX),1)),\n    activity('SelectStatementBuilder').output.firstRow.Query,\n    concat('SELECT * FROM [', item().Source.TableName,']')\n    )",
																	"type": "Expression"
																},
																"partitionOption": "None"
															},
															"sink": {
																"type": "ParquetSink",
																"storeSettings": {
																	"type": "AzureBlobFSWriteSettings"
																},
																"formatSettings": {
																	"type": "ParquetWriteSettings"
																}
															},
															"enableStaging": false,
															"validateDataConsistency": false,
															"translator": {
																"type": "TabularTranslator",
																"typeConversion": true,
																"typeConversionSettings": {
																	"allowDataTruncation": true,
																	"treatBooleanAsNumber": false
																}
															}
														},
														"inputs": [
															{
																"referenceName": "SqlServerDataSet",
																"type": "DatasetReference",
																"parameters": {
																	"ServerName": {
																		"value": "@variables('ServerName')",
																		"type": "Expression"
																	},
																	"DatabaseName": {
																		"value": "@variables('DatabaseName')",
																		"type": "Expression"
																	},
																	"UserName": {
																		"value": "@variables('UserName')",
																		"type": "Expression"
																	},
																	"CW_SchemaName": {
																		"value": "@variables('SchemaName')",
																		"type": "Expression"
																	},
																	"SecretName": {
																		"value": "@variables('SecretName')",
																		"type": "Expression"
																	},
																	"CW_TableName": {
																		"value": "@item().Source.TableName",
																		"type": "Expression"
																	}
																}
															}
														],
														"outputs": [
															{
																"referenceName": "DataLakeParquet",
																"type": "DatasetReference",
																"parameters": {
																	"CW_FolderName": {
																		"value": "@variables('FolderName')",
																		"type": "Expression"
																	},
																	"CW_FileName": {
																		"value": "@concat(item().Destination.FileName,'.parquet')",
																		"type": "Expression"
																	},
																	"ContainerName": {
																		"value": "@variables('TempContainerName')",
																		"type": "Expression"
																	},
																	"DataLake": {
																		"value": "@variables('DataLake')",
																		"type": "Expression"
																	}
																}
															}
														]
													}
												]
											}
										],
										"defaultActivities": [
											{
												"name": "Fail1",
												"type": "Fail",
												"dependsOn": [],
												"userProperties": [],
												"typeProperties": {
													"message": "ERROR",
													"errorCode": "500"
												}
											}
										]
									}
								}
							]
						}
					},
					{
						"name": "ProdNotebook",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "Iterate Data",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "IncrementalProdScript",
								"type": "NotebookReference"
							},
							"parameters": {
								"cw_FolderName": {
									"value": {
										"value": "@variables('FolderName')",
										"type": "Expression"
									},
									"type": "string"
								},
								"ContainerName": {
									"value": {
										"value": "@variables('ContainerName')",
										"type": "Expression"
									},
									"type": "string"
								},
								"DataLakeDF": {
									"value": {
										"value": "@replace(variables('DataLake'),'https://','')",
										"type": "Expression"
									},
									"type": "string"
								},
								"Items": {
									"value": {
										"value": "@{pipeline().parameters.cw_items}",
										"type": "Expression"
									},
									"type": "string"
								},
								"FullLoad": {
									"value": {
										"value": "@pipeline().parameters.Full_Load",
										"type": "Expression"
									},
									"type": "string"
								},
								"TempContainerName": {
									"value": {
										"value": "@variables('TempContainerName')",
										"type": "Expression"
									},
									"type": "string"
								}
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "TESTSparkPool",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": true,
								"spark.dynamicAllocation.minExecutors": 1,
								"spark.dynamicAllocation.maxExecutors": 4
							},
							"driverSize": "Small",
							"numExecutors": 1
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"parameters": {
					"CW_Items": {
						"type": "Array",
						"defaultValue": []
					},
					"TakeOnPeriod": {
						"type": "string",
						"defaultValue": " "
					},
					"Full_Load": {
						"type": "bool",
						"defaultValue": false
					},
					"ColumnsNamesBlankSpaces": {
						"type": "bool",
						"defaultValue": true
					},
					"LimitedLoadTopX": {
						"type": "string",
						"defaultValue": " "
					}
				},
				"variables": {
					"FolderName": {
						"type": "String",
						"defaultValue": "Unstructured Data/SAP"
					},
					"DataLake": {
						"type": "String",
						"defaultValue": "https://citylogisticsstorageprod.dfs.core.windows.net/"
					},
					"ServerName": {
						"type": "String",
						"defaultValue": "CCSQL01"
					},
					"DatabaseName": {
						"type": "String",
						"defaultValue": "SBK_CityLogistics "
					},
					"UserName": {
						"type": "String",
						"defaultValue": "Synapse@citylogistics.co.za"
					},
					"SecretName": {
						"type": "String",
						"defaultValue": "SAPSecret"
					},
					"SchemaName": {
						"type": "String",
						"defaultValue": "dbo"
					},
					"ContainerName": {
						"type": "String",
						"defaultValue": "synapse"
					},
					"ASAEndpoint": {
						"type": "String",
						"defaultValue": "citylogistics-synapseanalytics-workspace-prod-ondemand.sql.azuresynapse.net"
					},
					"ASADataBase": {
						"type": "String",
						"defaultValue": "master"
					},
					"TempContainerName": {
						"type": "String",
						"defaultValue": "temp"
					}
				},
				"folder": {
					"name": "PROD/BackEnd/Extract/SAP"
				},
				"annotations": [],
				"lastPublishTime": "2022-04-04T06:38:01Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/IncrementalProdScript')]",
				"[concat(variables('workspaceId'), '/bigDataPools/TESTSparkPool')]",
				"[concat(variables('workspaceId'), '/datasets/SqlServerDataSet')]",
				"[concat(variables('workspaceId'), '/datasets/AzureSqlConnection')]",
				"[concat(variables('workspaceId'), '/datasets/DataLakeParquet')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Sharepoint Full Take-on')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Get Bearer Token",
						"type": "WebActivity",
						"dependsOn": [],
						"policy": {
							"timeout": "0.03:00:00",
							"retry": 1,
							"retryIntervalInSeconds": 30,
							"secureOutput": true,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"url": {
								"value": "@concat('https://accounts.accesscontrol.windows.net/',variables('TenantID'),'/tokens/OAuth/2')\n",
								"type": "Expression"
							},
							"connectVia": {
								"referenceName": "AutoResolveIntegrationRuntime",
								"type": "IntegrationRuntimeReference"
							},
							"method": "POST",
							"headers": {
								"Content-Type": "application/x-www-form-urlencoded"
							},
							"body": {
								"value": "@concat('grant_type=client_credentials&client_id=',variables('client_id'),'@',variables('TenantID'),'&client_secret=',variables('client_secret'),'&resource=00000003-0000-0ff1-ce00-000000000000','/',variables('BaseURL'),'@',variables('TenantID'))",
								"type": "Expression"
							}
						}
					},
					{
						"name": "ForEach1",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "Get Bearer Token",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@pipeline().parameters.CW_Items",
								"type": "Expression"
							},
							"activities": [
								{
									"name": "Set variable SourceFolder",
									"type": "SetVariable",
									"dependsOn": [],
									"policy": {
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"variableName": "SharePointFolderName",
										"value": {
											"value": "@item().Source.TableName",
											"type": "Expression"
										}
									}
								},
								{
									"name": "Set variable DestFolder",
									"type": "SetVariable",
									"dependsOn": [],
									"policy": {
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"variableName": "DestFolderName",
										"value": {
											"value": "@item().Destination.FileName",
											"type": "Expression"
										}
									}
								},
								{
									"name": "Set variable SiteURL",
									"type": "SetVariable",
									"dependsOn": [],
									"policy": {
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"variableName": "SiteURL",
										"value": {
											"value": "@concat('sites/',pipeline().parameters.SharepointSiteName,'/')",
											"type": "Expression"
										}
									}
								}
							]
						}
					},
					{
						"name": "Get All the Files_copy1",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "Get List of File Names",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('Get List of File Names').output.value",
								"type": "Expression"
							},
							"activities": [
								{
									"name": "Copy data Folder 1_copy1",
									"type": "Copy",
									"dependsOn": [],
									"policy": {
										"timeout": "0.03:00:00",
										"retry": 1,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": true
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "BinarySource",
											"storeSettings": {
												"type": "HttpReadSettings",
												"requestMethod": "GET",
												"additionalHeaders": {
													"value": "@{concat('Authorization: Bearer ', activity('Get Bearer Token').output.access_token)}",
													"type": "Expression"
												},
												"requestTimeout": ""
											},
											"formatSettings": {
												"type": "BinaryReadSettings"
											}
										},
										"sink": {
											"type": "BinarySink",
											"storeSettings": {
												"type": "AzureBlobFSWriteSettings"
											}
										},
										"enableStaging": false
									},
									"inputs": [
										{
											"referenceName": "SharePointBinary",
											"type": "DatasetReference",
											"parameters": {
												"FileName": {
													"value": "@{item().ServerRelativeUrl}",
													"type": "Expression"
												},
												"BaseURL": {
													"value": "@variables('BaseURL')",
													"type": "Expression"
												},
												"SiteURL": {
													"value": "@variables('SiteURL')",
													"type": "Expression"
												}
											}
										}
									],
									"outputs": [
										{
											"referenceName": "DataLakeBinary",
											"type": "DatasetReference",
											"parameters": {
												"ContainerName": {
													"value": "@variables('ContainerName')",
													"type": "Expression"
												},
												"DestFolderName": {
													"value": "@variables('DestFolderName')",
													"type": "Expression"
												},
												"DataLake": {
													"value": "@variables('DataLake')",
													"type": "Expression"
												},
												"FileName": {
													"value": "@{item().Name}",
													"type": "Expression"
												}
											}
										}
									]
								}
							]
						}
					},
					{
						"name": "Get List of File Names",
						"type": "WebActivity",
						"dependsOn": [
							{
								"activity": "ForEach1",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.03:00:00",
							"retry": 1,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"url": {
								"value": "@concat('https://',variables('BaseURL'),'/',variables('SiteURL'),'_api/web/GetFolderByServerRelativeUrl(','''','/',variables('SiteURL'),variables('SharePointFolderName'),'''',')/Files')",
								"type": "Expression"
							},
							"connectVia": {
								"referenceName": "AutoResolveIntegrationRuntime",
								"type": "IntegrationRuntimeReference"
							},
							"method": "GET",
							"headers": {
								"Authorization": {
									"value": "@{concat('Bearer ', activity('Get Bearer Token').output.access_token)}",
									"type": "Expression"
								},
								"Accept": "application/json"
							},
							"body": "grant_type=client_credentials&client_id=bd146d1a-7d3f-4b57-8d4e-c794594a7f7f@0e23ddde-1cba-42eb-9760-e698ce0a9b00&client_secret=tI58Q~wsUQhC0tnGQ_KenYWvWfbqWPevgSKm6dsT&resource=00000003-0000-0ff1-ce00-000000000000/citycouriers365.sharepoint.com@0e23ddde-1cba-42eb-9760-e698ce0a9b00"
						}
					}
				],
				"concurrency": 1,
				"policy": {
					"elapsedTimeMetric": {}
				},
				"parameters": {
					"CW_Items": {
						"type": "array"
					},
					"SharepointSiteName": {
						"type": "string",
						"defaultValue": "DataWarehouse"
					}
				},
				"variables": {
					"TenantID": {
						"type": "String",
						"defaultValue": "0e23ddde-1cba-42eb-9760-e698ce0a9b00"
					},
					"client_id": {
						"type": "String",
						"defaultValue": "bd146d1a-7d3f-4b57-8d4e-c794594a7f7f"
					},
					"client_secret": {
						"type": "String",
						"defaultValue": "tI58Q~wsUQhC0tnGQ_KenYWvWfbqWPevgSKm6dsT"
					},
					"BaseURL": {
						"type": "String",
						"defaultValue": "citycouriers365.sharepoint.com"
					},
					"ContainerName": {
						"type": "String",
						"defaultValue": "synapse"
					},
					"DataLake": {
						"type": "String",
						"defaultValue": "https://citylogisticsstorageprod.dfs.core.windows.net/"
					},
					"SiteURL": {
						"type": "String"
					},
					"SharePointFolderName": {
						"type": "String"
					},
					"DestFolderName": {
						"type": "String"
					},
					"FileList": {
						"type": "Array"
					},
					"FolderContent": {
						"type": "Array"
					},
					"Temp": {
						"type": "Array"
					}
				},
				"folder": {
					"name": "PROD/BackEnd/Extract/SharePoint"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/datasets/SharePointBinary')]",
				"[concat(variables('workspaceId'), '/datasets/DataLakeBinary')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/TMS LOAD PROD')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Iterate Data",
						"type": "ForEach",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@pipeline().parameters.CW_Items",
								"type": "Expression"
							},
							"batchCount": 4,
							"activities": [
								{
									"name": "If ColumnsNamesBlankSpacesOrLimitedLoad",
									"type": "IfCondition",
									"dependsOn": [
										{
											"activity": "If FullLoad",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"expression": {
											"value": "@or(pipeline().parameters.ColumnsNamesBlankSpaces,greater(length(pipeline().parameters.LimitedLoadTopX),1))",
											"type": "Expression"
										},
										"ifTrueActivities": [
											{
												"name": "SelectStatementBuilder",
												"type": "Lookup",
												"dependsOn": [],
												"policy": {
													"timeout": "0.03:00:00",
													"retry": 3,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [],
												"typeProperties": {
													"source": {
														"type": "SqlServerSource",
														"sqlReaderQuery": {
															"value": "@concat(' \nDECLARE @s VARCHAR(MAX)\nDECLARE @tablename VARCHAR(MAX) = ''', item().Source.TableName,'''\nSELECT @s = ISNULL(@s + '', '','''') + ''['' + c.name + '']'' + '' as [''+replace(c.name,'' '',''_'')+''] ''\nFROM sys.all_columns c \njoin sys.objects t ON c.object_id = t.object_id \nWHERE t.name =  @tablename \nSELECT ''SELECT ', if(greater(length(pipeline().parameters.LimitedLoadTopX),0),concat('TOP ', pipeline().parameters.LimitedLoadTopX),''), ' '' + @s + '' FROM ['' +  @tablename + '']', if(greater(length(item().Increment.IdField),1),concat(' ORDER BY ', item().Increment.IdField, ' DESC'),''),''' as Query')",
															"type": "Expression"
														},
														"queryTimeout": "02:00:00",
														"partitionOption": "None"
													},
													"dataset": {
														"referenceName": "SqlServerDataSet",
														"type": "DatasetReference",
														"parameters": {
															"ServerName": {
																"value": "@variables('ServerName')",
																"type": "Expression"
															},
															"DatabaseName": {
																"value": "@variables('DatabaseName')",
																"type": "Expression"
															},
															"UserName": {
																"value": "@variables('UserName')",
																"type": "Expression"
															},
															"CW_SchemaName": {
																"value": "@variables('SchemaName')",
																"type": "Expression"
															},
															"SecretName": {
																"value": "@variables('SecretName')",
																"type": "Expression"
															},
															"CW_TableName": {
																"value": "@item().Source.TableName",
																"type": "Expression"
															}
														}
													}
												}
											}
										]
									}
								},
								{
									"name": "If FullLoad",
									"type": "IfCondition",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"expression": {
											"value": "@pipeline().parameters.Full_Load",
											"type": "Expression"
										},
										"ifFalseActivities": [
											{
												"name": "Lookup Last Max Field",
												"type": "Lookup",
												"dependsOn": [],
												"policy": {
													"timeout": "0.03:00:00",
													"retry": 0,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [],
												"typeProperties": {
													"source": {
														"type": "AzureSqlSource",
														"sqlReaderQuery": {
															"value": "@concat('SELECT MAX(',item().Increment.IncField,')  as Field \n FROM\n    OPENROWSET(\n        BULK ','''',variables('DataLake'),variables('ContainerName'),'/',variables('FolderName'),'/',item().destination.FileName,'.parquet','''',\n        ',FORMAT = ','''PARQUET''','\n    ) AS [result]')",
															"type": "Expression"
														},
														"queryTimeout": "02:00:00",
														"partitionOption": "None"
													},
													"dataset": {
														"referenceName": "AzureSqlConnection",
														"type": "DatasetReference",
														"parameters": {
															"ASAEndpoint": {
																"value": "@variables('ASAEndpoint')",
																"type": "Expression"
															},
															"ASADataBase": {
																"value": "@variables('ASADataBase')",
																"type": "Expression"
															}
														}
													}
												}
											},
											{
												"name": "Remove Old Data Incremental Data_SingleFile",
												"type": "Delete",
												"dependsOn": [
													{
														"activity": "Lookup Last Max Field",
														"dependencyConditions": [
															"Succeeded"
														]
													}
												],
												"policy": {
													"timeout": "0.03:00:00",
													"retry": 3,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [],
												"typeProperties": {
													"dataset": {
														"referenceName": "DataLakeParquet",
														"type": "DatasetReference",
														"parameters": {
															"CW_FolderName": {
																"value": "@concat(variables('FolderName'),'_Incremental_Data')",
																"type": "Expression"
															},
															"CW_FileName": {
																"value": "@concat(item().Destination.FileName,'.parquet')",
																"type": "Expression"
															},
															"ContainerName": {
																"value": "@variables('TempContainerName')",
																"type": "Expression"
															},
															"DataLake": {
																"value": "@variables('DataLake')",
																"type": "Expression"
															}
														}
													},
													"enableLogging": false,
													"storeSettings": {
														"type": "AzureBlobFSReadSettings",
														"recursive": true,
														"enablePartitionDiscovery": false
													}
												}
											}
										],
										"ifTrueActivities": [
											{
												"name": "Remove Old Data Data_Folder",
												"type": "Delete",
												"dependsOn": [
													{
														"activity": "Remove Old Data Data_SingleFile",
														"dependencyConditions": [
															"Failed"
														]
													}
												],
												"policy": {
													"timeout": "0.03:00:00",
													"retry": 0,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [],
												"typeProperties": {
													"dataset": {
														"referenceName": "DataLakeParquet",
														"type": "DatasetReference",
														"parameters": {
															"CW_FolderName": {
																"value": "@variables('FolderName')",
																"type": "Expression"
															},
															"CW_FileName": {
																"value": "@concat(item().Destination.FileName,'.parquet')",
																"type": "Expression"
															},
															"ContainerName": {
																"value": "@variables('TempContainerName')",
																"type": "Expression"
															},
															"DataLake": {
																"value": "@variables('DataLake')",
																"type": "Expression"
															}
														}
													},
													"enableLogging": false,
													"storeSettings": {
														"type": "AzureBlobFSReadSettings",
														"recursive": true,
														"wildcardFileName": {
															"value": "@{concat(variables('TempContainerName'),'/',variables('FolderName'),'/',item().Destination.FileName,'.parquet','/')}",
															"type": "Expression"
														},
														"enablePartitionDiscovery": false
													}
												}
											},
											{
												"name": "Remove Old Data Data_SingleFile",
												"type": "Delete",
												"dependsOn": [],
												"policy": {
													"timeout": "0.03:00:00",
													"retry": 0,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [],
												"typeProperties": {
													"dataset": {
														"referenceName": "DataLakeParquet",
														"type": "DatasetReference",
														"parameters": {
															"CW_FolderName": {
																"value": "@variables('FolderName')",
																"type": "Expression"
															},
															"CW_FileName": {
																"value": "@concat(item().Destination.FileName,'.parquet')",
																"type": "Expression"
															},
															"ContainerName": {
																"value": "@variables('TempContainerName')",
																"type": "Expression"
															},
															"DataLake": {
																"value": "@variables('DataLake')",
																"type": "Expression"
															}
														}
													},
													"enableLogging": false,
													"storeSettings": {
														"type": "AzureBlobFSReadSettings",
														"recursive": true,
														"enablePartitionDiscovery": false
													}
												}
											},
											{
												"name": "WaitBecause of Error",
												"type": "Wait",
												"dependsOn": [
													{
														"activity": "Remove Old Data Data_Folder",
														"dependencyConditions": [
															"Failed"
														]
													}
												],
												"userProperties": [],
												"typeProperties": {
													"waitTimeInSeconds": 1
												}
											}
										]
									}
								},
								{
									"name": "Switch1",
									"type": "Switch",
									"dependsOn": [
										{
											"activity": "If ColumnsNamesBlankSpacesOrLimitedLoad",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"on": {
											"value": "@if(pipeline().parameters.Full_Load,'FullTakeOn'\n   ,if(equals(item().Increment.IncField,item().Increment.IdField),'IdLoad'\n      ,'DateLoad'\n   )\n)",
											"type": "Expression"
										},
										"cases": [
											{
												"value": "DateLoad",
												"activities": [
													{
														"name": "Copy New incremental data",
														"type": "Copy",
														"dependsOn": [],
														"policy": {
															"timeout": "0.03:00:00",
															"retry": 3,
															"retryIntervalInSeconds": 30,
															"secureOutput": false,
															"secureInput": false
														},
														"userProperties": [],
														"typeProperties": {
															"source": {
																"type": "PostgreSqlSource",
																"query": {
																	"value": "@concat(if(or(pipeline().parameters.ColumnsNamesBlankSpaces, greater(length(pipeline().parameters.LimitedLoadTopX),1)),\n        activity('SelectStatementBuilder').output.firstRow.Query,concat('SELECT * FROM ', item().Source.TableName)\n        )\n    ,' WHERE ', item().Increment.IncField, ' > ',\n        if(greater(length(pipeline().parameters.TakeOnPeriod),1)\n        ,pipeline().parameters.TakeOnPeriod\n        ,concat('''',activity('Lookup Last Max Field').output.firstRow.Field,'''')\n        )\n    )",
																	"type": "Expression"
																}
															},
															"sink": {
																"type": "ParquetSink",
																"storeSettings": {
																	"type": "AzureBlobFSWriteSettings"
																},
																"formatSettings": {
																	"type": "ParquetWriteSettings"
																}
															},
															"enableStaging": false,
															"validateDataConsistency": false,
															"translator": {
																"type": "TabularTranslator",
																"typeConversion": true,
																"typeConversionSettings": {
																	"allowDataTruncation": true,
																	"treatBooleanAsNumber": false
																}
															}
														},
														"inputs": [
															{
																"referenceName": "PostgresSQL_Dataset",
																"type": "DatasetReference",
																"parameters": {
																	"ServerName": {
																		"value": "@variables('ServerName')",
																		"type": "Expression"
																	},
																	"DatabaseName": {
																		"value": "@variables('DatabaseName')",
																		"type": "Expression"
																	},
																	"UserName": {
																		"value": "@variables('UserName')",
																		"type": "Expression"
																	},
																	"CW_SchemaName": {
																		"value": "@variables('SchemaName')",
																		"type": "Expression"
																	},
																	"SecretName": {
																		"value": "@variables('SecretName')",
																		"type": "Expression"
																	},
																	"CW_TableName": {
																		"value": "@item().Source.TableName",
																		"type": "Expression"
																	},
																	"Port": {
																		"value": "@variables('Port')",
																		"type": "Expression"
																	}
																}
															}
														],
														"outputs": [
															{
																"referenceName": "DataLakeParquet",
																"type": "DatasetReference",
																"parameters": {
																	"CW_FolderName": {
																		"value": "@variables('FolderName')",
																		"type": "Expression"
																	},
																	"CW_FileName": {
																		"value": "@concat(item().Destination.FileName,'.parquet')",
																		"type": "Expression"
																	},
																	"ContainerName": {
																		"value": "@variables('TempContainerName')",
																		"type": "Expression"
																	},
																	"DataLake": {
																		"value": "@variables('DataLake')",
																		"type": "Expression"
																	}
																}
															}
														]
													}
												]
											},
											{
												"value": "IdLoad",
												"activities": [
													{
														"name": "Copy New incremental data_IDLoad",
														"type": "Copy",
														"dependsOn": [],
														"policy": {
															"timeout": "0.03:00:00",
															"retry": 3,
															"retryIntervalInSeconds": 30,
															"secureOutput": false,
															"secureInput": false
														},
														"userProperties": [],
														"typeProperties": {
															"source": {
																"type": "PostgreSqlSource",
																"query": {
																	"value": "@concat(if(or(pipeline().parameters.ColumnsNamesBlankSpaces,    greater(length(pipeline().parameters.LimitedLoadTopX),1)),\n        activity('SelectStatementBuilder').output.firstRow.Query, concat('SELECT * FROM ', item().Source.TableName)\n        )   \n    ,' WHERE ', item().Increment.IncField, ' > ',\n        if(greater(length(pipeline().parameters.TakeOnPeriod),1)\n        ,pipeline().parameters.TakeOnPeriod\n        ,activity('Lookup Last Max Field').output.firstRow.Field\n        )\n    )",
																	"type": "Expression"
																}
															},
															"sink": {
																"type": "ParquetSink",
																"storeSettings": {
																	"type": "AzureBlobFSWriteSettings"
																},
																"formatSettings": {
																	"type": "ParquetWriteSettings"
																}
															},
															"enableStaging": false,
															"validateDataConsistency": false,
															"translator": {
																"type": "TabularTranslator",
																"typeConversion": true,
																"typeConversionSettings": {
																	"allowDataTruncation": true,
																	"treatBooleanAsNumber": false
																}
															}
														},
														"inputs": [
															{
																"referenceName": "PostgresSQL_Dataset",
																"type": "DatasetReference",
																"parameters": {
																	"ServerName": {
																		"value": "@variables('ServerName')",
																		"type": "Expression"
																	},
																	"DatabaseName": {
																		"value": "@variables('DatabaseName')",
																		"type": "Expression"
																	},
																	"UserName": {
																		"value": "@variables('UserName')",
																		"type": "Expression"
																	},
																	"CW_SchemaName": {
																		"value": "@variables('SchemaName')",
																		"type": "Expression"
																	},
																	"SecretName": {
																		"value": "@variables('SecretName')",
																		"type": "Expression"
																	},
																	"CW_TableName": {
																		"value": "@item().Source.TableName",
																		"type": "Expression"
																	},
																	"Port": {
																		"value": "@variables('Port')",
																		"type": "Expression"
																	}
																}
															}
														],
														"outputs": [
															{
																"referenceName": "DataLakeParquet",
																"type": "DatasetReference",
																"parameters": {
																	"CW_FolderName": {
																		"value": "@variables('FolderName')",
																		"type": "Expression"
																	},
																	"CW_FileName": {
																		"value": "@concat(item().Destination.FileName,'.parquet')",
																		"type": "Expression"
																	},
																	"ContainerName": {
																		"value": "@variables('TempContainerName')",
																		"type": "Expression"
																	},
																	"DataLake": {
																		"value": "@variables('DataLake')",
																		"type": "Expression"
																	}
																}
															}
														]
													}
												]
											},
											{
												"value": "FullTakeOn",
												"activities": [
													{
														"name": "Copy_Data_Full_Take_On",
														"type": "Copy",
														"dependsOn": [],
														"policy": {
															"timeout": "7.00:00:00",
															"retry": 3,
															"retryIntervalInSeconds": 30,
															"secureOutput": false,
															"secureInput": false
														},
														"userProperties": [],
														"typeProperties": {
															"source": {
																"type": "PostgreSqlSource",
																"query": {
																	"value": "@if(or(pipeline().parameters.ColumnsNamesBlankSpaces, greater(length(pipeline().parameters.LimitedLoadTopX),1)),\n    activity('SelectStatementBuilder').output.firstRow.Query,\n    concat('SELECT * FROM ', item().Source.TableName)\n    )",
																	"type": "Expression"
																}
															},
															"sink": {
																"type": "ParquetSink",
																"storeSettings": {
																	"type": "AzureBlobFSWriteSettings"
																},
																"formatSettings": {
																	"type": "ParquetWriteSettings"
																}
															},
															"enableStaging": false,
															"validateDataConsistency": false,
															"translator": {
																"type": "TabularTranslator",
																"typeConversion": true,
																"typeConversionSettings": {
																	"allowDataTruncation": true,
																	"treatBooleanAsNumber": false
																}
															}
														},
														"inputs": [
															{
																"referenceName": "PostgresSQL_Dataset",
																"type": "DatasetReference",
																"parameters": {
																	"ServerName": {
																		"value": "@variables('ServerName')",
																		"type": "Expression"
																	},
																	"DatabaseName": {
																		"value": "@variables('DatabaseName')",
																		"type": "Expression"
																	},
																	"UserName": {
																		"value": "@variables('UserName')",
																		"type": "Expression"
																	},
																	"CW_SchemaName": {
																		"value": "@variables('SchemaName')",
																		"type": "Expression"
																	},
																	"SecretName": {
																		"value": "@variables('SecretName')",
																		"type": "Expression"
																	},
																	"CW_TableName": {
																		"value": "@item().Source.TableName",
																		"type": "Expression"
																	},
																	"Port": {
																		"value": "@variables('Port')",
																		"type": "Expression"
																	}
																}
															}
														],
														"outputs": [
															{
																"referenceName": "DataLakeParquet",
																"type": "DatasetReference",
																"parameters": {
																	"CW_FolderName": {
																		"value": "@variables('FolderName')",
																		"type": "Expression"
																	},
																	"CW_FileName": {
																		"value": "@concat(item().Destination.FileName,'.parquet')",
																		"type": "Expression"
																	},
																	"ContainerName": {
																		"value": "@variables('TempContainerName')",
																		"type": "Expression"
																	},
																	"DataLake": {
																		"value": "@variables('DataLake')",
																		"type": "Expression"
																	}
																}
															}
														]
													}
												]
											}
										],
										"defaultActivities": [
											{
												"name": "Fail1",
												"type": "Fail",
												"dependsOn": [],
												"userProperties": [],
												"typeProperties": {
													"message": "ERROR",
													"errorCode": "500"
												}
											}
										]
									}
								}
							]
						}
					},
					{
						"name": "ProdNotebook",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "Iterate Data",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "IncrementalProdScript",
								"type": "NotebookReference"
							},
							"parameters": {
								"cw_FolderName": {
									"value": {
										"value": "@variables('FolderName')",
										"type": "Expression"
									},
									"type": "string"
								},
								"ContainerName": {
									"value": {
										"value": "@variables('ContainerName')",
										"type": "Expression"
									},
									"type": "string"
								},
								"DataLakeDF": {
									"value": {
										"value": "@replace(variables('DataLake'),'https://','')",
										"type": "Expression"
									},
									"type": "string"
								},
								"Items": {
									"value": {
										"value": "@{pipeline().parameters.cw_items}",
										"type": "Expression"
									},
									"type": "string"
								},
								"FullLoad": {
									"value": {
										"value": "@pipeline().parameters.Full_Load",
										"type": "Expression"
									},
									"type": "string"
								},
								"TempContainerName": {
									"value": {
										"value": "@variables('TempContainerName')",
										"type": "Expression"
									},
									"type": "string"
								}
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "TESTSparkPool",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": true,
								"spark.dynamicAllocation.minExecutors": 1,
								"spark.dynamicAllocation.maxExecutors": 4
							},
							"driverSize": "Small",
							"numExecutors": 1
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"parameters": {
					"cw_items": {
						"type": "Array",
						"defaultValue": []
					},
					"TakeOnPeriod": {
						"type": "string",
						"defaultValue": " "
					},
					"Full_Load": {
						"type": "bool",
						"defaultValue": false
					},
					"ColumnsNamesBlankSpaces": {
						"type": "bool",
						"defaultValue": false
					},
					"LimitedLoadTopX": {
						"type": "string",
						"defaultValue": "1"
					}
				},
				"variables": {
					"FolderName": {
						"type": "String",
						"defaultValue": "Unstructured Data/TMS"
					},
					"DataLake": {
						"type": "String",
						"defaultValue": "https://citylogisticsstorageprod.dfs.core.windows.net/"
					},
					"ServerName": {
						"type": "String",
						"defaultValue": "orvdb.citylogistics.co.za"
					},
					"DatabaseName": {
						"type": "String",
						"defaultValue": "tmsapi"
					},
					"UserName": {
						"type": "String",
						"defaultValue": "synapse"
					},
					"SecretName": {
						"type": "String",
						"defaultValue": "TMSSecret"
					},
					"SchemaName": {
						"type": "String",
						"defaultValue": "public"
					},
					"ContainerName": {
						"type": "String",
						"defaultValue": "synapse"
					},
					"ASAEndpoint": {
						"type": "String",
						"defaultValue": "citylogistics-synapseanalytics-workspace-prod-ondemand.sql.azuresynapse.net"
					},
					"ASADataBase": {
						"type": "String",
						"defaultValue": "master"
					},
					"Port": {
						"type": "String",
						"defaultValue": "5433"
					},
					"TempContainerName": {
						"type": "String",
						"defaultValue": "temp"
					}
				},
				"folder": {
					"name": "PROD/BackEnd/Extract/TMS"
				},
				"annotations": [],
				"lastPublishTime": "2022-04-04T06:22:12Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/IncrementalProdScript')]",
				"[concat(variables('workspaceId'), '/bigDataPools/TESTSparkPool')]",
				"[concat(variables('workspaceId'), '/datasets/SqlServerDataSet')]",
				"[concat(variables('workspaceId'), '/datasets/AzureSqlConnection')]",
				"[concat(variables('workspaceId'), '/datasets/DataLakeParquet')]",
				"[concat(variables('workspaceId'), '/datasets/PostgresSQL_Dataset')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureSqlConnection')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureSqlDatabase",
					"type": "LinkedServiceReference",
					"parameters": {
						"ASAEndpoint": {
							"value": "@dataset().ASAEndpoint",
							"type": "Expression"
						},
						"ASADataBase": {
							"value": "@dataset().ASADataBase",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"ASAEndpoint": {
						"type": "string"
					},
					"ASADataBase": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [],
				"typeProperties": {}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureSqlDatabase')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DataLakeBinary')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "citylogistics_synapseanalytics_workspace_WorkspaceSecondaryStorage",
					"type": "LinkedServiceReference",
					"parameters": {
						"DataLake": {
							"value": "@dataset().DataLake",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"ContainerName": {
						"type": "string"
					},
					"DestFolderName": {
						"type": "string"
					},
					"DataLake": {
						"type": "string"
					},
					"FileName": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@dataset().FileName",
							"type": "Expression"
						},
						"folderPath": {
							"value": "@dataset().DestFolderName",
							"type": "Expression"
						},
						"fileSystem": {
							"value": "@dataset().ContainerName",
							"type": "Expression"
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/citylogistics_synapseanalytics_workspace_WorkspaceSecondaryStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DataLakeParquet')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "citylogistics_synapseanalytics_workspace_WorkspaceSecondaryStorage",
					"type": "LinkedServiceReference",
					"parameters": {
						"DataLake": {
							"value": "@dataset().DataLake",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"CW_FolderName": {
						"type": "string"
					},
					"CW_FileName": {
						"type": "string"
					},
					"ContainerName": {
						"type": "string"
					},
					"DataLake": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@dataset().CW_FileName",
							"type": "Expression"
						},
						"folderPath": {
							"value": "@dataset().CW_FolderName",
							"type": "Expression"
						},
						"fileSystem": {
							"value": "@dataset().ContainerName",
							"type": "Expression"
						}
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/citylogistics_synapseanalytics_workspace_WorkspaceSecondaryStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DestinationDataset_wdh')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "citylogistics-synapseanalytics-workspace-prod-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "Unstructured Data/TMS",
						"fileSystem": "synapse"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/citylogistics-synapseanalytics-workspace-prod-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/OnPremSambaBinary')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "OnPremFileServerLinkedServer",
					"type": "LinkedServiceReference",
					"parameters": {
						"ServerName": {
							"value": "@dataset().ServerName",
							"type": "Expression"
						},
						"UserName": {
							"value": "@dataset().UserName",
							"type": "Expression"
						},
						"SecretName": {
							"value": "@dataset().SecretName",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"ServerName": {
						"type": "string"
					},
					"UserName": {
						"type": "string"
					},
					"SecretName": {
						"type": "string"
					},
					"DirectoryName": {
						"type": "string"
					},
					"FileName": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "FileServerLocation",
						"fileName": {
							"value": "@dataset().FileName",
							"type": "Expression"
						},
						"folderPath": {
							"value": "@dataset().DirectoryName",
							"type": "Expression"
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/OnPremFileServerLinkedServer')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/PostgresSQL_Dataset')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "PostgreSQL",
					"type": "LinkedServiceReference",
					"parameters": {
						"ServerName": {
							"value": "@dataset().ServerName",
							"type": "Expression"
						},
						"DatabaseName": {
							"value": "@dataset().DatabaseName",
							"type": "Expression"
						},
						"UserName": {
							"value": "@dataset().UserName",
							"type": "Expression"
						},
						"SecretName": {
							"value": "@dataset().SecretName",
							"type": "Expression"
						},
						"Port": {
							"value": "@dataset().Port",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"ServerName": {
						"type": "string"
					},
					"DatabaseName": {
						"type": "string"
					},
					"UserName": {
						"type": "string"
					},
					"CW_SchemaName": {
						"type": "string"
					},
					"SecretName": {
						"type": "string"
					},
					"CW_TableName": {
						"type": "string"
					},
					"Port": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "PostgreSqlTable",
				"schema": [],
				"typeProperties": {
					"schema": {
						"value": "@dataset().CW_SchemaName",
						"type": "Expression"
					},
					"table": {
						"value": "@dataset().CW_TableName",
						"type": "Expression"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/PostgreSQL')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SharePointBinary')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "SharePointHTTPLS",
					"type": "LinkedServiceReference",
					"parameters": {
						"BaseUrl": {
							"value": "@dataset().BaseURL",
							"type": "Expression"
						},
						"FileName": {
							"value": "@dataset().FileName",
							"type": "Expression"
						},
						"SiteURL": {
							"value": "@dataset().SiteURL",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"FileName": {
						"type": "string"
					},
					"BaseURL": {
						"type": "string"
					},
					"SiteURL": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "HttpServerLocation"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/SharePointHTTPLS')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SourceDataset_wdh')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "citylogisticsstoragedevdatalake",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "Unstructured Data/TMS",
						"fileSystem": "synapse"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/citylogisticsstoragedevdatalake')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SqlServerDataSet')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "SqlServer_WindowsAuth",
					"type": "LinkedServiceReference",
					"parameters": {
						"ServerName": {
							"value": "@dataset().ServerName",
							"type": "Expression"
						},
						"DatabaseName": {
							"value": "@dataset().DatabaseName",
							"type": "Expression"
						},
						"UserName": {
							"value": "@dataset().UserName",
							"type": "Expression"
						},
						"SecretName": {
							"value": "@dataset().SecretName",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"ServerName": {
						"type": "string"
					},
					"DatabaseName": {
						"type": "string"
					},
					"UserName": {
						"type": "string"
					},
					"CW_SchemaName": {
						"type": "string"
					},
					"SecretName": {
						"type": "string"
					},
					"CW_TableName": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "SqlServerTable",
				"schema": [],
				"typeProperties": {
					"schema": {
						"value": "@dataset().CW_SchemaName",
						"type": "Expression"
					},
					"table": {
						"value": "@dataset().CW_TableName",
						"type": "Expression"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/SqlServer_WindowsAuth')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SqlServerDataSet_DBAuth')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "SqlServer_DataBaseAuth",
					"type": "LinkedServiceReference",
					"parameters": {
						"ServerName": {
							"value": "@dataset().ServerName",
							"type": "Expression"
						},
						"DatabaseName": {
							"value": "@dataset().DatabaseName",
							"type": "Expression"
						},
						"UserName": {
							"value": "@dataset().UserName",
							"type": "Expression"
						},
						"SecretName": {
							"value": "@dataset().SecretName",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"ServerName": {
						"type": "string"
					},
					"DatabaseName": {
						"type": "string"
					},
					"UserName": {
						"type": "string"
					},
					"CW_SchemaName": {
						"type": "string"
					},
					"SecretName": {
						"type": "string"
					},
					"CW_TableName": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "SqlServerTable",
				"schema": [],
				"typeProperties": {
					"schema": {
						"value": "@dataset().CW_SchemaName",
						"type": "Expression"
					},
					"table": {
						"value": "@dataset().CW_TableName",
						"type": "Expression"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/SqlServer_DataBaseAuth')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureKeyVaultCity')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureKeyVault",
				"typeProperties": {
					"baseUrl": "[parameters('AzureKeyVaultCity_properties_typeProperties_baseUrl')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureSqlDatabase')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"ASAEndpoint": {
						"type": "string"
					},
					"ASADataBase": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "AzureSqlDatabase",
				"typeProperties": {
					"connectionString": "[parameters('AzureSqlDatabase_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/OnPremFileServerLinkedServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"ServerName": {
						"type": "string"
					},
					"UserName": {
						"type": "string"
					},
					"SecretName": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "FileServer",
				"typeProperties": {
					"host": "[parameters('OnPremFileServerLinkedServer_properties_typeProperties_host')]",
					"userId": "[parameters('OnPremFileServerLinkedServer_properties_typeProperties_userId')]",
					"password": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "AzureKeyVaultCity",
							"type": "LinkedServiceReference"
						},
						"secretName": {
							"value": "@linkedService().SecretName",
							"type": "Expression"
						}
					}
				},
				"connectVia": {
					"referenceName": "SelfHostedIRCityIntra",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/SelfHostedIRCityIntra')]",
				"[concat(variables('workspaceId'), '/linkedServices/AzureKeyVaultCity')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/PostgreSQL')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"ServerName": {
						"type": "string"
					},
					"DatabaseName": {
						"type": "string"
					},
					"UserName": {
						"type": "string"
					},
					"SecretName": {
						"type": "string"
					},
					"Port": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "PostgreSql",
				"typeProperties": {
					"connectionString": "[parameters('PostgreSQL_connectionString')]",
					"password": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "AzureKeyVaultCity",
							"type": "LinkedServiceReference"
						},
						"secretName": {
							"value": "@linkedService().SecretName",
							"type": "Expression"
						}
					}
				},
				"connectVia": {
					"referenceName": "SelfHostedIRCityIntra",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/SelfHostedIRCityIntra')]",
				"[concat(variables('workspaceId'), '/linkedServices/AzureKeyVaultCity')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/PostgreSqlTestServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "PostgreSql",
				"typeProperties": {
					"connectionString": "[parameters('PostgreSqlTestServer_connectionString')]",
					"password": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "AzureKeyVaultCity",
							"type": "LinkedServiceReference"
						},
						"secretName": "ORVSecret"
					}
				},
				"connectVia": {
					"referenceName": "SelfHostedIRCityIntra",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/SelfHostedIRCityIntra')]",
				"[concat(variables('workspaceId'), '/linkedServices/AzureKeyVaultCity')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQLTestServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "SqlServer",
				"typeProperties": {
					"connectionString": "[parameters('SQLTestServer_connectionString')]",
					"userName": "[parameters('SQLTestServer_properties_typeProperties_userName')]",
					"password": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "AzureKeyVaultCity",
							"type": "LinkedServiceReference"
						},
						"secretName": "SAPSecret"
					}
				},
				"connectVia": {
					"referenceName": "SelfHostedIRCityIntra",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/SelfHostedIRCityIntra')]",
				"[concat(variables('workspaceId'), '/linkedServices/AzureKeyVaultCity')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SharePointHTTPLS')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"BaseUrl": {
						"type": "string"
					},
					"FileName": {
						"type": "string"
					},
					"SiteURL": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "HttpServer",
				"typeProperties": {
					"url": "[parameters('SharePointHTTPLS_properties_typeProperties_url')]",
					"enableServerCertificateValidation": true,
					"authenticationType": "Anonymous"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SqlServer_DataBaseAuth')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"ServerName": {
						"type": "string"
					},
					"DatabaseName": {
						"type": "string"
					},
					"UserName": {
						"type": "string"
					},
					"SecretName": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "SqlServer",
				"typeProperties": {
					"connectionString": "[parameters('SqlServer_DataBaseAuth_connectionString')]",
					"password": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "AzureKeyVaultCity",
							"type": "LinkedServiceReference"
						},
						"secretName": {
							"value": "@linkedService().SecretName",
							"type": "Expression"
						}
					}
				},
				"connectVia": {
					"referenceName": "SelfHostedIRCityIntra",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/SelfHostedIRCityIntra')]",
				"[concat(variables('workspaceId'), '/linkedServices/AzureKeyVaultCity')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SqlServer_WindowsAuth')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"ServerName": {
						"type": "string"
					},
					"DatabaseName": {
						"type": "string"
					},
					"UserName": {
						"type": "string"
					},
					"SecretName": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "SqlServer",
				"typeProperties": {
					"connectionString": "[parameters('SqlServer_WindowsAuth_connectionString')]",
					"userName": "[parameters('SqlServer_WindowsAuth_properties_typeProperties_userName')]",
					"password": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "AzureKeyVaultCity",
							"type": "LinkedServiceReference"
						},
						"secretName": {
							"value": "@linkedService().SecretName",
							"type": "Expression"
						}
					}
				},
				"connectVia": {
					"referenceName": "SelfHostedIRCityIntra",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/SelfHostedIRCityIntra')]",
				"[concat(variables('workspaceId'), '/linkedServices/AzureKeyVaultCity')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/citylogistics-synapseanalytics-workspace-prod-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('citylogistics-synapseanalytics-workspace-prod-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/citylogistics-synapseanalytics-workspace-prod-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('citylogistics-synapseanalytics-workspace-prod-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/citylogistics_synapseanalytics_workspace_WorkspaceSecondaryStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DataLake": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('citylogistics_synapseanalytics_workspace_WorkspaceSecondaryStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/citylogisticsstoragedevdatalake')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('citylogisticsstoragedevdatalake_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Ad_Hoc Trigger')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"runtimeState": "Stopped",
				"pipelines": [],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Month",
						"interval": 1,
						"startTime": "2023-09-21T15:00:00",
						"endTime": "2023-09-23T15:00:00",
						"timeZone": "South Africa Standard Time",
						"schedule": {
							"minutes": [
								0
							],
							"hours": [
								18
							],
							"monthDays": [
								21
							]
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Full Take-on PROD_Daily Trigger Midnight')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "This is the daily trigger at midnight",
				"annotations": [],
				"runtimeState": "Started",
				"pipelines": [
					{
						"pipelineReference": {
							"referenceName": "CLMasterData LOAD PROD",
							"type": "PipelineReference"
						},
						"parameters": {
							"cw_items": "[parameters('Full Take-on PROD_Daily Trigger Midnight_properties_CLMasterData LOAD PROD_parameters_cw_items')]",
							"TakeOnPeriod": "[parameters('Full Take-on PROD_Daily Trigger Midnight_properties_CLMasterData LOAD PROD_parameters_TakeOnPeriod')]",
							"Full_Load": "[parameters('Full Take-on PROD_Daily Trigger Midnight_properties_CLMasterData LOAD PROD_parameters_Full_Load')]",
							"ColumnsNamesBlankSpaces": "[parameters('Full Take-on PROD_Daily Trigger Midnight_properties_CLMasterData LOAD PROD_parameters_ColumnsNamesBlankSpaces')]",
							"LimitedLoadTopX": "[parameters('Full Take-on PROD_Daily Trigger Midnight_properties_CLMasterData LOAD PROD_parameters_LimitedLoadTopX')]"
						}
					},
					{
						"pipelineReference": {
							"referenceName": "LMS LOAD PROD",
							"type": "PipelineReference"
						},
						"parameters": {
							"CW_Items": "[parameters('Full Take-on PROD_Daily Trigger Midnight_properties_LMS LOAD PROD_parameters_CW_Items')]",
							"TakeOnPeriod": "[parameters('Full Take-on PROD_Daily Trigger Midnight_properties_LMS LOAD PROD_parameters_TakeOnPeriod')]",
							"Full_Load": "[parameters('Full Take-on PROD_Daily Trigger Midnight_properties_LMS LOAD PROD_parameters_Full_Load')]",
							"ColumnsNamesBlankSpaces": "[parameters('Full Take-on PROD_Daily Trigger Midnight_properties_LMS LOAD PROD_parameters_ColumnsNamesBlankSpaces')]",
							"LimitedLoadTopX": "[parameters('Full Take-on PROD_Daily Trigger Midnight_properties_LMS LOAD PROD_parameters_LimitedLoadTopX')]"
						}
					},
					{
						"pipelineReference": {
							"referenceName": "SAP LOAD PROD",
							"type": "PipelineReference"
						},
						"parameters": {
							"CW_Items": "[parameters('Full Take-on PROD_Daily Trigger Midnight_properties_SAP LOAD PROD_parameters_CW_Items')]",
							"Full_Load": "[parameters('Full Take-on PROD_Daily Trigger Midnight_properties_SAP LOAD PROD_parameters_Full_Load')]",
							"ColumnsNamesBlankSpaces": "[parameters('Full Take-on PROD_Daily Trigger Midnight_properties_SAP LOAD PROD_parameters_ColumnsNamesBlankSpaces')]"
						}
					},
					{
						"pipelineReference": {
							"referenceName": "Avis Full Take-on",
							"type": "PipelineReference"
						},
						"parameters": {}
					},
					{
						"pipelineReference": {
							"referenceName": "Sharepoint Full Take-on",
							"type": "PipelineReference"
						},
						"parameters": {
							"CW_Items": "[parameters('Full Take-on PROD_Daily Trigger Midnight_properties_Sharepoint Full Take-on_parameters_CW_Items')]",
							"SharepointSiteName": "[parameters('Full Take-on PROD_Daily Trigger Midnight_properties_Sharepoint Full Take-on_parameters_SharepointSiteName')]"
						}
					},
					{
						"pipelineReference": {
							"referenceName": "ORV LOAD PROD",
							"type": "PipelineReference"
						},
						"parameters": {
							"cw_items": "[parameters('Full Take-on PROD_Daily Trigger Midnight_properties_ORV LOAD PROD_parameters_cw_items')]",
							"TakeOnPeriod": "[parameters('Full Take-on PROD_Daily Trigger Midnight_properties_ORV LOAD PROD_parameters_TakeOnPeriod')]",
							"Full_Load": "[parameters('Full Take-on PROD_Daily Trigger Midnight_properties_ORV LOAD PROD_parameters_Full_Load')]",
							"ColumnsNamesBlankSpaces": "[parameters('Full Take-on PROD_Daily Trigger Midnight_properties_ORV LOAD PROD_parameters_ColumnsNamesBlankSpaces')]",
							"LimitedLoadTopX": "[parameters('Full Take-on PROD_Daily Trigger Midnight_properties_ORV LOAD PROD_parameters_LimitedLoadTopX')]"
						}
					},
					{
						"pipelineReference": {
							"referenceName": "TMS LOAD PROD",
							"type": "PipelineReference"
						},
						"parameters": {
							"cw_items": "[parameters('Full Take-on PROD_Daily Trigger Midnight_properties_TMS LOAD PROD_parameters_cw_items')]",
							"TakeOnPeriod": "[parameters('Full Take-on PROD_Daily Trigger Midnight_properties_TMS LOAD PROD_parameters_TakeOnPeriod')]",
							"Full_Load": "[parameters('Full Take-on PROD_Daily Trigger Midnight_properties_TMS LOAD PROD_parameters_Full_Load')]",
							"ColumnsNamesBlankSpaces": "[parameters('Full Take-on PROD_Daily Trigger Midnight_properties_TMS LOAD PROD_parameters_ColumnsNamesBlankSpaces')]",
							"LimitedLoadTopX": "[parameters('Full Take-on PROD_Daily Trigger Midnight_properties_TMS LOAD PROD_parameters_LimitedLoadTopX')]"
						}
					}
				],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Week",
						"interval": 1,
						"startTime": "2022-10-05T08:43:00",
						"timeZone": "South Africa Standard Time",
						"schedule": {
							"minutes": [
								0
							],
							"hours": [
								0
							],
							"weekDays": [
								"Monday",
								"Wednesday",
								"Tuesday",
								"Thursday",
								"Friday"
							]
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/CLMasterData LOAD PROD')]",
				"[concat(variables('workspaceId'), '/pipelines/LMS LOAD PROD')]",
				"[concat(variables('workspaceId'), '/pipelines/SAP LOAD PROD')]",
				"[concat(variables('workspaceId'), '/pipelines/Avis Full Take-on')]",
				"[concat(variables('workspaceId'), '/pipelines/Sharepoint Full Take-on')]",
				"[concat(variables('workspaceId'), '/pipelines/ORV LOAD PROD')]",
				"[concat(variables('workspaceId'), '/pipelines/TMS LOAD PROD')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Full Take-on PROD_Daily Trigger Rest')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "This is the Daily trigger for the rest of the day\n",
				"annotations": [],
				"runtimeState": "Stopped",
				"pipelines": [],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Week",
						"interval": 1,
						"startTime": "2022-10-05T08:44:00",
						"timeZone": "South Africa Standard Time",
						"schedule": {
							"minutes": [
								0
							],
							"hours": [
								10,
								12,
								15
							],
							"weekDays": [
								"Monday",
								"Tuesday",
								"Wednesday",
								"Thursday",
								"Friday",
								"Saturday"
							]
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Incre PROD_Daily Trigger Midnight')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "This is the daily trigger at midnight",
				"annotations": [],
				"runtimeState": "Started",
				"pipelines": [
					{
						"pipelineReference": {
							"referenceName": "LMS LOAD PROD",
							"type": "PipelineReference"
						},
						"parameters": {
							"CW_Items": "[parameters('Incre PROD_Daily Trigger Midnight_properties_LMS LOAD PROD_parameters_CW_Items')]",
							"TakeOnPeriod": "[parameters('Incre PROD_Daily Trigger Midnight_properties_LMS LOAD PROD_parameters_TakeOnPeriod')]",
							"Full_Load": "[parameters('Incre PROD_Daily Trigger Midnight_properties_LMS LOAD PROD_parameters_Full_Load')]",
							"ColumnsNamesBlankSpaces": "[parameters('Incre PROD_Daily Trigger Midnight_properties_LMS LOAD PROD_parameters_ColumnsNamesBlankSpaces')]",
							"LimitedLoadTopX": "[parameters('Incre PROD_Daily Trigger Midnight_properties_LMS LOAD PROD_parameters_LimitedLoadTopX')]"
						}
					},
					{
						"pipelineReference": {
							"referenceName": "SAP LOAD PROD",
							"type": "PipelineReference"
						},
						"parameters": {
							"CW_Items": "[parameters('Incre PROD_Daily Trigger Midnight_properties_SAP LOAD PROD_parameters_CW_Items')]",
							"TakeOnPeriod": "[parameters('Incre PROD_Daily Trigger Midnight_properties_SAP LOAD PROD_parameters_TakeOnPeriod')]",
							"Full_Load": "[parameters('Incre PROD_Daily Trigger Midnight_properties_SAP LOAD PROD_parameters_Full_Load')]",
							"ColumnsNamesBlankSpaces": "[parameters('Incre PROD_Daily Trigger Midnight_properties_SAP LOAD PROD_parameters_ColumnsNamesBlankSpaces')]",
							"LimitedLoadTopX": "[parameters('Incre PROD_Daily Trigger Midnight_properties_SAP LOAD PROD_parameters_LimitedLoadTopX')]"
						}
					},
					{
						"pipelineReference": {
							"referenceName": "ORV LOAD PROD",
							"type": "PipelineReference"
						},
						"parameters": {
							"cw_items": "[parameters('Incre PROD_Daily Trigger Midnight_properties_ORV LOAD PROD_parameters_cw_items')]",
							"TakeOnPeriod": "[parameters('Incre PROD_Daily Trigger Midnight_properties_ORV LOAD PROD_parameters_TakeOnPeriod')]",
							"Full_Load": "[parameters('Incre PROD_Daily Trigger Midnight_properties_ORV LOAD PROD_parameters_Full_Load')]",
							"ColumnsNamesBlankSpaces": "[parameters('Incre PROD_Daily Trigger Midnight_properties_ORV LOAD PROD_parameters_ColumnsNamesBlankSpaces')]",
							"LimitedLoadTopX": "[parameters('Incre PROD_Daily Trigger Midnight_properties_ORV LOAD PROD_parameters_LimitedLoadTopX')]"
						}
					},
					{
						"pipelineReference": {
							"referenceName": "TMS LOAD PROD",
							"type": "PipelineReference"
						},
						"parameters": {
							"cw_items": "[parameters('Incre PROD_Daily Trigger Midnight_properties_TMS LOAD PROD_parameters_cw_items')]",
							"TakeOnPeriod": "[parameters('Incre PROD_Daily Trigger Midnight_properties_TMS LOAD PROD_parameters_TakeOnPeriod')]",
							"Full_Load": "[parameters('Incre PROD_Daily Trigger Midnight_properties_TMS LOAD PROD_parameters_Full_Load')]",
							"ColumnsNamesBlankSpaces": "[parameters('Incre PROD_Daily Trigger Midnight_properties_TMS LOAD PROD_parameters_ColumnsNamesBlankSpaces')]",
							"LimitedLoadTopX": "[parameters('Incre PROD_Daily Trigger Midnight_properties_TMS LOAD PROD_parameters_LimitedLoadTopX')]"
						}
					},
					{
						"pipelineReference": {
							"referenceName": "CLMasterData LOAD PROD",
							"type": "PipelineReference"
						},
						"parameters": {
							"cw_items": "[parameters('Incre PROD_Daily Trigger Midnight_properties_CLMasterData LOAD PROD_parameters_cw_items')]",
							"TakeOnPeriod": "[parameters('Incre PROD_Daily Trigger Midnight_properties_CLMasterData LOAD PROD_parameters_TakeOnPeriod')]",
							"Full_Load": "[parameters('Incre PROD_Daily Trigger Midnight_properties_CLMasterData LOAD PROD_parameters_Full_Load')]",
							"ColumnsNamesBlankSpaces": "[parameters('Incre PROD_Daily Trigger Midnight_properties_CLMasterData LOAD PROD_parameters_ColumnsNamesBlankSpaces')]",
							"LimitedLoadTopX": "[parameters('Incre PROD_Daily Trigger Midnight_properties_CLMasterData LOAD PROD_parameters_LimitedLoadTopX')]"
						}
					}
				],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Week",
						"interval": 1,
						"startTime": "2022-10-05T08:43:00",
						"timeZone": "South Africa Standard Time",
						"schedule": {
							"minutes": [
								0
							],
							"hours": [
								0
							],
							"weekDays": [
								"Monday",
								"Wednesday",
								"Tuesday",
								"Friday",
								"Thursday"
							]
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/LMS LOAD PROD')]",
				"[concat(variables('workspaceId'), '/pipelines/SAP LOAD PROD')]",
				"[concat(variables('workspaceId'), '/pipelines/ORV LOAD PROD')]",
				"[concat(variables('workspaceId'), '/pipelines/TMS LOAD PROD')]",
				"[concat(variables('workspaceId'), '/pipelines/CLMasterData LOAD PROD')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Incre PROD_Daily Trigger Rest')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "This is the Daily trigger for the rest of the day\n",
				"annotations": [],
				"runtimeState": "Stopped",
				"pipelines": [],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Week",
						"interval": 1,
						"startTime": "2022-10-05T08:44:00",
						"timeZone": "South Africa Standard Time",
						"schedule": {
							"minutes": [
								0
							],
							"hours": [
								10,
								12,
								15
							],
							"weekDays": [
								"Monday",
								"Tuesday",
								"Wednesday",
								"Thursday",
								"Friday",
								"Saturday"
							]
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Incre PROD_Weekly Trigger Rest')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "This is the trigger for the rest of the day on a Sunday",
				"annotations": [],
				"runtimeState": "Stopped",
				"pipelines": [],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Week",
						"interval": 1,
						"startTime": "2022-10-09T07:33:00",
						"timeZone": "South Africa Standard Time",
						"schedule": {
							"minutes": [
								0
							],
							"hours": [
								10,
								15,
								0
							],
							"weekDays": [
								"Sunday"
							]
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Rolling PROD_Weekly Trigger')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "This is the trigger for once a week 12:00 on a Sunday",
				"annotations": [],
				"runtimeState": "Stopped",
				"pipelines": [
					{
						"pipelineReference": {
							"referenceName": "LMS LOAD PROD",
							"type": "PipelineReference"
						},
						"parameters": {
							"CW_Items": "[parameters('Rolling PROD_Weekly Trigger_properties_LMS LOAD PROD_parameters_CW_Items')]",
							"TakeOnPeriod": "[parameters('Rolling PROD_Weekly Trigger_properties_LMS LOAD PROD_parameters_TakeOnPeriod')]",
							"Full_Load": "[parameters('Rolling PROD_Weekly Trigger_properties_LMS LOAD PROD_parameters_Full_Load')]",
							"ColumnsNamesBlankSpaces": "[parameters('Rolling PROD_Weekly Trigger_properties_LMS LOAD PROD_parameters_ColumnsNamesBlankSpaces')]"
						}
					},
					{
						"pipelineReference": {
							"referenceName": "SAP LOAD PROD",
							"type": "PipelineReference"
						},
						"parameters": {
							"CW_Items": "[parameters('Rolling PROD_Weekly Trigger_properties_SAP LOAD PROD_parameters_CW_Items')]",
							"TakeOnPeriod": "[parameters('Rolling PROD_Weekly Trigger_properties_SAP LOAD PROD_parameters_TakeOnPeriod')]",
							"Full_Load": "[parameters('Rolling PROD_Weekly Trigger_properties_SAP LOAD PROD_parameters_Full_Load')]",
							"ColumnsNamesBlankSpaces": "[parameters('Rolling PROD_Weekly Trigger_properties_SAP LOAD PROD_parameters_ColumnsNamesBlankSpaces')]"
						}
					}
				],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Week",
						"interval": 1,
						"startTime": "2022-10-01T00:56:00",
						"timeZone": "South Africa Standard Time",
						"schedule": {
							"minutes": [
								0
							],
							"hours": [
								12
							]
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/LMS LOAD PROD')]",
				"[concat(variables('workspaceId'), '/pipelines/SAP LOAD PROD')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SoR RunOrder Trigger')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"runtimeState": "Started",
				"pipelines": [
					{
						"pipelineReference": {
							"referenceName": "RunOrder",
							"type": "PipelineReference"
						},
						"parameters": {}
					}
				],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Week",
						"interval": 1,
						"startTime": "2023-05-21T15:12:00",
						"timeZone": "South Africa Standard Time",
						"schedule": {
							"minutes": [
								30
							],
							"hours": [
								2
							],
							"weekDays": [
								"Monday",
								"Wednesday",
								"Tuesday",
								"Thursday",
								"Friday"
							]
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/RunOrder')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DataFlowIR')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 16,
							"timeToLive": 10,
							"cleanup": false,
							"customProperties": []
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DataFlowIRLarge')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "MemoryOptimized",
							"coreCount": 32,
							"timeToLive": 5,
							"cleanup": false,
							"customProperties": []
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SelfHostedIRCityIntra')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "SelfHosted",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/101_RptParcelLevel_centurionsorterreport')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Reports/ExternalTables"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.centurionsorterreport') )\n    DROP EXTERNAL TABLE dbo.centurionsorterreport\nGO\n\nCREATE EXTERNAL TABLE dbo.centurionsorterreport (\n[id] int,\n[autosystemid] int,\n[lpn] nvarchar(4000),\n[status] int,\n[date] datetime2(7),\n[dateupdated] datetime2(7),\n[plcdivertid] int,\n[cldivertid] int,\n[divertdescription] nvarchar(4000),\n[billingcustomerid] int,\n[billingcustomer] nvarchar(4000),\n[deliverycustomerid] int,\n[deliverycustomer] nvarchar(4000),\n[deliverycustcref] nvarchar(4000),\n[deliverycustsrouteid] int,\n[deliverycustsroutedescription] nvarchar(4000),\n[deliverycustsroutecode]  nvarchar(4000),\n\n\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/RPT/centurionsorterreport.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.centurionsorterreport \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "RPT",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/102_RptParcelLevelLevel_volumiserlogreport')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Reports/ExternalTables"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.volumiserlogreport') )\n    DROP EXTERNAL TABLE dbo.volumiserlogreport\nGO\n\nCREATE EXTERNAL TABLE dbo.volumiserlogreport (\n[id] int,\n[parcelbarcode] nvarchar(4000),\n[consignid] int,\n[billingcustomerid] int,\n[billingcustomer] nvarchar(4000),\n[date] datetime2(7),\n[lmsvolumiserlengthmm] numeric(36,6),\n[lmsvolumiserwidthmm] numeric(36,6),\n[lmsvolumiserheightmm] numeric(36,6),\n[lmsweightkg] float,\n[lmscalculatedvolumetricweightkg] numeric(36,6),\n[lmsgreaterweight] nvarchar(4000),\n[sickvolumiserlengthmm] numeric(36,6),\n[sickvolumiserwidthmm] numeric(36,6),\n[sickvolumiserheightmm] numeric(36,6),\n[sickvolumiserweightkg] float,\n[sickcalculatedvolumetricweightkg] numeric(36,6),\n[sickgreaterweight] nvarchar(4000),\n[scanlocationid] int,\n[scanlocation] nvarchar(4000),\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/RPT/volumiserlogreport.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.volumiserlogreport \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "RPT",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/103_RptParcelLevel_parceldamagereport')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Reports/ExternalTables"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.parceldamagereport') )\n    DROP EXTERNAL TABLE dbo.parceldamagereport\nGO\n\nCREATE EXTERNAL TABLE dbo.parceldamagereport (\n\n[id] int,\n[parcelbarcode] nvarchar(4000),\n[loadid] int,\n[date] datetime2(7),\n[msg] nvarchar(4000),\n[loaddate] datetime2(7),\n[fromdepotid] int,\n[fromdepot] nvarchar(4000),\n[todepotid] int,\n[todepot] nvarchar(4000),\n[loggedbyid] int,\n[loggedby] nvarchar(4000),\n[userdepotid] int,\n[userdepot] nvarchar(4000),\n\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/RPT/parceldamagereport.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.parceldamagereport\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "RPT",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/104_RptParcelLevel_qcparceldamagereport')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Reports/ExternalTables"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.qcparceldamagereport') )\n    DROP EXTERNAL TABLE dbo.qcparceldamagereport\nGO\n\nCREATE EXTERNAL TABLE dbo.qcparceldamagereport (\n\n[id] int,\n[parcelid] int,\n[parcelbarcode] nvarchar(4000),\n[billingcustomerid] int,\n[billingcustomer] nvarchar(4000),\n[auditbyid] int,\n[auditby] nvarchar(4000),\n[locationid] int,\n[location] nvarchar(4000),\n[date] datetime2(7),\n[parcelweightkg] float,\n[parcelwidthcm] numeric(36,6),\n[parcellengthcm] numeric(36,6),\n[parcelheightcm] numeric(36,6),\n[volumiserweight] numeric(36,6),\n[volumiserlengthcm] numeric(36,6),\n[volumiserheightcm] numeric(36,6),\n[volumiserwidthcm] numeric(36,6),\n[auditquantity] int,\n[auditweight] numeric(36,6),\n[flutemeasure] numeric(36,6),\n[shrinkwrap] nvarchar(4000),\n[comment] nvarchar(4000),\n[utilization] nvarchar(4000),\n[missing] int,\n[damaged] int,\n[flutedescription] nvarchar(4000),\n[tapetype] nvarchar(4000),\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/RPT/qcparceldamagereport.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.qcparceldamagereport\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "RPT",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/105_RptTrackLevel_branchvolumesreport')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Reports/ExternalTables"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.branchvolumesreport') )\n    DROP EXTERNAL TABLE dbo.branchvolumesreport\nGO\n\nCREATE EXTERNAL TABLE dbo.branchvolumesreport (\n\n\t[locationid] int,\n\t[location] nvarchar(4000),\n\t[date] DATETIME2,\n\t[thour]int,\n\t[aggrpid] int,\n\t[aggrparcelweightrecived] float,\n\t[aggrpidl] int,\n\t[aggrparcelweightlinehaul] float,\n\t[aggrpidd] int,\n\t[aggrparcelweightdelivery] float\n\t\n\t)\n\n\tWITH (\n\tLOCATION = 'Structured Data/RPT/branchvolumesreport.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.branchvolumesreport \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "RPT",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/106_RptLoadParentLevel_vehicleutilisationreport')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Reports/ExternalTables"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.vehicleutilisationreport') )\n    DROP EXTERNAL TABLE dbo.vehicleutilisationreport\nGO\n\nCREATE EXTERNAL TABLE dbo.vehicleutilisationreport (\n\n\t[parentloadid] int,\n\t[childloadids] nvarchar(4000),\n\t[date] datetime2(7),\n\t[estdeptdte] datetime2(7),\n\t[originalstartdate] datetime2(7),\n\t[locationid] int,\n\t[location] nvarchar(4000),\n\t[tolocationid] int,\n\t[tolocation] nvarchar(4000),\n\t[vehicleid] int,\n\t[vehicleuid] nvarchar(4000),\n\t[fleetcode] nvarchar(4000),\n\t[regno] nvarchar(4000),\n\t[vehicleinternalvolume] numeric(36,6),\n\t[lmsvehodo] float,\n\t[lmsvehodo2] float,\n\t[lmsodokms] float,\n\t[orvstartodo] int,\n\t[orvstopodo] int,\n\t[orvodokms] int,\n\t[orvdistance] float,\n\t[orvduration] float,\n\t[noparcels] int,\n\t[weight] numeric(36,6),\n\t[chargeweight] numeric(36,6),\n\t[nochildloads] int,\n\t[nodeliverycustomers] int,\n\t[expressdelivercustomers] int,\n\t[retailelivercustomers] int,\n\t[requiredvolume] nvarchar(4000),\n\t[utilisationprct] nvarchar(4000),\n\n\t\n\t)\n\n\tWITH (\n\tLOCATION = 'Structured Data/RPT/vehicleutilisationreport.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.vehicleutilisationreport \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "RPT",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/107_RptParcelLevel_dailyopsstrikeratereport')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Reports/ExternalTables"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.dailyopsstrikeratereport') )\n    DROP EXTERNAL TABLE dbo.dailyopsstrikeratereport\nGO\n\nCREATE EXTERNAL TABLE dbo.dailyopsstrikeratereport (\n\n[delcustomerref] nvarchar(4000),\n[billingcustomerid] int,\n[billingcustomername] nvarchar(4000),\n[deliverycustomerid] int,\n[deliverycustomername] nvarchar(4000),\n[deliverycustomersrouteid] int,\n[deliverycustomersroutecode] nvarchar(4000),\n[depot] nvarchar(4000),\n[date] date,\n[podyear] int,\n[podmonth] int,\n[podday] int,\n[noofparcels] int,\n\n\t\n\t)\n\n\tWITH (\n\tLOCATION = 'Structured Data/RPT/dailyopsstrikeratereport.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.dailyopsstrikeratereport \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "RPT",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/108_RptParcelLevel_revenueverticalreport')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Reports/ExternalTables"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.revenueverticalreport') )\n    DROP EXTERNAL TABLE dbo.revenueverticalreport\nGO\n\nCREATE EXTERNAL TABLE dbo.revenueverticalreport (\n\n[billingcustomerid] int,\n[billingcustomername] nvarchar(4000),\n[pickupcustid] int,\n[pickupcustomername] nvarchar(4000),\n[deliverycustid] int,\n[deliverycustomername] nvarchar(4000),\n[pickupsrouteid] int,\n[pickupsroutedescription] nvarchar(4000),\n[deliverysrouteid] int,\n[deliverysroutedescription] nvarchar(4000),\n[pickupzoneid] int,\n[pickupzonedescription] nvarchar(4000),\n[deliveryzoneid] int,\n[deliveryzonedescription] nvarchar(4000),\n[pickuplocationid] int,\n[pickuplocationdescription] nvarchar(4000),\n[deliverylocationid] int,\n[deliverylocationdescription] nvarchar(4000),\n[distributiondesc] nvarchar(4000),\n[date] date,\n[noofparcels] bigint,\n[kgsconsignedaweight] numeric(36,6),\n[kgsconsignedvweight] numeric(36,6),\n[kgsconsignedchargeweight] numeric(36,6),\n[normalchargeweight] numeric(36,6),\n[specialdeliverychargeweight] numeric(36,6),\n[totalcharge] numeric(36,6),\n[standardcharge] numeric(36,6),\n[sdconsolidatedcharge] numeric(36,6),\n[fuelsurcharge] numeric(36,6),\n[docsurcharge] numeric(36,6),\n[othersurcharge] numeric(36,6),\n[covidsurcharge] numeric(36,6),\n[exception] numeric(36,6),\n\n\n\n\t)\n\n\tWITH (\n\tLOCATION = 'Structured Data/RPT/revenueverticalreport.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.revenueverticalreport \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "RPT",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/109_RptParcelLevel_manifestweightanalysisreport')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Reports/ExternalTables"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.manifestweightanalysisreport') )\n    DROP EXTERNAL TABLE dbo.manifestweightanalysisreport\nGO\n\nCREATE EXTERNAL TABLE dbo.manifestweightanalysisreport (\n[manifestid] int,\n[billingcustomerid] int,\n[billingcustomer] nvarchar(4000),\n[deliverycustomerid] int,\n[deliverycustomer] nvarchar(4000),\n[[parcelno]] nvarchar(4000),\nconsignmentcref nvarchar(4000),\n[weight] float,\n[chargeweight] numeric(36,6),\n[volweight] float,\n[calculatedvolweight] numeric(36,6),\n[calculatedcubes] numeric(36,6),\n[customerbarcode] nvarchar(4000),\n[parceltype] nvarchar(4000),\n[ordercustomereference] nvarchar(4000),\n[ordercorderno] nvarchar(4000),\n[zonelocationdescription] nvarchar(4000),\n\n\t)\n\n\tWITH (\n\tLOCATION = 'Structured Data/RPT/manifestweightanalysisreport.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.manifestweightanalysisreport \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "RPT",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/110_RptParcelLevel_dcreceivingdailyreport')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Reports/ExternalTables"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.dcreceivingdailyreport') )\n    DROP EXTERNAL TABLE dbo.dcreceivingdailyreport\nGO\n\nCREATE EXTERNAL TABLE dbo.dcreceivingdailyreport (\n\t[billingcustomerid] int,\n\t[billingcustomername] nvarchar(4000),\n\t[deliverycustomerid] int,\n\t[deliverycustomername] nvarchar(4000),\n\t[date]  datetime2(7),\n\t[supplierdescription] nvarchar(4000),\n\t[purchaseordernumber] nvarchar(4000),\n\t[appointmentcode] nvarchar(4000),\n\t[appointmentstartdatetime] datetime2(7),\n\t[appointmentenddatetime] datetime2(7),\n\t[arrivedat] datetime2(7),\n\t[lastuploadeddate] datetime2(7),\n\t[dccode] nvarchar(4000),\n\t[ispandaenabled] int,\n\t[dcprepacklength] numeric(36,6),\n\t[dcprepackheight] numeric(36,6),\n\t[dcprepackwidth] numeric(36,6),\n\t[dctransfercode] nvarchar(4000),\n\t[dctransferacceptancedate] datetime2(7),\n\t[dctransferauditflag] int,\n\t[dctransfercounted] int,\n\t[dctransferuploaded] int,\n\t[parcelweight] float,\n\t[parcelvolweight] float,\n\t[parcelchargeweight] numeric(36,6),\n\t[waybillid] int,\n\t[consignid] int,\n\t[parcellocation] nvarchar(4000),\n\t)\n\n\tWITH (\n\tLOCATION = 'Structured Data/RPT/dcreceivingdailyreport.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.dcreceivingdailyreport \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "RPT",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/111_RptWaybillLevel_hiltiwbregionsummaryreport')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Reports/ExternalTables"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.hiltiwbregionsummaryreport') )\n    DROP EXTERNAL TABLE dbo.hiltiwbregionsummaryreport\nGO\n\nCREATE EXTERNAL TABLE dbo.hiltiwbregionsummaryreport (\n\t[billingcustomerid] int,\n\t[billingcustomername] nvarchar(4000),\n\n    [waybillid] int,\n    [hcode] nvarchar(4000),\n    [deliverycustomerid] int,\n    [deliverycustomername] nvarchar(4000),\n    [pickupcustomername] nvarchar(4000),\n    [date] DATETIME2,\n\n\t)\n\n\tWITH (\n\tLOCATION = 'Structured Data/RPT/hiltiwbregionsummaryreport.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.hiltiwbregionsummaryreport \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "RPT",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/112_RptParcelLevel_otdbillingreport')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Reports/ExternalTables"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.otdbillingreport') )\n    DROP EXTERNAL TABLE dbo.otdbillingreport\nGO\n\nCREATE EXTERNAL TABLE dbo.otdbillingreport (\n[barcode] nvarchar(4000),\n[billingcustomerid] int,\n[billingcustomername] nvarchar(4000),\n[deliverycustomerid] int,\n[deliverycustomername] nvarchar(4000),\n[cref] nvarchar(4000),\n[weight] float,\n[date] datetime2(7),\n[consigndate] datetime2(7),\n[poddate] datetime2(7),\n[deliverycustsrouteid] int,\n[deliverycustsroutedescription] nvarchar(4000),\n[currentlocationid] int,\n[currentlocation] nvarchar(4000),\n[deliverybranchid] int,\n[deliverybranch] nvarchar(4000),\n[localoutlying] nvarchar(4000),\n[parcelstatus] nvarchar(4000),\n[waybillid] int,\n\n\n\t)\n\n\tWITH (\n\tLOCATION = 'Structured Data/RPT/otdbillingreport.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.otdbillingreport \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "RPT",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/113_RptParcelLevel_ecombillingreport')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Reports/ExternalTables"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.ecombillingreport') )\n    DROP EXTERNAL TABLE dbo.ecombillingreport\nGO\n\nCREATE EXTERNAL TABLE dbo.ecombillingreport (\n[billingcustomerid] int,\n[billingcustomername] nvarchar(4000),\n[deliverycustomerid] int,\n[deliverycustomername] nvarchar(4000),\n[pickupcustomerid] int,\n[pickucustomername] nvarchar(4000),\n[barcode] nvarchar(4000),\n[weight] float,\n[volweight] float,\n[orderno] nvarchar(4000),\n[waybillid] int,\n[consigndate] datetime2(7),\n[date] datetime2(7),\n[deliverycreatedateldate] datetime2(7),\n[deliverypoddate] datetime2(7),\n[parceldescription] nvarchar(4000),\n[deliverycustomersroutedescription] nvarchar(4000),\n[ptype] int,\n[assembly] int,\n[deladres1] nvarchar(4000),\n[deladres2] nvarchar(4000),\n[delpcode] nvarchar(4000),\n[podcaptured] datetime2(7),\n[courier] nvarchar(4000),\n[parcellocationid] int,\n[parcellocationdescription] nvarchar(4000),\n[parceltypedescription] nvarchar(4000),\n\n\n\n\t)\n\n\tWITH (\n\tLOCATION = 'Structured Data/RPT/ecombillingreport.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.ecombillingreport \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "RPT",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/114_RptParcelLevel_mrpibtreport')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Reports/ExternalTables"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.mrpibtreport') )\n    DROP EXTERNAL TABLE dbo.mrpibtreport\nGO\n\nCREATE EXTERNAL TABLE dbo.mrpibtreport (\n[barcode] nvarchar(4000),\n[cref] nvarchar(4000),\n[date] datetime2(7),\n[consignmentdate] datetime2(7),\n[weight] float,\n[pickupcustomerid] int,\n[pickupcustomername] nvarchar(4000),\n[pickupcustomercref] nvarchar(4000),\n[pickupcustomersroute] nvarchar(4000),\n[delivercustomerid] int,\n[delivercustomername] nvarchar(4000),\n[delivercustomercref] nvarchar(4000),\n[delivercustomersroute] nvarchar(4000),\n[billcustomerid] int,\n[billcustomername] nvarchar(4000),\n[storeconfirmation] datetime2(7),\n[loadedflag] nvarchar(4000),\n[latestloaddate] datetime2(7),\n\n\n\t)\n\n\tWITH (\n\tLOCATION = 'Structured Data/RPT/mrpibtreport.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.mrpibtreport \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "RPT",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/301_RptTripLevel_linehaulleadtimereport')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Reports/ExternalTables"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.linehaulleadtimereport') )\n    DROP EXTERNAL TABLE dbo.linehaulleadtimereport\nGO\n\nCREATE EXTERNAL TABLE dbo.linehaulleadtimereport (\n\n[tripid] bigint,\n[date] datetime2(7),\n[lastonscan] datetime2(7),\n[lastoffscan] datetime2(7),\n[dispatchstartdate] datetime2(7),\n[exitdepotgeofence] datetime2(7),\n[loadclosedate] datetime2(7),\n[thirdpartyid] bigint,\n[thirdpartyname] nvarchar(4000),\n[bookingids] nvarchar(4000),\n[loadids] nvarchar(4000),\n[noofparcels] bigint,\n[weight] float,\n[chargeweight] numeric(36,6),\n[volweight] float,\n[volumiserweight] numeric(36,6),\n[lastonscantostartdispatch] bigint,\n[lastonscantoexitgeofence] bigint,\n[closeloadtostartdispatch] bigint,\n[closeloadtoexitgeofence] bigint,\n[effectiveloaddurationminutes] bigint,\n[effectiveoffloaddurationminutes] bigint,\n\t)\n\n\tWITH (\n\tLOCATION = 'Structured Data/RPT/linehaulleadtimereport.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.linehaulleadtimereport\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "RPT",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STBillZone')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Distribution"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STBillZone') )\n    DROP EXTERNAL TABLE dbo.D_STBillZone\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STBillZone (\n    [lms_billzone_id] int,\n    [lms_billzone_description] nvarchar(4000),\n    [lms_billzone_zonemasterid] int,\n    [lms_billzone_countryid] int\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stbillzone.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STBillZone \nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STBillZoneRoute')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Distribution"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STBillZoneRoute') )\n    DROP EXTERNAL TABLE dbo.D_STBillZoneRoute\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STBillZoneRoute (\n    [lms_billzoneroute_id] int,\n    [lms_billzoneroute_description] nvarchar(4000),\n    [lms_billzoneroute_zoneid] int,\n    [lms_billzoneroute_zrscode] nvarchar(4000),\n    [lms_billzoneroute_zonerouteorder] int,\n    [lms_billzoneroute_zonetype] nvarchar(4000),\n    [lms_billzoneroute_broutemasterid] int,\n    [lms_billzoneroute_broutecode] nvarchar(4000)\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stbillzoneroute.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STBillZoneRoute \nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STBusinessPartners')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Finance"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STBusinessPartners') )\n    DROP EXTERNAL TABLE dbo.D_STBusinessPartners\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STBusinessPartners (\n\t[sap_businesspartners_cardcode] nvarchar(4000),\n\t[sap_businesspartners_cardname] nvarchar(4000),\n\t[sap_businesspartners_cardtype] nvarchar(4000),\n\t[sap_businesspartners_groupcode] int,\n\t[sap_businesspartners_cmpprivate] nvarchar(4000),\n\t[sap_businesspartners_groupnum] int,\n\t[sap_businesspartners_creditline] numeric(18,2),\n\t[sap_businesspartners_debtline] numeric(18,2),\n\t[sap_businesspartners_discount] numeric(18,2),\n\t[sap_businesspartners_vatstatus] nvarchar(4000),\n\t[sap_businesspartners_lictradnum] nvarchar(4000),\n\t[sap_businesspartners_city] nvarchar(4000),\n\t[sap_businesspartners_u_boy_tb_0] nvarchar(4000),\n\t[sap_businesspartners_u_creditapp] nvarchar(4000),\n\t[sap_businesspartners_u_creditref] nvarchar(4000),\n\t[sap_businesspartners_u_creditcontract] nvarchar(4000),\n\t[sap_businesspartners_u_bee] int,\n\t[sap_businesspartners_u_depotmanager] nvarchar(4000),\n\t[sap_businesspartners_u_branchmanager] nvarchar(4000),\n\t[sap_businesspartners_u_petty_cash] numeric(18,2),\n\n\n    \n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/FINANCE/stbusinnesspartner.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STBusinessPartners \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STSAP",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STChartOfAccounts')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Finance"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STChartOfAccounts') )\n    DROP EXTERNAL TABLE dbo.D_STChartOfAccounts\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STChartOfAccounts (\n[sap_account_acctcode] nvarchar(4000),\n[sap_account_acctname] nvarchar(4000),\n[sap_account_accttype] nvarchar(4000),\n[sap_account_segment_0] nvarchar(4000),\n[sap_account_segment_1] nvarchar(4000),\n[sap_account_segment_2] nvarchar(4000),\n[sap_account_groupmask] int,\n[sap_account_fathernum] nvarchar(4000),\n[sap_account_frozenfor] nvarchar(4000),\n[sap_account_postable] nvarchar(4000),\n[sap_account_level5name] nvarchar(4000),\n[sap_account_level4name] nvarchar(4000),\n[sap_account_fathernum2] nvarchar(4000),\n[sap_account_level3name] nvarchar(4000),\n[sap_account_fathernum3] nvarchar(4000),\n[sap_account_level2name] nvarchar(4000),\n[sap_account_fathernum4] nvarchar(4000),\n[sap_account_level1name] nvarchar(4000),\n[sap_account_fathernum5] nvarchar(4000)\n\n    \n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/FINANCE/stchartofaccounts.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STChartOfAccounts \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STSAP",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STCustAcc')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Distribution"
				},
				"content": {
					"query": "\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STCustAcc') )\n    DROP EXTERNAL TABLE dbo.D_STCustAcc\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STCustAcc (\n    [lms_custacc_id] int,\n    [lms_custacc_description] nvarchar(4000),\n    [lms_custacc_billtype] int,\n    [lms_custacc_customerid] int,\n    [lms_custacc_cref] nvarchar(4000),\n    [lms_custacc_active] int,\n \n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stcustacc.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STCustAcc \nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STDepot')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Distribution"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STDepot') )\n    DROP EXTERNAL TABLE dbo.D_STDepot\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STDepot (\n    [md_depot_id] int,\n    [md_depot_itemlabel] nvarchar(4000),\n    [md_depot_itemvalue] nvarchar(4000),\n    [md_depot_depotname] nvarchar(4000),\n    [md_depot_parentdepotcode] nvarchar(4000),\n    [md_depot_division] nvarchar(4000),\n    [md_depot_address] nvarchar(4000),\n    [md_depot_latitude] numeric(18,2),\n    [md_depot_longitude] numeric(18,2),\n    [md_depot_sapdepreciationaccountcode] nvarchar(4000),\n    [md_depot_mfrauthoriser] nvarchar(4000),\n    [md_depot_mfrapplicationflag] int,\n    [md_depot_activeflag] int,\n    [md_depot_operationsmanager] nvarchar(4000),\n    [md_depot_branchmanager] nvarchar(4000),\n    [md_depot_fleetmanager] nvarchar(4000)\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stdepot.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STDepot \nGO\n\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STDepotMap')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Finance"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STDepotMap') )\n    DROP EXTERNAL TABLE dbo.D_STDepotMap\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STDepotMap (\n[excel_depotmap_locationid] int,\n[excel_depotmap_locationdescription]nvarchar(4000),\n[excel_depotmap_parentdepot]nvarchar(4000),\n[excel_depotmap_subdepot]nvarchar(4000),\n\n    \n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/FINANCE/stdepotmap.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STDepotMap \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STSAP",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STDepotOrv')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Distribution"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STDepotOrv') )\n    DROP EXTERNAL TABLE dbo.D_STDepotOrv\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STDepotOrv (\n    [orv_depot_id] bigint,\n    [orv_depot_name] nvarchar(4000),\n    [orv_depot_depcode] nvarchar(4000),\n    [orv_depot_address] nvarchar(4000),\n    [orv_depot_suburb] nvarchar(4000),\n    [orv_depot_city] nvarchar(4000),\n    [orv_depot_zipcode] nvarchar(4000),\n    [orv_depot_region] nvarchar(4000),\n    [orv_depot_lat] float,\n    [orv_depot_lng] float,\n    [orv_depot_placeid] nvarchar(4000),\n    [orv_depot_w3w] nvarchar(4000),\n    [orv_depot_insdate] datetime2(7),\n    [orv_depot_update] datetime2(7),\n    [orv_depot_lmsid] int,\n    [orv_depot_country] nvarchar(4000),\n    [orv_depot_status] nvarchar(4000)\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stdepotorv.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STDepotOrv \nGO\n\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STDispatchActionLog')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Linehaul"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STDispatchActionLog') )\n    DROP EXTERNAL TABLE dbo.D_STDispatchActionLog\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STDispatchActionLog (\n[orv_dispatchactionlog_id] bigint,\n [orv_dispatchactionlog_lat] float,\n [orv_dispatchactionlog_lng] float,\n [orv_dispatchactionlog_reason] nvarchar(4000),\n [orv_dispatchactionlog_mode] nvarchar(4000),\n [orv_dispatchactionlog_insdate] datetime2(7),\n [orv_dispatchactionlog_did] bigint,\n [orv_dispatchactionlog_action] nvarchar(4000),\n [orv_dispatchactionlog_uid] bigint\n\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stdispatchactionlog.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STDispatchActionLog \nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STDispatchSegment')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Distribution"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STDispatchSegment') )\n    DROP EXTERNAL TABLE dbo.D_STDispatchSegment\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STDispatchSegment (\n    [orv_dispatchsegment_id] bigint,\n    [orv_dispatchsegment_fromdelid] bigint,\n    [orv_dispatchsegment_todelid] bigint,\n    [orv_dispatchsegment_distance] int,\n    [orv_dispatchsegment_duration] int,\n    [orv_dispatchsegment_did] bigint,\n    [orv_dispatchsegment_tollcost] bigint,\n    [orv_dispatchsegment_fuelcost] bigint,\n    [orv_dispatchsegment_inscost] bigint,\n    [orv_dispatchsegment_labourcost] bigint,\n    [orv_dispatchsegment_maintcost] bigint,\n    [orv_dispatchsegment_cpicost] bigint,\n    [orv_dispatchsegment_vehicleclass] nvarchar(4000),\n    [orv_dispatchsegment_fromlat] float,\n    [orv_dispatchsegment_fromlng] float,\n    [orv_dispatchsegment_tolat] float,\n    [orv_dispatchsegment_tolng] float,\n    [orv_dispatchsegment_departuredate] datetime2(7),\n    [orv_dispatchsegment_arrivaldate] datetime2(7),\n    [orv_dispatchsegment_debriefeddate] datetime2(7),\n    [orv_dispatchsegment_optimized] int,\n    [orv_dispatchsegment_segmentorder] int,\n    [orv_dispatchsegment_customized] int,\n    [orv_dispatchsegment_parcelct] int\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stdispatchsegment.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STDispatchSegment\nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STDocumentCategories')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Finance"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STDocumentCategories') )\n    DROP EXTERNAL TABLE dbo.D_STDocumentCategories\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STDocumentCategories (\n    [sap_transactionsline_doccat] nvarchar(4000),\n    [sap_transactionsline_doccatname] nvarchar(4000),\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/FINANCE/stdocumentcategories.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STDocumentCategories \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STSAP",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STDriver')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Distribution"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STDriver') )\n    DROP EXTERNAL TABLE dbo.D_STDriver\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STDriver (\n[lms_driver_id] int,\n[lms_driver_ianyflag] int,\n[lms_driver_billcustid] int,\n[lms_driver_password] nvarchar(4000),\n[lms_driver_userid] nvarchar(4000),\n[lms_driver_staffid] nvarchar(4000),\n[lms_driver_branch] nvarchar(4000),\n[lms_driver_passportnum] nvarchar(4000),\n[lms_driver_passportcountry] nvarchar(4000),\n[lms_driver_residence] nvarchar(4000),\n[lms_driver_dob] datetime2(7),\n[lms_driver_passportexpiry] datetime2(7),\n[lms_driver_activeflag] int,\n[lms_driver_locid] int,\n[lms_driver_lastupdate] datetime2(7),\n[lms_driver_dateengaged] datetime2(7),\n[lms_driver_terminationdate] datetime2(7),\n[lms_driver_empid] int,\n[lms_driver_dname] nvarchar(4000),\n[lms_driver_fname] nvarchar(4000),\n[lms_driver_surname] nvarchar(4000),\n[lms_driver_knownasname] nvarchar(4000),\n[lms_driver_depotcode] nvarchar(4000),\n[lms_driver_jobcategory] nvarchar(4000),\n[lms_driver_jobtitle] nvarchar(4000),\n[lms_driver_employeecode] nvarchar(4000),\n[lms_driver_cellno] nvarchar(4000),\n[lms_driver_gender] nvarchar(4000),\n[lms_driver_employeestatus] nvarchar(4000),\n[lms_driver_nationnality] nvarchar(4000),\n[lms_driver_idno] nvarchar(4000),\n[lms_driver_statusflag] int,\n[lms_driver_idpassportno] nvarchar(4000),\n[lms_driver_ownerid] int,\n[lms_driver_islinehauldriver] int,\n[lms_driver_uid] nvarchar(4000),\n[orv_publicorvuser_id] int\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stdriver.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STDriver \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STDriverPay')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Linehaul"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STDriverPay') )\n    DROP EXTERNAL TABLE dbo.D_STDriverPay\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STDriverPay (\n    [tms_driverpay_id] bigint,\n    [tms_driverpay_routeid] bigint,\n    [tms_driverpay_triprate] int,\n    [tms_driverpay_mealrate] int,\n    [tms_driverpay_linkrate] int,\n    [tms_driverpay_sleepoverrate] int,\n    [tms_driverpay_numsleepover] int,\n    [tms_driverpay_nummeal] int,\n    [tms_driverpay_validfromdate] datetime2(7),\n    [tms_driverpay_validtodate] datetime2(7),\n    [tms_driverpay_insdate] datetime2(7),\n    [tms_driverpay_moddate] datetime2(7),\n    [tms_driverpay_insby] bigint,\n    [tms_driverpay_airtimerate] int,\n    [tms_driverpay_status] nvarchar(4000),\n    [tms_driverpay_legacyid] int,\n\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stdriverpay.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STDriverPay \nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STEndorsmentReasons')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Distribution"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STEndorsmentReasons') )\n    DROP EXTERNAL TABLE dbo.D_STEndorsmentReasons\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STEndorsmentReasons (\n\t[lmds_db_reasondetail_id] int,\n\t[lmds_db_reasondetail_description] nvarchar(4000),\n\t[lmds_db_reasondetail_reasongroupid] int,\n\t[lmds_db_reasondetail_orv_type] int,\n\t[lmds_db_reasondetail_active] int,\n\t[lmds_db_reasondetail_actionid] int,\n\t[lmds_db_reasondetail_cancelreason] int,\n\t[lmds_db_reasondetail_lmsreasonrule] int,\n\t[lmds_db_reasongroup_description] nvarchar(4000),\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stendorsmentreason.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STEndorsmentReasons \nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STFinanceDates')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Finance"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STFinanceDates') )\n    DROP EXTERNAL TABLE dbo.D_STFinanceDates\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STFinanceDates (\n[date_dim_id] int,\n[sap_transactionsheader_postingdate] date,\n[epoch] bigint,\n[actual_year] int,\n[actual_year_month] nvarchar(4000),\n[actual_month_name] nvarchar(4000),\n[actual_month_number] nvarchar(4000),\n[actual_day_name] nvarchar(4000),\n[actual_day_of_week] int,\n[is_week_day] int,\n[actual_day_of_month] int,\n[is_last_day_of_month] int,\n[last_day_of_month] nvarchar(4000),\n[actual_day_of_year] int,\n[actual_week_of_year] int,\n[actual_quarter_of_year] int,\n[fiscal_year] int,\n[fiscal_year_month] nvarchar(4000),\n[fiscal_month_sort] int,\n[actual_month_sort] int,\n[actual_week_of_year_sun] int,\n[actual_day_of_week_sun] int,\n\n\n    \n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/FINANCE/stfinancedates.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STFinanceDates \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STSAP",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STFuelCard')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Linehaul"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STFuelCard') )\n    DROP EXTERNAL TABLE dbo.D_STFuelCard\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STFuelCard (\n    [tms_fuelcard_id] bigint,\n    [tms_fuelcard_eid] bigint,\n    [tms_fuelcard_entity] nvarchar(4000),\n    [tms_fuelcard_baseprice] int,\n    [tms_fuelcard_routeid] bigint,\n    [tms_fuelcard_operatingpercentage] int,\n    [tms_fuelcard_insdate] datetime2(7),\n    [tms_fuelcard_activefromdate] datetime2(7),\n    [tms_fuelcard_activetodate] datetime2(7),\n    [tms_fuelcard_moddate] datetime2(7),\n    [tms_fuelcard_status] nvarchar(4000),\n    [tms_fuelcard_insby] bigint,\n    [tms_fuelcard_zoneid] bigint,\n    [tms_fuelcard_legacyid] bigint,\n\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stfuelcard.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STFuelCard \nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STFuelZone')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Linehaul"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STFuelZone') )\n    DROP EXTERNAL TABLE dbo.D_STFuelZone\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STFuelZone (\n    [tms_fuelzone_id] bigint,\n    [tms_fuelzone_zonename] nvarchar(4000),\n    [tms_fuelzone_rulingprice] int,\n    [tms_fuelzone_insdate] datetime2(7),\n    [tms_fuelzone_moddate] datetime2(7),\n    [tms_fuelzone_insby] bigint,\n    [tms_fuelzone_lastrulingprice] int,\n    [tms_fuelzone_legacyid] bigint,\n\n\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stfuelzone.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STFuelZone \nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STFuelZoneHistory')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Linehaul"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STFuelZoneHistory') )\n    DROP EXTERNAL TABLE dbo.D_STFuelZoneHistory\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STFuelZoneHistory (\n    [tms_fuelzonehistory_id] bigint,\n    [tms_fuelzonehistory_zoneid] bigint,\n    [tms_fuelzonehistory_rulingprice] int,\n    [tms_fuelzonehistory_validfromdate] bigint,\n    [tms_fuelzonehistory_validtodate] bigint,\n    [tms_fuelzonehistory_editby] bigint,\n    [tms_fuelzonehistory_insdate] datetime2(7),\n    [tms_fuelzonehistory_moddate] datetime2(7),\n\n\n\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stfuelzonehistory.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STFuelZoneHistory \nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STInServiceReasons')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Distribution"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STInServiceReasons') )\n    DROP EXTERNAL TABLE dbo.D_STInServiceReasons\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STInServiceReasons (\n\t[lms_inservicereason_id] int,\n\t[lms_inservicereason_reasongroupid] int,\n\t[lms_reasongroup_description] nvarchar(4000),\n\t[lms_inservicereason_reason] nvarchar(4000),\n\t[lms_inservicereason_valid] int\n\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stinservicereasons.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STInServiceReasons \nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STItems')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Finance"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STItems') )\n    DROP EXTERNAL TABLE dbo.D_STItems\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STItems (\n\t[sap_itemgroup_itmsgrpcod] int,\n\t[sap_itemgroup_itmsgrpnam] nvarchar(4000),\n\t[sap_item_itemcode] nvarchar(4000),\n\t[sap_item_itemname] nvarchar(4000),\n\t[sap_item_cstgrpcode] int,\n\t[sap_item_vatgourpsa] nvarchar(4000),\n\t[sap_item_codebars] nvarchar(4000),\n\t[sap_item_prchseitem] nvarchar(4000),\n\t[sap_item_sellitem] nvarchar(4000),\n\t[sap_item_cardcode] nvarchar(4000),\n\t[sap_item_suppcatnum] nvarchar(4000),\n\t[sap_item_invntitem] nvarchar(4000),\n\t[sap_item_u_boy_tb_0] nvarchar(4000),\n\t[sap_item_u_svctype] nvarchar(4000),\n\t[sap_item_u_depot] nvarchar(4000),\n\n    \n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/FINANCE/stitems.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STItems \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STSAP",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STLMSBillCustomer')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Distribution"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STLMSBillCustomer') )\n    DROP EXTERNAL TABLE dbo.D_STLMSBillCustomer\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STLMSBillCustomer (\n    [lms_customer_id] int,\n    [lms_customer_activeflag] int,\n    [lms_customer_adres1] nvarchar(4000),\n    [lms_customer_adres2] nvarchar(4000),\n    [lms_customer_appointment] int,\n    [lms_customer_areadesc] nvarchar(4000),\n    [lms_customer_autoemailflag] int,\n    [lms_customer_bill_routeid] int,\n    [lms_customer_bill_servicetypeid] int,\n    [lms_customer_bill_zonerouteid] int,\n    [lms_customer_brouteid] int,\n    [lms_customer_btype] int,\n    [lms_customer_ccdepot] nvarchar(4000),\n    [lms_customer_ccroute] nvarchar(4000),\n    [lms_customer_chainstoreflag] int,\n    [lms_customer_consperord] int,\n    [lms_customer_contactperson] nvarchar(4000),\n    [lms_customer_countrycode] nvarchar(4000),\n    [lms_customer_cref] nvarchar(4000),\n    [lms_customer_cref2] nvarchar(4000),\n    [lms_customer_crouteid] nvarchar(4000),\n    [lms_customer_ctype] int,\n    [lms_customer_custaccid] int,\n    [lms_customer_custgroupid] int,\n    [lms_customer_custmainid] int,\n    [lms_customer_custrateid] int,\n    [lms_customer_custroutegroupid] int,\n    [lms_customer_custservicedays] numeric(18,2),\n    [lms_customer_custservicetime] numeric(18,2),\n    [lms_customer_dc] int,\n    [lms_customer_defdelpcode] nvarchar(4000),\n    [lms_customer_deliverydepotid] int,\n    [lms_customer_deliverytype] nvarchar(4000),\n    [lms_customer_destinationlocationcode] nvarchar(4000),\n    [lms_customer_dualbillcourier] nvarchar(4000),\n    [lms_customer_email] nvarchar(4000),\n    [lms_customer_faxno] nvarchar(4000),\n    [lms_customer_fctype] int,\n    [lms_customer_flevy] numeric(18,2),\n    [lms_customer_incompleteorderchkflag] int,\n    [lms_customer_insertdate] datetime2(7),\n    [lms_customer_isbillto] int,\n    [lms_customer_isbooking] int,\n    [lms_customer_ishighvolume] int,\n    [lms_customer_ismanual] int,\n    [lms_customer_lastupdate] datetime2(7),\n    [lms_customer_locationtypecode] nvarchar(4000),\n    [lms_customer_maxcartons] int,\n    [lms_customer_name] nvarchar(4000),\n    [lms_customer_nddid] int,\n    [lms_customer_onholdid] int,\n    [lms_customer_pcode] nvarchar(4000),\n    [lms_customer_pcodeid] int,\n    [lms_customer_servrouteid] int,\n    [lms_customer_srouteid] int,\n    [lms_customer_storeid] int,\n    [lms_customer_suburb] nvarchar(4000),\n    [lms_customer_telno] nvarchar(4000),\n    [lms_customer_town] nvarchar(4000),\n    [lms_customer_volfact] numeric(18,2),\n    [lms_customer_wtype] int,\n    [orv_customer_id] bigint,\n    [orv_customer_name] nvarchar(4000),\n    [orv_customer_contact] nvarchar(4000),\n    [orv_customer_email] nvarchar(4000),\n    [orv_customer_mobileno] nvarchar(4000),\n    [orv_customer_landline] nvarchar(4000),\n    [orv_customer_ctype] nvarchar(4000),\n    [orv_customer_parent] bigint,\n    [orv_customer_lastupdate] datetime2(7),\n    [orv_customer_lmsid] int,\n    [orv_customer_srouteid] bigint,\n    [orv_customer_verified] int,\n    [orv_customer_defaultmaildomain] nvarchar(4000),\n    [orv_customer_autoassignticketid] bigint,\n    [orv_customer_sendinfodel] int,\n    [orv_customer_anonemailcustsupportemail] nvarchar(4000),\n    [orv_customer_anonemailcustsupporttel] nvarchar(4000),\n    [orv_customer_cref] int,\n    [orv_customer_division] int,\n    [orv_customer_ecomm] int,\n    [orv_customer_preverifiedaddid] bigint,\n    [orv_customer_mallname] nvarchar(4000),\n    [orv_customer_mallid] bigint,\n    [orv_customer_mallflag] int,\n    [orv_customer_malllmsid] bigint,\n    [orv_customer_highvolume] int,\n    [orv_customer_custreference] nvarchar(4000),\n    [orv_customer_unverdate] bigint,\n    [orv_customer_verdate] bigint,\n    [orv_customer_groupid] bigint,\n    [orv_customer_postalcodecustgroupid] nvarchar(4000),\n    [orv_customer_webhookauthcredential] nvarchar(4000),\n    [orv_customer_mergedinto] bigint,\n    [orv_customer_cusmergedate] bigint,\n    [orv_customer_customerref] nvarchar(4000),\n    [orv_customer_routemaint] int,\n    [lms_customer_billcustid] int,\n    [lms_customer_billcustbillcustid] int,\n    [lms_customer_billcustincovalue] int,\n    [lms_customer_billcustdaterangelimit] int,\n    [lms_customer_billcuststatus] int,\n    [lms_customer_billcustnote] nvarchar(4000),\n    [lms_customer_billcustcustgroupid] int,\n    [lms_customer_billcustactiveflag] int,\n    [lms_customer_billcustrouterateeffectivedate] datetime2(7),\n    [lms_customer_billcustratesaddedflag] int,\n    [lms_customer_billcustibtratesflag] int,\n    [lms_customer_billcustslidingscaleflag] int,\n    [lms_customer_billcustsapflag] int,\n    [lms_customer_billcustsapcode] nvarchar(4000),\n    [lms_customer_billcustsapcustname] nvarchar(4000),\n    [lms_customer_billcustaveragepclrateflag] int,\n    [lms_customer_billcustaveragepclkg] numeric(18,2),\n    [lms_customer_billcustbilldatestart] int,\n    [lms_customer_billcustitemcode] nvarchar(4000),\n    [lms_customer_billcustservicetypeflag] int,\n    [lms_customer_billcustreturnreverserateflag] int,\n    [lms_customer_billcustminnoofpcls] int,\n    [lms_customer_billcustproductcategoryid] int,\n    [lms_customer_billcustsamedayconsolidation] int,\n    [lms_customer_billcustrategroupid] int,\n    [lms_customer_billcustrisksurchargeflag] int,\n    [lms_customer_billcustapplyavgvolweight] int,\n    [lms_customer_billcustavgvolweight] numeric(18,2),\n    [lms_customer_billcustecommcustomerflag] int,\n    [lms_customer_billcustleadtimestructureid] int,\n    [lms_customer_billcustrecalcinprogress] int,\n    [lms_customer_billcustrecalcuserid] int,\n    [lms_customer_billcustneedwebhook] int,\n    [lms_customer_billcustroutemaint] int,\n    [lms_customer_billcustlhdistrflag] int,\n    [lms_customer_billcustgroupid] int,\n    [lms_customer_billcustgroupdescription] nvarchar(4000),\n    [lms_customer_billcustgroupgrouptypeid] int,\n    [lms_customer_billcustgrouptypeid] int,\n    [lms_customer_billcustgrouptypedescription] nvarchar(4000)\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stbillcustomer.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STLMSBillCustomer \nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STLMSBookingWeights')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Linehaul"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STLMSBookingWeights') )\n    DROP EXTERNAL TABLE dbo.D_STLMSBookingWeights\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STLMSBookingWeights (\n    [tms_lmsbookingweights_bookingid] bigint,\n    [tms_lmsbookingweights_loadid] bigint,\n\t[tms_lmsbookingweights_noofparcels] bigint,\n\t[tms_lmsbookingweights_weight] float,\n\t[tms_lmsbookingweights_chargeweight] decimal(36,6),\n\t[tms_lmsbookingweights_volweight] float,\n\t[tms_lmsbookingweights_volumiserweight] decimal(36,6),\n\t[tms_lmsbookingweights_billbustomer]  nvarchar(4000),\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stlmsbookingweights.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STLMSBookingWeights \nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STLMSDeliveryPickupCustomer')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Distribution"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STLMSDeliveryPickupCustomer') )\n    DROP EXTERNAL TABLE dbo.D_STLMSDeliveryPickupCustomer\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STLMSDeliveryPickupCustomer (\n    [lms_customer_id] int,\n    [lms_customer_activeflag] int,\n    [lms_customer_adres1] nvarchar(4000),\n    [lms_customer_adres2] nvarchar(4000),\n    [lms_customer_appointment] int,\n    [lms_customer_areadesc] nvarchar(4000),\n    [lms_customer_autoemailflag] int,\n    [lms_customer_bill_routeid] int,\n    [lms_customer_bill_servicetypeid] int,\n    [lms_customer_bill_zonerouteid] int,\n    [lms_customer_brouteid] int,\n    [lms_customer_btype] int,\n    [lms_customer_ccdepot] nvarchar(4000),\n    [lms_customer_ccroute] nvarchar(4000),\n    [lms_customer_chainstoreflag] int,\n    [lms_customer_consperord] int,\n    [lms_customer_contactperson] nvarchar(4000),\n    [lms_customer_countrycode] nvarchar(4000),\n    [lms_customer_cref] nvarchar(4000),\n    [lms_customer_cref2] nvarchar(4000),\n    [lms_customer_crouteid] nvarchar(4000),\n    [lms_customer_ctype] int,\n    [lms_customer_custaccid] int,\n    [lms_customer_custgroupid] int,\n    [lms_customer_custmainid] int,\n    [lms_customer_custrateid] int,\n    [lms_customer_custroutegroupid] int,\n    [lms_customer_custservicedays] numeric(18,2),\n    [lms_customer_custservicetime] numeric(18,2),\n    [lms_customer_dc] int,\n    [lms_customer_defdelpcode] nvarchar(4000),\n    [lms_customer_deliverydepotid] int,\n    [lms_customer_deliverytype] nvarchar(4000),\n    [lms_customer_destinationlocationcode] nvarchar(4000),\n    [lms_customer_dualbillcourier] nvarchar(4000),\n    [lms_customer_email] nvarchar(4000),\n    [lms_customer_faxno] nvarchar(4000),\n    [lms_customer_fctype] int,\n    [lms_customer_flevy] numeric(18,2),\n    [lms_customer_incompleteorderchkflag] int,\n    [lms_customer_insertdate] datetime2(7),\n    [lms_customer_isbillto] int,\n    [lms_customer_isbooking] int,\n    [lms_customer_ishighvolume] int,\n    [lms_customer_ismanual] int,\n    [lms_customer_lastupdate] datetime2(7),\n    [lms_customer_locationtypecode] nvarchar(4000),\n    [lms_customer_maxcartons] int,\n    [lms_customer_name] nvarchar(4000),\n    [lms_customer_nddid] int,\n    [lms_customer_onholdid] int,\n    [lms_customer_pcode] nvarchar(4000),\n    [lms_customer_pcodeid] int,\n    [lms_customer_servrouteid] int,\n    [lms_customer_srouteid] int,\n    [lms_customer_storeid] int,\n    [lms_customer_suburb] nvarchar(4000),\n    [lms_customer_telno] nvarchar(4000),\n    [lms_customer_town] nvarchar(4000),\n    [lms_customer_volfact] numeric(18,2),\n    [lms_customer_wtype] int,\n    [orv_customer_id] bigint,\n    [orv_customer_name] nvarchar(4000),\n    [orv_customer_contact] nvarchar(4000),\n    [orv_customer_email] nvarchar(4000),\n    [orv_customer_mobileno] nvarchar(4000),\n    [orv_customer_landline] nvarchar(4000),\n    [orv_customer_ctype] nvarchar(4000),\n    [orv_customer_parent] bigint,\n    [orv_customer_lastupdate] datetime2(7),\n    [orv_customer_lmsid] int,\n    [orv_customer_srouteid] bigint,\n    [orv_customer_verified] int,\n    [orv_customer_defaultmaildomain] nvarchar(4000),\n    [orv_customer_autoassignticketid] bigint,\n    [orv_customer_sendinfodel] int,\n    [orv_customer_anonemailcustsupportemail] nvarchar(4000),\n    [orv_customer_anonemailcustsupporttel] nvarchar(4000),\n    [orv_customer_cref] int,\n    [orv_customer_division] int,\n    [orv_customer_ecomm] int,\n    [orv_customer_preverifiedaddid] bigint,\n    [orv_customer_mallname] nvarchar(4000),\n    [orv_customer_mallid] bigint,\n    [orv_customer_mallflag] int,\n    [orv_customer_malllmsid] bigint,\n    [orv_customer_highvolume] int,\n    [orv_customer_custreference] nvarchar(4000),\n    [orv_customer_unverdate] bigint,\n    [orv_customer_verdate] bigint,\n    [orv_customer_groupid] bigint,\n    [orv_customer_postalcodecustgroupid] nvarchar(4000),\n    [orv_customer_webhookauthcredential] nvarchar(4000),\n    [orv_customer_mergedinto] bigint,\n    [orv_customer_cusmergedate] bigint,\n    [orv_customer_customerref] nvarchar(4000),\n    [orv_customer_routemaint] int,\n    [lms_customer_billcustid] int,\n    [lms_customer_billcustbillcustid] int,\n    [lms_customer_billcustincovalue] int,\n    [lms_customer_billcustdaterangelimit] int,\n    [lms_customer_billcuststatus] int,\n    [lms_customer_billcustnote] nvarchar(4000),\n    [lms_customer_billcustcustgroupid] int,\n    [lms_customer_billcustactiveflag] int,\n    [lms_customer_billcustrouterateeffectivedate] datetime2(7),\n    [lms_customer_billcustratesaddedflag] int,\n    [lms_customer_billcustibtratesflag] int,\n    [lms_customer_billcustslidingscaleflag] int,\n    [lms_customer_billcustsapflag] int,\n    [lms_customer_billcustsapcode] nvarchar(4000),\n    [lms_customer_billcustsapcustname] nvarchar(4000),\n    [lms_customer_billcustaveragepclrateflag] int,\n    [lms_customer_billcustaveragepclkg] numeric(18,2),\n    [lms_customer_billcustbilldatestart] int,\n    [lms_customer_billcustitemcode] nvarchar(4000),\n    [lms_customer_billcustservicetypeflag] int,\n    [lms_customer_billcustreturnreverserateflag] int,\n    [lms_customer_billcustminnoofpcls] int,\n    [lms_customer_billcustproductcategoryid] int,\n    [lms_customer_billcustsamedayconsolidation] int,\n    [lms_customer_billcustrategroupid] int,\n    [lms_customer_billcustrisksurchargeflag] int,\n    [lms_customer_billcustapplyavgvolweight] int,\n    [lms_customer_billcustavgvolweight] numeric(18,2),\n    [lms_customer_billcustecommcustomerflag] int,\n    [lms_customer_billcustleadtimestructureid] int,\n    [lms_customer_billcustrecalcinprogress] int,\n    [lms_customer_billcustrecalcuserid] int,\n    [lms_customer_billcustneedwebhook] int,\n    [lms_customer_billcustroutemaint] int,\n    [lms_customer_billcustlhdistrflag] int,\n    [lms_customer_billcustgroupid] int,\n    [lms_customer_billcustgroupdescription] nvarchar(4000),\n    [lms_customer_billcustgroupgrouptypeid] int,\n    [lms_customer_billcustgrouptypeid] int,\n    [lms_customer_billcustgrouptypedescription] nvarchar(4000)\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stdeliverypickupcustomer.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STLMSDeliveryPickupCustomer \nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STLMSParcelWeights')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Linehaul"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STLMSParcelWeights') )\n    DROP EXTERNAL TABLE dbo.D_STLMSParcelWeights\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STLMSParcelWeights (\n    [tms_lmsparcelweights_tripid] bigint,\n    [tms_lmsparcelweights_bookingid] bigint,\n    [tms_lmsparcelweights_loadid] bigint,\n    [tms_lmsparcelweights_parcelid] int,\n    [tms_lmsparcelweights_weight] float,\n    [tms_lmsparcelweights_chargeweight] decimal(36,6),\n    [tms_lmsparcelweights_volweight] float,\n    [tms_lmsparcelweights_volumiserweight] decimal(36,6),\n    [tms_lmsparcelweights_billbustomer] nvarchar(4000),\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stlmsparcelweights.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STLMSParcelWeights \nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STLMSTrackScanTime')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Linehaul"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STLMSTrackScanTime') )\n    DROP EXTERNAL TABLE dbo.D_STLMSTrackScanTime\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STLMSTrackScanTime (\n    [lms_track_loadid] int,\n    [lms_loads_masterloadid] int,\n    [lms_track_firstscan] datetime2(7),\n    [lms_track_lastscan] datetime2(7),\n    [lms_track_loadtime] bigint,\n   \n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stlmstrackscantime.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STLMSTrackScanTime \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STLocation')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Distribution"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STLocation') )\n    DROP EXTERNAL TABLE dbo.D_STLocation\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STLocation (\n    [lms_location_id] int,\n    [lms_location_description] nvarchar(4000),\n    [lms_location_code] nvarchar(4000),\n    [lms_location_collect] int,\n    [lms_location_pcode] nvarchar(4000),\n    [lms_location_custid] int,\n    [lms_location_retail] int,\n    [lms_location_scale] int,\n    [lms_location_custcode] int,\n    [lms_location_captureonly] int,\n    [lms_location_zoneid] int,\n    [lms_location_scantype] int,\n    [lms_location_billcustid] int,\n    [lms_location_latitude] nvarchar(4000),\n    [lms_location_longitude] nvarchar(4000),\n    [lms_location_holdlocid] int,\n    [lms_location_branchmanageruserid] int,\n    [lms_location_fwcostcentre] int,\n    [lms_location_fwuserid] int,\n    [lms_location_fwmultibusinessid] int,\n    [lms_location_glcode] nvarchar(4000),\n    [lms_location_hubcode] nvarchar(4000),\n    [lms_location_fwapikey] nvarchar(4000),\n    [lms_location_geolocationgroupid] int,\n    [lms_location_geolocationflag] int,\n    [lms_location_orvactive] int,\n    [lms_location_cref] nvarchar(4000),\n    [lms_location_tempcol] int,\n    [lms_location_holdlocparentlocid] int,\n    [lms_location_floorsize] int,\n    [lms_location_isstationidenabled] int,\n    [lms_location_activeflag] int,\n    [lms_location_masterlocid] int,\n    [lms_location_locationtypeid] int,\n    [lms_location_locationcode] nvarchar(4000)\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stlocation.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STLocation \nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STMDDepot')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/MasterData"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STMDDepot') )\n    DROP EXTERNAL TABLE dbo.D_STMDDepot\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STMDDepot (\n\n[md_depot_id] bigint,\n[md_depot_itemlabel] nvarchar(4000),\n[md_depot_depotname] nvarchar(4000),\n[md_depot_parentdepotcode] nvarchar(4000),\n[md_depot_division] nvarchar(4000),\n[md_depot_address] nvarchar(4000),\n[md_depot_latitude] numeric(18,2),\n[md_depot_longitude] numeric(18,2),\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/MD/stmddepot.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n--synapse@citylogisticsstorage.dfs.core.windows.net/Structured Data/MD/stmddepot.parquet\n\n\nSELECT TOP 100 * FROM dbo.D_STMDDepot \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STMD",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STMDPeople')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/MasterData"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STMDPeople') )\n    DROP EXTERNAL TABLE dbo.D_STMDPeople\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STMDPeople (\n    [md_people_id] bigint,\n    [md_people_empid] bigint,\n    [md_people_employeecode] nvarchar(4000),\n    [md_people_uid] nvarchar(4000),\n    [md_people_firstname] nvarchar(4000),\n    [md_people_lastname] nvarchar(4000),\n    [md_people_knownasname] nvarchar(4000),\n    [md_people_birthdate] bigint,\n    [md_people_gender] nvarchar(4000),\n    [md_people_cellno] nvarchar(4000),\n    [md_people_emailaddress] nvarchar(4000),\n    [md_people_nationality] nvarchar(4000),\n    [md_people_idnumber] nvarchar(4000),\n    [md_people_passportno] nvarchar(4000),\n    [md_people_passportcountrycode] nvarchar(4000),\n    [md_people_idpassportno] nvarchar(4000),\n    [md_people_dateengaged] datetime2(7),\n    [md_people_datejoinedgroup] datetime2(7),\n    [md_people_employeestatus] nvarchar(4000),\n    [md_people_terminationdate] datetime2(7),\n    [md_people_terminationreasonid] int,\n    [md_people_depotcode] nvarchar(4000),\n    [md_people_jobcategory] nvarchar(4000),\n    [md_people_jobtitle] nvarchar(4000),\n    [md_people_source] nvarchar(4000),\n    [md_people_insby] nvarchar(4000),\n    [md_people_islinehaul] int,\n    [md_people_companycode] nvarchar(4000),\n    [md_people_companydisplayname] nvarchar(4000),\n    [md_people_companyrulecode] nvarchar(4000),\n    [md_people_reportstoemployee] nvarchar(4000),\n    [md_people_reporttoempid] bigint,\n    [md_people_internalexternal] nvarchar(4000),\n    [md_people_driverid] bigint,\n    [md_people_licenseno] nvarchar(4000),\n    [md_people_driverrestriction] nvarchar(4000),\n    [md_people_issuecountry] nvarchar(4000),\n    [md_people_issuedate] datetime2(7),\n    [md_people_licensevalidfrom] datetime2(7),\n    [md_people_licensevalidto] datetime2(7),\n    [md_people_vehiclecode] nvarchar(4000),\n    [md_people_vehiclerestriction] nvarchar(4000),\n    [md_people_driverphoto] nvarchar(4000),\n    [md_people_licenseexpired] int,\n    [md_people_prdpcode] nvarchar(4000),\n    [md_people_prdpexpiry] datetime2(7),\n    [md_people_personalid] bigint,\n    [md_people_physicalblock] nvarchar(4000),\n    [md_people_physicalcitytown] nvarchar(4000),\n    [md_people_physicalcomplex] nvarchar(4000),\n    [md_people_physicalcountrycode] nvarchar(4000),\n    [md_people_physicaldistrictid] nvarchar(4000),\n    [md_people_physicallevelfloor] nvarchar(4000),\n    [md_people_physicalpostalcode] nvarchar(4000),\n    [md_people_physicalprovince] nvarchar(4000),\n    [md_people_physicalstreetfarmname] nvarchar(4000),\n    [md_people_physicalstreetnumber] nvarchar(4000),\n    [md_people_physicalsuburbdistrict] nvarchar(4000),\n    [md_people_physicalunitpostalnumber] nvarchar(4000),\n    [md_people_postaladdressid] nvarchar(4000),\n    [md_people_postaladdressservicetype] nvarchar(4000),\n    [md_people_postalblock] nvarchar(4000),\n    [md_people_postalcitytown] nvarchar(4000),\n    [md_people_postalcomplex] nvarchar(4000),\n    [md_people_postalconcat] nvarchar(4000),\n    [md_people_postaldistrictid] nvarchar(4000),\n    [md_people_postallevelfloor] nvarchar(4000),\n    [md_people_postalpostalcode] nvarchar(4000),\n    [md_people_postalprovince] nvarchar(4000),\n    [md_people_postalstreetfarmname] nvarchar(4000),\n    [md_people_postalstreetnumber] nvarchar(4000),\n    [md_people_postalsuburbdistrict] nvarchar(4000),\n    [md_people_postalunitpostalnumber] nvarchar(4000),\n    [md_people_racialgroup] nvarchar(4000),\n\n    \n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/MD/stmdpeople.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STMDPeople \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STMD",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STMDSapDepot')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Finance"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STMDSapDepot') )\n    DROP EXTERNAL TABLE dbo.D_STMDSapDepot\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STMDSapDepot (\n\n    [md_depot_id] bigint,\n    [md_depot_parentdepotcode] nvarchar(4000),\n\t[sap_transactionsline_subdepotcode] nvarchar(4000),\n\t[md_depot_depotname] nvarchar(4000),\n\t[md_depot_division] nvarchar(4000),\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/FINANCE/stmdsapdepot.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STMDSapDepot \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STSAP",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STMDVehicle')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/MasterData"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STMDVehicle') )\n    DROP EXTERNAL TABLE dbo.D_STMDVehicle\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STMDVehicle (\n\n    [md_vehicle_id] bigint,\n    [md_vehicle_uid] nvarchar(4000),\n    [md_vehicle_vehicletype] nvarchar(4000),\n    [md_vehicle_internalexternal] nvarchar(4000),\n    [md_vehicle_activeflag] bigint,\n    [md_vehicle_vehiclestatus] nvarchar(4000),\n    [md_vehicle_chassisnumber] nvarchar(4000),\n    [md_vehicle_depot] nvarchar(4000),\n    [md_vehicle_enginenumber] nvarchar(4000),\n    [md_vehicle_fleetcode] nvarchar(4000),\n    [md_vehicle_fueltype] nvarchar(4000),\n    [md_vehicle_registrationnumber] nvarchar(4000),\n    [md_vehicle_vehicleapplication] nvarchar(4000),\n    [md_vehicle_vehiclebodymake] nvarchar(4000),\n    [md_vehicle_vehiclecolour] nvarchar(4000),\n    [md_vehicle_vehiclemake] nvarchar(4000),\n    [md_vehicle_vehiclemodel] nvarchar(4000),\n    [md_vehicle_vehicleseries] nvarchar(4000),\n    [md_vehicle_year] int,\n    [md_vehicle_vehicleownerid] bigint,\n    [md_vehicle_vehicleowner] nvarchar(4000),\n    [md_vehicle_staffvehicleflag] int,\n    [md_vehicle_vehicletrailersize] nvarchar(4000),\n    [md_vehicle_vehicletrailertype] nvarchar(4000),\n    [md_vehicle_vehicleetollclass] nvarchar(4000),\n    [md_vehicle_brn] nvarchar(4000),\n    [md_vehicle_brnnumber] nvarchar(4000),\n    [md_vehicle_vehicleregisterno] nvarchar(4000),\n    [md_vehicle_vehiclebasicinsdate] datetime2(7),\n    [md_vehicle_insby] nvarchar(4000),\n    [md_vehicle_islinehaulvehicle] int,\n    [md_vehicle_internalheight] numeric(18,2),\n    [md_vehicle_internallength] numeric(18,2),\n    [md_vehicle_internalvolume] numeric(18,2),\n    [md_vehicle_internalwidth] numeric(18,2),\n    [md_vehicle_vehiclebasicmoddate] datetime2(7),\n    [md_vehicle_trailerflag] int,\n    [md_vehicle_workflowstatus] nvarchar(4000),\n    [md_vehicle_vehicleapproveddate] datetime2(7),\n    [md_vehicle_mmcode] nvarchar(4000),\n    [md_vehicle_tmplicenseexpiery] bigint,\n    [md_vehicle_currentadvertising] nvarchar(4000),\n    [md_vehicle_fuelcardnumber] nvarchar(4000),\n    [md_vehicle_fuelcardreceivedate] datetime2(7),\n    [md_vehicle_licenseexpirydate] datetime2(7),\n    [md_vehicle_licensefee] numeric(18,2),\n    [md_vehicle_vehicleadvertisingindicator] nvarchar(4000),\n    [md_vehicle_vehiclespeedgoverningdevice] nvarchar(4000),\n    [md_vehicle_oldregistrationnumber] nvarchar(4000),\n    [md_vehicle_oldchassisnumber] nvarchar(4000),\n    [md_vehicle_oldenginenumber] nvarchar(4000),\n    [md_vehicle_logbookrecord] nvarchar(4000),\n    [md_vehicle_adminstatus] nvarchar(4000),\n    [md_vehicle_vehicleadmincompletebydate] datetime2(7),\n    [md_vehicle_backdoorheight] numeric(18,2),\n    [md_vehicle_backdoorwidth] numeric(18,2),\n    [md_vehicle_externalheight] numeric(18,2),\n    [md_vehicle_externallength] numeric(18,2),\n    [md_vehicle_externalwidth] numeric(18,2),\n    [md_vehicle_fleximounts] int,\n    [md_vehicle_fuelcapacity] numeric(18,2),\n    [md_vehicle_grossvehiclemass] numeric(18,2),\n    [md_vehicle_numberoftyres] int,\n    [md_vehicle_numberoftyresrear] int,\n    [md_vehicle_numberoftyressteering] int,\n    [md_vehicle_payload] numeric(18,2),\n    [md_vehicle_peoplecarrier] int,\n    [md_vehicle_tareweight] numeric(18,2),\n    [md_vehicle_tyresize] nvarchar(4000),\n    [md_vehicle_tyresizealternative] nvarchar(4000),\n    [md_vehicle_targetfuelconsumption] numeric(18,2),\n    [md_vehicle_tankrange] numeric(18,2),\n    [md_vehicle_operationsstatus] nvarchar(4000),\n    [md_vehicle_vehiclevehicleoperationscompletebydate] datetime2(7),\n    [md_vehicle_assetnumber] nvarchar(4000),\n    [md_vehicle_contractenddate] datetime2(7),\n    [md_vehicle_contractnumber] nvarchar(4000),\n    [md_vehicle_contractstartdate] datetime2(7),\n    [md_vehicle_vehiclefinancecompany] nvarchar(4000),\n    [md_vehicle_budgetrmcpk] numeric(18,2),\n    [md_vehicle_budgettyrecpk] numeric(18,2),\n    [md_vehicle_retailvalue] numeric(18,2),\n    [md_vehicle_tradeinvalue] numeric(18,2),\n    [md_vehicle_loadbodyreplacementcost] numeric(18,2),\n    [md_vehicle_financestatus] nvarchar(4000),\n    [md_vehicle_vehiclefinancecompletebydate] datetime2(7),\n    [md_vehicle_trackingstatus] nvarchar(4000),\n    [md_vehicle_vehicletrackingcompletebydate] datetime2(7),\n    [md_vehicle_vehicletrackingmoddate] datetime2(7),\n    [md_vehicle_assetid] bigint,\n    [md_vehicle_providerid] bigint,\n    [md_vehicle_providername] nvarchar(4000),\n    [md_vehicle_serial] nvarchar(4000),\n    [md_vehicle_vehicletrackinginsdate] datetime2(7),\n    [md_vehicle_linkdate] datetime2(7),\n    [md_vehicle_linkby] bigint,\n    [md_vehicle_linkbyname] nvarchar(4000),\n    [md_vehicle_lmsid] int,\n    [md_vehicle_orvid] bigint,\n\n\n    \n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/MD/stmdvehicle.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STMDVehicle \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STMD",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STOpsRoute')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Linehaul"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STOpsRoute') )\n    DROP EXTERNAL TABLE dbo.D_STOpsRoute\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STOpsRoute (\n[tms_opsroute_id] bigint,\n[tms_opsroute_name] nvarchar(4000),\n[tms_opsroute_slug] nvarchar(4000),\n[tms_opsroute_parentroute] bigint,\n[tms_opsroute_insdate] datetime2(7),\n[tms_opsroute_moddate] datetime2(7),\n[tms_opsroute_insby] bigint,\n[tms_opsroute_status] nvarchar(4000),\n[tms_opsroute_crossborder] int,\n[tms_opsroute_roundtripdays] int,\n[tms_opsroute_distance] int,\n[tms_opsroute_duration] int,\n[tms_opsroute_numofuse] int,\n[tms_opsroute_legacyid] int,\n[tms_opsroute_local] int,\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stopsroute.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STOpsRoute \nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STPOLookup')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Finance"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STPOLookup') )\n    DROP EXTERNAL TABLE dbo.D_STPOLookup\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STPOLookup (\n\n[sap_polookup_polookupid] nvarchar(4000),\n[sap_polookup_baseref] nvarchar(4000),\n[sap_polookup_docnum]int,\n[sap_polookup_docdate] date\n\n    \n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/FINANCE/stpolookup.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STPOLookup \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STSAP",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STParentRoute')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Linehaul"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STParentRoute') )\n    DROP EXTERNAL TABLE dbo.D_STParentRoute\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STParentRoute (\n    [tms_parentroute_id] bigint,\n    [tms_parentroute_name] nvarchar(4000),\n    [tms_parentroute_slug] nvarchar(4000),\n    [tms_parentroute_parentroute] bigint,\n    [tms_parentroute_insdate] datetime2(7),\n    [tms_parentroute_moddate] datetime2(7),\n    [tms_parentroute_insby] bigint,\n    [tms_parentroute_status] nvarchar(4000),\n    [tms_parentroute_crossborder] int,\n    [tms_parentroute_roundtripdays] int,\n    [tms_parentroute_distance] int,\n    [tms_parentroute_duration] int,\n    [tms_parentroute_numofuse] int,\n    [tms_parentroute_legacyid] int,\n    [tms_parentroute_local] int\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stparentroute.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STParentRoute \nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STPlcDivertMapping')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Sorter"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STPlcDivertMapping') )\n    DROP EXTERNAL TABLE dbo.D_STPlcDivertMapping\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STPlcDivertMapping (\n[lms_plcdivertmapping_id] int,\n[lms_plcdivertmapping_autosystemid] int,\n[lms_plcdivertmapping_plcdivertid] int,\n[lms_plcdivertmapping_cldivertid] int,\n[lms_plcdivertmapping_divertdescription] nvarchar(4000),\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stplcdivertmapping.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STPlcDivertMapping \nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STPlcDivertSorting')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Sorter"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STPlcDivertSorting') )\n    DROP EXTERNAL TABLE dbo.D_STPlcDivertSorting\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STPlcDivertSorting (\n[excel_sorterdivertsorting_divertdescription] nvarchar(4000),\n[excel_sorterdivertsorting_divertside] nvarchar(4000),\n[excel_sorterdivertsorting_divertseq] int,\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stplcdivertsorting.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STPlcDivertSorting\nGO\n\n\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STPlcScanPoint')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Sorter"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STPlcScanPoint') )\n    DROP EXTERNAL TABLE dbo.D_STPlcScanPoint\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STPlcScanPoint (\n    [lms_plcscanpoint_id] int,\n    [lms_plcscanpoint_description] varchar(4000),\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stplcscanpoint.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STPlcScanPoint \nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STPlcStatus')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Sorter"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STPlcStatus') )\n    DROP EXTERNAL TABLE dbo.D_STPlcStatus\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STPlcStatus (\n    [lms_plcstatus_id] int,\n    [lms_plcstatus_description] nvarchar(4000),\n\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stplcstatus.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STPlcStatus \nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STPod')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Distribution"
				},
				"content": {
					"query": "\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STPod') )\n    DROP EXTERNAL TABLE dbo.D_STPod\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STPod (\n    [orv_pod_id] bigint,\n    [orv_pod_did] bigint,\n    [orv_pod_name] nvarchar(4000),\n    [orv_pod_poddate] datetime2(7),\n    [orv_pod_lat] float,\n    [orv_pod_lng] float,\n    [orv_pod_customerrating] int,\n    [orv_pod_customerfeedback] nvarchar(4000)\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stpod.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STPod \nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STRateComponent')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Linehaul"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STRateComponent') )\n    DROP EXTERNAL TABLE dbo.D_STRateComponent\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STRateComponent (\n    [tms_ratecomponent_id] bigint,\n    [tms_ratecomponent_profileid] bigint,\n    [tms_ratecomponent_ratetype] nvarchar(4000),\n    [tms_ratecomponent_amount] int,\n    [tms_ratecomponent_applyfuellevy] int,\n    [tms_ratecomponent_note] nvarchar(4000),\n    [tms_ratecomponent_insdate] datetime2(7),\n    [tms_ratecomponent_moddate] datetime2(7),\n    [tms_ratecomponent_insby] bigint,\n    [tms_ratecomponent_legacyid] int,\n\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stratecomponent.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STRateComponent \nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STRateProfile')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Linehaul"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STRateProfile') )\n    DROP EXTERNAL TABLE dbo.D_STRateProfile\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STRateProfile (\n    [tms_rateprofile_id] bigint,\n    [tms_rateprofile_eid] bigint,\n    [tms_rateprofile_entity] nvarchar(4000),\n    [tms_rateprofile_trailersize] nvarchar(4000),\n    [tms_rateprofile_routeid] bigint,\n    [tms_rateprofile_profiletype] nvarchar(4000),\n    [tms_rateprofile_fromdate] datetime2(7),\n    [tms_rateprofile_todate] datetime2(7),\n    [tms_rateprofile_insby] bigint,\n    [tms_rateprofile_status] nvarchar(4000),\n    [tms_rateprofile_insdate] datetime2(7),\n    [tms_rateprofile_moddate] datetime2(7),\n    [tms_rateprofile_note] nvarchar(4000),\n    [tms_rateprofile_legacyid] int,\n    [tms_rateprofile_usedfuelprice] int,\n\n\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/strateprofile.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STRateProfile \nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STRouteRateType')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Distribution"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STRouteRateType') )\n    DROP EXTERNAL TABLE dbo.D_STRouteRateType\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STRouteRateType (\n[lms_routeratetype_id] int,\n[lms_routeratetype_minkg] numeric(36,6),\n[lms_routeratetype_minrate] numeric(36,6),\n[lms_routeratetype_addrate] numeric(36,6),\n[lms_routeratetype_altaddrate] numeric(36,6),\n[lms_routeratetype_docfees] numeric(36,6),\n[lms_routeratetype_volumetric] int,\n[lms_routeratetype_description] nvarchar(4000),\n[lms_routeratetype_fuellevy] numeric(36,6),\n[lms_routeratetype_custaccid] int,\n[lms_routeratetype_billcustid] int,\n[lms_routeratetype_servicetime] numeric(36,6),\n[lms_routeratetype_servicedays] numeric(36,6),\n[lms_routeratetype_broutemasterid] int,\n[lms_routeratetype_cutofftime] numeric(36,6),\n\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/strouteratetype.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STRouteRateType \nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STSRoute')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Distribution"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STSRoute') )\n    DROP EXTERNAL TABLE dbo.D_STSRoute\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STSRoute (\n    [lms_sroute_id] int,\n    [lms_sroute_billcustid] int,\n    [lms_sroute_courierid] nvarchar(4000),\n    [lms_sroute_description] nvarchar(4000),\n    [lms_sroute_code] nvarchar(4000),\n    [lms_sroute_id2] int,\n    [lms_sroute_sroutegroupid] int,\n    [lms_sroute_zoneid] int,\n    [lms_sroute_broutemasterid] int,\n    [lms_sroute_localflag] int,\n    [lms_sroute_area] nvarchar(4000),\n    [lms_sroute_highvolume] int\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stsroute.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STSRoute \nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STTMSList')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Linehaul"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STTMSList') )\n    DROP EXTERNAL TABLE dbo.D_STTMSList\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STTMSList (\n    [tms_list_id] bigint,\n    [tms_list_slug] nvarchar(4000),\n    [tms_list_valuetype] nvarchar(4000),\n    [tms_list_descr] nvarchar(4000),\n    [tms_list_insdate] datetime2(7),\n    [tms_list_moddate] datetime2(7),\n    [tms_list_refcols] nvarchar(4000),\n    [tms_list_editable] int,\n    [tms_listitem_id] bigint,\n    [tms_listitem_lid] bigint,\n    [tms_listitem_itemlabel] nvarchar(4000),\n    [tms_listitem_itemvalue] nvarchar(4000),\n    [tms_listitem_sort] int,\n    [tms_listitem_mdid] bigint,\n\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/sttmslist.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STTMSList \nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STTMSPrimaryCustomer')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Linehaul"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STTMSPrimaryCustomer') )\n    DROP EXTERNAL TABLE dbo.D_STTMSPrimaryCustomer\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STTMSPrimaryCustomer (\n    [tms_customer_id] bigint,\n    [tms_customer_status] nvarchar(4000),\n    [tms_customer_name] nvarchar(4000),\n    [tms_customer_parentid] bigint,\n    [tms_customer_accountnumber] nvarchar(4000),\n    [tms_customer_revenuecode] nvarchar(4000),\n    [tms_customer_insdate] datetime2(7),\n    [tms_customer_moddate] datetime2(7),\n    [tms_customer_insby] bigint,\n    [tms_customer_opaqueid] nvarchar(4000),\n    [tms_customer_email] nvarchar(4000),\n    [tms_customer_areacode] nvarchar(4000),\n    [tms_customer_mobilenum] nvarchar(4000),\n    [tms_customer_legacyid] int,\n    [tms_customer_billingmethod] nvarchar(4000),\n    [tms_customer_adjustmetric] nvarchar(4000),\n    [tms_customer_adjustvalue] int,\n    [tms_customer_adjustnegativeallowed] int,\n    [tms_customer_adjustvalueusingprevmonth] int,\n    [tms_customer_excludefromnotinvoicedreport] int,\n    [tms_customer_logictype] nvarchar(4000),\n    [tms_customer_minliability] bigint\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stprimarycustomer.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STTMSPrimaryCustomer \nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STTMSSecondaryCustomer')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Linehaul"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STTMSSecondaryCustomer') )\n    DROP EXTERNAL TABLE dbo.D_STTMSSecondaryCustomer\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STTMSSecondaryCustomer (\n    [tms_customer_id] bigint,\n    [tms_customer_status] nvarchar(4000),\n    [tms_customer_name] nvarchar(4000),\n    [tms_customer_parentid] bigint,\n    [tms_customer_accountnumber] nvarchar(4000),\n    [tms_customer_revenuecode] nvarchar(4000),\n    [tms_customer_insdate] datetime2(7),\n    [tms_customer_moddate] datetime2(7),\n    [tms_customer_insby] bigint,\n    [tms_customer_opaqueid] nvarchar(4000),\n    [tms_customer_email] nvarchar(4000),\n    [tms_customer_areacode] nvarchar(4000),\n    [tms_customer_mobilenum] nvarchar(4000),\n    [tms_customer_legacyid] int,\n    [tms_customer_billingmethod] nvarchar(4000),\n    [tms_customer_adjustmetric] nvarchar(4000),\n    [tms_customer_adjustvalue] int,\n    [tms_customer_adjustnegativeallowed] int,\n    [tms_customer_adjustvalueusingprevmonth] int,\n    [tms_customer_excludefromnotinvoicedreport] int,\n    [tms_customer_logictype] nvarchar(4000),\n    [tms_customer_minliability] bigint\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stsecondarycustomer.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STTMSSecondaryCustomer \nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STThirdParty')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Linehaul"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STThirdParty') )\n    DROP EXTERNAL TABLE dbo.D_STThirdParty\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STThirdParty (\n    [tms_thirdparty_id] bigint,\n    [tms_thirdparty_name] nvarchar(4000),\n    [tms_thirdparty_areacode] nvarchar(4000),\n    [tms_thirdparty_telnumber] nvarchar(4000),\n    [tms_thirdparty_mobareacode] nvarchar(4000),\n    [tms_thirdparty_mobnumer] nvarchar(4000),\n    [tms_thirdparty_contact] nvarchar(4000),\n    [tms_thirdparty_address] nvarchar(4000),\n    [tms_thirdparty_streetno] nvarchar(4000),\n    [tms_thirdparty_zipcode] nvarchar(4000),\n    [tms_thirdparty_city] nvarchar(4000),\n    [tms_thirdparty_province] nvarchar(4000),\n    [tms_thirdparty_status] nvarchar(4000),\n    [tms_thirdparty_mdvoid] bigint,\n    [tms_thirdparty_registration] nvarchar(4000),\n    [tms_thirdparty_email] nvarchar(4000),\n    [tms_thirdparty_accnumber] nvarchar(4000),\n    [tms_thirdparty_insdate] datetime2(7),\n    [tms_thirdparty_moddate] datetime2(7),\n    [tms_thirdparty_gitstatus] nvarchar(4000),\n    [tms_thirdparty_legacyid] int,\n    [tms_thirdparty_lat] float,\n    [tms_thirdparty_lng] float,\n    [tms_thirdparty_note] nvarchar(4000),\n    [tms_thirdparty_revenuecode] nvarchar(4000),\n    [tms_thirdparty_country] nvarchar(4000)\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stthirdparty.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STThirdParty \nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STTransfers')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Distribution"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STTransfers') )\n    DROP EXTERNAL TABLE dbo.D_STTransfers\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STTransfers (\n\n[combinetransfers] nvarchar(4000),\n[prefix] nvarchar(4000),\n[lms_parcel_pdate] date,\n[transferoutdate] date,\n[existsinmrponly] int,\n[existsinlmsonly] int,\n[date]  date,\n[counter] int,\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/sttransfers.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STTransfers \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STUserOrv')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Distribution"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STUserOrv') )\n    DROP EXTERNAL TABLE dbo.D_STUserOrv\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STUserOrv (\n    [orv_orvuser_id] bigint,\n    [orv_orvuser_username] nvarchar(4000),\n    [orv_orvuser_fname] nvarchar(4000),\n    [orv_orvuser_lname] nvarchar(4000),\n    [orv_orvuser_empcode] nvarchar(4000),\n    [orv_orvuser_aka] nvarchar(4000),\n    [orv_orvuser_gender] nvarchar(4000),\n    [orv_orvuser_rsaid] nvarchar(4000),\n    [orv_orvuser_passportno] nvarchar(4000),\n    [orv_orvuser_idpassportno] nvarchar(4000),\n    [orv_orvuser_nationality] nvarchar(4000),\n    [orv_orvuser_termindate] datetime2(7),\n    [orv_orvuser_status] nvarchar(4000),\n    [orv_orvuser_depot] nvarchar(4000),\n    [orv_orvuser_jobcategory] nvarchar(4000),\n    [orv_orvuser_lmsid] bigint,\n    [orv_orvuser_vipid] bigint,\n    [orv_orvuser_crewlmsid] bigint,\n    [orv_orvuser_courierid] bigint\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/storvuser.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STUserOrv\nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STVehicle')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Distribution"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STVehicle') )\n    DROP EXTERNAL TABLE dbo.D_STVehicle\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STVehicle (\n\n    [lms_vehicle_id] int,\n    [lms_vehicle_fleetno] nvarchar(4000),\n    [lms_vehicle_regno] nvarchar(4000),\n    [lms_vehicle_uid] nvarchar(4000),\n    [orv_vehicle_id] bigint,\n    [orv_vehicle_tollclass] nvarchar(4000),\n    [orv_vehicle_sapcode] nvarchar(4000),\n    [orv_vehicle_lmsid] bigint,\n\n    \n    \n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stvehicle.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STVehicle \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STWaybillDelayReasons')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Distribution"
				},
				"content": {
					"query": "\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STWaybillDelayReasons') )\n    DROP EXTERNAL TABLE dbo.D_STWaybillDelayReasons\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STWaybillDelayReasons (\n    [lms_delayreason_id] bigint,\n    [lms_delayreason_description] nvarchar(4000),\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stwaybilldelayreasons.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STWaybillDelayReasons \nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STWaybillDeliveryStatusReasons')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Distribution"
				},
				"content": {
					"query": "\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STWaybillDeliveryStatusReasons') )\n    DROP EXTERNAL TABLE dbo.D_STWaybillDeliveryStatusReasons\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STWaybillDeliveryStatusReasons (\n    [lms_deliverystatus_id] bigint,\n    [lms_deliverystatus_description] nvarchar(4000),\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stwaybilldeliverystatusreasons.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STWaybillDeliveryStatusReasons \nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STWaybillInServiceReasons')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Distribution"
				},
				"content": {
					"query": "\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STWaybillInServiceReasons') )\n    DROP EXTERNAL TABLE dbo.D_STWaybillInServiceReasons\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STWaybillInServiceReasons (\n    [lms_inservicereason_id] bigint,\n    [lms_inservicereason_description] nvarchar(4000),\n    [lms_inservicereason_group] nvarchar(4000),\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stwaybillinservicereasons.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STWaybillInServiceReasons \nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STWaybillNotDBReasons')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Distribution"
				},
				"content": {
					"query": "\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STWaybillNotDBReasons') )\n    DROP EXTERNAL TABLE dbo.D_STWaybillNotDBReasons\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STWaybillNotDBReasons (\n    [lms_notdbreason_id] bigint,\n    [lms_notdbreason_description] nvarchar(4000),\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stwaybillnotdbreasons.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STWaybillNotDBReasons \nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STWaybillReasonDetails')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Distribution"
				},
				"content": {
					"query": "\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STWaybillReasonDetails') )\n    DROP EXTERNAL TABLE dbo.D_STWaybillReasonDetails\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STWaybillReasonDetails (\n    [lms_reasondetail_id] bigint,\n    [lms_reasondetail_description] nvarchar(4000),\n    [lms_reasondetail_group] nvarchar(4000),\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stwaybillreasondetails.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STWaybillReasonDetails \nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_STZone')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Distribution"
				},
				"content": {
					"query": "\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.D_STZone') )\n    DROP EXTERNAL TABLE dbo.D_STZone\nGO\n\nCREATE EXTERNAL TABLE dbo.D_STZone (\n    [lms_zone_id] int,\n    [lms_zone_code] nvarchar(4000),\n    [lms_zone_description] nvarchar(4000),\n    [lms_zone_isocode] nvarchar(4000),\n    [lms_zone_locid] int,\n    [lms_zone_zonemasterid] int \n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stzone.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.D_STZone \nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Initialise DB')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "CREATE MASTER KEY ENCRYPTION BY PASSWORD='@TY@p8ctNiHFM38tPWUoJs9sVU-PNAtVk6qwFXvcn';\n\n-- Create a database scoped credential in the Serverless SQL DB.\nCREATE DATABASE SCOPED CREDENTIAL SynapseUser WITH IDENTITY = 'Managed Identity';\n\nIF NOT EXISTS (SELECT * FROM sys.external_file_formats WHERE name = 'SynapseParquetFormat') \n\tCREATE EXTERNAL FILE FORMAT [SynapseParquetFormat] \n\tWITH ( FORMAT_TYPE = PARQUET)\nGO\n\nIF NOT EXISTS (SELECT * FROM sys.external_data_sources WHERE name = 'synapse_citylogisticsstorageprod_dfs_core_windows_net') \n\tCREATE EXTERNAL DATA SOURCE [synapse_citylogisticsstorage_dfs_core_windows_net] \n\tWITH (\n\t\tLOCATION = 'abfss://synapse@citylogisticsstorageprod.dfs.core.windows.net',\n\t\tCREDENTIAL = SynapseUser\n\t)\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "RPT",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/RptParcelLevel')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Reports/ExternalTables"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.RptParcelLevel') )\n    DROP EXTERNAL TABLE dbo.RptParcelLevel\nGO\n\nCREATE EXTERNAL TABLE dbo.RptParcelLevel (\n[lms_parcel_id] int,\n[lms_parcel_orderid] int,\n[lms_parcel_consignid] int,\n[lms_parcel_waybillid] int,\n[lms_parcel_locid] int,\n[lms_parcel_totcharge] numeric(36,6),\n[lms_parcel_dimms] float,\n[lms_parcel_noofpcls] int,\n[lms_parcel_pl] numeric(36,6),\n[lms_parcel_pw] numeric(36,6),\n[lms_parcel_ph] numeric(36,6),\n[lms_parcel_pdate] datetime2(7),\n[lms_parcel_acceptancedate] datetime2(7),\n[lms_parcel_barcode] nvarchar(4000),\n[lms_parcel_volweight] float,\n[lms_parcel_weight] float,\n[lms_parcel_chargeweight] numeric(36,6),\n[lms_parcel_pweight] float,\n[lms_parcel_ptype] int,\n[lms_parcel_handoverdate] datetime2(7),\n[lms_parcel_floorstatus] nvarchar(4000),\n[lms_parcel_claimid] int,\n[lms_parcel_datevolumised] datetime2(7),\n[lms_parcel_volumiserweight] numeric(36,6),\n[lms_parcel_volumiserlength] numeric(36,6),\n[lms_parcel_volumiserheight] numeric(36,6),\n[lms_parcel_volumiserwidth] numeric(36,6),\n[lms_parcel_volumiserid] int,\n[lms_parcel_lastupdate] datetime2(7),\n[lms_parcel_orderhodate] datetime2(7),\n[lms_parcel_orderpickupcustid] int,\n[lms_parcel_orderdelivercustid] int,\n[lms_parcel_orderbillcustid] int,\n[lms_parcel_orderspecdel] int,\n[lms_parcel_ordercustaccid] int,\n[lms_parcel_custaccdescription] nvarchar(4000),\n[lms_parcel_custacccref] nvarchar(4000),\n[lms_parcel_consignmentcdate] datetime2(7),\n[lms_parcel_consignmentdeliverby] datetime2(7),\n[lms_parcel_waybillpoddate] datetime2(7),\n[lms_parcel_waybilldate] datetime2(7),\n[lms_parcel_parceldetailid] int,\n[lms_parcel_parceldetailinservicestart] datetime2(7),\n[lms_parcel_parceldetaildeliverby] datetime2(7),\n[lms_parcel_parceldetailservicedays] int,\n[lms_parcel_parceldetailservicetime] numeric(36,6),\n[lms_parcel_parceldetailcutofftime] numeric(36,6),\n[lms_parcel_parceldetailappointment] datetime2(7),\n[lms_parcel_parceldetailnddid] int,\n[lms_parcel_parceldetailcustonholdstart] datetime2(7),\n[lms_parcel_parceldetailcustonholdend] datetime2(7),\n[lms_parcel_parceldetailsplitinitial] int,\n[lms_parcel_parceldetailweekendholidays] int,\n[lms_parcel_parceldetailinbound] int,\n[lms_parcel_parceldetailcourierid] nvarchar(4000),\n[lms_parcel_parceldetailserviceid] nvarchar(4000),\n[lms_parcel_parceldetailcustaccid] int,\n[lms_parcel_parceldetailbroutemasterid] int,\n[lms_parcel_parceldetailbroutemasterfromid] int,\n[lms_parcel_parceldetailbroutemastertoid] int,\n[lms_parcel_parceldetailleadtimeid] int,\n[lms_parcel_inserviceflag] int,\n[lms_parcel_actualleaddaysstartdate] datetime2(7),\n[lms_parcel_actualleaddaysenddate] datetime2(7),\n[lms_parcel_actualleaddaysinclwe] float,\n[orv_parcel_scandate] datetime2(7),\n[orv_parcel_scanstatus] nvarchar(4000),\n[orv_parcel_scanmode] nvarchar(4000),\n[orv_parcel_claimid] bigint,\n[orv_parcel_courierid] bigint,\n[orv_parcel_moddate] datetime2(7),\n[lms_parcel_parceldetaildeliverbyroundup] datetime2(7),\n[parcelagedifferenceinseconds] bigint,\n[lms_parcel_actualleaddays] float,\n[lms_parcel_age] nvarchar(4000),\n[lms_track_enddate] datetime2(7),\n[lms_track_floortimeend] datetime2(7),\n[lms_track_fromlocid] int,\n[lms_parcel_pdatetocdate] bigint,\n[lms_parcel_cdatetoenddate] bigint,\n[lms_parcel_startdatetoenddate] bigint,\n[lms_parcel_floortimeedndtoenddate] bigint,\n[lms_parcel_deliveryduration] bigint,\n[lms_cenplclpnrouting_id] int,\n[lms_cenplclpnrouting_autosystemid] int,\n[lms_cenplclpnrouting_lpn] nvarchar(4000),\n[lms_cenplclpnrouting_divertid] int,\n[lms_cenplclpnrouting_scanpoint] int,\n[lms_cenplclpnrouting_status] int,\n[lms_cenplclpnrouting_datereceived] datetime2(7),\n[lms_cenplclpnrouting_dateupdated] datetime2(7),\n[lms_cenplclpnrouting_plcdivertid] int,\n[lms_cenplclpnrouting_cldivertid] int,\n[lms_cenplclpnrouting_divertdescription] nvarchar(4000),\n[lms_cenplclpnrouting_elapsed_ms] bigint,\n[lms_cenplclpnrouting_elapsed1_ms] bigint,\n[lms_cenplclpnrouting_elapsed2_ms] bigint,\n[lms_cenplclpnrouting_elapsed3_ms] bigint,\n[lms_cenplclpnrouting_elapsed4_ms] bigint,\n[lms_cenplclpnrouting_elapsed5_ms] bigint,\n[lms_cenplclpnrouting_elapsed6_ms] bigint,\n[lms_parcel_orderbillcustname] nvarchar(4000),\n[lms_parcel_orderdelivercustname] nvarchar(4000),\n[lms_parcel_orderdelivercustcref] nvarchar(4000),\n[lms_parcel_orderdelivercustsrouteid] int,\n[lms_parcel_sroutedescriptiondelivercust] nvarchar(4000),\n[lms_parcel_orderpickupcustcref] nvarchar(4000),\n[lms_parcel_orderpickupcustsrouteid] int,\n[lms_parcel_sroutedescriptionpickupcust] nvarchar(4000),\n[lms_damagedparcelaudit_id] int,\n[lms_damagedparcelaudit_barcode] nvarchar(4000),\n[lms_damagedparcelaudit_auditdatetime] datetime2(7),\n[lms_damagedparcelaudit_quantity] int,\n[lms_damagedparcelaudit_weight] numeric(36,6),\n[lms_damagedparcelaudit_flutemeasure] numeric(36,6),\n[lms_damagedparcelaudit_shrinkwrap] int,\n[lms_damagedparcelaudit_comment] nvarchar(4000),\n[lms_damagedparcelaudit_utilization] float,\n[lms_damagedparcelaudit_missing] int,\n[lms_damagedparcelaudit_damaged] int,\n[lms_damagedparcelaudit_tapetype] nvarchar(4000),\n[lms_damagedparcelaudit_auditby] nvarchar(4000),\n[lms_damagedparcelaudit_location] nvarchar(4000),\n[lms_csdmessage_loadid] nvarchar(4000),\n[lms_csdmessage_msdate] datetime2(7),\n[lms_csdmessage_msbody] nvarchar(4000),\n[lms_csdmessage_tolocation] nvarchar(4000),\n[lms_csdmessage_fullname] nvarchar(4000),\n[lms_csdmessage_fromlocation] nvarchar(4000),\n[lms_parcel_ordercorderno] nvarchar(4000),\n[lms_parcel_calculatedvolumetric] numeric(36,6),\n[lms_parcel_originalgreater] nvarchar(4000),\n[lms_parcel_volumiserlengthcm] numeric(36,6),\n[lms_parcel_volumiserwidthcm] numeric(36,6),\n[lms_parcel_volumiserheightcm] numeric(36,6),\n[lms_parcel_weightkg] numeric(36,6),\n[lms_parcel_calculatedvolumisedvolumetric] numeric(36,6),\n[lms_parcel_volumisedgreater] nvarchar(4000),\n[lms_volumiserlocation_location] nvarchar(4000),\n[lms_volumiserlocation_description] nvarchar(4000),\n[lms_parcel_locationdescription] nvarchar(4000),\n\n\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/RPT/rptparcellevel.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.RptParcelLevel \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "RPT",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/T_STBooking')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Linehaul"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.T_STBooking') )\n    DROP EXTERNAL TABLE dbo.T_STBooking\nGO\n\nCREATE EXTERNAL TABLE dbo.T_STBooking (\n    [tms_booking_id] bigint,\n    [tms_booking_allocatedby] bigint,\n    [tms_booking_allocateddate] datetime2(7),\n    [tms_booking_assetreleasedate] datetime2(7),\n    [tms_booking_billingmethod] nvarchar(4000),\n    [tms_booking_billingstatus] nvarchar(4000),\n    [tms_booking_convertedbookingid] bigint,\n    [tms_booking_cargoweight] float,\n    [tms_booking_customerid] bigint,\n    [tms_booking_childcustomerid] bigint,\n    [tms_booking_insby] bigint,\n    [tms_booking_insbyname] nvarchar(4000),\n    [tms_booking_cref1] nvarchar(4000),\n    [tms_booking_cref2] nvarchar(4000),\n    [tms_booking_cubicvolume] float,\n    [tms_booking_customeragreedrate] bigint,\n    [tms_booking_customerrate] bigint,\n    [tms_booking_deliverydate] datetime2(7),\n    [tms_booking_directbooking] int,\n    [tms_booking_dispatchdate] datetime2(7),\n    [tms_booking_docdate] datetime2(7),\n    [tms_booking_docstatus] nvarchar(4000),\n    [tms_booking_emailrequester] nvarchar(4000),\n    [tms_booking_frombranch] nvarchar(4000),\n    [tms_booking_frombranchid] bigint,\n    [tms_booking_insdate] datetime2(7),\n    [tms_booking_invoiceddate] datetime2(7),\n    [tms_booking_invoiceid] bigint,\n    [tms_booking_legacyid] bigint,\n    [tms_booking_local] int,\n    [tms_booking_manifests] nvarchar(4000),\n    [tms_booking_moddate] datetime2(7),\n    [tms_booking_note] nvarchar(4000),\n    [tms_booking_opsrouteid] bigint,\n    [tms_booking_opsroutename] nvarchar(4000),\n    [tms_booking_originalpod] int,\n    [tms_booking_pickupdate] datetime2(7),\n    [tms_booking_ponumber] nvarchar(4000),\n    [tms_booking_routeid] bigint,\n    [tms_booking_routename] nvarchar(4000),\n    [tms_booking_status] nvarchar(4000),\n    [tms_booking_thirdpartyagreedrate] bigint,\n    [tms_booking_thirdpartydriverflag] int,\n    [tms_booking_thirdpartyid] bigint,\n    [tms_booking_thirdpartyrate] bigint,\n    [tms_booking_thirdpartyreason] nvarchar(4000),\n    [tms_booking_thirdpartytrailerflag] int,\n    [tms_booking_thirdpartyvehicleflag] int,\n    [tms_booking_tobranch] nvarchar(4000),\n    [tms_booking_tobranchid] bigint,\n    [tms_booking_trailersize] nvarchar(4000),\n    [tms_booking_trailertype] nvarchar(4000),\n    [tms_booking_transporter] nvarchar(4000),\n    [tms_booking_tripid] bigint,\n    [tms_booking_reason] nvarchar(4000),\n    [tms_booking_source] nvarchar(4000),\n    [tms_booking_type] nvarchar(4000),\n    [tms_booking_billingrouteid] bigint,\n    [tms_booking_billingroutename] nvarchar(4000)\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stbooking.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.T_STBooking \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/T_STCollection')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Distribution"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.T_STCollection') )\n    DROP EXTERNAL TABLE dbo.T_STCollection\nGO\n\nCREATE EXTERNAL TABLE dbo.T_STCollection (\n[lms_collect_id] int,\n[lms_collect_date] datetime2(7),\n[lms_collect_cservice] nvarchar(4000),\n[lms_collect_billcust] int,\n[lms_collect_pickupcustid] int,\n[lms_collect_delivercustid] int,\n[lms_collect_locationid] int,\n[lms_collect_noofparcels] nvarchar(4000),\n[lms_collect_weight] nvarchar(4000),\n[lms_collect_timeready] nvarchar(4000),\n[lms_collect_collectrefno] nvarchar(4000),\n[lms_collect_timeclosed] nvarchar(4000),\n[lms_collect_courierid] nvarchar(4000),\n[lms_collect_confirmeddate] datetime2(7),\n[lms_collect_senddate] datetime2(7),\n[lms_collect_orderid] int,\n[lms_collect_reason] nvarchar(4000),\n[lms_collect_cubes] numeric(36,6),\n[lms_collect_custref] nvarchar(4000),\n[lms_collect_custref3] nvarchar(4000),\n[lms_collect_custaccid] int,\n[lms_collect_collectparentid] int,\n[lms_collect_duedate] datetime2(7),\n[lms_collect_collectstatusid] int,\n[lms_collect_collectstatus] nvarchar(4000),\n[lms_collect_bill_collectstatusid] int,\n[lms_collect_cancelreason] int,\n[lms_collect_cancelcomments] nvarchar(4000),\n[lms_collect_loadid] int,\n[lms_collect_custref2] nvarchar(4000),\n[lms_collect_created] datetime2(7),\n[lms_collect_oncollection] datetime2(7),\n[lms_collect_collected] datetime2(7),\n[lms_collect_complete] datetime2(7),\n[lms_collect_cancelled] datetime2(7),\n[lms_collect_failed] datetime2(7),\n[lms_collect_ctype] int,\n[lms_collect_quoteid] int,\n[lms_collect_lastupdate] datetime2(7),\n[lms_collect_notes] nvarchar(4000),\n[lms_collect_docurl] nvarchar(4000),\n[lms_collect_docdate] datetime2(7),\n[lms_collect_oldloadid] int,\n[orv_draftcollection_id] bigint,\n[orv_draftcollection_tolocid] int,\n[orv_draftcollection_fromlocid] int,\n[orv_draftcollection_loadid] int,\n[orv_draftcollection_status] nvarchar(4000),\n[orv_draftcollection_importedate] datetime2(7),\n[orv_draftcollection_fromstr] nvarchar(4000),\n[orv_draftcollection_tostr] nvarchar(4000),\n[orv_draftcollection_did] bigint,\n[orv_draftcollection_mergedlmsid] int,\n[orv_draftcollection_rejectdate] datetime2(7),\n[orv_draftcollection_confirmdate] datetime2(7),\n[orv_draftcollection_nocollection] int,\n[orv_draftcollection_customerref] nvarchar(4000),\n[orv_draftcollection_collectionwaybillid] bigint,\n[orv_draftcollection_collectbydate] datetime2(7),\n[orv_draftcollection_rejectreason] nvarchar(4000),\n[orv_draftcollection_cancelctr] int,\n[orv_draftcollection_courierid] bigint,\n[orv_draftcollection_collectstatusid] int,\n[orv_delivery_id] bigint,\n[orv_delivery_did] bigint,\n[orv_delivery_waybillid] bigint,\n[orv_delivery_address] nvarchar(4000),\n[orv_delivery_town] nvarchar(4000),\n[orv_delivery_province] nvarchar(4000),\n[orv_delivery_country] nvarchar(4000),\n[orv_delivery_lat] float,\n[orv_delivery_lng] float,\n[orv_delivery_placeid] nvarchar(4000),\n[orv_delivery_what3words] nvarchar(4000),\n[orv_delivery_zipcode] nvarchar(4000),\n[orv_delivery_eta] datetime2(7),\n[orv_delivery_actualeta] datetime2(7),\n[orv_delivery_deliverydate] datetime2(7),\n[orv_delivery_deliverby] datetime2(7),\n[orv_delivery_deliveryorder] int,\n[orv_delivery_tsgeofenceenter] datetime2(7),\n[orv_delivery_tsscanningstart] datetime2(7),\n[orv_delivery_tsscanningstop] datetime2(7),\n[orv_delivery_tspodsignature] datetime2(7),\n[orv_delivery_aid] bigint,\n[orv_delivery_numprcls] int,\n[orv_delivery_geocodingstatus] nvarchar(4000),\n[orv_delivery_skipdate] datetime2(7),\n[orv_delivery_skipreason] nvarchar(4000),\n[orv_delivery_status] nvarchar(4000),\n[orv_delivery_partialmatch] int,\n[orv_delivery_approximated] int,\n[orv_delivery_lmsorder] int,\n[orv_delivery_optimizedorder] int,\n[orv_delivery_seal] nvarchar(4000),\n[orv_delivery_customerinvoice] int,\n[orv_delivery_customerdn] int,\n[orv_delivery_grv] nvarchar(4000),\n[orv_delivery_chepslip] int,\n[orv_delivery_cleandelivery] int,\n[orv_delivery_trackingcode] nvarchar(4000),\n[orv_delivery_mode] nvarchar(4000),\n[orv_delivery_customorder] int,\n[orv_delivery_verified] int,\n[orv_delivery_altered] int,\n[orv_delivery_orderid] int,\n[orv_delivery_mergeid] bigint,\n[orv_delivery_orderref] nvarchar(4000),\n[orv_delivery_drivernote] nvarchar(4000),\n[orv_delivery_guid] bigint,\n[orv_delivery_timedefinite] int,\n[orv_delivery_orvcode] int,\n[orv_delivery_vaid] bigint,\n[orv_delivery_delegatedate] datetime2(7),\n[orv_delivery_delegatemode] nvarchar(4000),\n[orv_delivery_debriefmode] nvarchar(4000),\n[orv_delivery_debriefed] int,\n[orv_delivery_insdate] datetime2(7),\n[orv_delivery_reviseddebrief] int,\n[orv_delivery_lmsdebriefed] int,\n[orv_delivery_uncanceldate] datetime2(7),\n[orv_delivery_podlaterdate] datetime2(7),\n[orv_delivery_podmanualdate] datetime2(7),\n[orv_delivery_manualdebriefreason] nvarchar(4000),\n[orv_delivery_submanualdebriefreason] nvarchar(4000),\n[orv_delivery_outofgeofencereason] nvarchar(4000),\n[orv_delivery_tsentergeofenceapp] datetime2(7),\n[orv_delivery_tsexitgeofenceapp] datetime2(7),\n[orv_delivery_comebacklater] bigint,\n[orv_delivery_uncancelusername] nvarchar(4000),\n[orv_delivery_cref] int,\n[orv_delivery_division] int,\n[orv_delivery_descriptionofgoods] nvarchar(4000),\n[orv_delivery_skiplng] float,\n[orv_delivery_skiplat] float,\n[orv_delivery_courierid] bigint,\n[orv_delivery_posreason] nvarchar(4000),\n[orv_delivery_moddate] datetime2(7),\n[orv_delivery_mallid] int,\n[orv_delivery_mallgid] bigint,\n[orv_delivery_lmsidskipreason] bigint,\n[orv_delivery_deleteforimport] int,\n[orv_delivery_podoutofgeofence] int,\n[orv_delivery_originalcustomerid] bigint,\n[orv_pod_id] bigint,\n[orv_pod_name] nvarchar(4000),\n[orv_pod_poddate] datetime2(7),\n[orv_pod_lat] float,\n[orv_pod_lng] float,\n[orv_pod_customerrating] int,\n[orv_pod_customerfeedback] nvarchar(4000),\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stcollection.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.T_STCollection \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/T_STCollection_3MYoY')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Distribution"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.T_STCollection_3MYoY') )\n    DROP EXTERNAL TABLE dbo.T_STCollection_3MYoY\nGO\n\nCREATE EXTERNAL TABLE dbo.T_STCollection_3MYoY (\n[lms_collect_id] int,\n[lms_collect_date] datetime2(7),\n[lms_collect_cservice] nvarchar(4000),\n[lms_collect_billcust] int,\n[lms_collect_pickupcustid] int,\n[lms_collect_delivercustid] int,\n[lms_collect_locationid] int,\n[lms_collect_noofparcels] nvarchar(4000),\n[lms_collect_weight] nvarchar(4000),\n[lms_collect_timeready] nvarchar(4000),\n[lms_collect_collectrefno] nvarchar(4000),\n[lms_collect_timeclosed] nvarchar(4000),\n[lms_collect_courierid] nvarchar(4000),\n[lms_collect_confirmeddate] datetime2(7),\n[lms_collect_senddate] datetime2(7),\n[lms_collect_orderid] int,\n[lms_collect_reason] nvarchar(4000),\n[lms_collect_cubes] numeric(36,6),\n[lms_collect_custref] nvarchar(4000),\n[lms_collect_custref3] nvarchar(4000),\n[lms_collect_custaccid] int,\n[lms_collect_collectparentid] int,\n[lms_collect_duedate] datetime2(7),\n[lms_collect_collectstatusid] int,\n[lms_collect_collectstatus] nvarchar(4000),\n[lms_collect_bill_collectstatusid] int,\n[lms_collect_cancelreason] int,\n[lms_collect_cancelcomments] nvarchar(4000),\n[lms_collect_loadid] int,\n[lms_collect_custref2] nvarchar(4000),\n[lms_collect_created] datetime2(7),\n[lms_collect_oncollection] datetime2(7),\n[lms_collect_collected] datetime2(7),\n[lms_collect_complete] datetime2(7),\n[lms_collect_cancelled] datetime2(7),\n[lms_collect_failed] datetime2(7),\n[lms_collect_ctype] int,\n[lms_collect_quoteid] int,\n[lms_collect_lastupdate] datetime2(7),\n[lms_collect_notes] nvarchar(4000),\n[lms_collect_docurl] nvarchar(4000),\n[lms_collect_docdate] datetime2(7),\n[lms_collect_oldloadid] int,\n[orv_draftcollection_id] bigint,\n[orv_draftcollection_tolocid] int,\n[orv_draftcollection_fromlocid] int,\n[orv_draftcollection_loadid] int,\n[orv_draftcollection_status] nvarchar(4000),\n[orv_draftcollection_importedate] datetime2(7),\n[orv_draftcollection_fromstr] nvarchar(4000),\n[orv_draftcollection_tostr] nvarchar(4000),\n[orv_draftcollection_did] bigint,\n[orv_draftcollection_mergedlmsid] int,\n[orv_draftcollection_rejectdate] datetime2(7),\n[orv_draftcollection_confirmdate] datetime2(7),\n[orv_draftcollection_nocollection] int,\n[orv_draftcollection_customerref] nvarchar(4000),\n[orv_draftcollection_collectionwaybillid] bigint,\n[orv_draftcollection_collectbydate] datetime2(7),\n[orv_draftcollection_rejectreason] nvarchar(4000),\n[orv_draftcollection_cancelctr] int,\n[orv_draftcollection_courierid] bigint,\n[orv_draftcollection_collectstatusid] int,\n[orv_delivery_id] bigint,\n[orv_delivery_did] bigint,\n[orv_delivery_waybillid] bigint,\n[orv_delivery_address] nvarchar(4000),\n[orv_delivery_town] nvarchar(4000),\n[orv_delivery_province] nvarchar(4000),\n[orv_delivery_country] nvarchar(4000),\n[orv_delivery_lat] float,\n[orv_delivery_lng] float,\n[orv_delivery_placeid] nvarchar(4000),\n[orv_delivery_what3words] nvarchar(4000),\n[orv_delivery_zipcode] nvarchar(4000),\n[orv_delivery_eta] datetime2(7),\n[orv_delivery_actualeta] datetime2(7),\n[orv_delivery_deliverydate] datetime2(7),\n[orv_delivery_deliverby] datetime2(7),\n[orv_delivery_deliveryorder] int,\n[orv_delivery_tsgeofenceenter] datetime2(7),\n[orv_delivery_tsscanningstart] datetime2(7),\n[orv_delivery_tsscanningstop] datetime2(7),\n[orv_delivery_tspodsignature] datetime2(7),\n[orv_delivery_aid] bigint,\n[orv_delivery_numprcls] int,\n[orv_delivery_geocodingstatus] nvarchar(4000),\n[orv_delivery_skipdate] datetime2(7),\n[orv_delivery_skipreason] nvarchar(4000),\n[orv_delivery_status] nvarchar(4000),\n[orv_delivery_partialmatch] int,\n[orv_delivery_approximated] int,\n[orv_delivery_lmsorder] int,\n[orv_delivery_optimizedorder] int,\n[orv_delivery_seal] nvarchar(4000),\n[orv_delivery_customerinvoice] int,\n[orv_delivery_customerdn] int,\n[orv_delivery_grv] nvarchar(4000),\n[orv_delivery_chepslip] int,\n[orv_delivery_cleandelivery] int,\n[orv_delivery_trackingcode] nvarchar(4000),\n[orv_delivery_mode] nvarchar(4000),\n[orv_delivery_customorder] int,\n[orv_delivery_verified] int,\n[orv_delivery_altered] int,\n[orv_delivery_orderid] int,\n[orv_delivery_mergeid] bigint,\n[orv_delivery_orderref] nvarchar(4000),\n[orv_delivery_drivernote] nvarchar(4000),\n[orv_delivery_guid] bigint,\n[orv_delivery_timedefinite] int,\n[orv_delivery_orvcode] int,\n[orv_delivery_vaid] bigint,\n[orv_delivery_delegatedate] datetime2(7),\n[orv_delivery_delegatemode] nvarchar(4000),\n[orv_delivery_debriefmode] nvarchar(4000),\n[orv_delivery_debriefed] int,\n[orv_delivery_insdate] datetime2(7),\n[orv_delivery_reviseddebrief] int,\n[orv_delivery_lmsdebriefed] int,\n[orv_delivery_uncanceldate] datetime2(7),\n[orv_delivery_podlaterdate] datetime2(7),\n[orv_delivery_podmanualdate] datetime2(7),\n[orv_delivery_manualdebriefreason] nvarchar(4000),\n[orv_delivery_submanualdebriefreason] nvarchar(4000),\n[orv_delivery_outofgeofencereason] nvarchar(4000),\n[orv_delivery_tsentergeofenceapp] datetime2(7),\n[orv_delivery_tsexitgeofenceapp] datetime2(7),\n[orv_delivery_comebacklater] bigint,\n[orv_delivery_uncancelusername] nvarchar(4000),\n[orv_delivery_cref] int,\n[orv_delivery_division] int,\n[orv_delivery_descriptionofgoods] nvarchar(4000),\n[orv_delivery_skiplng] float,\n[orv_delivery_skiplat] float,\n[orv_delivery_courierid] bigint,\n[orv_delivery_posreason] nvarchar(4000),\n[orv_delivery_moddate] datetime2(7),\n[orv_delivery_mallid] int,\n[orv_delivery_mallgid] bigint,\n[orv_delivery_lmsidskipreason] bigint,\n[orv_delivery_deleteforimport] int,\n[orv_delivery_podoutofgeofence] int,\n[orv_delivery_originalcustomerid] bigint,\n[orv_pod_id] bigint,\n[orv_pod_name] nvarchar(4000),\n[orv_pod_poddate] datetime2(7),\n[orv_pod_lat] float,\n[orv_pod_lng] float,\n[orv_pod_customerrating] int,\n[orv_pod_customerfeedback] nvarchar(4000),\n\n\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stcollection3myoy.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.T_STCollection_3MYoY where orv_draftcollection_id = 467725\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/T_STConsignment')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Distribution"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.T_STConsignment') )\n    DROP EXTERNAL TABLE dbo.T_STConsignment\nGO\n\nCREATE EXTERNAL TABLE dbo.T_STConsignment (\n[lms_consignment_id] int,\n[lms_consignment_cref] nvarchar(4000),\n[lms_consignment_cdate] datetime2(7),\n[lms_consignment_userid] int,\n[lms_consignment_appointment] datetime2(7),\n[lms_consignment_deliverby] datetime2(7),\n[lms_consignment_deliverbyoriginal] datetime2(7),\n[lms_consignment_custaccid] int,\n[lms_consignment_billcust] int,\n[lms_consignment_delivercustid] int,\n[lms_consignment_inservicereasonid] int,\n[lms_consignment_inservicecomment] nvarchar(4000),\n[lms_consignment_lastupdate] datetime2(7),\n[lms_consignment_collectid] int,\n[lms_consignment_delivertypeid] int,\n[lms_consignment_billtypeid] int,\n[lms_consignment_loadid] int,\n[lms_consignment_totalcharge] numeric(36,8),\n[lms_consignment_charge] numeric(36,8),\n[lms_consignment_docfees] numeric(36,8),\n[lms_consignment_fuellevy] numeric(36,8),\n[lms_consignment_totothersurcharges] numeric(36,8),\n[lms_parcelbyconsignment_aggrweight] numeric(36,8),\n[lms_parcelbyconsignment_aggrchargeweight] numeric(36,8),\n[lms_parcelbyconsignment_aggrvolweight] numeric(36,8),\n[lms_parcelbyconsignment_aggrvolumiserweight] numeric(36,8),\n[lms_parcelbyconsignment_aggrmaxweight] numeric(36,8),\n[lms_parcelbyconsignment_aggrnoparcels] bigint,\n[lms_parcelbyconsignment_aggrweightexclspecdel] numeric(36,8),\n[lms_parcelbyconsignment_aggrchargeweightexclspecdel] numeric(36,8),\n[lms_parcelbyconsignment_aggrvolweightexclspecdel] numeric(36,8),\n[lms_parcelbyconsignment_aggrvolumiserweightexclspecdel] numeric(36,8),\n[lms_parcelbyconsignment_aggrmaxweightexclspecdel] numeric(36,8),\n[lms_parcelbyconsignment_aggrnoparcelsexclspecdel] bigint,\n[lms_billconsignmentr_id] int,\n[lms_billconsignmentr_consignid] int,\n[lms_billconsignmentr_updatedate] datetime2(7),\n[lms_billconsignmentr_pickuprouteid] int,\n[lms_billconsignmentr_deliverrouteid] int,\n[lms_billconsignmentr_chargeweight] numeric(36,8),\n[lms_billconsignmentr_noofparcels] int,\n[lms_billconsignmentr_tripdistance] numeric(36,8),\n[lms_billconsignmentr_consignmentvalue] numeric(36,8),\n[lms_billconsignmentr_billcust] int,\n[lms_billconsignmentr_redflag] int,\n[lms_billconsignmentr_updind] int,\n[lms_billconsignmentr_servicetypeid] int,\n[lms_billconsignmentr_routemissingind] int,\n[lms_billconsignmentr_chargescalcind] int,\n[lms_billconsignmentr_cdate1] datetime2(7),\n[lms_billconsignmentr_invalidpcode] int,\n[lms_billconsignmentr_programno] int,\n[lms_billconsignmentr_totrate] numeric(36,8),\n[lms_billconsignmentr_routecharge] numeric(36,8),\n[lms_billconsignmentr_minweight] numeric(36,8),\n[lms_billconsignmentr_minrate] numeric(36,8),\n[lms_billconsignmentr_corderno] nvarchar(4000),\n[lms_billconsignmentr_custref] nvarchar(4000),\n[lms_billconsignmentr_manualupdateflag] int,\n[lms_billconsignmentr_prevbillcustid] int,\n[lms_billconsignmentr_specdelvehicleid] int,\n[lms_billconsignmentr_specdelmissingrouteflag] int,\n[lms_billconsignmentr_overborderflag] int,\n[lms_billconsignmentr_samedeliverid] int,\n[lms_billconsignmentr_combineroutecharge] numeric(36,8),\n[lms_billconsignmentr_waybillid] int,\n[lms_billconsignmentr_routelinecount] int,\n[lms_billconsignmentr_samedelcount] int,\n[lms_billconsignmentr_aweight] numeric(36,8),\n[lms_billconsignmentr_vweight] numeric(36,8),\n[lms_billconsignmentr_custref3] nvarchar(4000),\n[lms_billconsignmentr_units] int,\n[lms_billconsignmentr_totsurcharges] numeric(36,8),\n[lms_billconsignmentr_noofpallets] int,\n[lms_billconsignmentr_invalidpudel] int,\n[lms_billconsignmentr_sharikaflag] int,\n[lms_billconsignmentr_sapid] int,\n[lms_billconsignmentr_totfuelsurcharge] numeric(36,8),\n[lms_billconsignmentr_totdocsurcharge] numeric(36,8),\n[lms_billconsignmentr_totothersurcharge] numeric(36,8),\n[lms_billconsignmentr_specdel] int,\n[lms_billconsignmentr_quoteflag] int,\n[lms_billconsignmentr_billperiod] nvarchar(4000),\n[lms_billconsignmentr_billweekr] int,\n[lms_billconsignmentr_billperiodflag] int,\n[lms_billconsignmentr_surhargeflag] int,\n[lms_billconsignmentr_recalcflag] int,\n[lms_billconsignmentr_quotemissingflag] int,\n[lms_billconsignmentr_delivertypeid] int,\n[lms_billconsignmentr_totcovidsurcharge] numeric(36,8),\n[lms_consignment_maxpoddate] datetime2(7),\n[lms_consignment_inserviceflag] int,\n[lms_consignment_actualleaddaysstartdate] datetime2(7),\n[lms_consignment_actualleaddaysenddate] datetime2(7),\n[lms_consignment_actualleaddays] float,\n[lms_consignment_actualleaddaysinclwe] float,\n[lms_consignment_cdateroundup] datetime2(7),\n[lms_consignment_age] nvarchar(4000),\n[consignagedifferenceinseconds] bigint,\n[lms_billbillcustomersr_sapcode] nvarchar(4000),\n[lms_consignment_tlocationid] int,\n[lms_consignment_tlocation] nvarchar(4000),\n[lms_billconsignmentr_aggrtotalcharge] numeric(36,8),\n[lms_billconsignmentr_aggrstandardcharge] numeric(36,8),\n[lms_billconsignmentr_aggrsdconsolidatedcharge] numeric(36,8),\n[lms_billconsignmentr_aggrfuelsurcharge] numeric(36,8),\n[lms_billconsignmentr_aggrdocsurcharge] numeric(36,8),\n[lms_billconsignmentr_aggrothersurcharge] numeric(36,8),\n[lms_billconsignmentr_aggrcovidsurcharge] numeric(36,8),\n\n\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stconsignment.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.T_STConsignment \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/T_STConsignment_3MYoY')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Distribution"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.T_STConsignment_3MYoY') )\n    DROP EXTERNAL TABLE dbo.T_STConsignment_3MYoY\nGO\n\nCREATE EXTERNAL TABLE dbo.T_STConsignment_3MYoY (\n[lms_consignment_id] int,\n[lms_consignment_cref] nvarchar(4000),\n[lms_consignment_cdate] datetime2(7),\n[lms_consignment_userid] int,\n[lms_consignment_appointment] datetime2(7),\n[lms_consignment_deliverby] datetime2(7),\n[lms_consignment_deliverbyoriginal] datetime2(7),\n[lms_consignment_custaccid] int,\n[lms_consignment_billcust] int,\n[lms_consignment_delivercustid] int,\n[lms_consignment_inservicereasonid] int,\n[lms_consignment_inservicecomment] nvarchar(4000),\n[lms_consignment_lastupdate] datetime2(7),\n[lms_consignment_collectid] int,\n[lms_consignment_delivertypeid] int,\n[lms_consignment_billtypeid] int,\n[lms_consignment_loadid] int,\n[lms_consignment_totalcharge] numeric(36,8),\n[lms_consignment_charge] numeric(36,8),\n[lms_consignment_docfees] numeric(36,8),\n[lms_consignment_fuellevy] numeric(36,8),\n[lms_consignment_totothersurcharges] numeric(36,8),\n[lms_parcelbyconsignment_aggrweight] numeric(36,8),\n[lms_parcelbyconsignment_aggrchargeweight] numeric(36,8),\n[lms_parcelbyconsignment_aggrvolweight] numeric(36,8),\n[lms_parcelbyconsignment_aggrvolumiserweight] numeric(36,8),\n[lms_parcelbyconsignment_aggrmaxweight] numeric(36,8),\n[lms_parcelbyconsignment_aggrnoparcels] bigint,\n[lms_parcelbyconsignment_aggrweightexclspecdel] numeric(36,8),\n[lms_parcelbyconsignment_aggrchargeweightexclspecdel] numeric(36,8),\n[lms_parcelbyconsignment_aggrvolweightexclspecdel] numeric(36,8),\n[lms_parcelbyconsignment_aggrvolumiserweightexclspecdel] numeric(36,8),\n[lms_parcelbyconsignment_aggrmaxweightexclspecdel] numeric(36,8),\n[lms_parcelbyconsignment_aggrnoparcelsexclspecdel] bigint,\n[lms_billconsignmentr_id] int,\n[lms_billconsignmentr_consignid] int,\n[lms_billconsignmentr_updatedate] datetime2(7),\n[lms_billconsignmentr_pickuprouteid] int,\n[lms_billconsignmentr_deliverrouteid] int,\n[lms_billconsignmentr_chargeweight] numeric(36,8),\n[lms_billconsignmentr_noofparcels] int,\n[lms_billconsignmentr_tripdistance] numeric(36,8),\n[lms_billconsignmentr_consignmentvalue] numeric(36,8),\n[lms_billconsignmentr_billcust] int,\n[lms_billconsignmentr_redflag] int,\n[lms_billconsignmentr_updind] int,\n[lms_billconsignmentr_servicetypeid] int,\n[lms_billconsignmentr_routemissingind] int,\n[lms_billconsignmentr_chargescalcind] int,\n[lms_billconsignmentr_cdate1] datetime2(7),\n[lms_billconsignmentr_invalidpcode] int,\n[lms_billconsignmentr_programno] int,\n[lms_billconsignmentr_totrate] numeric(36,8),\n[lms_billconsignmentr_routecharge] numeric(36,8),\n[lms_billconsignmentr_minweight] numeric(36,8),\n[lms_billconsignmentr_minrate] numeric(36,8),\n[lms_billconsignmentr_corderno] nvarchar(4000),\n[lms_billconsignmentr_custref] nvarchar(4000),\n[lms_billconsignmentr_manualupdateflag] int,\n[lms_billconsignmentr_prevbillcustid] int,\n[lms_billconsignmentr_specdelvehicleid] int,\n[lms_billconsignmentr_specdelmissingrouteflag] int,\n[lms_billconsignmentr_overborderflag] int,\n[lms_billconsignmentr_samedeliverid] int,\n[lms_billconsignmentr_combineroutecharge] numeric(36,8),\n[lms_billconsignmentr_waybillid] int,\n[lms_billconsignmentr_routelinecount] int,\n[lms_billconsignmentr_samedelcount] int,\n[lms_billconsignmentr_aweight] numeric(36,8),\n[lms_billconsignmentr_vweight] numeric(36,8),\n[lms_billconsignmentr_custref3] nvarchar(4000),\n[lms_billconsignmentr_units] int,\n[lms_billconsignmentr_totsurcharges] numeric(36,8),\n[lms_billconsignmentr_noofpallets] int,\n[lms_billconsignmentr_invalidpudel] int,\n[lms_billconsignmentr_sharikaflag] int,\n[lms_billconsignmentr_sapid] int,\n[lms_billconsignmentr_totfuelsurcharge] numeric(36,8),\n[lms_billconsignmentr_totdocsurcharge] numeric(36,8),\n[lms_billconsignmentr_totothersurcharge] numeric(36,8),\n[lms_billconsignmentr_specdel] int,\n[lms_billconsignmentr_quoteflag] int,\n[lms_billconsignmentr_billperiod] nvarchar(4000),\n[lms_billconsignmentr_billweekr] int,\n[lms_billconsignmentr_billperiodflag] int,\n[lms_billconsignmentr_surhargeflag] int,\n[lms_billconsignmentr_recalcflag] int,\n[lms_billconsignmentr_quotemissingflag] int,\n[lms_billconsignmentr_delivertypeid] int,\n[lms_billconsignmentr_totcovidsurcharge] numeric(36,8),\n[lms_consignment_maxpoddate] datetime2(7),\n[lms_consignment_inserviceflag] int,\n[lms_consignment_actualleaddaysstartdate] datetime2(7),\n[lms_consignment_actualleaddaysenddate] datetime2(7),\n[lms_consignment_actualleaddays] float,\n[lms_consignment_actualleaddaysinclwe] float,\n[lms_consignment_cdateroundup] datetime2(7),\n[lms_consignment_age] nvarchar(4000),\n[consignagedifferenceinseconds] bigint,\n[lms_billbillcustomersr_sapcode] nvarchar(4000),\n[lms_consignment_tlocationid] int,\n[lms_consignment_tlocation] nvarchar(4000),\n[lms_billconsignmentr_aggrtotalcharge] numeric(36,8),\n[lms_billconsignmentr_aggrstandardcharge] numeric(36,8),\n[lms_billconsignmentr_aggrsdconsolidatedcharge] numeric(36,8),\n[lms_billconsignmentr_aggrfuelsurcharge] numeric(36,8),\n[lms_billconsignmentr_aggrdocsurcharge] numeric(36,8),\n[lms_billconsignmentr_aggrothersurcharge] numeric(36,8),\n[lms_billconsignmentr_aggrcovidsurcharge] numeric(36,8),\n\n\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stconsignment3myoy.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.T_STConsignment_3MYoY \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/T_STDCTransfer')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Distribution"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.T_STDCTransfer') )\n    DROP EXTERNAL TABLE dbo.T_STDCTransfer\nGO\n\nCREATE EXTERNAL TABLE dbo.T_STDCTransfer (\n    [lms_dctransfer_id] int,\n    [lms_dctransfer_destinationstoreid] nvarchar(4000),\n    [lms_dctransfer_transfercode] nvarchar(4000),\n    [lms_dctransfer_audittype] nvarchar(4000),\n    [lms_dctransfer_qty] int,\n    [lms_dctransfer_dc_prepackid] int,\n    [lms_dctransfer_acceptancedate] datetime2(7),\n    [lms_dctransfer_auditflag] int,\n    [lms_dctransfer_audituserid] int,\n    [lms_dctransfer_auditlocid] int,\n    [lms_dctransfer_counted] int,\n    [lms_dctransfer_damagecount] int,\n    [lms_dctransfer_uploaded] int,\n    [lms_dctransfer_dc_torderid] int,\n    [lms_dctransfer_duedate] datetime2(7),\n    [lms_dctransfer_destinationlocationcode] nvarchar(4000),\n    [lms_dctransfer_destinationlocationtypecode] nvarchar(4000),\n    [lms_dctransfer_processfamilycode] nvarchar(4000),\n    [lms_dctransfer_grnversion] int\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stdctransfer.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.T_STDCTransfer \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/T_STDocuments')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Finance"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.T_STDocuments') )\n    DROP EXTERNAL TABLE dbo.T_STDocuments\nGO\n\nCREATE EXTERNAL TABLE dbo.T_STDocuments (\n\n\t[sap_documentsheader_docentry] int,\n\t[sap_documentsheader_docnum] int,\n\t[sap_documentsheader_doctype] nvarchar(4000),\n\t[sap_documentsheader_objtype] nvarchar(4000),\n\t[sap_documentsheader_postingdate] datetime2(7),\n\t[sap_documentsheader_duedate] datetime2(7),\n\t[sap_documentsheader_cardcode] nvarchar(4000),\n\t[sap_documentsheader_cardref] nvarchar(4000),\n\t[sap_documentsheader_discprcnt] numeric(18,2),\n\t[sap_documentsheader_discsum] numeric(18,2),\n\t[sap_documentsheader_doctotal] numeric(18,2),\n\t[sap_documentsheader_comments] nvarchar(4000),\n\t[sap_documentsheader_transid] int,\n\t[sap_documentsheader_u_expense_depot] nvarchar(4000),\n\t[sap_documentsline_linenum] int,\n\t[sap_documentsline_linestatus] nvarchar(4000),\n\t[sap_documentsline_itemcode] nvarchar(4000),\n\t[sap_documentsline_dscription] nvarchar(4000),\n\t[sap_documentsline_quantity] numeric(18,2),\n\t[sap_documentsline_openqty] numeric(18,2),\n\t[sap_documentsline_price] numeric(18,2),\n\t[sap_documentsline_rate] numeric(18,2),\n\t[sap_documentsline_discprcnt] numeric(18,2),\n\t[sap_documentsline_linetotal] numeric(18,2),\n\t[sap_documentsline_opensum] numeric(18,2),\n\t[sap_documentsline_pricebefdi] numeric(18,2),\n\t[sap_documentsline_docdate] datetime2(7),\n\t[sap_documentsline_project] nvarchar(4000),\n\t[sap_documentsline_vatprcnt] numeric(18,2),\n\t[sap_documentsline_vatgroup] nvarchar(4000),\n\t[sap_documentsline_vatamount] numeric(18,2),\n\t[sap_documentsline_u_reason] nvarchar(4000),\n\t[sap_documentsline_u_expenseitem] nvarchar(4000),\n\t[sap_documentsline_u_fueldate] datetime2(7),\n\t[sap_documentsline_acctcode] nvarchar(4000),\n\t[sap_documentsline_doccat] nvarchar(4000),\n\t[sap_documentsline_subdepotcode] nvarchar(4000),\n\t[sap_documentsline_u_rate] nvarchar(4000),\n\t[sap_documentsline_u_ir_number] int,\n\t[sap_documentsline_discsum] numeric(18,2),\n\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/FINANCE/stdocuments.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.T_STDocuments \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STSAP",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/T_STDriverPayItem')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Linehaul"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.T_STDriverPayItem') )\n    DROP EXTERNAL TABLE dbo.T_STDriverPayItem\nGO\n\nCREATE EXTERNAL TABLE dbo.T_STDriverPayItem (\n    [tms_driverpayitem_id] bigint,\n    [tms_driverpayitem_driverid] nvarchar(4000),\n    [tms_driverpayitem_tripid] bigint,\n    [tms_driverpayitem_triprate] int,\n    [tms_driverpayitem_mealrate] int,\n    [tms_driverpayitem_sleepoverrate] int,\n    [tms_driverpayitem_airtimerate] int,\n    [tms_driverpayitem_linkrate] int,\n    [tms_driverpayitem_totalrate] int,\n    [tms_driverpayitem_routeid] bigint,\n    [tms_driverpayitem_routename] nvarchar(4000),\n    [tms_driverpayitem_insdate] datetime2(7),\n    [tms_driverpayitem_moddate] datetime2(7),\n    [tms_driverpayitem_horse] nvarchar(4000),\n    [tms_driverpayitem_trailers] nvarchar(4000),\n    [tms_driverpayitem_linkused] int,\n    [tms_driverpayitem_manualadjustment] int,\n    [tms_driverpayitem_editby] bigint,\n    [tms_driverpayitem_drivername] nvarchar(4000),\n    [tms_driverpayitem_dispatchdate] datetime2(7),\n    [tms_driverpayitem_bookings] nvarchar(4000),\n    [tms_driverpayitem_employeecode] nvarchar(4000),\n    [tms_driverpayitem_originatedfrom] nvarchar(4000),\n    [tms_driverpayitem_reportid] bigint,\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stdriverpayitem.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.T_STDriverPayItem \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/T_STEndorsments')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Distribution"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.T_STEndorsments') )\n    DROP EXTERNAL TABLE dbo.T_STEndorsments\nGO\n\nCREATE EXTERNAL TABLE dbo.T_STEndorsments (\n\t[lms_db_endorsement_id] int,\n\t[lms_db_endorsement_description] nvarchar(4000),\n\t[lms_wbendorsement_id] int,\n\t[lms_wbendorsement_barcode] nvarchar(4000),\n\t[lms_wbendorsement_waybillid] int,\n\t[orv_claim_id] bigint,\n\t[orv_claim_delid] bigint,\n\t[orv_claim_claimtype] nvarchar(4000),\n\t[orv_claim_claimdate] datetime2(7),\n\t[orv_claim_did] bigint,\n\t[orv_claim_parcelid] bigint,\n\t[orv_claim_barcode] nvarchar(4000),\n\t[orv_claim_attachid] bigint,\n\t[orv_claim_note] nvarchar(4000)\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stendorsments.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.T_STEndorsments \nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/T_STFinanceData')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Linehaul"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.T_STFinanceData') )\n    DROP EXTERNAL TABLE dbo.T_STFinanceData\nGO\n\nCREATE EXTERNAL TABLE dbo.T_STFinanceData (\n    [tms_financedata_id] bigint,\n    [tms_financedata_bookingid] bigint,\n    [tms_financedata_insdate] datetime2(7),\n    [tms_financedata_peaksurchagrebuy] bigint,\n    [tms_financedata_peaksurchagresell] bigint,\n    [tms_financedata_manualentryroadhaulsell] bigint,\n    [tms_financedata_manualentryahocsell] bigint,\n    [tms_financedata_systemfuellevy] bigint,\n    [tms_financedata_systemratefuellevy] bigint,\n    [tms_financedata_systemratefuelexlevy] bigint,\n    [tms_financedata_systemratefuelinclevy] bigint,\n    [tms_financedata_systemsellrate] bigint,\n    [tms_financedata_totalsellrate] bigint,\n    [tms_financedata_moddate] datetime2(7),\n    [tms_financedata_totalbuyrate] bigint,\n    [tms_financedata_rateprofileid] bigint\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stfinancedata.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.T_STFinanceData \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/T_STInstruction')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Linehaul"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.T_STInstruction') )\n    DROP EXTERNAL TABLE dbo.T_STInstruction\nGO\n\nCREATE EXTERNAL TABLE dbo.T_STInstruction (\n[tms_instruction_id] bigint,\n[tms_instruction_bookingid] bigint,\n[tms_instruction_addressid] bigint,\n[tms_instruction_instrtype] nvarchar(4000),\n[tms_instruction_onroute] int,\n[tms_instruction_onrouteid] bigint,\n[tms_instruction_sort] int,\n[tms_instruction_onroutestatus] nvarchar(4000),\n[tms_instruction_note] nvarchar(4000),\n[tms_instruction_citybranchid] bigint,\n[tms_instruction_citybranchname] nvarchar(4000),\n[tms_instruction_insdate] datetime2(7),\n[tms_instruction_moddate] datetime2(7),\n[tms_instruction_insby] bigint,\n[tms_instruction_bookingpickupdeliverydate] datetime2(7),\n[orv_lhdelivery_id] bigint,\n[orv_lhdelivery_did] bigint,\n[orv_lhdelivery_lat] float,\n[orv_lhdelivery_lng] float,\n[orv_lhdelivery_actualeta] datetime2(7),\n[orv_lhdelivery_eta] datetime2(7),\n[orv_lhdelivery_tsgeofenceenter] datetime2(7),\n[orv_lhdelivery_tsdocuments] datetime2(7),\n[orv_lhdelivery_customerref1] nvarchar(4000),\n[orv_lhdelivery_customerref2] nvarchar(4000),\n[orv_lhdelivery_trailersize] nvarchar(4000),\n[orv_lhdelivery_cargoweight] float,\n[orv_lhdelivery_cubicvolume] float,\n[orv_lhdelivery_geocodingstatus] nvarchar(4000),\n[orv_lhdelivery_status] nvarchar(4000),\n[orv_lhdelivery_optimizedorder] int,\n[orv_lhdelivery_triporder] int,\n[orv_lhdelivery_sequence] int,\n[orv_lhdelivery_bookingid] bigint,\n[orv_lhdelivery_grv] int,\n[orv_lhdelivery_chepslip] int,\n[orv_lhdelivery_type] nvarchar(4000),\n[orv_lhdelivery_insdate] datetime2(7),\n[orv_lhdelivery_isdirect] int,\n[orv_lhdelivery_notes] nvarchar(4000),\n[orv_lhdelivery_contactperson] nvarchar(4000),\n[orv_lhdelivery_contactnumber] nvarchar(4000),\n[orv_lhdelivery_trailerfleetcode] nvarchar(4000),\n[orv_lhdelivery_primarycustomerid] bigint,\n[orv_lhdelivery_pcustomername] nvarchar(4000),\n[orv_lhdelivery_pcustomeraccountnumber] nvarchar(4000),\n[orv_lhdelivery_pcustomerrevenuecode] nvarchar(4000),\n[orv_lhdelivery_secondarycustomerid] bigint,\n[orv_lhdelivery_scustomername] nvarchar(4000),\n[orv_lhdelivery_scustomeraccountnumber] nvarchar(4000),\n[orv_lhdelivery_scustomerrevenuecode] nvarchar(4000),\n[orv_lhdelivery_actiondate] datetime2(7),\n[orv_lhdelivery_tsgeofenceexit] datetime2(7),\n[orv_lhdelivery_pctmsid] bigint,\n[orv_lhdelivery_sctmsid] bigint,\n[orv_lhdelivery_customorder] int,\n[orv_lhdelivery_skipdate] datetime2(7),\n[orv_lhdelivery_skipreason] nvarchar(4000),\n[orv_lhdelivery_outofgeofencereason] nvarchar(4000),\n[orv_lhdelivery_tsarrivedcustomer] datetime2(7),\n[orv_lhdelivery_tsdroppedoff] datetime2(7),\n[orv_lhdelivery_tsstartoffloading] datetime2(7),\n[orv_lhdelivery_tsstartloading] datetime2(7),\n[orv_lhdelivery_tsloaded] datetime2(7),\n[orv_lhdelivery_tsfinishedoffloading] datetime2(7),\n[orv_lhdelivery_tsdocumnetshanddovedriver] datetime2(7),\n[orv_lhdelivery_tsleavingcustomer] datetime2(7),\n[orv_lhdelivery_actionsubtype] nvarchar(4000),\n[orv_lhdelivery_tsstart] datetime2(7),\n[orv_lhdelivery_signame] nvarchar(4000),\n[orv_lhdelivery_signcomment] nvarchar(4000),\n[orv_lhdelivery_signdate] datetime2(7),\n[orv_lhdelivery_tsendtate] datetime2(7),\n[orv_lhdelivery_tsentergeofenceapp] datetime2(7),\n[orv_lhdelivery_tsexitgeofenceapp] datetime2(7),\n[orv_lhdelivery_tmsid] bigint,\n[orv_lhdelivery_subtype] nvarchar(4000),\n[orv_lhdelivery_needbotrailerver] int,\n[orv_lhdelivery_secondtrailerfleetcode] nvarchar(4000),\n[orv_lhdelivery_trailerverlat] float,\n[orv_lhdelivery_trailerverlng] float,\n[orv_lhdelivery_tstrailerver] bigint,\n[orv_lhdelivery_podlat] float,\n[orv_lhdelivery_podlng] float,\n[orv_lhdelivery_skiplat] float,\n[orv_lhdelivery_skiplng] float,\n[orv_lhdelivery_uncanceldate] datetime2(7),\n[tms_instruction_inputpickupduration] bigint,\n[tms_instruction_loadingduration] bigint,\n[tms_instruction_geopickupduration] bigint,\n[tms_instruction_actionpickupduration] bigint,\n[tms_instruction_inservice] nvarchar(4000),\n[tms_instruction_driverinputservice] nvarchar(4000),\n[tms_instruction_geofenceservice] nvarchar(4000),\n[tms_instruction_duedate] datetime2(7),\n[tms_instruction_actualdate] datetime2(7),\n\n\n\n\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stinstruction.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.T_STInstruction \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/T_STInvoice')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Linehaul"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.T_STInvoice') )\n    DROP EXTERNAL TABLE dbo.T_STInvoice\nGO\n\nCREATE EXTERNAL TABLE dbo.T_STInvoice (\n    [tms_invoice_id] bigint,\n    [tms_invoice_customerid] bigint,\n    [tms_invoice_invoicedate] datetime2(7),\n    [tms_invoice_reference] nvarchar(4000),\n    [tms_invoice_status] nvarchar(4000),\n    [tms_invoice_insby] bigint,\n    [tms_invoice_insdate] datetime2(7),\n    [tms_invoice_total] bigint,\n    [tms_invoice_vat] bigint,\n    [tms_invoice_invoicenum] nvarchar(4000),\n    [tms_invoice_opaqueid] nvarchar(4000),\n    [tms_invoice_sapstatus] nvarchar(4000),\n    [tms_invoice_sapid] nvarchar(4000),\n    [tms_invoice_sendsapdate] datetime2(7),\n    [tms_invoice_legacyid] bigint,\n    [tms_invoice_customercode] nvarchar(4000),\n    [tms_invoice_revennuecode] nvarchar(4000),\n    [tms_invoice_depotcode] nvarchar(4000),\n    [tms_invoiceitem_id] bigint,\n    [tms_invoiceitem_invoiceid] bigint,\n    [tms_invoiceitem_bookingid] bigint,\n    [tms_invoiceitem_amount] bigint,\n    [tms_invoiceitem_vat] bigint,\n    [tms_invoiceitem_linetotal] bigint,\n    [tms_invoiceitem_insdate] datetime2(7),\n    [tms_invoiceitem_moddate] datetime2(7),\n    [tms_invoiceitem_insby] bigint,\n    [tms_invoiceitem_linenum] int,\n    [tms_invoiceitem_legacyid] bigint,\n    [tms_invoiceitem_sapid] int,\n    [tms_invoiceitem_vatable] int,\n    [tms_invoiceitem_routeid] bigint,\n    [tms_invoiceitem_routename] nvarchar(4000),\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stinvoice.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.T_STInvoice \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/T_STKMsandLTs')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Finance"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.T_STKMsandLTs') )\n    DROP EXTERNAL TABLE dbo.T_STKMsandLTs\nGO\n\nCREATE EXTERNAL TABLE dbo.T_STKMsandLTs (\n[excel_kmsandlts_registrationnumber] nvarchar(4000),\n[excel_kmsandlts_fleetcode] nvarchar(4000),\n[excel_kmsandlts_date] date,\n[excel_kmsandlts_time] nvarchar(4000),\n[excel_kmsandlts_fuelcompany] nvarchar(4000),\n[excel_kmsandlts_voucherno] nvarchar(4000),\n[excel_kmsandlts_fuelstation] nvarchar(4000),\n[excel_kmsandlts_previousodo] int,\n[excel_kmsandlts_odo] int,\n[excel_kmsandlts_tripkm] int,\n[excel_kmsandlts_consumptionper100kms] numeric(18,2),\n[excel_kmsandlts_consumptionperunit] numeric(18,2),\n[excel_kmsandlts_ltsloss] numeric(18,2),\n[excel_kmsandlts_randsloss] numeric(18,2),\n[excel_kmsandlts_triplts] float,\n[excel_kmsandlts_priceperunit] numeric(18,2),\n[excel_kmsandlts_fuelamount] numeric(18,2),\n[excel_kmsandlts_pickuptype] nvarchar(4000),\n[excel_kmsandlts_account] nvarchar(4000),\n[excel_kmsandlts_costcentre] nvarchar(4000),\n[excel_kmsandlts_division] nvarchar(4000),\n[excel_kmsandlts_make] nvarchar(4000),\n[excel_kmsandlts_range] nvarchar(4000),\n[excel_kmsandlts_model] nvarchar(4000),\n[excel_kmsandlts_consumptionexpected] numeric(18,2),\n\n)\n\tWITH (\n\tLOCATION = 'Structured Data/FINANCE/stkmsandlts.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t);\n\nGO\n\n\nSELECT TOP 100 * FROM dbo.T_STKMsandLTs\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STSAP",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/T_STLMSActivityDetail')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Finance"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.T_STLMSActivityDetail') )\n    DROP EXTERNAL TABLE dbo.T_STLMSActivityDetail\nGO\n\nCREATE EXTERNAL TABLE dbo.T_STLMSActivityDetail (\n[lms_activity_transid] nvarchar(4000),\n[lms_activity_transidtype] nvarchar(4000),\n[lms_activity_date] date,\n[lms_activity_billcustid] int,\n[lms_activity_billcustname] nvarchar(4000),\n[lms_activity_delivercustid] int,\n[lms_activity_delivercustname] nvarchar(4000),\n[lms_activity_movementtype] nvarchar(4000),\n[lms_activity_linehualbookingid] nvarchar(4000),\n[lms_activity_location] nvarchar(4000),\n[lms_activity_aggrweight] numeric(18,2),\n[lms_activity_aggrchargeweight] numeric(18,2),\n[lms_activity_aggrvolweight] numeric(18,2),\n[lms_activity_aggrvolumiserweight] numeric(18,2),\n[lms_activity_aggrmaxweight] numeric(18,2),\n[lms_activity_aggrnoparcels] bigint,\n[lms_activity_aggrweightexclspecdel] numeric(18,2),\n[lms_activity_aggrchargeweightexclspecdel] numeric(18,2),\n[lms_activity_aggrvolweightexclspecdel] numeric(18,2),\n[lms_activity_aggrvolumiserweightexclspecdel] numeric(18,2),\n[lms_activity_aggrmaxweightexclspecdel] numeric(18,2),\n[lms_activity_aggrnoparcelsexclspecdel] bigint,\n\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/FINANCE/stactivitydetail.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.T_STLMSActivityDetail \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STSAP",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/T_STLMSActivitySummary')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Finance"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.T_STLMSActivitySummary') )\n    DROP EXTERNAL TABLE dbo.T_STLMSActivitySummary\nGO\n\nCREATE EXTERNAL TABLE dbo.T_STLMSActivitySummary (\n\n[lms_activity_date] date,\n[lms_activity_movementtype] nvarchar(4000),\n[lms_activity_location] nvarchar(4000),\n[lms_activity_aggrweight] numeric(18,2),\n[lms_activity_aggrchargeweight] numeric(18,2),\n[lms_activity_aggrvolweight] numeric(18,2),\n[lms_activity_aggrvolumiserweight] numeric(18,2),\n[lms_activity_aggrmaxweight] numeric(18,2),\n[lms_activity_aggrnoparcels] bigint,\n[lms_activity_aggrweightexclspecdel] numeric(18,2),\n[lms_activity_aggrchargeweightexclspecdel] numeric(18,2),\n[lms_activity_aggrvolweightexclspecdel] numeric(18,2),\n[lms_activity_aggrvolumiserweightexclspecdel] numeric(18,2),\n[lms_activity_aggrmaxweightexclspecdel] numeric(18,2),\n[lms_activity_aggrnoparcelsexclspecdel] bigint,\n[lms_activity_billcustname] nvarchar(4000),\n\n\n)\n\tWITH (\n\tLOCATION = 'Structured Data/FINANCE/stactivitysummary.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t);\n\nGO\n\n\nSELECT TOP 100 * FROM dbo.T_STLMSActivitySummary \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STSAP",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/T_STLMSAggrigateMeasures')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Distribution"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.T_STLMSAggrigateMeasures') )\n    DROP EXTERNAL TABLE dbo.T_STLMSAggrigateMeasures\nGO\n\nCREATE EXTERNAL TABLE dbo.T_STLMSAggrigateMeasures (\ndaymonth  nvarchar(4000),\nlms_parcel_ho_orders_cy bigint, \nlms_parcel_ho_consignments_cy bigint, \nlms_parcel_ho_parcels_cy bigint, \nlms_parcel_ho_waybills_cy bigint, \nlms_parcel_ho_weight_cy float, \nlms_parcel_ho_chargeweight_cy numeric(36,6), \nlms_parcel_ho_orders_cy_cum bigint, \nlms_parcel_ho_consignments_cy_cum bigint, \nlms_parcel_ho_parcels_cy_cum bigint, \nlms_parcel_ho_waybills_cy_cum bigint, \nlms_parcel_ho_weight_cy_cum float, \nlms_parcel_ho_chargeweight_cy_cum numeric(36,6), \nlms_parcel_ho_orders_py bigint, \nlms_parcel_ho_consignments_py bigint, \nlms_parcel_ho_parcels_py bigint, \nlms_parcel_ho_waybills_py bigint, \nlms_parcel_ho_weight_py float, \nlms_parcel_ho_chargeweight_py numeric(36,6), \nlms_parcel_ho_orders_py_cum bigint, \nlms_parcel_ho_consignments_py_cum bigint, \nlms_parcel_ho_parcels_py_cum bigint, \nlms_parcel_ho_waybills_py_cum bigint, \nlms_parcel_ho_weight_py_cum float, \nlms_parcel_ho_chargeweight_py_cum numeric(36,6), \nlms_parcel_p_orders_cy bigint, \nlms_parcel_p_consignments_cy bigint, \nlms_parcel_p_parcels_cy bigint, \nlms_parcel_p_waybills_cy bigint, \nlms_parcel_p_weight_cy float, \nlms_parcel_p_chargeweight_cy numeric(36,6), \nlms_parcel_p_orders_cy_cum bigint, \nlms_parcel_p_consignments_cy_cum bigint, \nlms_parcel_p_parcels_cy_cum bigint, \nlms_parcel_p_waybills_cy_cum bigint, \nlms_parcel_p_weight_cy_cum float, \nlms_parcel_p_chargeweight_cy_cum numeric(36,6), \nlms_parcel_p_orders_py bigint, \nlms_parcel_p_consignments_py bigint, \nlms_parcel_p_parcels_py bigint, \nlms_parcel_p_waybills_py bigint, \nlms_parcel_p_weight_py float, \nlms_parcel_p_chargeweight_py numeric(36,6), \nlms_parcel_p_orders_py_cum bigint, \nlms_parcel_p_consignments_py_cum bigint, \nlms_parcel_p_parcels_py_cum bigint, \nlms_parcel_p_waybills_py_cum bigint, \nlms_parcel_p_weight_py_cum float, \nlms_parcel_p_chargeweight_py_cum numeric(36,6), \nlms_parcel_c_orders_cy bigint, \nlms_parcel_c_consignments_cy bigint, \nlms_parcel_c_parcels_cy bigint, \nlms_parcel_c_waybills_cy bigint, \nlms_parcel_c_weight_cy float, \nlms_parcel_c_chargeweight_cy numeric(36,6), \nlms_parcel_c_orders_cy_cum bigint, \nlms_parcel_c_consignments_cy_cum bigint, \nlms_parcel_c_parcels_cy_cum bigint, \nlms_parcel_c_waybills_cy_cum bigint, \nlms_parcel_c_weight_cy_cum float, \nlms_parcel_c_chargeweight_cy_cum numeric(36,6), \nlms_parcel_c_orders_py bigint, \nlms_parcel_c_consignments_py bigint, \nlms_parcel_c_parcels_py bigint, \nlms_parcel_c_waybills_py bigint, \nlms_parcel_c_weight_py float, \nlms_parcel_c_chargeweight_py numeric(36,6), \nlms_parcel_c_orders_py_cum bigint, \nlms_parcel_c_consignments_py_cum bigint, \nlms_parcel_c_parcels_py_cum bigint, \nlms_parcel_c_waybills_py_cum bigint, \nlms_parcel_c_weight_py_cum float, \nlms_parcel_c_chargeweight_py_cum numeric(36,6), \nlms_parcel_wb_parcels_cy bigint, \nlms_parcel_wb_waybills_cy bigint, \nlms_parcel_wb_weight_cy float, \nlms_parcel_wb_chargeweight_cy numeric(36,6), \nlms_parcel_wb_parcels_cy_cum bigint, \nlms_parcel_wb_waybills_cy_cum bigint, \nlms_parcel_wb_weight_cy_cum float, \nlms_parcel_wb_chargeweight_cy_cum numeric(36,6), \nlms_parcel_wb_parcels_py bigint, \nlms_parcel_wb_waybills_py bigint, \nlms_parcel_wb_weight_py float, \nlms_parcel_wb_chargeweight_py numeric(36,6), \nlms_parcel_wb_parcels_py_cum bigint, \nlms_parcel_wb_waybills_py_cum bigint, \nlms_parcel_wb_weight_py_cum float, \nlms_parcel_wb_chargeweight_py_cum numeric(36,6), \nlms_track_t_totallinehaulloads_cy bigint, \nlms_track_t_totallinehaulparcels_cy bigint, \nlms_track_t_totallinehaulweight_cy float,\nlms_track_t_totallinehaulchargeweight_cy numeric(36,6), \nlms_track_t_totallinehaulloads_cy_cum bigint, \nlms_track_t_totallinehaulparcels_cy_cum bigint, \nlms_track_t_totallinehaulweight_cy_cum float,\nlms_track_t_totallinehaulchargeweight_cy_cum numeric(36,6),\nlms_track_t_totallinehaulloads_py bigint, \nlms_track_t_totallinehaulparcels_py bigint, \nlms_track_t_totallinehaulweight_py float, \nlms_track_t_totallinehaulchargeweight_py numeric(36,6), \nlms_track_t_totallinehaulloads_py_cum bigint, \nlms_track_t_totallinehaulparcels_py_cum bigint, \nlms_track_t_totallinehaulweight_py_cum float, \nlms_track_t_totallinehaulchargeweight_py_cum numeric(36,6), \nlms_track_t_totaldistributionloads_cy bigint, \nlms_track_t_totaldistributionparcels_cy bigint, \nlms_track_t_totaldistributionweight_cy float, \nlms_track_t_totaldistributionchargeweight_cy numeric(36,6), \nlms_track_t_totaldistributionloads_cy_cum bigint, \nlms_track_t_totaldistributionparcels_cy_cum bigint, \nlms_track_t_totaldistributionweight_cy_cum float, \nlms_track_t_totaldistributionchargeweight_cy_cum numeric(36,6), \nlms_track_t_totaldistributionloads_py bigint, \nlms_track_t_totaldistributionparcels_py bigint, \nlms_track_t_totaldistributionweight_py float, \nlms_track_t_totaldistributionchargeweight_py numeric(36,6), \nlms_track_t_totaldistributionloads_py_cum bigint, \nlms_track_t_totaldistributionparcels_py_cum bigint, \nlms_track_t_totaldistributionweight_py_cum float, \nlms_track_t_totaldistributionchargeweight_py_cum numeric(36,6),\n\n\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/staggrigatemeasures.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.T_STLMSAggrigateMeasures \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/T_STLMSTrack')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Distribution"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.T_STLMSTrack') )\n    DROP EXTERNAL TABLE dbo.T_STLMSTrack\nGO\n\nCREATE EXTERNAL TABLE dbo.T_STLMSTrack (\n[lms_track_id] int,\n[lms_track_loadid] int,\n[lms_track_parcelid] int,\n[lms_track_waybillsperparcelkey] nvarchar(4000),\n[lms_track_fromlocid] int,\n[lms_track_tolocid] int,\n[lms_track_tracktypeid] int,\n[lms_track_tracktypeid2] int,\n[lms_track_opendt] datetime2(7),\n[lms_track_closedt] datetime2(7),\n[lms_track_specdel] int,\n[lms_track_loadparenetloadid] int,\n[lms_track_parcelweight] float,\n[lms_track_parcelchargeweight] numeric(36,6),\n[lms_track_parcelvolweight] float,\n[lms_track_parceltotcharge] numeric(36,6),\n[lms_track_orderbillcustid] int,\n[lms_track_orderpickupcustid] int,\n[lms_track_orderdelivercustid] int,\n[lms_track_parcelconsignid] int,\n[lms_track_parcelorderid] int,\n[lms_track_lastmilebranchflag] int,\n[lms_track_waybillid] int,\n[lms_track_rdate] datetime2(7),\n[lms_track_returnedflag] int,\n[lms_track_dooruserid] int,\n[lms_track_doordeliverydate] datetime2(7),\n[lms_track_stationid] int,\n[lms_track_doorreasondetailid] int,\n[lms_track_doorcomment] nvarchar(4000),\n[lms_track_debriefuserid] int,\n[lms_track_debriefdeliverydate] datetime2(7),\n[lms_track_debriefcomment] nvarchar(4000),\n[lms_track_debriefreasondetailid] int,\n[lms_track_reasonupdates] int,\n[lms_track_callprogid] int,\n\n\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stlmstrack.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.T_STLMSTrack \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/T_STLMSTrack_3MYoY')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Distribution"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.T_STLMSTrack_3MYoY') )\n    DROP EXTERNAL TABLE dbo.T_STLMSTrack_3MYoY\nGO\n\nCREATE EXTERNAL TABLE dbo.T_STLMSTrack_3MYoY (\n[lms_track_id] int,\n[lms_track_loadid] int,\n[lms_track_parcelid] int,\n[lms_track_waybillsperparcelkey] nvarchar(4000),\n[lms_track_fromlocid] int,\n[lms_track_tolocid] int,\n[lms_track_tracktypeid] int,\n[lms_track_tracktypeid2] int,\n[lms_track_opendt] datetime2(7),\n[lms_track_closedt] datetime2(7),\n[lms_track_specdel] int,\n[lms_track_loadparenetloadid] int,\n[lms_track_parcelweight] float,\n[lms_track_parcelchargeweight] numeric(36,6),\n[lms_track_parcelvolweight] float,\n[lms_track_parceltotcharge] numeric(36,6),\n[lms_track_orderbillcustid] int,\n[lms_track_orderpickupcustid] int,\n[lms_track_orderdelivercustid] int,\n[lms_track_parcelconsignid] int,\n[lms_track_parcelorderid] int,\n[lms_track_lastmilebranchflag] int,\n[lms_track_waybillid] int,\n[lms_track_rdate] datetime2(7),\n[lms_track_returnedflag] int,\n[lms_track_dooruserid] int,\n[lms_track_doordeliverydate] datetime2(7),\n[lms_track_stationid] int,\n[lms_track_doorreasondetailid] int,\n[lms_track_doorcomment] nvarchar(4000),\n[lms_track_debriefuserid] int,\n[lms_track_debriefdeliverydate] datetime2(7),\n[lms_track_debriefcomment] nvarchar(4000),\n[lms_track_debriefreasondetailid] int,\n[lms_track_reasonupdates] int,\n[lms_track_callprogid] int,\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stlmstrack3myoy.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.T_STLMSTrack_3MYoY \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/T_STLoadChild')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Distribution"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.T_STLoadChild') )\n    DROP EXTERNAL TABLE dbo.T_STLoadChild\nGO\n\nCREATE EXTERNAL TABLE dbo.T_STLoadChild (\n[lms_loadchild_id] int,\n[lms_loadchild_routeid] int,\n[lms_loadchild_driverid] int,\n[lms_loadchild_sealno] nvarchar(4000),\n[lms_loadchild_vehicleid] int,\n[lms_loadchild_ttypeid] int,\n[lms_loadchild_userid] int,\n[lms_loadchild_ldate] datetime2(7),\n[lms_loadchild_courierid] nvarchar(4000),\n[lms_loadchild_site] nvarchar(4000),\n[lms_loadchild_fromlocid] int,\n[lms_loadchild_tolocid] int,\n[lms_loadchild_cdate] datetime2(7),\n[lms_loadchild_trailer1] int,\n[lms_loadchild_trailer2] int,\n[lms_loadchild_cref] nvarchar(4000),\n[lms_loadchild_debriefed] int,\n[lms_loadchild_mrpflag] int,\n[lms_loadchild_mrpcflag] int,\n[lms_loadchild_finalflag] int,\n[lms_loadchild_alidaflag] int,\n[lms_loadchild_puibts] int,\n[lms_loadchild_pucols] int,\n[lms_loadchild_planningref] nvarchar(4000),\n[lms_loadchild_interfacedate] datetime2(7),\n[lms_loadchild_nosealsret] int,\n[lms_loadchild_excludeopsi] int,\n[lms_loadchild_dccref] nvarchar(4000),\n[lms_loadchild_srouteid] int,\n[lms_loadchild_fuellitres] numeric(36,8),\n[lms_loadchild_outsourcedcrew] int,\n[lms_loadchild_masterloadid] int,\n[lms_loadchild_securitygatedespatched] datetime2(7),\n[lms_loadchild_securitygateuserid] int,\n[lms_loadchild_securitygatearrived] datetime2(7),\n[lms_loadchild_securitygatearriveduserid] int,\n[lms_loadchild_lastupdate] datetime2(7),\n[lms_loadchild_bayno] nvarchar(4000),\n[lms_loadchild_costs] numeric(36,8),\n[lms_loadchild_estdeparturedatetime] datetime2(7),\n[lms_loadchild_totalweight] numeric(36,8),\n[lms_loadchild_ovrignore] int,\n[lms_loadchild_childloadid] int,\n[lms_loadchild_orvcode] int,\n[lms_loadchild_companyid] int,\n[lms_loadchild_editloaddate] datetime2(7),\n[lms_loadchild_reopeneddate] datetime2(7),\n[lms_loadchild_reopeneduserid] int,\n[lms_loadchild_novehicle] int,\n[lms_loadchild_mobilefeedbackreasonid] int,\n[lms_loadchild_direct] int,\n[lms_loadchild_noofpcls] int,\n[lms_loadchild_debriefdt] datetime2(7),\n[lms_loadchild_httpcode] int,\n[lms_loadchild_parentloadid] int,\n[lms_loadchild_mrpobversion] int,\n[lms_loadchild_aggrweight] numeric(36,8),\n[lms_loadchild_aggrchargeweight] numeric(36,8),\n[lms_loadchild_aggrvolweight] numeric(36,8),\n[lms_loadchild_aggrvolumiserweight] numeric(36,8),\n[lms_loadchild_aggrmaxweight] numeric(36,8),\n[lms_loadchild_aggrnoparcels] bigint,\n[lms_loadchild_aggrweightexclspecdel] numeric(36,8),\n[lms_loadchild_aggrchargeweightexclspecdel] numeric(36,8),\n[lms_loadchild_aggrvolweightexclspecdel] numeric(36,8),\n[lms_loadchild_aggrvolumiserweightexclspecdel] numeric(36,8),\n[lms_loadchild_aggrmaxweightexclspecdel] numeric(36,8),\n[lms_loadchild_aggrnoparcelsexclspecdel] bigint,\n[lms_loadchild_effectiveloaddurationminutes] int,\n[lms_loadchild_effectiveoffloaddurationminutes] int\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stloadchild.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.T_STLoadChild \nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/T_STLoadChild_3MYoY')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Distribution"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.T_STLoadChild_3MYoY') )\n    DROP EXTERNAL TABLE dbo.T_STLoadChild_3MYoY\nGO\n\nCREATE EXTERNAL TABLE dbo.T_STLoadChild_3MYoY (\n[lms_loadchild_id] int,\n[lms_loadchild_routeid] int,\n[lms_loadchild_driverid] int,\n[lms_loadchild_sealno] nvarchar(4000),\n[lms_loadchild_vehicleid] int,\n[lms_loadchild_ttypeid] int,\n[lms_loadchild_userid] int,\n[lms_loadchild_ldate] datetime2(7),\n[lms_loadchild_courierid] nvarchar(4000),\n[lms_loadchild_site] nvarchar(4000),\n[lms_loadchild_fromlocid] int,\n[lms_loadchild_tolocid] int,\n[lms_loadchild_cdate] datetime2(7),\n[lms_loadchild_trailer1] int,\n[lms_loadchild_trailer2] int,\n[lms_loadchild_cref] nvarchar(4000),\n[lms_loadchild_debriefed] int,\n[lms_loadchild_mrpflag] int,\n[lms_loadchild_mrpcflag] int,\n[lms_loadchild_finalflag] int,\n[lms_loadchild_alidaflag] int,\n[lms_loadchild_puibts] int,\n[lms_loadchild_pucols] int,\n[lms_loadchild_planningref] nvarchar(4000),\n[lms_loadchild_interfacedate] datetime2(7),\n[lms_loadchild_nosealsret] int,\n[lms_loadchild_excludeopsi] int,\n[lms_loadchild_dccref] nvarchar(4000),\n[lms_loadchild_srouteid] int,\n[lms_loadchild_fuellitres] numeric(36,8),\n[lms_loadchild_outsourcedcrew] int,\n[lms_loadchild_masterloadid] int,\n[lms_loadchild_securitygatedespatched] datetime2(7),\n[lms_loadchild_securitygateuserid] int,\n[lms_loadchild_securitygatearrived] datetime2(7),\n[lms_loadchild_securitygatearriveduserid] int,\n[lms_loadchild_lastupdate] datetime2(7),\n[lms_loadchild_bayno] nvarchar(4000),\n[lms_loadchild_costs] numeric(36,8),\n[lms_loadchild_estdeparturedatetime] datetime2(7),\n[lms_loadchild_totalweight] numeric(36,8),\n[lms_loadchild_ovrignore] int,\n[lms_loadchild_childloadid] int,\n[lms_loadchild_orvcode] int,\n[lms_loadchild_companyid] int,\n[lms_loadchild_editloaddate] datetime2(7),\n[lms_loadchild_reopeneddate] datetime2(7),\n[lms_loadchild_reopeneduserid] int,\n[lms_loadchild_novehicle] int,\n[lms_loadchild_mobilefeedbackreasonid] int,\n[lms_loadchild_direct] int,\n[lms_loadchild_noofpcls] int,\n[lms_loadchild_debriefdt] datetime2(7),\n[lms_loadchild_httpcode] int,\n[lms_loadchild_parentloadid] int,\n[lms_loadchild_mrpobversion] int,\n[lms_loadchild_aggrweight] numeric(36,8),\n[lms_loadchild_aggrchargeweight] numeric(36,8),\n[lms_loadchild_aggrvolweight] numeric(36,8),\n[lms_loadchild_aggrvolumiserweight] numeric(36,8),\n[lms_loadchild_aggrmaxweight] numeric(36,8),\n[lms_loadchild_aggrnoparcels] bigint,\n[lms_loadchild_aggrweightexclspecdel] numeric(36,8),\n[lms_loadchild_aggrchargeweightexclspecdel] numeric(36,8),\n[lms_loadchild_aggrvolweightexclspecdel] numeric(36,8),\n[lms_loadchild_aggrvolumiserweightexclspecdel] numeric(36,8),\n[lms_loadchild_aggrmaxweightexclspecdel] numeric(36,8),\n[lms_loadchild_aggrnoparcelsexclspecdel] bigint,\n[lms_loadchild_effectiveloaddurationminutes] int,\n[lms_loadchild_effectiveoffloaddurationminutes] int\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stloadchild3myoy.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.T_STLoadChild_3MYoY \nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/T_STLoadParent')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Distribution"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.T_STLoadParent') )\n    DROP EXTERNAL TABLE dbo.T_STLoadParent\nGO\n\nCREATE EXTERNAL TABLE dbo.T_STLoadParent (\n[lms_loadparent_parentloadid] int,\n[lms_loadparent_vehicleid] int,\n[lms_loadparent_ldate] datetime2(7),\n[lms_loadparent_vehodo] float,\n[lms_loadparent_vehodo2] float,\n[lms_loadparent_fromlocid] int,\n[lms_loadparent_tolocid] int,\n[lms_loadparent_type]  nvarchar(4000),\n[lms_loadparent_ttypeid] int,\n[lms_loadparent_aggrweight] numeric(36,8),\n[lms_loadparent_aggrchargeweight] numeric(36,8),\n[lms_loadparent_aggrvolweight] numeric(36,8),\n[lms_loadparent_aggrvolumiserweight] numeric(36,8),\n[lms_loadparent_aggrmaxweight] numeric(36,8),\n[lms_loadparent_aggrnoparcels] bigint,\n[lms_loadparent_aggrweightexclspecdel] numeric(36,8),\n[lms_loadparent_aggrchargeweightexclspecdel] numeric(36,8),\n[lms_loadparent_aggrvolweightexclspecdel] numeric(36,8),\n[lms_loadparent_aggrvolumiserweightexclspecdel] numeric(36,8),\n[lms_loadparent_aggrmaxweightexclspecdel] numeric(36,8),\n[lms_loadparent_aggrnoparcelsexclspecdel] bigint,\n[md_vehicle_vehicletype] nvarchar(4000),\n[md_vehicle_internalvolume]  numeric(36,8),\n[orv_dispatch_id] bigint,\n[orv_dispatch_lmsid] bigint,\n[orv_dispatch_vid] bigint,\n[orv_dispatch_did] bigint,\n[orv_dispatch_status] nvarchar(4000),\n[orv_dispatch_startdate] datetime2(7),\n[orv_dispatch_stopdate] datetime2(7),\n[orv_dispatch_vehicletype] int,\n[orv_dispatch_uid] bigint,\n[orv_dispatch_estdepdate] datetime2(7),\n[orv_dispatch_trailer] bigint,\n[orv_dispatch_trailer2] bigint,\n[orv_dispatch_fromlocid] bigint,\n[orv_dispatch_tolocid] bigint,\n[orv_dispatch_site] nvarchar(4000),\n[orv_dispatch_lastupdate] datetime2(7),\n[orv_dispatch_etastatus] nvarchar(4000),\n[orv_dispatch_startinglat] float,\n[orv_dispatch_startinglng] float,\n[orv_dispatch_endinglat] float,\n[orv_dispatch_endinglng] float,\n[orv_dispatch_startingfrom] nvarchar(4000),\n[orv_dispatch_goingto] nvarchar(4000),\n[orv_dispatch_importissue] nvarchar(4000),\n[orv_dispatch_insdate] datetime2(7),\n[orv_dispatch_estarrdate] datetime2(7),\n[orv_dispatch_duration] bigint,\n[orv_dispatch_distance] bigint,\n[orv_dispatch_cost] bigint,\n[orv_dispatch_stoplat] float,\n[orv_dispatch_stoplng] float,\n[orv_dispatch_stopreason] nvarchar(4000),\n[orv_dispatch_startodo] bigint,\n[orv_dispatch_stopodo] bigint,\n[orv_dispatch_expduration] bigint,\n[orv_dispatch_expdistance] bigint,\n[orv_dispatch_expcost] bigint,\n[orv_dispatch_collectionid] bigint,\n[orv_dispatch_crew] int,\n[orv_dispatch_crewnames] nvarchar(4000),\n[orv_dispatch_bocloseuid] bigint,\n[orv_dispatch_routing] nvarchar(4000),\n[orv_dispatch_swapct] int,\n[orv_dispatch_debriefed] int,\n[orv_dispatch_debriefnotes] nvarchar(4000),\n[orv_dispatch_bopin] nvarchar(4000),\n[orv_dispatch_cpicost] bigint,\n[orv_dispatch_labourcost] bigint,\n[orv_dispatch_maintcost] bigint,\n[orv_dispatch_inscost] bigint,\n[orv_dispatch_tollcost] bigint,\n[orv_dispatch_fuelcost] bigint,\n[orv_dispatch_internal] int,\n[orv_dispatch_childids] nvarchar(4000),\n[orv_dispatch_guid] bigint,\n[orv_dispatch_timedefinite] int,\n[orv_dispatch_orvcode] int,\n[orv_dispatch_tsstopdepoenter] datetime2(7),\n[orv_dispatch_tsstartdepoexit] datetime2(7),\n[orv_dispatch_reroutect] int,\n[orv_dispatch_actualarrdate] datetime2(7),\n[orv_dispatch_physicalstartlat] float,\n[orv_dispatch_physicalstartlng] float,\n[orv_dispatch_tsstartdepoexitapp] datetime2(7),\n[orv_dispatch_tsstopdepoenterapp] datetime2(7),\n[orv_dispatch_uncanceldate] datetime2(7),\n[orv_dispatch_uncancelusername] nvarchar(4000),\n[orv_dispatch_lmsdebrifed] int,\n[orv_dispatch_reviseddebrief] int,\n[orv_dispatch_lhtransporter] nvarchar(4000),\n[orv_dispatch_triptype] nvarchar(4000),\n[orv_dispatch_startreason] nvarchar(4000),\n[orv_dispatch_route] nvarchar(4000),\n[orv_dispatch_tmstraileridentify] nvarchar(4000),\n[orv_dispatch_tmstrailer2identify] nvarchar(4000),\n[orv_dispatch_tmscontroller] nvarchar(4000),\n[orv_dispatch_routekey] nvarchar(4000),\n[orv_dispatch_courierid] bigint,\n[orv_dispatch_originalstartdate] datetime2(7),\n[orv_dispatch_originalextarrival] datetime2(7),\n[orv_dispatch_recomputeoriginals] int,\n[orv_dispatch_originalextdistance] bigint,\n[orv_dispatch_originalextduration] bigint,\n[orv_dispatch_moddate] datetime2(7),\n[orv_dispatch_tmsmode] int,\n[orv_dispatch_driversignature] bigint,\n[orv_dispatch_exitdepot] datetime2(7),\n[orv_dispatch_enterdepot] datetime2(7),\n[orv_dispatch_fdeliverydate] datetime2(7),\n[orv_dispatch_fdeltsgeofenceenter] datetime2(7),\n[orv_dispatch_fdeltsscanningstart] datetime2(7),\n[orv_dispatch_fdeltsscanningstop] datetime2(7),\n[orv_dispatch_fdeltspodsignature] datetime2(7),\n[orv_dispatch_fdelnumprcls] int,\n[orv_dispatch_deliverydate] datetime2(7),\n[orv_dispatch_ldeltsgeofenceenter] datetime2(7),\n[orv_dispatch_ldeltsscanningstart] datetime2(7),\n[orv_dispatch_ldeltsscanningstop] datetime2(7),\n[orv_dispatch_ldeltspodsignature] datetime2(7),\n[orv_dispatch_ldelnumprcls] int,\n[orv_dispatch_fromdelid] bigint,\n[orv_dispatch_todelid] bigint,\n[orv_dispatch_distancemeters] int,\n[orv_dispatch_durationhours] float,\n[orv_dispatch_timeduebackatdepot] datetime2(7),\n[orv_dispatch_lastcanceleddeltodepotime] datetime2(7),\n[orv_dispatch_difstopdatevstimedueback] float,\n[orv_dispatch_completesameday] int,\n[orv_dispatch_latedepotexitflag] int,\n[orv_dispatch_latedepotenterflag] int,\n[orv_dispatch_latedepotexitflagnull] int,\n[orv_dispatch_latedepotenterflagnull] int,\n[orv_dispatch_cexternal] int,\n[orv_dispatch_latestopdateflag] int\n\n\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stloadparent.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.T_STLoadParent\nGO\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/T_STLoadParent_3MYoY')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Distribution"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.T_STLoadParent_3MYoY') )\n    DROP EXTERNAL TABLE dbo.T_STLoadParent_3MYoY\nGO\n\nCREATE EXTERNAL TABLE dbo.T_STLoadParent_3MYoY (\n[lms_loadparent_parentloadid] int,\n[lms_loadparent_vehicleid] int,\n[lms_loadparent_ldate] datetime2(7),\n[lms_loadparent_vehodo] float,\n[lms_loadparent_vehodo2] float,\n[lms_loadparent_fromlocid] int,\n[lms_loadparent_tolocid] int,\n[lms_loadparent_type]  nvarchar(4000),\n[lms_loadparent_ttypeid] int,\n[lms_loadparent_aggrweight] numeric(36,8),\n[lms_loadparent_aggrchargeweight] numeric(36,8),\n[lms_loadparent_aggrvolweight] numeric(36,8),\n[lms_loadparent_aggrvolumiserweight] numeric(36,8),\n[lms_loadparent_aggrmaxweight] numeric(36,8),\n[lms_loadparent_aggrnoparcels] bigint,\n[lms_loadparent_aggrweightexclspecdel] numeric(36,8),\n[lms_loadparent_aggrchargeweightexclspecdel] numeric(36,8),\n[lms_loadparent_aggrvolweightexclspecdel] numeric(36,8),\n[lms_loadparent_aggrvolumiserweightexclspecdel] numeric(36,8),\n[lms_loadparent_aggrmaxweightexclspecdel] numeric(36,8),\n[lms_loadparent_aggrnoparcelsexclspecdel] bigint,\n[md_vehicle_vehicletype] nvarchar(4000),\n[md_vehicle_internalvolume]  numeric(36,8),\n[orv_dispatch_id] bigint,\n[orv_dispatch_lmsid] bigint,\n[orv_dispatch_vid] bigint,\n[orv_dispatch_did] bigint,\n[orv_dispatch_status] nvarchar(4000),\n[orv_dispatch_startdate] datetime2(7),\n[orv_dispatch_stopdate] datetime2(7),\n[orv_dispatch_vehicletype] int,\n[orv_dispatch_uid] bigint,\n[orv_dispatch_estdepdate] datetime2(7),\n[orv_dispatch_trailer] bigint,\n[orv_dispatch_trailer2] bigint,\n[orv_dispatch_fromlocid] bigint,\n[orv_dispatch_tolocid] bigint,\n[orv_dispatch_site] nvarchar(4000),\n[orv_dispatch_lastupdate] datetime2(7),\n[orv_dispatch_etastatus] nvarchar(4000),\n[orv_dispatch_startinglat] float,\n[orv_dispatch_startinglng] float,\n[orv_dispatch_endinglat] float,\n[orv_dispatch_endinglng] float,\n[orv_dispatch_startingfrom] nvarchar(4000),\n[orv_dispatch_goingto] nvarchar(4000),\n[orv_dispatch_importissue] nvarchar(4000),\n[orv_dispatch_insdate] datetime2(7),\n[orv_dispatch_estarrdate] datetime2(7),\n[orv_dispatch_duration] bigint,\n[orv_dispatch_distance] bigint,\n[orv_dispatch_cost] bigint,\n[orv_dispatch_stoplat] float,\n[orv_dispatch_stoplng] float,\n[orv_dispatch_stopreason] nvarchar(4000),\n[orv_dispatch_startodo] bigint,\n[orv_dispatch_stopodo] bigint,\n[orv_dispatch_expduration] bigint,\n[orv_dispatch_expdistance] bigint,\n[orv_dispatch_expcost] bigint,\n[orv_dispatch_collectionid] bigint,\n[orv_dispatch_crew] int,\n[orv_dispatch_crewnames] nvarchar(4000),\n[orv_dispatch_bocloseuid] bigint,\n[orv_dispatch_routing] nvarchar(4000),\n[orv_dispatch_swapct] int,\n[orv_dispatch_debriefed] int,\n[orv_dispatch_debriefnotes] nvarchar(4000),\n[orv_dispatch_bopin] nvarchar(4000),\n[orv_dispatch_cpicost] bigint,\n[orv_dispatch_labourcost] bigint,\n[orv_dispatch_maintcost] bigint,\n[orv_dispatch_inscost] bigint,\n[orv_dispatch_tollcost] bigint,\n[orv_dispatch_fuelcost] bigint,\n[orv_dispatch_internal] int,\n[orv_dispatch_childids] nvarchar(4000),\n[orv_dispatch_guid] bigint,\n[orv_dispatch_timedefinite] int,\n[orv_dispatch_orvcode] int,\n[orv_dispatch_tsstopdepoenter] datetime2(7),\n[orv_dispatch_tsstartdepoexit] datetime2(7),\n[orv_dispatch_reroutect] int,\n[orv_dispatch_actualarrdate] datetime2(7),\n[orv_dispatch_physicalstartlat] float,\n[orv_dispatch_physicalstartlng] float,\n[orv_dispatch_tsstartdepoexitapp] datetime2(7),\n[orv_dispatch_tsstopdepoenterapp] datetime2(7),\n[orv_dispatch_uncanceldate] datetime2(7),\n[orv_dispatch_uncancelusername] nvarchar(4000),\n[orv_dispatch_lmsdebrifed] int,\n[orv_dispatch_reviseddebrief] int,\n[orv_dispatch_lhtransporter] nvarchar(4000),\n[orv_dispatch_triptype] nvarchar(4000),\n[orv_dispatch_startreason] nvarchar(4000),\n[orv_dispatch_route] nvarchar(4000),\n[orv_dispatch_tmstraileridentify] nvarchar(4000),\n[orv_dispatch_tmstrailer2identify] nvarchar(4000),\n[orv_dispatch_tmscontroller] nvarchar(4000),\n[orv_dispatch_routekey] nvarchar(4000),\n[orv_dispatch_courierid] bigint,\n[orv_dispatch_originalstartdate] datetime2(7),\n[orv_dispatch_originalextarrival] datetime2(7),\n[orv_dispatch_recomputeoriginals] int,\n[orv_dispatch_originalextdistance] bigint,\n[orv_dispatch_originalextduration] bigint,\n[orv_dispatch_moddate] datetime2(7),\n[orv_dispatch_tmsmode] int,\n[orv_dispatch_driversignature] bigint,\n[orv_dispatch_exitdepot] datetime2(7),\n[orv_dispatch_enterdepot] datetime2(7),\n[orv_dispatch_fdeliverydate] datetime2(7),\n[orv_dispatch_fdeltsgeofenceenter] datetime2(7),\n[orv_dispatch_fdeltsscanningstart] datetime2(7),\n[orv_dispatch_fdeltsscanningstop] datetime2(7),\n[orv_dispatch_fdeltspodsignature] datetime2(7),\n[orv_dispatch_fdelnumprcls] int,\n[orv_dispatch_deliverydate] datetime2(7),\n[orv_dispatch_ldeltsgeofenceenter] datetime2(7),\n[orv_dispatch_ldeltsscanningstart] datetime2(7),\n[orv_dispatch_ldeltsscanningstop] datetime2(7),\n[orv_dispatch_ldeltspodsignature] datetime2(7),\n[orv_dispatch_ldelnumprcls] int,\n[orv_dispatch_fromdelid] bigint,\n[orv_dispatch_todelid] bigint,\n[orv_dispatch_distancemeters] int,\n[orv_dispatch_durationhours] float,\n[orv_dispatch_timeduebackatdepot] datetime2(7),\n[orv_dispatch_lastcanceleddeltodepotime] datetime2(7),\n[orv_dispatch_difstopdatevstimedueback] float,\n[orv_dispatch_completesameday] int,\n[orv_dispatch_latedepotexitflag] int,\n[orv_dispatch_latedepotenterflag] int,\n[orv_dispatch_latedepotexitflagnull] int,\n[orv_dispatch_latedepotenterflagnull] int,\n[orv_dispatch_cexternal] int,\n[orv_dispatch_latestopdateflag] int\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stloadparent3myoy.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\nSELECT TOP 100 * FROM dbo.T_STLoadParent_3MYoY \nGO\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/T_STMRPCSVFiles')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Distribution"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.T_STMRPCSVFiles') )\n    DROP EXTERNAL TABLE dbo.T_STMRPCSVFiles\nGO\n\nCREATE EXTERNAL TABLE dbo.T_STMRPCSVFiles (\n[billingextractid] int,\n[retailyear] int,\n[retailweek] int,\n[consignmentnumber] nvarchar(4000),\n[transfernumber] nvarchar(4000),\n[transferoutdate] datetime2(7),\n[quantity] int,\n[volumetricweight] decimal(36,6),\n[weigtmeasuretypeid] int,\n[islinehaul] nvarchar(4000),\n[isdistribution] nvarchar(4000),\n[fromstoreid] int,\n[destinationstoreid] int,\n[routecode] nvarchar(4000),\n[routedesc] nvarchar(4000),\n[subroutecode] nvarchar(4000),\n[subroutedesc] nvarchar(4000),\n[zoneid] int,\n[zrsnumber] nvarchar(4000),\n[volumetricrateexdc] decimal(36,6),\n[volumetricrateexdepot] decimal(36,6),\n[volumetricrateexdccarriagecost] decimal(36,6),\n[volumetricrateexdepotcarriagecost] decimal(36,6),\n[lhcarriagecost] decimal(36,6),\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stmrpcsvfiles.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.T_STMRPCSVFiles \nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/T_STManualJournals')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Finance"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.T_STManualJournals') )\n    DROP EXTERNAL TABLE dbo.T_STManualJournals\nGO\n\nCREATE EXTERNAL TABLE dbo.T_STManualJournals (\n\t[sap_manualjournalsheader_postingdate] datetime2(7),\n\t[sap_manualjournalsheader_duedate] datetime2(7),\n\t[sap_manualjournalsheader_doctotal] numeric(18,2),\n\t[sap_manualjournalsheader_doctype] nvarchar(4000),\n\t[sap_manualjournalsheader_transid] int,\n\t[sap_manualjournalsline_linenum] int,\n\t[sap_manualjournalsline_acctcode] nvarchar(4000),\n\t[sap_manualjournalsheader_objtype] nvarchar(4000),\n\t[sap_manualjournalsline_project] nvarchar(4000),\n\t[sap_manualjournalsline_vatgroup] nvarchar(4000),\n\t[sap_manualjournalsline_vatrate] numeric(18,2),\n\t[sap_manualjournalsline_vatamount] numeric(18,2),\n\t[sap_manualjournalsline_description] nvarchar(4000),\n\t[sap_manualjournalsline_jnlref1] nvarchar(4000),\n\t[sap_manualjournalsline_jnlref2] nvarchar(4000),\n\t[sap_manualjournalsline_subdepotcode] nvarchar(4000),\n\t[sap_manualjournalsline_linetotal] numeric(18,2),\n\t[sap_manualjournalsline_doccat] nvarchar(4000),\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/FINANCE/stmanualjournal.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.T_STManualJournals \nGO\n\n\n\n\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STSAP",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/T_STOpsFinTransactions')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Distribution"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.T_STOpsFinTransactions') )\n    DROP EXTERNAL TABLE dbo.T_STOpsFinTransactions\nGO\n\nCREATE EXTERNAL TABLE dbo.T_STOpsFinTransactions (\n    [sap_transactionsline_acctcode] nvarchar(4000),\n    [sap_transactionsheader_cardcode] nvarchar(4000),\n    [sap_transactionsheader_cardref] nvarchar(4000),\n    [sap_transactionsline_subdepotcode] nvarchar(4000),\n    [sap_transactionsline_doccat] nvarchar(4000),\n    [sap_transactionsheader_postingdate] date,\n    [sap_transactionsline_docdate] date,\n    [sap_transactionsline_linetotalfinalexclgjpo] numeric(38,6),\n    [sap_businesspartners_cardname] nvarchar(4000),\n    [md_depot_depotname] nvarchar(4000),\n    [md_depot_division] nvarchar(4000),\n    [md_depot_parentdepotcode] nvarchar(4000),\n    [sap_account_level1name] nvarchar(4000),\n    [sap_account_level4name] nvarchar(4000),\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stopsfintransactions.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.T_STOpsFinTransactions \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/T_STOrder')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Distribution"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.T_STOrder') )\n    DROP EXTERNAL TABLE dbo.T_STOrder\nGO\n\nCREATE EXTERNAL TABLE dbo.T_STOrder (\n[lms_order_id] int,\n[lms_order_corderno] nvarchar(4000),\n[lms_order_custref] nvarchar(4000),\n[lms_order_pickupcustid] int,\n[lms_order_delivercustid] int,\n[lms_order_billcustid] int,\n[lms_order_weight] numeric(36,8),\n[lms_order_ovalue] numeric(36,8),\n[lms_order_hodate] datetime2(7),\n[lms_order_cservice] nvarchar(4000),\n[lms_order_courierid] nvarchar(4000),\n[lms_order_custservice] nvarchar(4000),\n[lms_order_userid] int,\n[lms_order_appointment] datetime2(7),\n[lms_order_deliverby] datetime2(7),\n[lms_order_dconfirmed] nvarchar(4000),\n[lms_order_dcomment] nvarchar(4000),\n[lms_order_drefno] nvarchar(4000),\n[lms_order_duedate] datetime2(7),\n[lms_order_batchno] int,\n[lms_order_custaccid] int,\n[lms_order_airfreightflag] int,\n[lms_order_specdel] int,\n[lms_order_quote] numeric(36,8),\n[lms_order_quoteweight] numeric(36,8),\n[lms_order_quotefuellevy] numeric(36,8),\n[lms_order_quoteothersurcharges] numeric(36,8),\n[lms_order_quotedocfees] numeric(36,8),\n[lms_order_billvehicletype] int,\n[lms_parcelbyorder_aggrweight] numeric(36,8),\n[lms_parcelbyorder_aggrchargeweight] numeric(36,8),\n[lms_parcelbyorder_aggrvolweight] numeric(36,8),\n[lms_parcelbyorder_aggrvolumiserweight] numeric(36,8),\n[lms_parcelbyorder_aggrmaxweight] numeric(36,8),\n[lms_parcelbyorder_aggrnoparcels] bigint,\n[lms_parcelbyorder_aggrweightexclspecdel] numeric(36,8),\n[lms_parcelbyorder_aggrchargeweightexclspecdel] numeric(36,8),\n[lms_parcelbyorder_aggrvolweightexclspecdel] numeric(36,8),\n[lms_parcelbyorder_aggrvolumiserweightexclspecdel] numeric(36,8),\n[lms_parcelbyorder_aggrmaxweightexclspecdel] numeric(36,8),\n[lms_parcelbyorder_aggrnoparcelsexclspecdel] bigint,\n[lms_order_maxpoddate] datetime2(7),\n[lms_order_inserviceflag] int,\n[lms_order_actualleaddaysstartdate] datetime2(7),\n[lms_order_actualleaddaysenddate] datetime2(7),\n[lms_order_actualleaddays] float,\n[lms_order_actualleaddaysinclwe] float,\n[lms_order_hodateroundup] datetime2(7),\n[lms_order_age] nvarchar(4000),\n[orderagedifferenceinseconds] bigint,\n\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/storder.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.T_STOrder \nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/T_STOrder_3MYoY')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Distribution"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.T_STOrder_3MYoY') )\n    DROP EXTERNAL TABLE dbo.T_STOrder_3MYoY\nGO\n\nCREATE EXTERNAL TABLE dbo.T_STOrder_3MYoY (\n[lms_order_id] int,\n[lms_order_corderno] nvarchar(4000),\n[lms_order_custref] nvarchar(4000),\n[lms_order_pickupcustid] int,\n[lms_order_delivercustid] int,\n[lms_order_billcustid] int,\n[lms_order_weight] numeric(36,8),\n[lms_order_ovalue] numeric(36,8),\n[lms_order_hodate] datetime2(7),\n[lms_order_cservice] nvarchar(4000),\n[lms_order_courierid] nvarchar(4000),\n[lms_order_custservice] nvarchar(4000),\n[lms_order_userid] int,\n[lms_order_appointment] datetime2(7),\n[lms_order_deliverby] datetime2(7),\n[lms_order_dconfirmed] nvarchar(4000),\n[lms_order_dcomment] nvarchar(4000),\n[lms_order_drefno] nvarchar(4000),\n[lms_order_duedate] datetime2(7),\n[lms_order_batchno] int,\n[lms_order_custaccid] int,\n[lms_order_airfreightflag] int,\n[lms_order_specdel] int,\n[lms_order_quote] numeric(36,8),\n[lms_order_quoteweight] numeric(36,8),\n[lms_order_quotefuellevy] numeric(36,8),\n[lms_order_quoteothersurcharges] numeric(36,8),\n[lms_order_quotedocfees] numeric(36,8),\n[lms_order_billvehicletype] int,\n[lms_parcelbyorder_aggrweight] numeric(36,8),\n[lms_parcelbyorder_aggrchargeweight] numeric(36,8),\n[lms_parcelbyorder_aggrvolweight] numeric(36,8),\n[lms_parcelbyorder_aggrvolumiserweight] numeric(36,8),\n[lms_parcelbyorder_aggrmaxweight] numeric(36,8),\n[lms_parcelbyorder_aggrnoparcels] bigint,\n[lms_parcelbyorder_aggrweightexclspecdel] numeric(36,8),\n[lms_parcelbyorder_aggrchargeweightexclspecdel] numeric(36,8),\n[lms_parcelbyorder_aggrvolweightexclspecdel] numeric(36,8),\n[lms_parcelbyorder_aggrvolumiserweightexclspecdel] numeric(36,8),\n[lms_parcelbyorder_aggrmaxweightexclspecdel] numeric(36,8),\n[lms_parcelbyorder_aggrnoparcelsexclspecdel] bigint,\n[lms_order_maxpoddate] datetime2(7),\n[lms_order_inserviceflag] int,\n[lms_order_actualleaddaysstartdate] datetime2(7),\n[lms_order_actualleaddaysenddate] datetime2(7),\n[lms_order_actualleaddays] float,\n[lms_order_actualleaddaysinclwe] float,\n[lms_order_hodateroundup] datetime2(7),\n[lms_order_age] nvarchar(4000),\n[orderagedifferenceinseconds] bigint,\n\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/storder3myoy.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.T_STOrder_3MYoY \nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/T_STPRPClockingHoursDaily')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Finance"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.T_STPRPClockingHoursDaily') )\n    DROP EXTERNAL TABLE dbo.T_STPRPClockingHoursDaily\nGO\n\nCREATE EXTERNAL TABLE dbo.T_STPRPClockingHoursDaily (\n\tprp_clockinghoursdaily_division nvarchar(4000),\n\tprp_clockinghoursdaily_costcentredescription nvarchar(4000),\n\tprp_clockinghoursdaily_costcentrecode nvarchar(4000),\n\tprp_clockinghoursdaily_teamname nvarchar(4000),\n\tprp_clockinghoursdaily_payroll nvarchar(4000),\n\tprp_clockinghoursdaily_vendor nvarchar(4000),\n\tprp_clockinghoursdaily_jobcategory nvarchar(4000),\n\tprp_clockinghoursdaily_date date,\n\tprp_clockinghoursdaily_week int,\n\tprp_clockinghoursdaily_approvalstatus nvarchar(4000),\n\tprp_clockinghoursdaily_totinvval numeric(36,6),\n\tprp_clockinghoursdaily_finalflag int\n\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/FINANCE/stprpclockinghoursdaily.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.T_STPRPClockingHoursDaily \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STSAP",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/T_STPRPClockingHoursDetail')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Finance"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.T_STPRPClockingHoursDetail') )\n    DROP EXTERNAL TABLE dbo.T_STPRPClockingHoursDetail\nGO\n\nCREATE EXTERNAL TABLE dbo.T_STPRPClockingHoursDetail (\n\n\tprp_clockinghoursdetail_id int,\n\tprp_clockinghoursdetail_division nvarchar(4000),\n\tprp_clockinghoursdetail_costcentredescription nvarchar(4000),\n\tprp_clockinghoursdetail_costcentrecode nvarchar(4000),\n\tprp_clockinghoursdetail_teamname nvarchar(4000),\n\tprp_clockinghoursdetail_employee nvarchar(4000),\n\tprp_clockinghoursdetail_assignment nvarchar(4000),\n\tprp_clockinghoursdetail_idpassportno nvarchar(4000),\n\tprp_clockinghoursdetail_employeenumber nvarchar(4000),\n\tprp_clockinghoursdetail_payroll nvarchar(4000),\n\tprp_clockinghoursdetail_vendorempnumber nvarchar(4000),\n\tprp_clockinghoursdetail_vendor nvarchar(4000),\n\tprp_clockinghoursdetail_jobcategory nvarchar(4000),\n\tprp_clockinghoursdetail_rate nvarchar(4000),\n\tprp_clockinghoursdetail_date date,\n\tprp_clockinghoursdetail_week int,\n\tprp_clockinghoursdetail_shift nvarchar(4000),\n\tprp_clockinghoursdetail_shifttime nvarchar(4000),\n\tprp_clockinghoursdetail_calctime nvarchar(4000),\n\tprp_clockinghoursdetail_clockin nvarchar(4000),\n\tprp_clockinghoursdetail_clockout nvarchar(4000),\n\tprp_clockinghoursdetail_approvalstatus nvarchar(4000),\n\tprp_clockinghoursdetail_totinvval numeric(36,6),\n\tprp_clockinghoursdetail_finalflag int\n\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/FINANCE/stprpclockinghoursdetail.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.T_STPRPClockingHoursDetail \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STSAP",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/T_STParcel')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Distribution"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.T_STParcel') )\n    DROP EXTERNAL TABLE dbo.T_STParcel\nGO\n\nCREATE EXTERNAL TABLE dbo.T_STParcel (\n[lms_parcel_id] int,\n[lms_parcel_orderid] int,\n[lms_parcel_consignid] int,\n[lms_parcel_waybillid] int,\n[lms_parcel_locid] int,\n[lms_parcel_totcharge] numeric(36,8),\n[lms_parcel_dimms] float,\n[lms_parcel_noofpcls] int,\n[lms_parcel_pl] numeric(36,8),\n[lms_parcel_pw] numeric(36,8),\n[lms_parcel_ph] numeric(36,8),\n[lms_parcel_pdate] datetime2(7),\n[lms_parcel_acceptancedate] datetime2(7),\n[lms_parcel_acceptancedate2] datetime2(7),\n[lms_parcel_barcode] nvarchar(4000),\n[lms_parcel_volweight] float,\n[lms_parcel_weight] float,\n[lms_parcel_chargeweight] numeric(36,8),\n[lms_parcel_pweight] float,\n[lms_parcel_ptype] int,\n[lms_parcel_handoverdate] datetime2(7),\n[lms_parcel_floorstatus] nvarchar(4000),\n[lms_parcel_claimid] int,\n[lms_parcel_datevolumised] datetime2(7),\n[lms_parcel_volumiserweight] numeric(36,8),\n[lms_parcel_volumiserlength] numeric(36,8),\n[lms_parcel_volumiserheight] numeric(36,8),\n[lms_parcel_volumiserwidth] numeric(36,8),\n[lms_parcel_volumiserid] int,\n[lms_parcel_lastupdate] datetime2(7),\n[lms_parcel_description] nvarchar(4000),\n[lms_parcel_oldwbid] int,\n[lms_parcel_disflag] int,\n[lms_parcel_customerbarcode] nvarchar(4000),\n[lms_parcel_cref] nvarchar(4000),\n[lms_parcel_orderhodate] datetime2(7),\n[lms_parcel_orderpickupcustid] int,\n[lms_parcel_orderdelivercustid] int,\n[lms_parcel_orderbillcustid] int,\n[lms_parcel_orderspecdel] int,\n[lms_parcel_ordercustaccid] int,\n[lms_parcel_ordercustref] nvarchar(4000),\n[lms_parcel_ordercorderno] nvarchar(4000),\n[lms_parcel_custaccdescription] nvarchar(4000),\n[lms_parcel_custacccref] nvarchar(4000),\n[lms_parcel_consignmentcdate] datetime2(7),\n[lms_parcel_consignmentdeliverby] datetime2(7),\n[lms_parcel_consignmentcref] nvarchar(4000),\n[lms_parcel_waybillpoddate] datetime2(7),\n[lms_parcel_waybilldate] datetime2(7),\n[lms_parcel_waybillcref] nvarchar(4000),\n[lms_parcel_parceldetailid] int,\n[lms_parcel_parceldetailinservicestart] datetime2(7),\n[lms_parcel_parceldetaildeliverby] datetime2(7),\n[lms_parcel_parceldetailservicedays] int,\n[lms_parcel_parceldetailservicetime] numeric(36,8),\n[lms_parcel_parceldetailcutofftime] numeric(36,8),\n[lms_parcel_parceldetailappointment] datetime2(7),\n[lms_parcel_parceldetailnddid] int,\n[lms_parcel_parceldetailcustonholdstart] datetime2(7),\n[lms_parcel_parceldetailcustonholdend] datetime2(7),\n[lms_parcel_parceldetailsplitinitial] int,\n[lms_parcel_parceldetailweekendholidays] int,\n[lms_parcel_parceldetailinbound] int,\n[lms_parcel_parceldetailcourierid] nvarchar(4000),\n[lms_parcel_parceldetailserviceid] nvarchar(4000),\n[lms_parcel_parceldetailcustaccid] int,\n[lms_parcel_parceldetailbroutemasterid] int,\n[lms_parcel_parceldetailbroutemasterfromid] int,\n[lms_parcel_parceldetailbroutemastertoid] int,\n[lms_parcel_parceldetailleadtimeid] int,\n[lms_parcel_inserviceflag] int,\n[lms_parcel_actualleaddaysstartdate] datetime2(7),\n[lms_parcel_actualleaddaysenddate] datetime2(7),\n[lms_parcel_actualleaddaysinclwe] float,\n[lms_parcel_actualleaddays] float,\n[orv_parcel_scandate] datetime2(7),\n[orv_parcel_scanstatus] nvarchar(4000),\n[orv_parcel_scanmode] nvarchar(4000),\n[orv_parcel_claimid] bigint,\n[orv_parcel_courierid] bigint,\n[orv_parcel_moddate] datetime2(7),\n[lms_parcel_parceldetaildeliverbyroundup] datetime2(7),\n[lms_parcel_age] nvarchar(4000),\n[lms_track_enddate] datetime2(7),\n[lms_track_floortimeend] datetime2(7),\n[lms_track_fromlocid] int,\n[lms_parcel_pdatetocdate] bigint,\n[lms_parcel_cdatetoenddate] bigint,\n[lms_parcel_startdatetoenddate] bigint,\n[lms_parcel_floortimeedndtoenddate] bigint,\n[lms_parcel_deliveryduration] bigint,\n[parcelagedifferenceinseconds] bigint,\n[lms_parcel_splitconsignmentflag] int,\n[lms_parcel_splitdeliveredflag] int,\n[lms_location_consignmentlocid] int,\n[lms_location_consignmentlocdescription] nvarchar(4000),\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stparcel.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.T_STParcel \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/T_STParcelDamagedAudit')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Distribution"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.T_STParcelDamagedAudit') )\n    DROP EXTERNAL TABLE dbo.T_STParcelDamagedAudit\nGO\n\nCREATE EXTERNAL TABLE dbo.T_STParcelDamagedAudit (\n[lms_damagedparcelaudit_id] int,\n[lms_damagedparcelaudit_parcelid] int,\n[lms_damagedparcelaudit_userid] int,\n[lms_damagedparcelaudit_auditdatetime] datetime2(7),\n[lms_damagedparcelaudit_locid] int,\n[lms_damagedparcelaudit_loadid] int,\n[lms_damagedparcelaudit_quantity] int,\n[lms_damagedparcelaudit_flutemeasure] numeric(18,2),\n[lms_damagedparcelaudit_weight] numeric(18,2),\n[lms_damagedparcelaudit_length] numeric(18,2),\n[lms_damagedparcelaudit_width] numeric(18,2),\n[lms_damagedparcelaudit_height] numeric(18,2),\n[lms_damagedparcelaudit_shrinkwrap] int,\n[lms_damagedparcelaudit_tapeusedid] int,\n[lms_damagedparcelaudit_missing] int,\n[lms_damagedparcelaudit_damaged] int,\n[lms_damagedparcelaudit_parcelutilization] int,\n[lms_damagedparcelaudit_damagetypeid] int,\n[lms_damagedparcelaudit_typeid] int,\n[lms_parcel_orderdelivercustid] int,\n[lms_parcel_orderbillcustid] int,\n[lms_location_description] nvarchar(4000),\n[lms_damagedparcelaudittype_description] nvarchar(4000),\n[lms_damagedparcelaudittapetype_description] nvarchar(4000)\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stdamagedparcelaudit.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.T_STParcelDamagedAudit\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/T_STParcelDamagedAudit_3MYoY')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Distribution"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.T_STParcelDamagedAudit_3MYoY') )\n    DROP EXTERNAL TABLE dbo.T_STParcelDamagedAudit_3MYoY\nGO\n\nCREATE EXTERNAL TABLE dbo.T_STParcelDamagedAudit_3MYoY (\n[lms_damagedparcelaudit_id] int,\n[lms_damagedparcelaudit_parcelid] int,\n[lms_damagedparcelaudit_userid] int,\n[lms_damagedparcelaudit_auditdatetime] datetime2(7),\n[lms_damagedparcelaudit_locid] int,\n[lms_damagedparcelaudit_loadid] int,\n[lms_damagedparcelaudit_quantity] int,\n[lms_damagedparcelaudit_flutemeasure] numeric(18,2),\n[lms_damagedparcelaudit_weight] numeric(18,2),\n[lms_damagedparcelaudit_length] numeric(18,2),\n[lms_damagedparcelaudit_width] numeric(18,2),\n[lms_damagedparcelaudit_height] numeric(18,2),\n[lms_damagedparcelaudit_shrinkwrap] int,\n[lms_damagedparcelaudit_tapeusedid] int,\n[lms_damagedparcelaudit_missing] int,\n[lms_damagedparcelaudit_damaged] int,\n[lms_damagedparcelaudit_parcelutilization] int,\n[lms_damagedparcelaudit_damagetypeid] int,\n[lms_damagedparcelaudit_typeid] int,\n[lms_parcel_orderdelivercustid] int,\n[lms_parcel_orderbillcustid] int,\n[lms_location_description] nvarchar(4000),\n[lms_damagedparcelaudittype_description] nvarchar(4000),\n[lms_damagedparcelaudittapetype_description] nvarchar(4000)\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stdamagedparcelaudit3myoy.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.T_STParcelDamagedAudit_3MYoY\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/T_STParcelMRPRecon')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Distribution"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.T_STParcelMRPRecon') )\n    DROP EXTERNAL TABLE dbo.T_STParcelMRPRecon\nGO\n\nCREATE EXTERNAL TABLE dbo.T_STParcelMRPRecon (\n[lms_parcel_id] int,\n[lms_parcel_orderid] int,\n[lms_parcel_consignid] int,\n[lms_parcel_waybillid] int,\n[lms_parcel_locid] int,\n[lms_parcel_totcharge] numeric(36,8),\n[lms_parcel_dimms] float,\n[lms_parcel_noofpcls] int,\n[lms_parcel_pl] numeric(36,8),\n[lms_parcel_pw] numeric(36,8),\n[lms_parcel_ph] numeric(36,8),\n[lms_parcel_pdate] datetime2(7),\n[lms_parcel_acceptancedate] datetime2(7),\n[lms_parcel_acceptancedate2] datetime2(7),\n[lms_parcel_barcode] nvarchar(4000),\n[lms_parcel_volweight] float,\n[lms_parcel_weight] float,\n[lms_parcel_chargeweight] numeric(36,8),\n[lms_parcel_pweight] float,\n[lms_parcel_ptype] int,\n[lms_parcel_handoverdate] datetime2(7),\n[lms_parcel_floorstatus] nvarchar(4000),\n[lms_parcel_claimid] int,\n[lms_parcel_datevolumised] datetime2(7),\n[lms_parcel_volumiserweight] numeric(36,8),\n[lms_parcel_volumiserlength] numeric(36,8),\n[lms_parcel_volumiserheight] numeric(36,8),\n[lms_parcel_volumiserwidth] numeric(36,8),\n[lms_parcel_volumiserid] int,\n[lms_parcel_lastupdate] datetime2(7),\n[lms_parcel_description] nvarchar(4000),\n[lms_parcel_oldwbid] int,\n[lms_parcel_disflag] int,\n[lms_parcel_customerbarcode] nvarchar(4000),\n[lms_parcel_cref] nvarchar(4000),\n[lms_parcel_orderhodate] datetime2(7),\n[lms_parcel_orderpickupcustid] int,\n[lms_parcel_orderdelivercustid] int,\n[lms_parcel_orderbillcustid] int,\n[lms_parcel_orderspecdel] int,\n[lms_parcel_ordercustaccid] int,\n[lms_parcel_ordercustref] nvarchar(4000),\n[lms_parcel_ordercorderno] nvarchar(4000),\n[lms_parcel_custaccdescription] nvarchar(4000),\n[lms_parcel_custacccref] nvarchar(4000),\n[lms_parcel_consignmentcdate] datetime2(7),\n[lms_parcel_consignmentdeliverby] datetime2(7),\n[lms_parcel_consignmentcref] nvarchar(4000),\n[lms_parcel_waybillpoddate] datetime2(7),\n[lms_parcel_waybilldate] datetime2(7),\n[lms_parcel_waybillcref] nvarchar(4000),\n[lms_parcel_parceldetailid] int,\n[lms_parcel_parceldetailinservicestart] datetime2(7),\n[lms_parcel_parceldetaildeliverby] datetime2(7),\n[lms_parcel_parceldetailservicedays] int,\n[lms_parcel_parceldetailservicetime] numeric(36,8),\n[lms_parcel_parceldetailcutofftime] numeric(36,8),\n[lms_parcel_parceldetailappointment] datetime2(7),\n[lms_parcel_parceldetailnddid] int,\n[lms_parcel_parceldetailcustonholdstart] datetime2(7),\n[lms_parcel_parceldetailcustonholdend] datetime2(7),\n[lms_parcel_parceldetailsplitinitial] int,\n[lms_parcel_parceldetailweekendholidays] int,\n[lms_parcel_parceldetailinbound] int,\n[lms_parcel_parceldetailcourierid] nvarchar(4000),\n[lms_parcel_parceldetailserviceid] nvarchar(4000),\n[lms_parcel_parceldetailcustaccid] int,\n[lms_parcel_parceldetailbroutemasterid] int,\n[lms_parcel_parceldetailbroutemasterfromid] int,\n[lms_parcel_parceldetailbroutemastertoid] int,\n[lms_parcel_parceldetailleadtimeid] int,\n[lms_parcel_inserviceflag] int,\n[lms_parcel_actualleaddaysstartdate] datetime2(7),\n[lms_parcel_actualleaddaysenddate] datetime2(7),\n[lms_parcel_actualleaddaysinclwe] float,\n[lms_parcel_actualleaddays] float,\n[orv_parcel_scandate] datetime2(7),\n[orv_parcel_scanstatus] nvarchar(4000),\n[orv_parcel_scanmode] nvarchar(4000),\n[orv_parcel_claimid] bigint,\n[orv_parcel_courierid] bigint,\n[orv_parcel_moddate] datetime2(7),\n[lms_parcel_parceldetaildeliverbyroundup] datetime2(7),\n[lms_parcel_age] nvarchar(4000),\n[lms_track_enddate] datetime2(7),\n[lms_track_floortimeend] datetime2(7),\n[lms_track_fromlocid] int,\n[lms_parcel_pdatetocdate] bigint,\n[lms_parcel_cdatetoenddate] bigint,\n[lms_parcel_startdatetoenddate] bigint,\n[lms_parcel_floortimeedndtoenddate] bigint,\n[lms_parcel_deliveryduration] bigint,\n[parcelagedifferenceinseconds] bigint,\n[lms_parcel_splitconsignmentflag] int,\n[lms_parcel_splitdeliveredflag] int\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stparcelmrprecon.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.T_STParcelMRPRecon \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/T_STParcel_3MYoY')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Distribution"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.T_STParcel_3MYoY') )\n    DROP EXTERNAL TABLE dbo.T_STParcel_3MYoY\nGO\n\nCREATE EXTERNAL TABLE dbo.T_STParcel_3MYoY (\n[lms_parcel_id] int,\n[lms_parcel_orderid] int,\n[lms_parcel_consignid] int,\n[lms_parcel_waybillid] int,\n[lms_parcel_locid] int,\n[lms_parcel_totcharge] numeric(36,8),\n[lms_parcel_dimms] float,\n[lms_parcel_noofpcls] int,\n[lms_parcel_pl] numeric(36,8),\n[lms_parcel_pw] numeric(36,8),\n[lms_parcel_ph] numeric(36,8),\n[lms_parcel_pdate] datetime2(7),\n[lms_parcel_acceptancedate] datetime2(7),\n[lms_parcel_acceptancedate2] datetime2(7),\n[lms_parcel_barcode] nvarchar(4000),\n[lms_parcel_volweight] float,\n[lms_parcel_weight] float,\n[lms_parcel_chargeweight] numeric(36,8),\n[lms_parcel_pweight] float,\n[lms_parcel_ptype] int,\n[lms_parcel_handoverdate] datetime2(7),\n[lms_parcel_floorstatus] nvarchar(4000),\n[lms_parcel_claimid] int,\n[lms_parcel_datevolumised] datetime2(7),\n[lms_parcel_volumiserweight] numeric(36,8),\n[lms_parcel_volumiserlength] numeric(36,8),\n[lms_parcel_volumiserheight] numeric(36,8),\n[lms_parcel_volumiserwidth] numeric(36,8),\n[lms_parcel_volumiserid] int,\n[lms_parcel_lastupdate] datetime2(7),\n[lms_parcel_description] nvarchar(4000),\n[lms_parcel_oldwbid] int,\n[lms_parcel_disflag] int,\n[lms_parcel_customerbarcode] nvarchar(4000),\n[lms_parcel_cref] nvarchar(4000),\n[lms_parcel_orderhodate] datetime2(7),\n[lms_parcel_orderpickupcustid] int,\n[lms_parcel_orderdelivercustid] int,\n[lms_parcel_orderbillcustid] int,\n[lms_parcel_orderspecdel] int,\n[lms_parcel_ordercustaccid] int,\n[lms_parcel_ordercustref] nvarchar(4000),\n[lms_parcel_ordercorderno] nvarchar(4000),\n[lms_parcel_custaccdescription] nvarchar(4000),\n[lms_parcel_custacccref] nvarchar(4000),\n[lms_parcel_consignmentcdate] datetime2(7),\n[lms_parcel_consignmentdeliverby] datetime2(7),\n[lms_parcel_consignmentcref] nvarchar(4000),\n[lms_parcel_waybillpoddate] datetime2(7),\n[lms_parcel_waybilldate] datetime2(7),\n[lms_parcel_waybillcref] nvarchar(4000),\n[lms_parcel_parceldetailid] int,\n[lms_parcel_parceldetailinservicestart] datetime2(7),\n[lms_parcel_parceldetaildeliverby] datetime2(7),\n[lms_parcel_parceldetailservicedays] int,\n[lms_parcel_parceldetailservicetime] numeric(36,8),\n[lms_parcel_parceldetailcutofftime] numeric(36,8),\n[lms_parcel_parceldetailappointment] datetime2(7),\n[lms_parcel_parceldetailnddid] int,\n[lms_parcel_parceldetailcustonholdstart] datetime2(7),\n[lms_parcel_parceldetailcustonholdend] datetime2(7),\n[lms_parcel_parceldetailsplitinitial] int,\n[lms_parcel_parceldetailweekendholidays] int,\n[lms_parcel_parceldetailinbound] int,\n[lms_parcel_parceldetailcourierid] nvarchar(4000),\n[lms_parcel_parceldetailserviceid] nvarchar(4000),\n[lms_parcel_parceldetailcustaccid] int,\n[lms_parcel_parceldetailbroutemasterid] int,\n[lms_parcel_parceldetailbroutemasterfromid] int,\n[lms_parcel_parceldetailbroutemastertoid] int,\n[lms_parcel_parceldetailleadtimeid] int,\n[lms_parcel_inserviceflag] int,\n[lms_parcel_actualleaddaysstartdate] datetime2(7),\n[lms_parcel_actualleaddaysenddate] datetime2(7),\n[lms_parcel_actualleaddaysinclwe] float,\n[lms_parcel_actualleaddays] float,\n[orv_parcel_scandate] datetime2(7),\n[orv_parcel_scanstatus] nvarchar(4000),\n[orv_parcel_scanmode] nvarchar(4000),\n[orv_parcel_claimid] bigint,\n[orv_parcel_courierid] bigint,\n[orv_parcel_moddate] datetime2(7),\n[lms_parcel_parceldetaildeliverbyroundup] datetime2(7),\n[lms_parcel_age] nvarchar(4000),\n[lms_track_enddate] datetime2(7),\n[lms_track_floortimeend] datetime2(7),\n[lms_track_fromlocid] int,\n[lms_parcel_pdatetocdate] bigint,\n[lms_parcel_cdatetoenddate] bigint,\n[lms_parcel_startdatetoenddate] bigint,\n[lms_parcel_floortimeedndtoenddate] bigint,\n[lms_parcel_deliveryduration] bigint,\n[parcelagedifferenceinseconds] bigint,\n[lms_parcel_splitconsignmentflag] int,\n[lms_parcel_splitdeliveredflag] int,\n[lms_location_consignmentlocid] int,\n[lms_location_consignmentlocdescription] nvarchar(4000),\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stparcel3myoy.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.T_STParcel_3MYoY \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/T_STPlcLpnRouting')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Sorter"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.T_STPlcLpnRouting') )\n    DROP EXTERNAL TABLE dbo.T_STPlcLpnRouting\nGO\n\nCREATE EXTERNAL TABLE dbo.T_STPlcLpnRouting (\n    [lms_plclpnrouting_id] int,\n    [lms_plclpnrouting_autosystemid] int,\n    [lms_plclpnrouting_lpn] nvarchar(4000),\n    [lms_plclpnrouting_divertid] int,\n    [lms_plclpnrouting_scanpoint] int,\n    [lms_plclpnrouting_status] int,\n    [lms_plclpnrouting_datereceived] datetime2(7),\n    [lms_plclpnrouting_dateupdated] datetime2(7),\n    [lms_plclpnrouting_plcdivertid] int,\n    [lms_plclpnrouting_cldivertid] int,\n    [lms_plclpnrouting_divertdescription] nvarchar(4000),\n    [lms_parcel_id] int,\n    [lms_parcel_orderid] int,\n    [lms_parcel_consignid] int,\n    [lms_parcel_waybillid] int,\n    [lms_parcel_pdate] datetime2(7),\n    [lms_parcel_orderbillcustid] int,\n    [lms_parcel_orderdelivercustid] int,\n    [excel_sorterchutes_location] nvarchar(4000),\n    [excel_sortertimestudy_averageminsperparcel] decimal(36,6),\n    [excel_sorterstaffallocation_generalworkerallocation] nvarchar(4000),\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stplclpnrouting.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.T_STPlcLpnRouting \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/T_STRetroAnalysis')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Distribution"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.T_STRetroAnalysis') )\n    DROP EXTERNAL TABLE dbo.T_STRetroAnalysis\nGO\n\nCREATE EXTERNAL TABLE dbo.T_STRetroAnalysis (\n\n[orv_delivery_id] int,\n[orv_delivery_waybillid] int,\n[orv_delivery_deliveryid] int,\n[orv_delivery_manifest] int,\n[orv_delivery_dispatchid] int,\n[orv_delivery_depot] nvarchar(4000),\n[orv_delivery_fleetcode] nvarchar(4000),\n[orv_delivery_driver] nvarchar(4000),\n[orv_delivery_progress] nvarchar(4000),\n[orv_delivery_originaldeparturedate] datetime2(7),\n[orv_delivery_dispatchstartdate] datetime2(7),\n[orv_delivery_exitdepodate] datetime2(7),\n[orv_delivery_enterdepodate] datetime2(7),\n[orv_delivery_totalpausetime] int,\n[orv_delivery_etatodepot] datetime2(7),\n[orv_delivery_delcustomer] nvarchar(4000),\n[orv_delivery_deloriginalorder] int,\n[orv_delivery_deloptimisedorder] int,\n[orv_delivery_delactualorder] int,\n[orv_delivery_distancetravelled] int,\n[orv_delivery_delactualeta] datetime2(7),\n[orv_delivery_retroexpeta] datetime2(7),\n[orv_delivery_firstscandate] datetime2(7),\n[orv_delivery_deltaretroexpetatofisrtscan] int,\n[orv_delivery_delnoparcels] int,\n[orv_delivery_targetdeldate] datetime2(7),\n[orv_delivery_actualdeltime] int,\n[orv_delivery_deliverydate] datetime2(7),\n[orv_delivery_targetexittime] datetime2(7),\n[orv_delivery_exittime] datetime2(7),\n[orv_delivery_delstatus] nvarchar(4000),\n[orv_delivery_deltalastscantopod] int,\n[orv_delivery_poddate] datetime2(7),\n[orv_delivery_deltadispatchstartdate] int,\n[orv_delivery_deltadispatchexitandstart] int,\n[orv_delivery_deltaetatodepoandenterdepo] int,\n[orv_delivery_stoppedindepo] nvarchar(4000),\n[orv_delivery_targetdeltime] int,\n[orv_delivery_actualtime] int,\n[orv_delivery_deltaactualtimeandshouldtake] int,\n[orv_delivery_shouldtake] int,\n[orv_delivery_insdate] datetime2(7),\n[orv_delivery_originalorderkm] float,\n[orv_delivery_optimisedorderkm] float,\n[orv_delivery_totaldistancetravelledkm] float,\n[orv_delivery_lastdeliverytime] datetime2(7),\n[orv_delivery_lastexitgeofencedeliverytime] datetime2(7),\n[orv_delivery_totalactualdeliverytime] int,\n[orv_delivery_totalestimateddeliverytime] int,\n[orv_delivery_deltadeliverytime] int,\n[orv_delivery_distancefromlastdelivery] int,\n[orv_delivery_lastdeliverytimetodepo] int,\n[orv_delivery_stopdispatch] datetime2(7),\n[orv_delivery_totaldistanceactualkm] float,\n[orv_delivery_totalestimateddrivetime] int,\n[orv_delivery_lastdeliverycustomer] nvarchar(4000),\n[orv_delivery_actualdistancekm] float,\n[orv_delivery_estimateddrivetime] int,\n[orv_delivery_billingcustomer] nvarchar(4000)\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stretroanalysis.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.T_STRetroAnalysis \nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/T_STStockRevaluations')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Finance"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.T_STStockRevaluations') )\n    DROP EXTERNAL TABLE dbo.T_STStockRevaluations\nGO\n\nCREATE EXTERNAL TABLE dbo.T_STStockRevaluations (\n\t[sap_stockrevaluationsheader_docentry] int,\n\t[sap_stockrevaluationsheader_transid] int,\n\t[sap_stockrevaluationsheader_docnum] int,\n\t[sap_stockrevaluationsheader_postingdate] datetime2(7),\n\t[sap_stockrevaluationsheader_comments] nvarchar(4000),\n\t[sap_stockrevaluationsheader_objtype] nvarchar(4000),\n\t[sap_stockrevaluationsline_linenum] int,\n\t[sap_stockrevaluationsline_dscription] nvarchar(4000),\n\t[sap_stockrevaluationsline_itemcode] nvarchar(4000),\n\t[sap_stockrevaluationsline_subdepotcode] nvarchar(4000),\n\t[sap_stockrevaluationsline_linetotal] numeric(18,2),\n\t[sap_stockrevaluationsline_acctcode] nvarchar(4000),\n\t[sap_stockrevaluationsline_doccat] nvarchar(4000),\n\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/FINANCE/ststockrevaluations.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.T_STStockRevaluations \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STSAP",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/T_STStockTransactions')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Finance"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.T_STStockTransactions') )\n    DROP EXTERNAL TABLE dbo.T_STStockTransactions\nGO\n\nCREATE EXTERNAL TABLE dbo.T_STStockTransactions (\n\t[sap_stocktransactionsheader_docentry] int,\n\t[sap_stocktransactionsheader_docnum] int,\n\t[sap_stocktransactionsheader_doctype] nvarchar(4000),\n\t[sap_stocktransactionsheader_objtype] int,\n\t[sap_stocktransactionsheader_postingdate] datetime2(7),\n\t[sap_stocktransactionsheader_duedate] datetime2(7),\n\t[sap_stocktransactionsheader_cardcode] nvarchar(4000),\n\t[sap_stocktransactionsheader_cardref] nvarchar(4000),\n\t[sap_stocktransactionsheader_discprcnt] numeric(18,2),\n\t[sap_stocktransactionsheader_discsum] numeric(18,2),\n\t[sap_stocktransactionsheader_doctotal] numeric(18,2),\n\t[sap_stocktransactionsheader_comments] nvarchar(4000),\n\t[sap_stocktransactionsheader_transid] int,\n\t[sap_stocktransactionsheader_u_expense_depot] nvarchar(4000),\n\t[sap_stocktransactionsline_linenum] int,\n\t[sap_stocktransactionsline_linestatus] nvarchar(4000),\n\t[sap_stocktransactionsline_itemcode] nvarchar(4000),\n\t[sap_stocktransactionsline_dscription] nvarchar(4000),\n\t[sap_stocktransactionsline_quantity] numeric(18,2),\n\t[sap_stocktransactionsline_openqty] numeric(18,2),\n\t[sap_stocktransactionsline_price] numeric(18,2),\n\t[sap_stocktransactionsline_rate] numeric(18,2),\n\t[sap_stocktransactionsline_discprcnt] numeric(18,2),\n\t[sap_stocktransactionsline_linetotal] numeric(18,2),\n\t[sap_stocktransactionsline_opensum] numeric(18,2),\n\t[sap_stocktransactionsline_pricebefdi] numeric(18,2),\n\t[sap_stocktransactionsline_docdate] datetime2(7),\n\t[sap_stocktransactionsline_project] nvarchar(4000),\n\t[sap_stocktransactionsline_vatprcnt] numeric(18,2),\n\t[sap_stocktransactionsline_vatgroup] nvarchar(4000),\n\t[sap_stocktransactionsline_vatamount] numeric(18,2),\n\t[sap_stocktransactionsline_u_reason] nvarchar(4000),\n\t[sap_stocktransactionsline_u_expenseitem] nvarchar(4000),\n\t[sap_stocktransactionsline_u_fueldate] datetime2(7),\n\t[sap_stocktransactionsline_acctcode] nvarchar(4000),\n\t[sap_stocktransactionsline_doccat] nvarchar(4000),\n\t[sap_stocktransactionsline_subdepotcode] nvarchar(4000),\n\t[sap_stocktransactionsline_discsum] numeric(18,2),\n\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/FINANCE/ststocktransactions.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.T_STStockTransactions \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STSAP",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/T_STTMSActivityDetail')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Finance"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.T_STTMSActivityDetail') )\n    DROP EXTERNAL TABLE dbo.T_STTMSActivityDetail\nGO\n\nCREATE EXTERNAL TABLE dbo.T_STTMSActivityDetail (\n[tms_activity_tripid] bigint,\n[tms_activity_tripdriverid] nvarchar(4000),\n[tms_activity_tripdrivername] nvarchar(4000),\n[tms_activity_tripvehicleid] nvarchar(4000),\n[tms_activity_tripvehiclefleetcode] nvarchar(4000),\n[tms_activity_triptrailer1id] nvarchar(4000),\n[tms_activity_triptrailer1fleetcode] nvarchar(4000),\n[tms_activity_triptrailer2id] nvarchar(4000),\n[tms_activity_triptrailer2fleetcode] nvarchar(4000),\n[tms_activity_triptrailersize] nvarchar(4000),\n[tms_activity_tripstatus] nvarchar(4000),\n[tms_activity_tripparenetrouteid] bigint,\n[tms_activity_tripparentroute] nvarchar(4000),\n[tms_activity_tripopsrouteid] bigint,\n[tms_activity_tripopsroute] nvarchar(4000),\n[tms_activity_tripdispatchdate] date,\n[tms_activity_triptransporter] nvarchar(4000),\n[tms_activity_tripsource] nvarchar(4000),\n[tms_activity_bookingid] bigint,\n[tms_activity_bookingtype] nvarchar(4000),\n[tms_activity_bookingtrailersize] nvarchar(4000),\n[tms_activity_bookingcubicvolume] float,\n[tms_activity_directbooking] int,\n[tms_activity_localbooking] int,\n[tms_activity_bookingsource] nvarchar(4000),\n[tms_activity_bookingallocateddate] date,\n[tms_activity_customerid] bigint,\n[tms_activity_primarycustomer] nvarchar(4000),\n[tms_activity_childcustomerid] bigint,\n[tms_activity_secondarycustomer] nvarchar(4000),\n\n)\n\tWITH (\n\tLOCATION = 'Structured Data/FINANCE/sttmsactivitydetail.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t);\n\nGO\n\n\nSELECT TOP 100 * FROM dbo.T_STTMSActivityDetail \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STSAP",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/T_STTMSActivitySummary')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Finance"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.T_STTMSActivitySummary') )\n    DROP EXTERNAL TABLE dbo.T_STTMSActivitySummary\nGO\n\nCREATE EXTERNAL TABLE dbo.T_STTMSActivitySummary (\n[tms_activity_tripid] bigint,\n[tms_activity_bookingid] bigint,\n[tms_activity_tripdrivername] nvarchar(4000),\n[tms_activity_tripstatus] nvarchar(4000),\n[tms_activity_tripopsroute] nvarchar(4000),\n[tms_activity_tripdispatchdate] date,\n[tms_activity_tripsource] nvarchar(4000),\n[tms_activity_directbooking] int,\n[tms_activity_localbooking] int,\n[tms_activity_triptrailersize] nvarchar(4000),\n[tms_activity_triptransporter] nvarchar(4000),\n[tms_activity_bookingtype] nvarchar(4000),\n[tms_activity_aggrbookingcubicvolume] float,\n[tms_activity_aggrbookingid] bigint,\n[tms_activity_bookingallocateddate] date,\n[tms_activity_primarycustomer] nvarchar(4000),\n[tms_activity_secondarycustomer] nvarchar(4000),\n\n)\n\tWITH (\n\tLOCATION = 'Structured Data/FINANCE/sttmsactivitysummary.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t);\n\nGO\n\n\nSELECT TOP 100 * FROM dbo.T_STTMSActivitySummary \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STSAP",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/T_STTMSTrack')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Linehaul"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.T_STTMSTrack') )\n    DROP EXTERNAL TABLE dbo.T_STTMSTrack\nGO\n\nCREATE EXTERNAL TABLE dbo.T_STTMSTrack (\n    [tms_track_id] bigint,\n    [tms_track_tripid] bigint,\n    [tms_track_source] nvarchar(4000),\n    [tms_track_trackdate] datetime2(7),\n    [tms_track_tracktype] nvarchar(4000),\n    [tms_track_userid] bigint,\n    [tms_track_lat] float,\n    [tms_track_lng] float,\n    [tms_track_address] nvarchar(4000),\n    [tms_track_note] nvarchar(4000),\n\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/sttmstrack.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.T_STTMSTrack \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/T_STTransactionsDetail')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Finance"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.T_STTransactionsDetail') )\n    DROP EXTERNAL TABLE dbo.T_STTransactionsDetail\nGO\n\nCREATE EXTERNAL TABLE dbo.T_STTransactionsDetail (\n\t[sap_transactionsline_polookupid] nvarchar(4000),\n\t[sap_transactionsheader_docentry] int,\n\t[sap_transactionsheader_docnum] int,\n\t[sap_transactionsheader_doctype] nvarchar(4000),\n\t[sap_transactionsheader_objtype] nvarchar(4000),\n\t[sap_transactionsheader_postingdate] date,\n\t[sap_transactionsheader_duedate] date,\n\t[sap_transactionsheader_cardcode] nvarchar(4000),\n\t[sap_transactionsheader_cardref] nvarchar(4000),\n\t[sap_transactionsheader_discprcnt] numeric(18,2),\n\t[sap_transactionsheader_discsum] numeric(18,2),\n\t[sap_transactionsheader_doctotal] numeric(18,2),\n\t[sap_transactionsheader_comments] nvarchar(4000),\n\t[sap_transactionsheader_transid] int,\n\t[sap_transactionsheader_u_expense_depot] nvarchar(4000),\n\t[sap_transactionsline_linenum] int,\n\t[sap_transactionsline_linestatus] nvarchar(4000),\n\t[sap_transactionsline_itemcode] nvarchar(4000),\n\t[sap_transactionsline_dscription] nvarchar(4000),\n\t[sap_transactionsline_quantity] numeric(18,2),\n\t[sap_transactionsline_openqty] numeric(18,2),\n\t[sap_transactionsline_price] numeric(18,2),\n\t[sap_transactionsline_rate] numeric(18,2),\n\t[sap_transactionsline_discprcnt] numeric(18,2),\n\t[sap_transactionsline_linetotal] numeric(18,2),\n\t[sap_transactionsline_linetotallessdiscount] numeric(18,2),\n\t[sap_transactionsline_linetotalfinal] numeric(18,2),\n\t[sap_transactionsline_linetotalfinalexclgjpo] numeric(18,2),\n\t[sap_transactionsline_opensum] numeric(18,2),\n\t[sap_transactionsline_pricebefdi] numeric(18,2),\n\t[sap_transactionsline_docdate] date,\n\t[sap_transactionsline_project] nvarchar(4000),\n\t[sap_transactionsline_vatprcnt] numeric(18,2),\n\t[sap_transactionsline_vatgroup] nvarchar(4000),\n\t[sap_transactionsline_vatamount] numeric(18,2),\n\t[sap_transactionsline_u_reason] nvarchar(4000),\n\t[sap_transactionsline_u_expenseitem] nvarchar(4000),\n\t[sap_transactionsline_u_fueldate] date,\n\t[sap_transactionsline_acctcode] nvarchar(4000),\n\t[sap_transactionsline_doccat] nvarchar(4000),\n\t[sap_transactionsline_subdepotcode] nvarchar(4000),\n\t[sap_transactionsline_u_rate] nvarchar(4000),\n\t[sap_transactionsline_u_ir_number] int,\n\t[sap_transactionsline_discsum] numeric(18,2),\n\t[sap_transactionsline_vatrate] numeric(18,2),\n\t[sap_transactionsline_description] nvarchar(4000),\n\t[sap_transactionsline_jnlref1] nvarchar(4000),\n\t[sap_transactionsline_jnlref2] nvarchar(4000),\n\t[sap_transactionsline_invntitem] nvarchar(4000),\n\t[sap_transactionsline_exception] nvarchar(4000)\n\n\n\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/FINANCE/sttransactionsdetail.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.T_STTransactionsDetail \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STSAP",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/T_STTransactionsSummary')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Finance"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.T_STTransactionsSummary') )\n    DROP EXTERNAL TABLE dbo.T_STTransactionsSummary\nGO\n\nCREATE EXTERNAL TABLE dbo.T_STTransactionsSummary (\n[sap_transactionsline_acctcode] nvarchar(4000),\n[sap_transactionsheader_postingdate] date,\n[sap_transactionsline_subdepotcode] nvarchar(4000),\n[sap_transactionsline_doccat] nvarchar(4000),\n[sap_transactionsline_linetotal] numeric(18,2),\n[sap_transactionsline_linetotalfinal] numeric(18,2),\n[sap_transactionsline_linetotalfinalexclgjpo] numeric(18,2),\n)\n\tWITH (\n\tLOCATION = 'Structured Data/FINANCE/sttransactionssummary.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t);\n\nGO\n\n\nSELECT TOP 100 * FROM dbo.T_STTransactionsSummary \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STSAP",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/T_STTrip')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Linehaul"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.T_STTrip') )\n    DROP EXTERNAL TABLE dbo.T_STTrip\nGO\n\nCREATE EXTERNAL TABLE dbo.T_STTrip (\n[tms_trip_id] bigint,\n[tms_trip_driverid] nvarchar(4000),\n[tms_trip_drivername] nvarchar(4000),\n[tms_trip_vehicleid] nvarchar(4000),\n[tms_trip_vehiclefleetcode] nvarchar(4000),\n[tms_trip_trailerid] nvarchar(4000),\n[tms_trip_trailer1fleetcode] nvarchar(4000),\n[tms_trip_sectrailerid] nvarchar(4000),\n[tms_trip_trailer2fleetcode] nvarchar(4000),\n[tms_trip_trailersize] nvarchar(4000),\n[tms_trip_status] nvarchar(4000),\n[tms_trip_routeid] bigint,\n[tms_trip_opsrouteid] bigint,\n[tms_trip_startfromaddress] nvarchar(4000),\n[tms_trip_startfromlat] nvarchar(4000),\n[tms_trip_startfromlng] nvarchar(4000),\n[tms_trip_dispatchdate] datetime2(7),\n[tms_trip_stopaddress] nvarchar(4000),\n[tms_trip_stoplat] nvarchar(4000),\n[tms_trip_stoplng] nvarchar(4000),\n[tms_trip_insdate] datetime2(7),\n[tms_trip_moddate] datetime2(7),\n[tms_trip_insby] bigint,\n[tms_trip_onrouteid] bigint,\n[tms_trip_uuid] nvarchar(4000),\n[tms_trip_insbyuser] nvarchar(4000),\n[tms_trip_thirdpartyid] bigint,\n[tms_trip_timehash] int,\n[tms_trip_onroutestatus] nvarchar(4000),\n[tms_trip_lastupdate] datetime2(7),\n[tms_trip_lastlng] float,\n[tms_trip_lastlat] float,\n[tms_trip_controllerid] bigint,\n[tms_trip_trackstatus] nvarchar(4000),\n[tms_trip_lastrackid] bigint,\n[tms_trip_customer] nvarchar(4000),\n[tms_trip_bookings] nvarchar(4000),\n[tms_trip_controller] nvarchar(4000),\n[tms_trip_transporter] nvarchar(4000),\n[tms_trip_legacyid] bigint,\n[tms_trip_originatedfrom] nvarchar(4000),\n[tms_trip_local] int,\n[tms_trip_startdate] datetime2(7),\n[tms_trip_enddate] datetime2(7),\n[tms_trip_actualdeliverydate] datetime2(7),\n[tms_trip_note] nvarchar(4000),\n[tms_trip_deliverydate] datetime2(7),\n[tms_trip_internalexternal] nvarchar(4000),\n[tms_trip_pickupstartaddress] nvarchar(4000),\n[tms_trip_originaletadate] datetime2(7),\n[tms_trip_currentetadate] datetime2(7),\n[tms_trip_lastetaupdate] datetime2(7),\n[tms_trip_source] nvarchar(4000),\n[orv_dispatch_id] bigint,\n[orv_dispatch_lmsid] bigint,\n[orv_dispatch_vid] bigint,\n[orv_dispatch_did] bigint,\n[orv_dispatch_status] nvarchar(4000),\n[orv_dispatch_startdate] datetime2(7),\n[orv_dispatch_stopdate] datetime2(7),\n[orv_dispatch_vehicletype] int,\n[orv_dispatch_uid] bigint,\n[orv_dispatch_estdepdate] datetime2(7),\n[orv_dispatch_trailer] bigint,\n[orv_dispatch_trailer2] bigint,\n[orv_dispatch_fromlocid] bigint,\n[orv_dispatch_tolocid] bigint,\n[orv_dispatch_site] nvarchar(4000),\n[orv_dispatch_lastupdate] datetime2(7),\n[orv_dispatch_etastatus] nvarchar(4000),\n[orv_dispatch_startinglat] float,\n[orv_dispatch_startinglng] float,\n[orv_dispatch_endinglat] float,\n[orv_dispatch_endinglng] float,\n[orv_dispatch_startingfrom] nvarchar(4000),\n[orv_dispatch_goingto] nvarchar(4000),\n[orv_dispatch_importissue] nvarchar(4000),\n[orv_dispatch_insdate] datetime2(7),\n[orv_dispatch_estarrdate] datetime2(7),\n[orv_dispatch_duration] bigint,\n[orv_dispatch_distance] bigint,\n[orv_dispatch_cost] bigint,\n[orv_dispatch_stoplat] float,\n[orv_dispatch_stoplng] float,\n[orv_dispatch_stopreason] nvarchar(4000),\n[orv_dispatch_startodo] bigint,\n[orv_dispatch_stopodo] bigint,\n[orv_dispatch_expduration] bigint,\n[orv_dispatch_expdistance] bigint,\n[orv_dispatch_expcost] bigint,\n[orv_dispatch_collectionid] bigint,\n[orv_dispatch_crew] int,\n[orv_dispatch_crewnames] nvarchar(4000),\n[orv_dispatch_bocloseuid] bigint,\n[orv_dispatch_routing] nvarchar(4000),\n[orv_dispatch_swapct] int,\n[orv_dispatch_debriefed] int,\n[orv_dispatch_debriefnotes] nvarchar(4000),\n[orv_dispatch_bopin] nvarchar(4000),\n[orv_dispatch_cpicost] bigint,\n[orv_dispatch_labourcost] bigint,\n[orv_dispatch_maintcost] bigint,\n[orv_dispatch_inscost] bigint,\n[orv_dispatch_tollcost] bigint,\n[orv_dispatch_fuelcost] bigint,\n[orv_dispatch_internal] int,\n[orv_dispatch_childids] nvarchar(4000),\n[orv_dispatch_guid] bigint,\n[orv_dispatch_timedefinite] int,\n[orv_dispatch_orvcode] int,\n[orv_dispatch_tsstopdepoenter] datetime2(7),\n[orv_dispatch_tsstartdepoexit] datetime2(7),\n[orv_dispatch_reroutect] int,\n[orv_dispatch_actualarrdate] datetime2(7),\n[orv_dispatch_physicalstartlat] float,\n[orv_dispatch_physicalstartlng] float,\n[orv_dispatch_tsstartdepoexitapp] datetime2(7),\n[orv_dispatch_tsstopdepoenterapp] datetime2(7),\n[orv_dispatch_uncanceldate] datetime2(7),\n[orv_dispatch_uncancelusername] nvarchar(4000),\n[orv_dispatch_lmsdebrifed] int,\n[orv_dispatch_reviseddebrief] int,\n[orv_dispatch_lhtransporter] nvarchar(4000),\n[orv_dispatch_triptype] nvarchar(4000),\n[orv_dispatch_startreason] nvarchar(4000),\n[orv_dispatch_route] nvarchar(4000),\n[orv_dispatch_tmstraileridentify] nvarchar(4000),\n[orv_dispatch_tmstrailer2identify] nvarchar(4000),\n[orv_dispatch_tmscontroller] nvarchar(4000),\n[orv_dispatch_routekey] nvarchar(4000),\n[orv_dispatch_courierid] bigint,\n[orv_dispatch_originalstartdate] datetime2(7),\n[orv_dispatch_originalextarrival] datetime2(7),\n[orv_dispatch_recomputeoriginals] int,\n[orv_dispatch_originalextdistance] bigint,\n[orv_dispatch_originalextduration] bigint,\n[orv_dispatch_moddate] datetime2(7),\n[orv_dispatch_tmsmode] int,\n[orv_dispatch_driversignature] bigint,\n[tms_trip_appduration] bigint,\n[tms_trip_driverduration] bigint,\n[tms_trip_controlroomduration] bigint,\n[tms_trip_arrivedontime] nvarchar(4000),\n[orv_dispatch_actualstartfromexpected] float,\n[tms_trip_directtrip] int,\n[tms_trip_trailer1size] nvarchar(4000),\n[tms_trip_trailer2size] nvarchar(4000),\n[tms_track_completetrackdate] datetime2(7),\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/sttrip.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.T_STTrip \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/T_STTripTrackingHistory')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Linehaul"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.T_STTripTrackingHistory') )\n    DROP EXTERNAL TABLE dbo.T_STTripTrackingHistory\nGO\n\nCREATE EXTERNAL TABLE dbo.T_STTripTrackingHistory (\n    [tms_triptrackinghistory_id] bigint,\n    [tms_triptrackinghistory_tripid] bigint,\n    [tms_triptrackinghistory_controllerid] bigint,\n    [tms_triptrackinghistory_fromdate] datetime2(7),\n    [tms_triptrackinghistory_todate] datetime2(7),\n    [tms_triptrackinghistory_actionedby] bigint,\n    [tms_triptrackinghistory_controllername] nvarchar(4000),\n    [tms_triptrackinghistory_actionedbyname] nvarchar(4000),\n\n\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/sttriptrackinghistory.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.T_STTripTrackingHistory \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/T_STWaybill')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Distribution"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.T_STWaybill') )\n    DROP EXTERNAL TABLE dbo.T_STWaybill\nGO\n\nCREATE EXTERNAL TABLE dbo.T_STWaybill (\n[lms_waybill_id] int,\n[lms_waybill_date] datetime2(7),\n[lms_waybill_customerid] int,\n[lms_waybill_bservice] nvarchar(4000),\n[lms_waybill_cservice] nvarchar(4000),\n[lms_waybill_originid] nvarchar(4000),\n[lms_waybill_defuser] nvarchar(4000),\n[lms_waybill_distance] float,\n[lms_waybill_courierid] nvarchar(4000),\n[lms_waybill_courierwbno] nvarchar(4000),\n[lms_waybill_billtype] int,\n[lms_waybill_capdate] datetime2(7),\n[lms_waybill_ordertype] int,\n[lms_waybill_billcust] int,\n[lms_waybill_pickupcustid] int,\n[lms_waybill_delivercustid] int,\n[lms_waybill_billto] nvarchar(4000),\n[lms_waybill_claimable] int,\n[lms_waybill_override] int,\n[lms_waybill_vehiclecapacity] int,\n[lms_waybill_poddate] datetime2(7),\n[lms_waybill_signee] nvarchar(4000),\n[lms_waybill_grvno] nvarchar(4000),\n[lms_waybill_endorsements] nvarchar(4000),\n[lms_waybill_loadid] int,\n[lms_waybill_routeid] int,\n[lms_waybill_mrpflag] int,\n[lms_waybill_printed] int,\n[lms_waybill_sreasonid] int,\n[lms_waybill_hasprinted] nvarchar(4000),\n[lms_waybill_site] nvarchar(4000),\n[lms_waybill_deliveryno] int,\n[lms_waybill_custaccid] int,\n[lms_waybill_scanned] int,\n[lms_waybill_userid] int,\n[lms_waybill_autoprinted] int,\n[lms_waybill_cref] nvarchar(4000),\n[lms_waybill_ushort] int,\n[lms_waybill_delays] nvarchar(4000),\n[lms_waybill_debriefed] int,\n[lms_waybill_debriefdt] datetime2(7),\n[lms_waybill_dbctnsret] int,\n[lms_waybill_uploaded] int,\n[lms_waybill_serviceexclude] int,\n[lms_waybill_failed] int,\n[lms_waybill_delayreasonid] int,\n[lms_waybill_notdebrievreasonid] int,\n[lms_waybill_deliverystatid] int,\n[lms_waybill_reasongroupid] int,\n[lms_waybill_emailcust] int,\n[lms_waybill_mobileissuereportedid] int,\n[lms_waybill_ismobile] int,\n[lms_waybill_custcomment] nvarchar(4000),\n[lms_waybill_custfeedback] int,\n[lms_waybill_reasondetailid] int,\n[lms_waybill_mobilecomments] nvarchar(4000),\n[lms_waybill_inservicereasonid] int,\n[lms_waybill_notdebrievcom] nvarchar(4000),\n[lms_waybill_lastupdate] datetime2(7),\n[lms_waybill_dropsequence] int,\n[lms_waybill_noofparcelssreturned] int,\n[lms_parcelbywaybill_aggrweight] numeric(36,8),\n[lms_parcelbywaybill_aggrchargeweight] numeric(36,8),\n[lms_parcelbywaybill_aggrvolweight] numeric(36,8),\n[lms_parcelbywaybill_aggrvolumiserweight] numeric(36,8),\n[lms_parcelbywaybill_aggrmaxweight] numeric(36,8),\n[lms_parcelbywaybill_aggrnoparcels] bigint,\n[lms_parcelbywaybill_aggrweightexclspecdel] numeric(36,8),\n[lms_parcelbywaybill_aggrchargeweightexclspecdel] numeric(36,8),\n[lms_parcelbywaybill_aggrvolweightexclspecdel] numeric(36,8),\n[lms_parcelbywaybill_aggrvolumiserweightexclspecdel] numeric(36,8),\n[lms_parcelbywaybill_aggrmaxweightexclspecdel] numeric(36,8),\n[lms_parcelbywaybill_aggrnoparcelsexclspecdel] bigint,\n[orv_delivery_id] bigint,\n[orv_delivery_did] bigint,\n[orv_delivery_waybillid] bigint,\n[orv_delivery_address] nvarchar(4000),\n[orv_delivery_town] nvarchar(4000),\n[orv_delivery_province] nvarchar(4000),\n[orv_delivery_country] nvarchar(4000),\n[orv_delivery_lat] float,\n[orv_delivery_lng] float,\n[orv_delivery_placeid] nvarchar(4000),\n[orv_delivery_what3words] nvarchar(4000),\n[orv_delivery_zipcode] nvarchar(4000),\n[orv_delivery_eta] datetime2(7),\n[orv_delivery_actualeta] datetime2(7),\n[orv_delivery_deliverydate] datetime2(7),\n[orv_delivery_deliverby] datetime2(7),\n[orv_delivery_deliveryorder] int,\n[orv_delivery_tsgeofenceenter] datetime2(7),\n[orv_delivery_tsscanningstart] datetime2(7),\n[orv_delivery_tsscanningstop] datetime2(7),\n[orv_delivery_tspodsignature] datetime2(7),\n[orv_delivery_aid] bigint,\n[orv_delivery_numprcls] int,\n[orv_delivery_geocodingstatus] nvarchar(4000),\n[orv_delivery_skipdate] datetime2(7),\n[orv_delivery_skipreason] nvarchar(4000),\n[orv_delivery_status] nvarchar(4000),\n[orv_delivery_partialmatch] int,\n[orv_delivery_approximated] int,\n[orv_delivery_lmsorder] int,\n[orv_delivery_optimizedorder] int,\n[orv_delivery_seal] nvarchar(4000),\n[orv_delivery_customerinvoice] int,\n[orv_delivery_customerdn] int,\n[orv_delivery_grv] nvarchar(4000),\n[orv_delivery_chepslip] int,\n[orv_delivery_cleandelivery] int,\n[orv_delivery_trackingcode] nvarchar(4000),\n[orv_delivery_mode] nvarchar(4000),\n[orv_delivery_customorder] int,\n[orv_delivery_verified] int,\n[orv_delivery_altered] int,\n[orv_delivery_orderid] int,\n[orv_delivery_mergeid] bigint,\n[orv_delivery_orderref] nvarchar(4000),\n[orv_delivery_drivernote] nvarchar(4000),\n[orv_delivery_guid] bigint,\n[orv_delivery_timedefinite] int,\n[orv_delivery_orvcode] int,\n[orv_delivery_vaid] bigint,\n[orv_delivery_delegatedate] datetime2(7),\n[orv_delivery_delegatemode] nvarchar(4000),\n[orv_delivery_debriefmode] nvarchar(4000),\n[orv_delivery_debriefed] int,\n[orv_delivery_insdate] datetime2(7),\n[orv_delivery_reviseddebrief] int,\n[orv_delivery_lmsdebriefed] int,\n[orv_delivery_uncanceldate] datetime2(7),\n[orv_delivery_podlaterdate] datetime2(7),\n[orv_delivery_podmanualdate] datetime2(7),\n[orv_delivery_manualdebriefreason] nvarchar(4000),\n[orv_delivery_submanualdebriefreason] nvarchar(4000),\n[orv_delivery_outofgeofencereason] nvarchar(4000),\n[orv_delivery_tsentergeofenceapp] datetime2(7),\n[orv_delivery_tsexitgeofenceapp] datetime2(7),\n[orv_delivery_comebacklater] bigint,\n[orv_delivery_uncancelusername] nvarchar(4000),\n[orv_delivery_cref] int,\n[orv_delivery_division] int,\n[orv_delivery_descriptionofgoods] nvarchar(4000),\n[orv_delivery_skiplng] float,\n[orv_delivery_skiplat] float,\n[orv_delivery_courierid] bigint,\n[orv_delivery_posreason] nvarchar(4000),\n[orv_delivery_moddate] datetime2(7),\n[orv_delivery_mallid] int,\n[orv_delivery_mallgid] bigint,\n[orv_delivery_lmsidskipreason] bigint,\n[orv_delivery_deleteforimport] int,\n[orv_delivery_podoutofgeofence] int,\n[orv_delivery_originalcustomerid] bigint,\n[orv_pod_id] bigint,\n[orv_pod_name] nvarchar(4000),\n[orv_pod_poddate] datetime2(7),\n[orv_pod_lat] float,\n[orv_pod_lng] float,\n[orv_pod_customerrating] int,\n[orv_pod_customerfeedback] nvarchar(4000),\n[orv_delivery_poddistancefromdeliverypoint] float,\n[orv_delivery_datediffpoddeldate] float,\n[orv_delivery_deliveryenterexitdiff] float\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stwaybill.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.T_STWaybill \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/T_STWaybill_3MYoY')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/Ops/Distribution"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.T_STWaybill_3MYoY') )\n    DROP EXTERNAL TABLE dbo.T_STWaybill_3MYoY\nGO\n\nCREATE EXTERNAL TABLE dbo.T_STWaybill_3MYoY (\n [lms_waybill_id] int,\n[lms_waybill_date] datetime2(7),\n[lms_waybill_customerid] int,\n[lms_waybill_bservice] nvarchar(4000),\n[lms_waybill_cservice] nvarchar(4000),\n[lms_waybill_originid] nvarchar(4000),\n[lms_waybill_defuser] nvarchar(4000),\n[lms_waybill_distance] float,\n[lms_waybill_courierid] nvarchar(4000),\n[lms_waybill_courierwbno] nvarchar(4000),\n[lms_waybill_billtype] int,\n[lms_waybill_capdate] datetime2(7),\n[lms_waybill_ordertype] int,\n[lms_waybill_billcust] int,\n[lms_waybill_pickupcustid] int,\n[lms_waybill_delivercustid] int,\n[lms_waybill_billto] nvarchar(4000),\n[lms_waybill_claimable] int,\n[lms_waybill_override] int,\n[lms_waybill_vehiclecapacity] int,\n[lms_waybill_poddate] datetime2(7),\n[lms_waybill_signee] nvarchar(4000),\n[lms_waybill_grvno] nvarchar(4000),\n[lms_waybill_endorsements] nvarchar(4000),\n[lms_waybill_loadid] int,\n[lms_waybill_routeid] int,\n[lms_waybill_mrpflag] int,\n[lms_waybill_printed] int,\n[lms_waybill_sreasonid] int,\n[lms_waybill_hasprinted] nvarchar(4000),\n[lms_waybill_site] nvarchar(4000),\n[lms_waybill_deliveryno] int,\n[lms_waybill_custaccid] int,\n[lms_waybill_scanned] int,\n[lms_waybill_userid] int,\n[lms_waybill_autoprinted] int,\n[lms_waybill_cref] nvarchar(4000),\n[lms_waybill_ushort] int,\n[lms_waybill_delays] nvarchar(4000),\n[lms_waybill_debriefed] int,\n[lms_waybill_debriefdt] datetime2(7),\n[lms_waybill_dbctnsret] int,\n[lms_waybill_uploaded] int,\n[lms_waybill_serviceexclude] int,\n[lms_waybill_failed] int,\n[lms_waybill_delayreasonid] int,\n[lms_waybill_notdebrievreasonid] int,\n[lms_waybill_deliverystatid] int,\n[lms_waybill_reasongroupid] int,\n[lms_waybill_emailcust] int,\n[lms_waybill_mobileissuereportedid] int,\n[lms_waybill_ismobile] int,\n[lms_waybill_custcomment] nvarchar(4000),\n[lms_waybill_custfeedback] int,\n[lms_waybill_reasondetailid] int,\n[lms_waybill_mobilecomments] nvarchar(4000),\n[lms_waybill_inservicereasonid] int,\n[lms_waybill_notdebrievcom] nvarchar(4000),\n[lms_waybill_lastupdate] datetime2(7),\n[lms_waybill_dropsequence] int,\n[lms_waybill_noofparcelssreturned] int,\n[lms_parcelbywaybill_aggrweight] numeric(36,8),\n[lms_parcelbywaybill_aggrchargeweight] numeric(36,8),\n[lms_parcelbywaybill_aggrvolweight] numeric(36,8),\n[lms_parcelbywaybill_aggrvolumiserweight] numeric(36,8),\n[lms_parcelbywaybill_aggrmaxweight] numeric(36,8),\n[lms_parcelbywaybill_aggrnoparcels] bigint,\n[lms_parcelbywaybill_aggrweightexclspecdel] numeric(36,8),\n[lms_parcelbywaybill_aggrchargeweightexclspecdel] numeric(36,8),\n[lms_parcelbywaybill_aggrvolweightexclspecdel] numeric(36,8),\n[lms_parcelbywaybill_aggrvolumiserweightexclspecdel] numeric(36,8),\n[lms_parcelbywaybill_aggrmaxweightexclspecdel] numeric(36,8),\n[lms_parcelbywaybill_aggrnoparcelsexclspecdel] bigint,\n[orv_delivery_id] bigint,\n[orv_delivery_did] bigint,\n[orv_delivery_waybillid] bigint,\n[orv_delivery_address] nvarchar(4000),\n[orv_delivery_town] nvarchar(4000),\n[orv_delivery_province] nvarchar(4000),\n[orv_delivery_country] nvarchar(4000),\n[orv_delivery_lat] float,\n[orv_delivery_lng] float,\n[orv_delivery_placeid] nvarchar(4000),\n[orv_delivery_what3words] nvarchar(4000),\n[orv_delivery_zipcode] nvarchar(4000),\n[orv_delivery_eta] datetime2(7),\n[orv_delivery_actualeta] datetime2(7),\n[orv_delivery_deliverydate] datetime2(7),\n[orv_delivery_deliverby] datetime2(7),\n[orv_delivery_deliveryorder] int,\n[orv_delivery_tsgeofenceenter] datetime2(7),\n[orv_delivery_tsscanningstart] datetime2(7),\n[orv_delivery_tsscanningstop] datetime2(7),\n[orv_delivery_tspodsignature] datetime2(7),\n[orv_delivery_aid] bigint,\n[orv_delivery_numprcls] int,\n[orv_delivery_geocodingstatus] nvarchar(4000),\n[orv_delivery_skipdate] datetime2(7),\n[orv_delivery_skipreason] nvarchar(4000),\n[orv_delivery_status] nvarchar(4000),\n[orv_delivery_partialmatch] int,\n[orv_delivery_approximated] int,\n[orv_delivery_lmsorder] int,\n[orv_delivery_optimizedorder] int,\n[orv_delivery_seal] nvarchar(4000),\n[orv_delivery_customerinvoice] int,\n[orv_delivery_customerdn] int,\n[orv_delivery_grv] nvarchar(4000),\n[orv_delivery_chepslip] int,\n[orv_delivery_cleandelivery] int,\n[orv_delivery_trackingcode] nvarchar(4000),\n[orv_delivery_mode] nvarchar(4000),\n[orv_delivery_customorder] int,\n[orv_delivery_verified] int,\n[orv_delivery_altered] int,\n[orv_delivery_orderid] int,\n[orv_delivery_mergeid] bigint,\n[orv_delivery_orderref] nvarchar(4000),\n[orv_delivery_drivernote] nvarchar(4000),\n[orv_delivery_guid] bigint,\n[orv_delivery_timedefinite] int,\n[orv_delivery_orvcode] int,\n[orv_delivery_vaid] bigint,\n[orv_delivery_delegatedate] datetime2(7),\n[orv_delivery_delegatemode] nvarchar(4000),\n[orv_delivery_debriefmode] nvarchar(4000),\n[orv_delivery_debriefed] int,\n[orv_delivery_insdate] datetime2(7),\n[orv_delivery_reviseddebrief] int,\n[orv_delivery_lmsdebriefed] int,\n[orv_delivery_uncanceldate] datetime2(7),\n[orv_delivery_podlaterdate] datetime2(7),\n[orv_delivery_podmanualdate] datetime2(7),\n[orv_delivery_manualdebriefreason] nvarchar(4000),\n[orv_delivery_submanualdebriefreason] nvarchar(4000),\n[orv_delivery_outofgeofencereason] nvarchar(4000),\n[orv_delivery_tsentergeofenceapp] datetime2(7),\n[orv_delivery_tsexitgeofenceapp] datetime2(7),\n[orv_delivery_comebacklater] bigint,\n[orv_delivery_uncancelusername] nvarchar(4000),\n[orv_delivery_cref] int,\n[orv_delivery_division] int,\n[orv_delivery_descriptionofgoods] nvarchar(4000),\n[orv_delivery_skiplng] float,\n[orv_delivery_skiplat] float,\n[orv_delivery_courierid] bigint,\n[orv_delivery_posreason] nvarchar(4000),\n[orv_delivery_moddate] datetime2(7),\n[orv_delivery_mallid] int,\n[orv_delivery_mallgid] bigint,\n[orv_delivery_lmsidskipreason] bigint,\n[orv_delivery_deleteforimport] int,\n[orv_delivery_podoutofgeofence] int,\n[orv_delivery_originalcustomerid] bigint,\n[orv_pod_id] bigint,\n[orv_pod_name] nvarchar(4000),\n[orv_pod_poddate] datetime2(7),\n[orv_pod_lat] float,\n[orv_pod_lng] float,\n[orv_pod_customerrating] int,\n[orv_pod_customerfeedback] nvarchar(4000),\n[orv_delivery_poddistancefromdeliverypoint] float,\n[orv_delivery_datediffpoddeldate] float,\n[orv_delivery_deliveryenterexitdiff] float\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/OPS/stwaybill3myoy.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.T_STWaybill_3MYoY \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/User Management - DB lvl Access')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT * FROM sys.database_scoped_credentials;\n\nCREATE USER [michelledb@citylogistics.co.za] FROM LOGIN [michelledb@citylogistics.co.za];\nALTER ROLE db_datareader ADD MEMBER  [michelledb@citylogistics.co.za];\nGRANT VIEW DEFINITION TO [michelledb@citylogistics.co.za];\nGRANT CONTROL ON DATABASE SCOPED CREDENTIAL :: SynapseUser to [michelledb@citylogistics.co.za];\n\nCREATE USER [nerakshab@citylogistics.co.za] FROM LOGIN [nerakshab@citylogistics.co.za];\nALTER ROLE db_datareader ADD MEMBER [nerakshab@citylogistics.co.za];\nGRANT VIEW DEFINITION TO [nerakshab@citylogistics.co.za];\nGRANT CONTROL ON DATABASE SCOPED CREDENTIAL :: SynapseUser to [nerakshab@citylogistics.co.za];\n\nCREATE USER [Carlyd@citylogistics.co.za] FROM LOGIN [Carlyd@citylogistics.co.za];\nALTER ROLE db_datareader ADD MEMBER [Carlyd@citylogistics.co.za];\nGRANT VIEW DEFINITION TO [Carlyd@citylogistics.co.za];\nGRANT CONTROL ON DATABASE SCOPED CREDENTIAL :: SynapseUser to [Carlyd@citylogistics.co.za];\n\nCREATE USER [riaans@citylogistics.co.za] FROM LOGIN [riaans@citylogistics.co.za];\nALTER ROLE db_datareader ADD MEMBER [riaans@citylogistics.co.za];\nGRANT VIEW DEFINITION TO [riaans@citylogistics.co.za];\nGRANT CONTROL ON DATABASE SCOPED CREDENTIAL :: SynapseUser to [riaans@citylogistics.co.za];\n\nCREATE USER [brianv@citylogistics.co.za] FROM LOGIN [brianv@citylogistics.co.za];\nALTER ROLE db_datareader ADD MEMBER [brianv@citylogistics.co.za];\nGRANT VIEW DEFINITION TO [brianv@citylogistics.co.za];\nGRANT CONTROL ON DATABASE SCOPED CREDENTIAL :: SynapseUser to [brianv@citylogistics.co.za];\n\nCREATE USER [ryang@citylogistics.co.za] FROM LOGIN [ryang@citylogistics.co.za];\nALTER ROLE db_datareader ADD MEMBER [ryang@citylogistics.co.za];\nGRANT VIEW DEFINITION TO [ryang@citylogistics.co.za];\nGRANT CONTROL ON DATABASE SCOPED CREDENTIAL :: SynapseUser to [ryang@citylogistics.co.za];\n\nCREATE USER [lucaa@citytech.co.za] FROM LOGIN [lucaa@citytech.co.za];\nALTER ROLE db_datareader ADD MEMBER [lucaa@citytech.co.za];\nGRANT VIEW DEFINITION TO [lucaa@citytech.co.za];\nGRANT CONTROL ON DATABASE SCOPED CREDENTIAL :: SynapseUser to [lucaa@citytech.co.za];\n\nCREATE USER [kevinm@citylogistics.co.za] FROM LOGIN [kevinm@citylogistics.co.za];\nALTER ROLE db_datareader ADD MEMBER [kevinm@citylogistics.co.za];\nGRANT VIEW DEFINITION TO [kevinm@citylogistics.co.za];\nGRANT CONTROL ON DATABASE SCOPED CREDENTIAL :: SynapseUser to [kevinm@citylogistics.co.za];\n\nCREATE USER [powerbiserviceSyn] FROM LOGIN [powerbiserviceSyn]; -- With Password = 'u4z7S@LZg%rXoO^XCqZQWEa^3f7566893tNcnhz%On9Y8yjIIa';\nALTER ROLE db_datareader ADD MEMBER [powerbiserviceSyn];\nGRANT VIEW DEFINITION TO [powerbiserviceSyn];\nGRANT CONTROL ON DATABASE SCOPED CREDENTIAL :: SynapseUser to [powerbiserviceSyn];\n\nCREATE USER [powerbiservice@citytech.co.za] FROM LOGIN [powerbiservice@citytech.co.za]; -- With Password = 'u4z7S@LZg%rXoO^XCqZQWEa^3f7566893tNcnhz%On9Y8yjIIa';\nALTER ROLE db_datareader ADD MEMBER [powerbiservice@citytech.co.za];\nGRANT VIEW DEFINITION TO [powerbiservice@citytech.co.za];\nGRANT CONTROL ON DATABASE SCOPED CREDENTIAL :: SynapseUser to [powerbiservice@citytech.co.za];",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "RPT",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/UserManagement - Master Logins')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "CREATE LOGIN [michelledb@citylogistics.co.za] FROM EXTERNAL PROVIDER;\nCREATE LOGIN [nerakshab@citylogistics.co.za] FROM EXTERNAL PROVIDER;\nCREATE LOGIN [markb@citylogistics.co.za] FROM EXTERNAL PROVIDER;\nCREATE LOGIN [Carlyd@citylogistics.co.za] FROM EXTERNAL PROVIDER;\nCREATE LOGIN [riaans@citylogistics.co.za] FROM EXTERNAL PROVIDER;\nCREATE LOGIN [brianv@citylogistics.co.za] FROM EXTERNAL PROVIDER;\nCREATE LOGIN [ryang@citylogistics.co.za] FROM EXTERNAL PROVIDER;\nCREATE LOGIN [lucaa@citytech.co.za] FROM EXTERNAL PROVIDER;\nCREATE LOGIN [kevinm@citylogistics.co.za] FROM EXTERNAL PROVIDER;\nDROP LOGIN [marks@citylogistics.co.za];\nDROP LOGIN [markb@citylogistics.co.za];\nCREATE LOGIN [powerbiserviceSyn] WITH PASSWORD = 'u4z7S@LZg%rXoO^XCqZQWEa^3f7566893tNcnhz%On9Y8yjIIa';\nCREATE LOGIN [powerbiservice@citytech.co.za] FROM EXTERNAL PROVIDER;",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/reconconsignment')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/v2Recon"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.reconconsignment') )\n    DROP EXTERNAL TABLE dbo.reconconsignment\nGO\n\nCREATE EXTERNAL TABLE dbo.reconconsignment (\n[v1id] int,\n[v1CRef] nvarchar(4000),\n[v1CDate] datetime2(7),\n[v1UserID] int,\n[v1Appointment] datetime2(7),\n[v1DeliverBy] datetime2(7),\n[v1DConfirmed] nvarchar(4000),\n[v1DComment] nvarchar(4000),\n[v1DRefNo] nvarchar(4000),\n[v1DUserID] int,\n[v1LoadID] int,\n[v1DeliverCustID] int,\n[v1mrpCFlag] int,\n[v1mrpFlag] int,\n[v1ChargeWeight] numeric(36,6),\n[v1VolumetricWeight] numeric(36,6),\n[v1NoOfParcels] int,\n[v1Weight] numeric(36,6),\n[v1CustAccID] int,\n[v1MinKg] numeric(36,6),\n[v1MinRate] numeric(36,6),\n[v1AddRate] numeric(36,6),\n[v1DocFees] numeric(36,6),\n[v1Charge] numeric(36,6),\n[v1FuelLevy] numeric(36,6),\n[v1TotFuelLevy] numeric(36,6),\n[v1BillCust] int,\n[v1TotalCharge] numeric(36,6),\n[v1Units] int,\n[v1xFromCustID] int,\n[v1Scanned] int,\n[v1ConsConsignID] int,\n[v1ApptCode] nvarchar(4000),\n[v1FinalInservice] int,\n[v1DeliverByOriginal] datetime2(7),\n[v1Bill_ServiceType] int,\n[v1ConsignmentValue] numeric(36,6),\n[v1TripDistance] numeric(36,6),\n[v1Endorsement] nvarchar(4000),\n[v1InServiceReasonID] int,\n[v1InServiceComment] nvarchar(4000),\n[v1ReturnFlag] int,\n[v1CallProgID] int,\n[v1CallProgUpdDate] datetime2(7),\n[v1CServiceID] nvarchar(4000),\n[v1LastUpdate] datetime2(7),\n[v1BillFlag] int,\n[v1MissingFlag] int,\n[v1RouteRateID] int,\n[v1OverBorderFlag] int,\n[v1CollectID] int,\n[v1BillUserID] int,\n[v1SpecDel] int,\n[v1QuoteFlag] int,\n[v1VehicleTypeID] int,\n[v1DeliverTypeID] int,\n[v1BillTypeID] int,\n[v1TotOtherSurcharges] numeric(36,6),\n[v1LHAddRate] numeric(36,6),\n[v1LHCharge] numeric(36,6),\n[v1SAPBillPeriodID] int,\n[v1LHRouteRateID] int,\n[v1TotCovidSurcharges] numeric(36,6),\n[v1BillPeriod] nvarchar(4000),\n[v1BillWeek] int,\n[v2id] bigint,\n[v2customerid] bigint,\n[v2puid] bigint,\n[v2delid] bigint,\n[v2manifestid] bigint,\n[v2serviceid] bigint,\n[v2servicename] nvarchar(4000),\n[v2costcenterid] bigint,\n[v2costcenter] nvarchar(4000),\n[v2pickuprouteid] bigint,\n[v2deliverrouteid] bigint,\n[v2distributiontype] nvarchar(4000),\n[v2consigndate] bigint,\n[v2status] nvarchar(4000),\n[v2cref] nvarchar(4000),\n[v2cref2] nvarchar(4000),\n[v2cref3] nvarchar(4000),\n[v2cref4] nvarchar(4000),\n[v2volweight] float,\n[v2chargeweight] float,\n[v2value] int,\n[v2numpcls] int,\n[v2units] int,\n[v2batchdelivery] int,\n[v2sapid] bigint,\n[v2insdate] bigint,\n[v2moddate] bigint,\n[v2insby] int,\n[v2legacyid] bigint,\n[v2billtypeid] int,\n[v2totothersurcharges] int,\n[v2lhaddrate] int,\n[v2lhcharge] int,\n[v2billperiod] nvarchar(4000),\n[v2billweek] int,\n[v2minkg] float,\n[v2minrate] int,\n[v2addrate] int,\n[v2docfees] int,\n[v2charge] int,\n[v2fuellevy] int,\n[v2missingflag] int,\n[v2qouteflag] int,\n[v2vehicletypeid] int,\n[v2redflag] int,\n[v2samedeliverid] int,\n[v2combineroutecharge] int,\n[v2samedelcount] int,\n[v2serviceratetype] nvarchar(4000),\n[v2migrationstatus] nvarchar(4000),\n[v2customername] nvarchar(4000),\n[v2puname] nvarchar(4000),\n[v2delname] nvarchar(4000),\n[v2weight] float,\n[v2overborder] int,\n[v2vehicletype] nvarchar(4000),\n[v2masterconsignid] int,\n[v2masterconsignflag] int,\n[v2legacypickuprouteid] int,\n[v2legacydeliverrouteid] int,\n[v2type] nvarchar(4000),\n[notinv2] int,\n\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/LMSV2/reconconsignment.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.reconconsignment \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/reconloads')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/v2Recon"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.reconloads') )\n    DROP EXTERNAL TABLE dbo.reconloads\nGO\n\nCREATE EXTERNAL TABLE dbo.reconloads (\n[v1ID] int,\n[v1FromID] int,\n[v1ToID] int,\n[v1RouteID] int,\n[v1DriverID] int,\n[v1SealNo] nvarchar(4000),\n[v1VehicleID] int,\n[v1TTypeID] int,\n[v1UserID] int,\n[v1LDate] datetime2(7),\n[v1CourierID] nvarchar(4000),\n[v1ServiceID] nvarchar(4000),\n[v1Site] nvarchar(4000),\n[v1Spanner] int,\n[v1SpareWheel] int,\n[v1Tyres] int,\n[v1FromLocID] int,\n[v1ToLocID] int,\n[v1CDate] datetime2(7),\n[v1UserID2] int,\n[v1Trailer1] int,\n[v1Trailer2] int,\n[v1CRef] nvarchar(4000),\n[v1Debriefed] int,\n[v1mrpFlag] int,\n[v1OldLoadID] int,\n[v1VehicleRegNo] nvarchar(4000),\n[v1DriverName] nvarchar(4000),\n[v1mrpCFlag] int,\n[v1finalFlag] int,\n[v1AlidaFlag] int,\n[v1vehodo] float,\n[v1vehodo2] float,\n[v1PUIBTs] int,\n[v1PUCOLs] int,\n[v1DBUserID] int,\n[v1Crew] nvarchar(4000),\n[v1PlanningRef] nvarchar(4000),\n[v1InterfaceDate] datetime2(7),\n[v1NoSealsRet] int,\n[v1ExcludeOpsi] int,\n[v1DCCRef] nvarchar(4000),\n[v1SRouteID] int,\n[v1FuelLitres] numeric(36,6),\n[v1outsourcedCrew] int,\n[v1MasterLoadID] int,\n[v1SecurityGateDespatched] datetime2(7),\n[v1SecurityGateUserID] int,\n[v1SecurityGateArrived] datetime2(7),\n[v1SecurityGateArrivedUserID] int,\n[v1LastUpdate] datetime2(7),\n[v1BayNo] nvarchar(4000),\n[v1Costs] numeric(36,6),\n[v1ESTDepartureDateTime] datetime2(7),\n[v1TotalWeight] numeric(36,6),\n[v1SupUserID] int,\n[v1ovrIgnore] int,\n[v1orvcleandispatch] int,\n[v1orvskippeddeliveries] int,\n[v1orvendorsments] int,\n[v1orvreturns] int,\n[v1orvduration] int,\n[v1orvincident] int,\n[v1ChildLoadID] int,\n[v1ORVCode] int,\n[v1CompanyID] int,\n[v1EditLoadDate] datetime2(7),\n[v1ReOpenedDate] datetime2(7),\n[v1ReOpenedUserID] int,\n[v1ORVID] int,\n[v1NoVehicle] int,\n[v1MobileFeedbackReasonID] int,\n[v1Direct] int,\n[v1ORV_Status] nvarchar(4000),\n[v1ORVStatus] nvarchar(4000),\n[v1NoOfPcls] int,\n[v1DebriefDt] datetime2(7),\n[v1HTTPCode] int,\n[v1PODUserID] int,\n[v1ParentLoadID] int,\n[v1ORVDevice] nvarchar(4000),\n[v1Remarks] nvarchar(4000),\n[v1EffectiveLoadDurationMinutes] int,\n[v1EffectiveOffLoadDurationMinutes] int,\n[v1MRPOBVersion] int,\n[v1actualdistance] int,\n[v1PclsLoaded] int,\n[v1DepotEnter] datetime2(7),\n[v1DepotExit] datetime2(7),\n[v1supUserID2] int,\n[v2id] bigint,\n[v2fromlocid] bigint,\n[v2tolocid] bigint,\n[v2driveruid] nvarchar(4000),\n[v2vehicleuid] nvarchar(4000),\n[v2traileruid] nvarchar(4000),\n[v2sectraileruid] nvarchar(4000),\n[v2tmsid] bigint,\n[v2parentid] bigint,\n[v2thirdpartyid] bigint,\n[v2lastclosedate] bigint,\n[v2estdepartdate] bigint,\n[v2debriefdate] bigint,\n[v2vehiclefleetcode] nvarchar(4000),\n[v2drivername] nvarchar(4000),\n[v2trailerfleetcode] nvarchar(4000),\n[v2sectrailerfleetcode] nvarchar(4000),\n[v2type] nvarchar(4000),\n[v2status] nvarchar(4000),\n[v2bayid] bigint,\n[v2batchdelivery] int,\n[v2cref] nvarchar(4000),\n[v2totweight] float,\n[v2numpcls] int,\n[v2excluderouting] int,\n[v2loadstartdate] bigint,\n[v2loadstopdate] bigint,\n[v2offloadstartdate] bigint,\n[v2offloadstopdate] bigint,\n[v2startodo] int,\n[v2stopodo] int,\n[v2supervisorid] bigint,\n[v2supervisorname] nvarchar(4000),\n[v2orvid] bigint,\n[v2orvstatus] nvarchar(4000),\n[v2orvignore] int,\n[v2novehicle] int,\n[v2insdate] bigint,\n[v2moddate] bigint,\n[v2insby] int,\n[v2legacyid] bigint,\n[v2migrationstatus] nvarchar(4000),\n[v2fromlocname] nvarchar(4000),\n[v2tolocname] nvarchar(4000),\n[v2thirdparty] nvarchar(4000),\n[v2bayname] nvarchar(4000),\n[v2totchargeweight] float,\n[v2debriefstatus] nvarchar(4000),\n[v2movementid] bigint,\n[notinv2] int,\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/LMSV2/reconloads.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.reconloads \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/reconmovement')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/v2Recon"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.reconmovement') )\n    DROP EXTERNAL TABLE dbo.reconmovement\nGO\n\nCREATE EXTERNAL TABLE dbo.reconmovement (\n[v2id] bigint,\n[v2tolocid] bigint,\n[v2tolocname] nvarchar(4000),\n[v2fromlocid] bigint,\n[v2fromlocname] nvarchar(4000),\n[v2tofloorareaid] bigint,\n[v2tofloorareaname] nvarchar(4000),\n[v2fromfloorareaid] bigint,\n[v2fromfloorareaname] nvarchar(4000),\n[v2status] nvarchar(4000),\n[v2typeid] bigint,\n[v2type] nvarchar(4000),\n[v2bayid] bigint,\n[v2bayname] nvarchar(4000),\n[v2insdate] bigint,\n[v2moddate] bigint,\n[v2closedate] bigint,\n[v2insby] bigint,\n[v2legacyid] bigint,\n[v2migrationstatus] nvarchar(4000),\n\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/LMSV2/reconmovement.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.reconmovement \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/reconorder')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/v2Recon"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.reconorder') )\n    DROP EXTERNAL TABLE dbo.reconorder\nGO\n\nCREATE EXTERNAL TABLE dbo.reconorder (\n[v1id] int,\n[v1IDold] nvarchar(4000),\n[v1COrderNo] nvarchar(4000),\n[v1CustRef] nvarchar(4000),\n[v1WaybillID] int,\n[v1CustomerName] nvarchar(4000),\n[v1Adres1] nvarchar(4000),\n[v1Adres2] nvarchar(4000),\n[v1Town1] nvarchar(4000),\n[v1NoOfPcls] int,\n[v1PickUpCustID] int,\n[v1DeliverCustID] int,\n[v1BillCustID] int,\n[v1Weight] numeric(36,6),\n[v1OValue] numeric(36,6),\n[v1HODate] datetime2(7),\n[v1CService] nvarchar(4000),\n[v1CourierID] nvarchar(4000),\n[v1CustService] nvarchar(4000),\n[v1oldID] nvarchar(4000),\n[v1UserID] int,\n[v1PCode] nvarchar(4000),\n[v1Appointment] datetime2(7),\n[v1DeliverBy] datetime2(7),\n[v1DConfirmed] nvarchar(4000),\n[v1DComment] nvarchar(4000),\n[v1DRefNo] nvarchar(4000),\n[v1DUserID] int,\n[v1NoPODs] int,\n[v1DueDate] datetime2(7),\n[v1BatchNo] int,\n[v1CustAccID] int,\n[v1Units] int,\n[v1Airfreightflag] int,\n[v1CustRef3] nvarchar(4000),\n[v1SpecDel] int,\n[v1FLUnits] int,\n[v1FLKgs] int,\n[v1InService] datetime2(7),\n[v1CustDocs] int,\n[v1Scanned] datetime2(7),\n[v1ScanImageName1] nvarchar(4000),\n[v1ScanImageName] nvarchar(4000),\n[v1ChangeReasonID] int,\n[v1Bill_ServiceType] int,\n[v1Bill_VehicleType] int,\n[v1Quote] numeric(36,6),\n[v1ConsignLabel] int,\n[v1Custref2] nvarchar(4000),\n[v1CallProgID] int,\n[v1CallProgUpdDate] datetime2(7),\n[v1SubmitEDI] int,\n[v1LastUpdate] datetime2(7),\n[v1QuoteFlag] int,\n[v1Bill_MinRate] numeric(36,6),\n[v1Bill_AddRate] numeric(36,6),\n[v1Bill_MinKG] numeric(36,6),\n[v1Notes] nvarchar(4000),\n[v1QuoteWeight] numeric(36,6),\n[v1OrderConsolidationID] int,\n[v1QuoteFuelLevy] numeric(36,6),\n[v1QuoteOtherSurcharges] numeric(36,6),\n[v1QuoteDocFees] numeric(36,6),\n[v1PTypeO] int,\n[v1labelformat] nvarchar(4000),\n[v2id] bigint,\n[v2customerid] bigint,\n[v2puid] bigint,\n[v2delid] bigint,\n[v2cref] nvarchar(4000),\n[v2customername] nvarchar(4000),\n[v2puname] nvarchar(4000),\n[v2delname] nvarchar(4000),\n[v2hodate] bigint,\n[v2reqdeliverydate] bigint,\n[v2customerduedate] bigint,\n[v2status] nvarchar(4000),\n[v2numpcls] int,\n[v2ordervalue] int,\n[v2units] int,\n[v2cityservice] nvarchar(4000),\n[v2cityserviceid] bigint,\n[v2customerservice] nvarchar(4000),\n[v2batchno] nvarchar(4000),\n[v2batchdelivery] int,\n[v2costcenter] nvarchar(4000),\n[v2insdate] bigint,\n[v2moddate] bigint,\n[v2insby] bigint,\n[v2quoteid] bigint,\n[v2cref2] nvarchar(4000),\n[v2cref3] nvarchar(4000),\n[v2cref4] nvarchar(4000),\n[v2legacyid] bigint,\n[v2migrationstatus] nvarchar(4000),\n[v2batchid] bigint,\n[v2customerserviceid] bigint,\n[v2costcenterid] bigint,\n[v2insbyname] nvarchar(4000),\n[v2bookingcomment] nvarchar(4000),\n[v2bookingreference] nvarchar(4000),\n[v2bookingby] bigint,\n[v2bookingconfirmname] nvarchar(4000),\n[notinv2] int,\n\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/LMSV2/reconorder.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.reconorder \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/reconparcel')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/v2Recon"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.reconparcel') )\n    DROP EXTERNAL TABLE dbo.reconparcel\nGO\n\nCREATE EXTERNAL TABLE dbo.reconparcel (\n[v1id] int,\n[v1IDold] nvarchar(4000),\n[v1Weight] float,\n[v1DIMMS] float,\n[v1WaybillID] int,\n[v1OrderID] int,\n[v1NoOfPcls] int,\n[v1AcceptanceDate] datetime2(7),\n[v1SysID] int,\n[v1UserID] int,\n[v1Barcode] nvarchar(4000),\n[v1Endorsement] nvarchar(4000),\n[v1VolWeight] float,\n[v1ChargeWeight] numeric(36,6),\n[v1PType] int,\n[v1Description] nvarchar(4000),\n[v1LocID] int,\n[v1HandOverDate] datetime2(7),\n[v1AcceptanceDate2] datetime2(7),\n[v1SeqNo] int,\n[v1FloorStatus] nvarchar(4000),\n[v1OldWBID] int,\n[v1ConsignID] int,\n[v1CRef] nvarchar(4000),\n[v1Orphan] int,\n[v1ClaimID] int,\n[v1DueDate] datetime2(7),\n[v1CODDate] datetime2(7),\n[v1disFlag] int,\n[v1AStoreID] int,\n[v1mrpFlag] int,\n[v1NMRFlag] int,\n[v1ConsignFlag] int,\n[v1CManifest] int,\n[v1CBranchID] int,\n[v1ClaimRef] nvarchar(4000),\n[v1Cost] numeric(36,6),\n[v1CReason] nvarchar(4000),\n[v1CustomerBarcode] nvarchar(4000),\n[v1DBShorts] int,\n[v1HoldDate] datetime2(7),\n[v1AFlag] int,\n[v1StationID] int,\n[v1DBShortsDesc] nvarchar(4000),\n[v1DStatusID] int,\n[v1DFlag] int,\n[v1CallProgID] int,\n[v1CallProgUpdDate] datetime2(7),\n[v1oldOrderID] int,\n[v1DuplFlag] int,\n[v1LastUpdate] datetime2(7),\n[v1DateVolumised] datetime2(7),\n[v1VolumiserWeight] numeric(36,6),\n[v1VolumiserLength] numeric(36,6),\n[v1VolumiserHeight] numeric(36,6),\n[v1VolumiserWidth] numeric(36,6),\n[v1Volumiserid] int,\n[v1PL] numeric(36,6),\n[v1PH] numeric(36,6),\n[v1PW] numeric(36,6),\n[v1TotCharge] numeric(36,6),\n[v1DontRecalc] int,\n[v1containerID] int,\n[v1AvgVolWeight] numeric(36,6),\n[v2id] bigint,\n[v2orderid] bigint,\n[v2currentdeliverynoteid] bigint,\n[v2consignmentid] bigint,\n[v2customerid] bigint,\n[v2puid] bigint,\n[v2delid] bigint,\n[v2palletid] bigint,\n[v2parcelstatusid] bigint,\n[v2locid] bigint,\n[v2lastvolumizerid] bigint,\n[v2acceptancedate] bigint,\n[v2handoverdate] bigint,\n[v2volumiseddate] bigint,\n[v2coddate] bigint,\n[v2deliverafterdate] bigint,\n[v2barcode] nvarchar(4000),\n[v2customerbarcode] nvarchar(4000),\n[v2parceltype] nvarchar(4000),\n[v2cref] nvarchar(4000),\n[v2source] nvarchar(4000),\n[v2consignflag] int,\n[v2blockscan] int,\n[v2timehash] int,\n[v2deliverycount] int,\n[v2description] nvarchar(4000),\n[v2weight] float,\n[v2volweight] float,\n[v2chargeweight] float,\n[v2volumiserweight] float,\n[v2length] float,\n[v2height] float,\n[v2width] float,\n[v2vollength] float,\n[v2volheight] float,\n[v2volwidth] float,\n[v2pvalue] int,\n[v2seqno] int,\n[v2units] int,\n[v2insdate] bigint,\n[v2moddate] bigint,\n[v2insby] bigint,\n[v2legacyid] bigint,\n[v2distributiontype] nvarchar(4000),\n[v2migrationstatus] nvarchar(4000),\n[v2currentmanifestid] bigint,\n[v2customername] nvarchar(4000),\n[v2puname] nvarchar(4000),\n[v2delname] nvarchar(4000),\n[v2parcelstatus] nvarchar(4000),\n[v2locname] nvarchar(4000),\n[v2floorareaid] bigint,\n[v2poddate] bigint,\n[v2parceltypeid] int,\n[notinv2] int,\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/LMSV2/reconparcel.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.reconparcel \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/recontrack')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/v2Recon"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.recontrack') )\n    DROP EXTERNAL TABLE dbo.recontrack\nGO\n\nCREATE EXTERNAL TABLE dbo.recontrack (\n[v1ID] int,\n[v1OpenDt] datetime2(7),\n[v1CloseDt] datetime2(7),\n[v1LoadID] int,\n[v1UserID] int,\n[v1ParcelID] int,\n[v1tolocid] int,\n[v1touserid] int,\n[v1fromlocid] int,\n[v1TrackTypeID2] int,\n[v1TrackTypeID] int,\n[v1LastUpdate] datetime2(7),\n[v1ToLoadid] int,\n[v1OpenStationID] int,\n[v1CloseStationID] int,\n[v2id] bigint,\n[v2parcelid] bigint,\n[v2manifestid] bigint,\n[v2entity] nvarchar(4000),\n[v2eid] bigint,\n[v2trackdate] bigint,\n[v2parcelstatusid] bigint,\n[v2parcelstatus] nvarchar(4000),\n[v2tracktype] nvarchar(4000),\n[v2description] nvarchar(4000),\n[v2insdate] bigint,\n[v2moddate] bigint,\n[v2insby] bigint,\n[v2legacyid] bigint,\n[v2migrationstatus] nvarchar(4000),\n[v2legacytype] nvarchar(4000),\n[v2locname] nvarchar(4000),\n[v2locid] bigint,\n[v2bayid] bigint,\n[v2bay] nvarchar(4000),\n[v2movementid] bigint,\n[v2tracktypeid] bigint,\n[v2movementtypeid] bigint,\n[notinv2] int,\n\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/LMSV2/recontrack.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.recontrack \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/reconwaybill')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Structured Schema on Read/v2Recon"
				},
				"content": {
					"query": "IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dbo.reconwaybill') )\n    DROP EXTERNAL TABLE dbo.reconwaybill\nGO\n\nCREATE EXTERNAL TABLE dbo.reconwaybill (\n[v1ID] int,\n[v1Date] datetime2(7),\n[v1CustomerID] int,\n[v1BService] nvarchar(4000),\n[v1CService] nvarchar(4000),\n[v1OriginID] nvarchar(4000),\n[v1DefUser] nvarchar(4000),\n[v1Distance] float,\n[v1CourierID] nvarchar(4000),\n[v1CourierWBNo] nvarchar(4000),\n[v1Weight] numeric(36,6),\n[v1Volume] float,\n[v1VolumetricWeight] numeric(36,6),\n[v1ChargeWeight] numeric(36,6),\n[v1BillType] int,\n[v1NoOfParcels] int,\n[v1CapDate] datetime2(7),\n[v1Surcharge] numeric(36,6),\n[v1Insurance] numeric(36,6),\n[v1Tax] numeric(36,6),\n[v1TotalCharge] numeric(36,6),\n[v1DestinationName] nvarchar(4000),\n[v1OrderType] int,\n[v1PickupCustID] int,\n[v1DeliverCustID] int,\n[v1BillTo] nvarchar(4000),\n[v1Claimable] int,\n[v1Override] int,\n[v1VehicleCapacity] int,\n[v1CustPODDate] datetime2(7),\n[v1PODDate] datetime2(7),\n[v1Signee] nvarchar(4000),\n[v1GRVNo] nvarchar(4000),\n[v1Endorsements] nvarchar(4000),\n[v1BillCust] int,\n[v1COURIER_FROM] nvarchar(4000),\n[v1COURIER_TO] nvarchar(4000),\n[v1Charge] numeric(36,6),\n[v1TVALUE] numeric(36,6),\n[v1InService] datetime2(7),\n[v1DBTotalCharge] numeric(36,6),\n[v1LoadID] int,\n[v1RouteID] int,\n[v1mrpFlag] int,\n[v1Printed] int,\n[v1SReasonID] int,\n[v1MinKg] numeric(36,6),\n[v1MinRate] numeric(36,6),\n[v1Addrate] numeric(36,6),\n[v1DocFees] numeric(36,6),\n[v1Volumetric] numeric(36,6),\n[v1HasPrinted] nvarchar(4000),\n[v1Site] nvarchar(4000),\n[v1DeliveryNo] int,\n[v1FuelLevy] numeric(36,6),\n[v1TotFuelLevy] numeric(36,6),\n[v1CustAccID] int,\n[v1PODADDED] datetime2(7),\n[v1Scanned] int,\n[v1UserID] int,\n[v1Autoprinted] int,\n[v1PodUserID] int,\n[v1ScanUserID] int,\n[v1cref] nvarchar(4000),\n[v1UShort] int,\n[v1Delays] nvarchar(4000),\n[v1DeBriefed] int,\n[v1DeBriefDt] datetime2(7),\n[v1DBCtnsRet] int,\n[v1Uploaded] int,\n[v1ServiceExclude] int,\n[v1Failed] int,\n[v1PODURL] nvarchar(4000),\n[v1DelayReasonID] int,\n[v1NotDebrievReasonID] int,\n[v1DeliveryStatID] int,\n[v1ReasonGroupID] int,\n[v1eMailCust] int,\n[v1MobileIssueReportedID] int,\n[v1isMobile] int,\n[v1CustComment] nvarchar(4000),\n[v1CustFeedback] int,\n[v1ReasonDetailID] int,\n[v1MobileComments] nvarchar(4000),\n[v1InServiceReasonID] int,\n[v1NotDebrievCom] nvarchar(4000),\n[v1CallProgID] int,\n[v1LastUpdate] datetime2(7),\n[v1Dropsequence] int,\n[v1Noofparcelssreturned] int,\n[v1lat] float,\n[v1lng] float,\n[v1SignatureURL] nvarchar(4000),\n[v1Comment] nvarchar(4000),\n[v1ORVCode] int,\n[v1DeliverBy] datetime2(7),\n[v1ORVStatus] nvarchar(4000),\n[v1podpdf_base64] nvarchar(4000),\n[v1podsignature_base64] nvarchar(4000),\n[v1ToLocID] int,\n[v1NoEndorsementsFLag] int,\n[v1ORVDebriefDate] datetime2(7),\n[v1debrifmode] nvarchar(4000),\n[v1ORVCustComment] nvarchar(4000),\n[v1ORVCustFeedback] nvarchar(4000),\n[v1ORVPODDate] datetime2(7),\n[v1PclsLoaded] int,\n[v2id] bigint,\n[v2customerid] bigint,\n[v2puid] bigint,\n[v2delid] bigint,\n[v2manifestid] bigint,\n[v2status] nvarchar(4000),\n[v2loadsequence] int,\n[v2cref] nvarchar(4000),\n[v2cref2] nvarchar(4000),\n[v2insdate] bigint,\n[v2moddate] bigint,\n[v2scandate] bigint,\n[v2debriefdate] bigint,\n[v2poddate] bigint,\n[v2customerpoddate] bigint,\n[v2uploaddate] bigint,\n[v2numpcls] int,\n[v2signee] nvarchar(4000),\n[v2grvno] nvarchar(4000),\n[v2endorsment] nvarchar(4000),\n[v2customerfeedback] nvarchar(4000),\n[v2customernote] nvarchar(4000),\n[v2comment] nvarchar(4000),\n[v2uuid] nvarchar(4000),\n[v2extpodurl] nvarchar(4000),\n[v2orvcode] nvarchar(4000),\n[v2orvstatus] nvarchar(4000),\n[v2orvdebriefdate] bigint,\n[v2orvpoddate] bigint,\n[v2orvcustcomment] nvarchar(4000),\n[v2orvcustfeedback] nvarchar(4000),\n[v2insby] bigint,\n[v2scanby] bigint,\n[v2debriefby] bigint,\n[v2legacyid] bigint,\n[v2puibtcount] int,\n[v2pucollectcount] int,\n[v2migrationstatus] nvarchar(4000),\n[v2customername] nvarchar(4000),\n[v2puname] nvarchar(4000),\n[v2delname] nvarchar(4000),\n[v2weight] float,\n[v2type] nvarchar(4000),\n[v2debriefmode] nvarchar(4000),\n[notinv2] int,\n\n\n\t)\n\tWITH (\n\tLOCATION = 'Structured Data/LMSV2/reconwaybill.parquet',\n\tDATA_SOURCE = [synapse_citylogisticsstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.reconwaybill \nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "STOPS",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/00_LMSv1v2Recon_RunOrder')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Ops/LMSv1v2Recon"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "f8ddc606-c305-4de0-a749-c4e53b6f3049"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse_Prod/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace-prod/bigDataPools/PRDSparkPool",
						"name": "PRDSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/PRDSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"if 'prod' in Environment:\r\n",
							"    StorageAccount = 'citylogisticsstorageprod'\r\n",
							"    StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"    StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"    StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"else:\r\n",
							"    StorageAccount = 'citylogisticsstorage'\r\n",
							"    StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"    StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"    StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql import functions as F\r\n",
							"from pyspark.sql import DataFrame"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the v2customerorder LMSV2 Table\r\n",
							"v2customerorder = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/CLMasterData/v2customerorder.parquet', format='parquet')\r\n",
							"columns=v2customerorder.columns\r\n",
							"for i in range(len(columns)):v2customerorder=v2customerorder.withColumnRenamed(columns[i],'v2'+ columns[i])\r\n",
							"v2customerorder.createOrReplaceTempView(\"v2customerorder\")\r\n",
							"\r\n",
							"#Create DataFrame for the v2deliverynote LMSV2 Table\r\n",
							"v2deliverynote = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/CLMasterData/v2deliverynote.parquet', format='parquet')\r\n",
							"columns1=v2deliverynote.columns\r\n",
							"for i in range(len(columns1)):v2deliverynote=v2deliverynote.withColumnRenamed(columns1[i],'v2'+ columns1[i])\r\n",
							"v2deliverynote.createOrReplaceTempView(\"v2deliverynote\")\r\n",
							"\r\n",
							"#Create DataFrame for the v2manifest LMSV2 Table\r\n",
							"v2manifest = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/CLMasterData/v2manifest.parquet', format='parquet')\r\n",
							"columns2=v2manifest.columns\r\n",
							"for i in range(len(columns2)):v2manifest=v2manifest.withColumnRenamed(columns2[i],'v2'+ columns2[i])\r\n",
							"v2manifest.createOrReplaceTempView(\"v2manifest\")\r\n",
							"\r\n",
							"#Create DataFrame for the v2consignment LMSV2 Table\r\n",
							"v2consignment = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/CLMasterData/v2consignment.parquet', format='parquet')\r\n",
							"columns3=v2consignment.columns\r\n",
							"for i in range(len(columns3)):v2consignment=v2consignment.withColumnRenamed(columns3[i],'v2'+ columns3[i])\r\n",
							"v2consignment.createOrReplaceTempView(\"v2consignment\")\r\n",
							"\r\n",
							"#Create DataFrame for the v2parcel LMSV2 Table\r\n",
							"v2parcel = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/CLMasterData/v2parcel.parquet', format='parquet')\r\n",
							"columns8=v2parcel.columns\r\n",
							"for i in range(len(columns8)):v2parcel=v2parcel.withColumnRenamed(columns8[i],'v2'+ columns8[i])\r\n",
							"v2parcel.createOrReplaceTempView(\"v2parcel\")\r\n",
							"\r\n",
							"#Create DataFrame for the v2track LMSV2 Table\r\n",
							"v2track = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/CLMasterData/v2track.parquet', format='parquet')\r\n",
							"columns9=v2track.columns\r\n",
							"for i in range(len(columns9)):v2track=v2track.withColumnRenamed(columns9[i],'v2'+ columns9[i])\r\n",
							"v2track.createOrReplaceTempView(\"v2track\")\r\n",
							"\r\n",
							"#Create DataFrame for the v2movement LMSV2 Table\r\n",
							"v2movement = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/CLMasterData/v2movement.parquet', format='parquet')\r\n",
							"columns10=v2movement.columns\r\n",
							"for i in range(len(columns10)):v2movement=v2movement.withColumnRenamed(columns10[i],'v2'+ columns10[i])\r\n",
							"v2movement.createOrReplaceTempView(\"v2movement\")\r\n",
							"\r\n",
							"\r\n",
							"######################################################################################################\r\n",
							"\r\n",
							"#Create DataFrame for the dbowaybill LMS Table\r\n",
							"dbowaybill = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbowaybill.parquet', format='parquet')\r\n",
							"columns4=dbowaybill.columns\r\n",
							"for i in range(len(columns4)):dbowaybill=dbowaybill.withColumnRenamed(columns4[i],'v1'+ columns4[i])\r\n",
							"dbowaybill.createOrReplaceTempView(\"dbowaybill\")\r\n",
							"\r\n",
							"#Create DataFrame for the dboloads LMS Table\r\n",
							"dboloads = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboloads.parquet', format='parquet')\r\n",
							"columns5=dboloads.columns\r\n",
							"for i in range(len(columns5)):dboloads=dboloads.withColumnRenamed(columns5[i],'v1'+ columns5[i])\r\n",
							"dboloads.createOrReplaceTempView(\"dboloads\")\r\n",
							"\r\n",
							"#Create DataFrame for the dboconsignment LMS Table\r\n",
							"dboconsignment = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboconsignment.parquet', format='parquet')\r\n",
							"columns6=dboconsignment.columns\r\n",
							"for i in range(len(columns6)):dboconsignment=dboconsignment.withColumnRenamed(columns6[i],'v1'+ columns6[i])\r\n",
							"dboconsignment.createOrReplaceTempView(\"dboconsignment\")\r\n",
							"\r\n",
							"#Create DataFrame for the dboorder LMS Table\r\n",
							"dboorder = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboorder.parquet', format='parquet')\r\n",
							"columns7=dboorder.columns\r\n",
							"for i in range(len(columns7)):dboorder=dboorder.withColumnRenamed(columns7[i],'v1'+ columns7[i])\r\n",
							"dboorder.createOrReplaceTempView(\"dboorder\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dboparcel LMS Table\r\n",
							"# dboparcel = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboparcel.parquet', format='parquet')\r\n",
							"# columns11=dboparcel.columns\r\n",
							"# for i in range(len(columns11)):dboparcel=dboparcel.withColumnRenamed(columns11[i],'v1'+ columns11[i])\r\n",
							"# dboparcel.createOrReplaceTempView(\"dboparcel\")\r\n",
							"\r\n",
							"#Create DataFrame for the sstparcelunion LMS Table\r\n",
							"sstparcelunion = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Semi Structured Data/OPS/sstparcelunion.parquet', format='parquet')\r\n",
							"columns11=sstparcelunion.columns\r\n",
							"for i in range(len(columns11)):sstparcelunion=sstparcelunion.withColumnRenamed(columns11[i],'v1'+ columns11[i])\r\n",
							"sstparcelunion.createOrReplaceTempView(\"sstparcelunion\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbotrack LMS Table\r\n",
							"# dbotrack = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbotrack.parquet', format='parquet')\r\n",
							"# columns12=dbotrack.columns\r\n",
							"# for i in range(len(columns12)):dbotrack=dbotrack.withColumnRenamed(columns12[i],'v1'+ columns12[i])\r\n",
							"# dbotrack.createOrReplaceTempView(\"dbotrack\")\r\n",
							"\r\n",
							"#Create DataFrame for the ssttrackunion LMS Table\r\n",
							"ssttrackunion = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Semi Structured Data/OPS/ssttrackunion.parquet', format='parquet')\r\n",
							"columns12=ssttrackunion.columns\r\n",
							"for i in range(len(columns12)):ssttrackunion=ssttrackunion.withColumnRenamed(columns12[i],'v1'+ columns12[i])\r\n",
							"ssttrackunion.createOrReplaceTempView(\"ssttrackunion\")"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Ops/LMSv1v2Recon/01_LMSv1v2Recon_ETL"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"reconparcel.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/LMSV2/reconparcel.parquet', mode = \"overwrite\")\r\n",
							"reconwaybill.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/LMSV2/reconwaybill.parquet', mode = \"overwrite\")\r\n",
							"reconorder.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/LMSV2/reconorder.parquet', mode = \"overwrite\")\r\n",
							"reconconsignment.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/LMSV2/reconconsignment.parquet', mode = \"overwrite\")\r\n",
							"reconloads.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/LMSV2/reconloads.parquet', mode = \"overwrite\")\r\n",
							"recontrack.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/LMSV2/recontrack.parquet', mode = \"overwrite\")\r\n",
							"reconmovement.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/LMSV2/reconmovement.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"SUCCESS\")"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/00_ReportsRunOrder')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Reports"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "7a365da5-7514-43e0-affb-d44301afd5e5"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse_Prod/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace-prod/bigDataPools/PRDSparkPool",
						"name": "PRDSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/PRDSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"if 'prod' in Environment:\r\n",
							"    StorageAccount = 'citylogisticsstorageprod'\r\n",
							"    StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"    StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"    StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"else:\r\n",
							"    StorageAccount = 'citylogisticsstorage'\r\n",
							"    StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"    StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"    StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import *\r\n",
							"from pyspark.sql.window import Window"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the stparcel LMS Table\r\n",
							"stparcel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stparcel.parquet', format='parquet')\r\n",
							"stparcel.createOrReplaceTempView(\"stparcel\")\r\n",
							"\r\n",
							"#Create DataFrame for the stdeliverypickupcustomer LMS Table\r\n",
							"stdeliverypickupcustomer = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stdeliverypickupcustomer.parquet', format='parquet')\r\n",
							"stdeliverypickupcustomer.createOrReplaceTempView(\"stdeliverypickupcustomer\")\r\n",
							"\r\n",
							"#Create DataFrame for the stbillcustomer LMS Table\r\n",
							"stbillcustomer = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stbillcustomer.parquet', format='parquet')\r\n",
							"stbillcustomer.createOrReplaceTempView(\"stbillcustomer\")\r\n",
							"\r\n",
							"#create dataframe for the stsroute lms table\r\n",
							"stsroute = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stsroute.parquet', format='parquet')\r\n",
							"stsroute.createOrReplaceTempView(\"stsroute\")\r\n",
							"\r\n",
							"#create dataframe for the storder lms table\r\n",
							"storder = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/storder.parquet', format='parquet')\r\n",
							"storder.createOrReplaceTempView(\"storder\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Reports/LMS/_01_RptParcelLevel"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"rptparcellevel.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/RPT/rptparcellevel.parquet', mode = \"overwrite\")\r\n",
							"rptparcellevellimitload.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/RPT/rptparcellevellimitload.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the stlmstrack LMS Table\r\n",
							"stlmstrack = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stlmstrack.parquet', format='parquet')\r\n",
							"stlmstrack.createOrReplaceTempView(\"stlmstrack\")\r\n",
							"\r\n",
							"#Create DataFrame for the stparcel LMS Table\r\n",
							"stparcel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stparcel.parquet', format='parquet')\r\n",
							"stparcel.createOrReplaceTempView(\"stparcel\")\r\n",
							"\r\n",
							"#Create DataFrame for the stbillcustomer LMS Table\r\n",
							"stbillcustomer = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stbillcustomer.parquet', format='parquet')\r\n",
							"stbillcustomer.createOrReplaceTempView(\"stbillcustomer\")\r\n",
							"\r\n",
							"#Create DataFrame for the stdeliverypickupcustomer LMS Table\r\n",
							"stdeliverypickupcustomer = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stdeliverypickupcustomer.parquet', format='parquet')\r\n",
							"stdeliverypickupcustomer.createOrReplaceTempView(\"stdeliverypickupcustomer\")\r\n",
							"\r\n",
							"#create dataframe for the stsroute lms table\r\n",
							"stsroute = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stsroute.parquet', format='parquet')\r\n",
							"stsroute.createOrReplaceTempView(\"stsroute\")\r\n",
							"\r\n",
							"#Create DataFrame for the stzone LMS Table\r\n",
							"stzone = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stzone.parquet', format='parquet')\r\n",
							"stzone.createOrReplaceTempView(\"stzone\")\r\n",
							"\r\n",
							"#Create DataFrame for the stlocation LMS Table\r\n",
							"stlocation = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stlocation.parquet', format='parquet')\r\n",
							"stlocation.createOrReplaceTempView(\"stlocation\")"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Reports/LMS/_02_RptTrackLevel"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"rpttracklevel.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/RPT/rpttracklevel.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the stloadchild LMS Table\r\n",
							"stloadchild = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stloadchild.parquet', format='parquet')\r\n",
							"stloadchild.createOrReplaceTempView(\"stloadchild\")\r\n",
							"\r\n",
							"#Create DataFrame for the stloadparent LMS Table\r\n",
							"stloadparent = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stloadparent.parquet', format='parquet')\r\n",
							"stloadparent.createOrReplaceTempView(\"stloadparent\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbolocation LMS Table\r\n",
							"dbolocation = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbolocation.parquet', format='parquet')\r\n",
							"dbolocation.createOrReplaceTempView(\"dbolocation\")\r\n",
							"\r\n",
							"#Create DataFrame for the stmdvehicle LMS Table\r\n",
							"stmdvehicle = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/MD/stmdvehicle.parquet', format='parquet')\r\n",
							"stmdvehicle.createOrReplaceTempView(\"stmdvehicle\")\r\n",
							"\r\n",
							"#Create DataFrame for the stwaybill LMS Table\r\n",
							"stwaybill = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stwaybill.parquet', format='parquet')\r\n",
							"stwaybill.createOrReplaceTempView(\"stwaybill\")\r\n",
							"\r\n",
							"#Create DataFrame for the stbillcustomer LMS Table\r\n",
							"stbillcustomer = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stbillcustomer.parquet', format='parquet')\r\n",
							"stbillcustomer.createOrReplaceTempView(\"stbillcustomer\")"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Reports/LMS/_03_RptLoadLevel"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"rptloadparentlevel.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/RPT/rptloadparentlevel.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the stconsignment LMS Table\r\n",
							"stconsignment = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stconsignment.parquet', format='parquet')\r\n",
							"stconsignment.createOrReplaceTempView(\"stconsignment\")\r\n",
							"\r\n",
							"#Create DataFrame for the stbillcustomer LMS Table\r\n",
							"stbillcustomer = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stbillcustomer.parquet', format='parquet')\r\n",
							"stbillcustomer.createOrReplaceTempView(\"stbillcustomer\")\r\n",
							"\r\n",
							"#Create DataFrame for the stdeliverypickupcustomer LMS Table\r\n",
							"stdeliverypickupcustomer = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stdeliverypickupcustomer.parquet', format='parquet')\r\n",
							"stdeliverypickupcustomer.createOrReplaceTempView(\"stdeliverypickupcustomer\")\r\n",
							"\r\n",
							"#create dataframe for the stsroute lms table\r\n",
							"stsroute = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stsroute.parquet', format='parquet')\r\n",
							"stsroute.createOrReplaceTempView(\"stsroute\")\r\n",
							"\r\n",
							"#create dataframe for the stzone lms table\r\n",
							"stzone = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stzone.parquet', format='parquet')\r\n",
							"stzone.createOrReplaceTempView(\"stzone\")\r\n",
							"\r\n",
							"#create dataframe for the stlocation lms table\r\n",
							"stlocation = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stlocation.parquet', format='parquet')\r\n",
							"stlocation.createOrReplaceTempView(\"stlocation\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbobillconsignmentr LMS Table\r\n",
							"dbobillconsignmentr = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbobill_consignmentr.parquet', format='parquet')\r\n",
							"dbobillconsignmentr.createOrReplaceTempView(\"dbobillconsignmentr\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbobillexceptionr LMS Table\r\n",
							"dbobillexceptionr = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbobill_exceptionr.parquet', format='parquet')\r\n",
							"dbobillexceptionr.createOrReplaceTempView(\"dbobillexceptionr\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbobillzoneroute LMS Table\r\n",
							"dbobillzoneroute = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbobill_zoneroute.parquet', format='parquet')\r\n",
							"dbobillzoneroute.createOrReplaceTempView(\"dbobillzoneroute\")"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Reports/LMS/_04_RptConsignmentLevel"
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"rptconsignmentlevel.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/RPT/rptconsignmentlevel.parquet', mode = \"overwrite\")\r\n",
							"rptbillconsignmentlevel.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/RPT/rptbillconsignmentlevel.parquet', mode = \"overwrite\")\r\n",
							"rptbillconsignmentexceptionlevel.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/RPT/rptbillconsignmentexceptionlevel.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the stwaybill LMS Table\r\n",
							"stwaybill = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stwaybill.parquet', format='parquet')\r\n",
							"stwaybill.createOrReplaceTempView(\"stwaybill\")\r\n",
							"\r\n",
							"#Create DataFrame for the stparcel LMS Table\r\n",
							"stparcel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stparcel.parquet', format='parquet')\r\n",
							"stparcel.createOrReplaceTempView(\"stparcel\")\r\n",
							"\r\n",
							"#Create DataFrame for the stbillcustomer LMS Table\r\n",
							"stbillcustomer = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stbillcustomer.parquet', format='parquet')\r\n",
							"stbillcustomer.createOrReplaceTempView(\"stbillcustomer\")\r\n",
							"\r\n",
							"#Create DataFrame for the stdeliverypickupcustomer LMS Table\r\n",
							"stdeliverypickupcustomer = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stdeliverypickupcustomer.parquet', format='parquet')\r\n",
							"stdeliverypickupcustomer.createOrReplaceTempView(\"stdeliverypickupcustomer\")\r\n",
							"\r\n",
							"#Create DataFrame for the dborouteratetype LMS Table\r\n",
							"dborouteratetype = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dborouteratetype.parquet', format='parquet')\r\n",
							"dborouteratetype.createOrReplaceTempView(\"dborouteratetype\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbocustacc LMS Table\r\n",
							"dbocustacc = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbocustacc.parquet', format='parquet')\r\n",
							"dbocustacc.createOrReplaceTempView(\"dbocustacc\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Reports/LMS/_05_RptWaybillLevel"
						],
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"rptwaybilllevel.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/RPT/rptwaybilllevel.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"<mark>**TMS**</mark>"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the sttrip LMS Table\r\n",
							"sttrip = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/sttrip.parquet', format='parquet')\r\n",
							"sttrip.createOrReplaceTempView(\"sttrip\")\r\n",
							"\r\n",
							"#Create DataFrame for the stlmsbookingweights LMS Table\r\n",
							"stlmsbookingweights = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stlmsbookingweights.parquet', format='parquet')\r\n",
							"stlmsbookingweights.createOrReplaceTempView(\"stlmsbookingweights\")\r\n",
							"\r\n",
							"#Create DataFrame for the stbooking LMS Table\r\n",
							"stbooking = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stbooking.parquet', format='parquet')\r\n",
							"stbooking.createOrReplaceTempView(\"stbooking\")\r\n",
							"\r\n",
							"#Create DataFrame for the stlmstrack LMS Table\r\n",
							"stlmstrack = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stlmstrack.parquet', format='parquet')\r\n",
							"stlmstrack.createOrReplaceTempView(\"stlmstrack\")\r\n",
							"\r\n",
							"#Create DataFrame for the stloadchild LMS Table\r\n",
							"stloadchild = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stloadchild.parquet', format='parquet')\r\n",
							"stloadchild.createOrReplaceTempView(\"stloadchild\")\r\n",
							"\r\n",
							"#Create DataFrame for the stloadparent LMS Table\r\n",
							"stloadparent = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stloadparent.parquet', format='parquet')\r\n",
							"stloadparent.createOrReplaceTempView(\"stloadparent\")\r\n",
							"\r\n",
							"#Create DataFrame for the stthirdparty LMS Table\r\n",
							"stthirdparty = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stthirdparty.parquet', format='parquet')\r\n",
							"stthirdparty.createOrReplaceTempView(\"stthirdparty\")\r\n",
							"\r\n",
							"#Create DataFrame for the publiclmsdata LMS Table\r\n",
							"publiclmsdata = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publiclmsdata.parquet', format='parquet')\r\n",
							"publiclmsdata.createOrReplaceTempView(\"publiclmsdata\")"
						],
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Reports/TMS/_01_RptTripLevel"
						],
						"outputs": [],
						"execution_count": 27
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"rpttriplevel.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/RPT/rpttriplevel.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**<mark>DETAILED REPORTS LMS</mark>**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the rptparcellevel LMS Table\r\n",
							"rptparcellevel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/RPT/rptparcellevel.parquet', format='parquet')\r\n",
							"rptparcellevel.createOrReplaceTempView(\"rptparcellevel\")\r\n",
							"\r\n",
							"#Create DataFrame for the dboplclpnroutinghistorycenturion LMS Table\r\n",
							"dboplclpnroutinghistorycenturion = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboplc_lpnrouting_history_centurion.parquet', format='parquet')\r\n",
							"dboplclpnroutinghistorycenturion.createOrReplaceTempView(\"dboplclpnroutinghistorycenturion\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 28
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Reports/LMS/101_RptParcelLevel_CenturionSorterReport"
						],
						"outputs": [],
						"execution_count": 29
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"centurionsorterreport.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/RPT/centurionsorterreport.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the rptparcellevel LMS Table\r\n",
							"rptparcellevel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/RPT/rptparcellevel.parquet', format='parquet')\r\n",
							"rptparcellevel.createOrReplaceTempView(\"rptparcellevel\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbovolumiserlog  LMS Table\r\n",
							"dbovolumiserlog = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbovolumiserlog.parquet', format='parquet')\r\n",
							"dbovolumiserlog.createOrReplaceTempView(\"dbovolumiserlog\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbovolumiserlocation  LMS Table\r\n",
							"dbovolumiserlocation = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbovolumiserlocation.parquet', format='parquet')\r\n",
							"dbovolumiserlocation.createOrReplaceTempView(\"dbovolumiserlocation\")"
						],
						"outputs": [],
						"execution_count": 30
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Reports/LMS/102_RptParcelLevel_VolumiserLogReport"
						],
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"volumiserlogreport.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/RPT/volumiserlogreport.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the rptparcellevel LMS Table\r\n",
							"rptparcellevel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/RPT/rptparcellevel.parquet', format='parquet')\r\n",
							"rptparcellevel.createOrReplaceTempView(\"rptparcellevel\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbolocation LMS Table\r\n",
							"dbolocation = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbolocation.parquet', format='parquet')\r\n",
							"dbolocation.createOrReplaceTempView(\"dbolocation\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbocsdmessageparcel LMS Table\r\n",
							"dbocsdmessageparcel = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbocsd_messageparcel.parquet', format='parquet')\r\n",
							"dbocsdmessageparcel.createOrReplaceTempView(\"dbocsdmessageparcel\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbocsdmessage  LMS Table\r\n",
							"dbocsdmessage = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbocsd_message.parquet', format='parquet')\r\n",
							"dbocsdmessage.createOrReplaceTempView(\"dbocsdmessage\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbousers LMS Table\r\n",
							"dbousers = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbousers.parquet', format='parquet')\r\n",
							"dbousers.createOrReplaceTempView(\"dbousers\")\r\n",
							"\r\n",
							"#Create DataFrame for the stloadchild LMS Table\r\n",
							"stloadchild = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stloadchild.parquet', format='parquet')\r\n",
							"stloadchild.createOrReplaceTempView(\"stloadchild\")"
						],
						"outputs": [],
						"execution_count": 32
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Reports/LMS/103_RptParcelLevel_ParcelDamageReport"
						],
						"outputs": [],
						"execution_count": 33
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"parceldamagereport.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/RPT/parceldamagereport.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the rptparcellevel LMS Table\r\n",
							"rptparcellevel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/RPT/rptparcellevel.parquet', format='parquet')\r\n",
							"rptparcellevel.createOrReplaceTempView(\"rptparcellevel\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbodamagedparcelaudit LMS Table\r\n",
							"dbodamagedparcelaudit = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodamaged_parcel_audit.parquet', format='parquet')\r\n",
							"dbodamagedparcelaudit.createOrReplaceTempView(\"dbodamagedparcelaudit\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbodamagedparcelaudittapetype LMS Table\r\n",
							"dbodamagedparcelaudittapetype = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodamaged_parcel_audit_tapetype.parquet', format='parquet')\r\n",
							"dbodamagedparcelaudittapetype.createOrReplaceTempView(\"dbodamagedparcelaudittapetype\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbovolumiserlocation  LMS Table\r\n",
							"dbovolumiserlocation = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbovolumiserlocation.parquet', format='parquet')\r\n",
							"dbovolumiserlocation.createOrReplaceTempView(\"dbovolumiserlocation\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbodamagedparcelauditfluteprofiles  LMS Table\r\n",
							"dbodamagedparcelauditfluteprofiles = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodamaged_parcel_audit_flute_profiles.parquet', format='parquet')\r\n",
							"dbodamagedparcelauditfluteprofiles.createOrReplaceTempView(\"dbodamagedparcelauditfluteprofiles\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbousers LMS Table\r\n",
							"dbousers = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbousers.parquet', format='parquet')\r\n",
							"dbousers.createOrReplaceTempView(\"dbousers\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbolocation LMS Table\r\n",
							"dbolocation = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbolocation.parquet', format='parquet')\r\n",
							"dbolocation.createOrReplaceTempView(\"dbolocation\")\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 34
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Reports/LMS/104_RptParcelLevel_QCParcelDamageReport"
						],
						"outputs": [],
						"execution_count": 35
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"qcparceldamagereport.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/RPT/qcparceldamagereport.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the stlmstrack LMS Table\r\n",
							"stlmstrack = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stlmstrack.parquet', format='parquet')\r\n",
							"stlmstrack.createOrReplaceTempView(\"stlmstrack\")\r\n",
							"\r\n",
							"#Create DataFrame for the stparcel LMS Table\r\n",
							"stparcel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stparcel.parquet', format='parquet')\r\n",
							"stparcel.createOrReplaceTempView(\"stparcel\")\r\n",
							"\r\n",
							"#Create DataFrame for the stdeliverypickupcustomer LMS Table\r\n",
							"stdeliverypickupcustomer = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stdeliverypickupcustomer.parquet', format='parquet')\r\n",
							"stdeliverypickupcustomer.createOrReplaceTempView(\"stdeliverypickupcustomer\")\r\n",
							"\r\n",
							"#Create DataFrame for the stbillcustomer LMS Table\r\n",
							"stbillcustomer = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stbillcustomer.parquet', format='parquet')\r\n",
							"stbillcustomer.createOrReplaceTempView(\"stbillcustomer\")\r\n",
							"\r\n",
							"#Create DataFrame for the stlocation LMS Table\r\n",
							"stlocation = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stlocation.parquet', format='parquet')\r\n",
							"stlocation.createOrReplaceTempView(\"stlocation\")\r\n",
							"\r\n",
							"#create dataframe for the stsroute lms table\r\n",
							"stsroute = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stsroute.parquet', format='parquet')\r\n",
							"stsroute.createOrReplaceTempView(\"stsroute\")\r\n",
							"\r\n",
							"#Create DataFrame for the stzone LMS Table\r\n",
							"stzone = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stzone.parquet', format='parquet')\r\n",
							"stzone.createOrReplaceTempView(\"stzone\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbodcorder LMS Table\r\n",
							"dbodcorder = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodc_order.parquet', format='parquet')\r\n",
							"dbodcorder.createOrReplaceTempView(\"dbodcorder\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbodcprepack LMS Table\r\n",
							"dbodcprepack = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodc_prepack.parquet', format='parquet')\r\n",
							"dbodcprepack.createOrReplaceTempView(\"dbodcprepack\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbodctransfer LMS Table\r\n",
							"dbodctransfer = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodc_transfer.parquet', format='parquet')\r\n",
							"dbodctransfer.createOrReplaceTempView(\"dbodctransfer\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 36
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Reports/LMS/105_RptTrackLevel_BranchVolumesReport"
						],
						"outputs": [],
						"execution_count": 37
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"branchvolumesreport.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/RPT/branchvolumesreport.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the rptloadparentlevel LMS Table\r\n",
							"rptloadparentlevel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/RPT/rptloadparentlevel.parquet', format='parquet')\r\n",
							"rptloadparentlevel.createOrReplaceTempView(\"rptloadparentlevel\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 38
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Reports/LMS/106_RptLoadLevel_VehicleUtilisationReport"
						],
						"outputs": [],
						"execution_count": 39
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"vehicleutilisationreport.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/RPT/vehicleutilisationreport.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the rptparcellevel LMS Table\r\n",
							"rptparcellevel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/RPT/rptparcellevel.parquet', format='parquet')\r\n",
							"rptparcellevel.createOrReplaceTempView(\"rptparcellevel\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbolocation LMS Table\r\n",
							"dbolocation = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbolocation.parquet', format='parquet')\r\n",
							"dbolocation.createOrReplaceTempView(\"dbolocation\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbozone LMS Table\r\n",
							"dbozone = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbozone.parquet', format='parquet')\r\n",
							"dbozone.createOrReplaceTempView(\"dbozone\")"
						],
						"outputs": [],
						"execution_count": 40
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Reports/LMS/107_RptParcelLevel_DailyOpsStrikeRateReport"
						],
						"outputs": [],
						"execution_count": 41
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"dailyopsstrikeratereport.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/RPT/dailyopsstrikeratereport.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the rptbillconsignmentlevel LMS Table\r\n",
							"rptbillconsignmentlevel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/RPT/rptbillconsignmentlevel.parquet', format='parquet')\r\n",
							"rptbillconsignmentlevel.createOrReplaceTempView(\"rptbillconsignmentlevel\")\r\n",
							"\r\n",
							"#Create DataFrame for the rptbillconsignmentexceptionlevel LMS Table\r\n",
							"rptbillconsignmentexceptionlevel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/RPT/rptbillconsignmentexceptionlevel.parquet', format='parquet')\r\n",
							"rptbillconsignmentexceptionlevel.createOrReplaceTempView(\"rptbillconsignmentexceptionlevel\")"
						],
						"outputs": [],
						"execution_count": 42
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Reports/LMS/108_RptBillConsignmentLevel_RevenueVerticalReport"
						],
						"outputs": [],
						"execution_count": 43
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"revenueverticalreport.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/RPT/revenueverticalreport.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the rptparcellevel LMS Table\r\n",
							"rptparcellevel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/RPT/rptparcellevel.parquet', format='parquet')\r\n",
							"rptparcellevel.createOrReplaceTempView(\"rptparcellevel\")\r\n",
							"\r\n",
							"#Create DataFrame for the rpttracklevel LMS Table\r\n",
							"rpttracklevel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/RPT/rpttracklevel.parquet', format='parquet')\r\n",
							"rpttracklevel.createOrReplaceTempView(\"rpttracklevel\")\r\n",
							"\r\n",
							"#create dataframe for the stsroute lms table\r\n",
							"stsroute = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stsroute.parquet', format='parquet')\r\n",
							"stsroute.createOrReplaceTempView(\"stsroute\")\r\n",
							"\r\n",
							"#create dataframe for the stzone lms table\r\n",
							"stzone = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stzone.parquet', format='parquet')\r\n",
							"stzone.createOrReplaceTempView(\"stzone\")\r\n",
							"\r\n",
							"#create dataframe for the stlocation lms table\r\n",
							"stlocation = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stlocation.parquet', format='parquet')\r\n",
							"stlocation.createOrReplaceTempView(\"stlocation\")"
						],
						"outputs": [],
						"execution_count": 44
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Reports/LMS/109_RptParcelLevel_ManifestWeightAnalysis"
						],
						"outputs": [],
						"execution_count": 45
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"manifestweightanalysisreport.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/RPT/manifestweightanalysisreport.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the rptparcellevel LMS Table\r\n",
							"rptparcellevel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/RPT/rptparcellevel.parquet', format='parquet')\r\n",
							"rptparcellevel.createOrReplaceTempView(\"rptparcellevel\")\r\n",
							"\r\n",
							"#Create DataFrame for the rpttracklevel LMS Table\r\n",
							"rpttracklevel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/RPT/rpttracklevel.parquet', format='parquet')\r\n",
							"rpttracklevel.createOrReplaceTempView(\"rpttracklevel\")\r\n",
							"\r\n",
							"#create dataframe for the stlocation lms table\r\n",
							"stlocation = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stlocation.parquet', format='parquet')\r\n",
							"stlocation.createOrReplaceTempView(\"stlocation\")\r\n",
							"\r\n",
							"#create dataframe for the dbodcorder lms table\r\n",
							"dbodcorder = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodc_order.parquet', format='parquet')\r\n",
							"dbodcorder.createOrReplaceTempView(\"dbodcorder\")\r\n",
							"\r\n",
							"#create dataframe for the dbodctransfer lms table\r\n",
							"dbodctransfer = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodc_transfer.parquet', format='parquet')\r\n",
							"dbodctransfer.createOrReplaceTempView(\"dbodctransfer\")\r\n",
							"\r\n",
							"#create dataframe for the dbodcprepack lms table\r\n",
							"dbodcprepack = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodc_prepack.parquet', format='parquet')\r\n",
							"dbodcprepack.createOrReplaceTempView(\"dbodcprepack\")"
						],
						"outputs": [],
						"execution_count": 46
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Reports/LMS/110_RptParcelLevel_DCReceivingDaily"
						],
						"outputs": [],
						"execution_count": 47
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"dcreceivingdailyreport.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/RPT/dcreceivingdailyreport.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# #Create DataFrame for the rptwaybilllevel LMS Table\r\n",
							"# rptwaybilllevel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/RPT/rptwaybilllevel.parquet', format='parquet')\r\n",
							"# rptwaybilllevel.createOrReplaceTempView(\"rptwaybilllevel\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbohiltigroupcode LMS Table\r\n",
							"# dbohiltigroupcode = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbohiltigroupcode.parquet', format='parquet')\r\n",
							"# dbohiltigroupcode.createOrReplaceTempView(\"dbohiltigroupcode\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbohiltigroup LMS Table\r\n",
							"# dbohiltigroup = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbohiltigroup.parquet', format='parquet')\r\n",
							"# dbohiltigroup.createOrReplaceTempView(\"dbohiltigroup\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbohiltirouteratetype LMS Table\r\n",
							"# dbohiltirouteratetype = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbohiltirouteratetype.parquet', format='parquet')\r\n",
							"# dbohiltirouteratetype.createOrReplaceTempView(\"dbohiltirouteratetype\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# %run TRANSFORM/02 STRUCTURED/Reports/LMS/111_RptWaybillLevel_HiltiWBSummary"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# hiltiwbregionsummaryreport.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/RPT/hiltiwbregionsummaryreport.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the rptparcellevel LMS Table\r\n",
							"rptparcellevel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/RPT/rptparcellevel.parquet', format='parquet')\r\n",
							"rptparcellevel.createOrReplaceTempView(\"rptparcellevel\")\r\n",
							"\r\n",
							"#Create DataFrame for the stlocation LMS Table\r\n",
							"stlocation = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stlocation.parquet', format='parquet')\r\n",
							"stlocation.createOrReplaceTempView(\"stlocation\")\r\n",
							"\r\n",
							"#Create DataFrame for the stzone LMS Table\r\n",
							"stzone = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stzone.parquet', format='parquet')\r\n",
							"stzone.createOrReplaceTempView(\"stzone\")\r\n",
							"\r\n",
							"#Create DataFrame for the stsroute LMS Table\r\n",
							"stsroute = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stsroute.parquet', format='parquet')\r\n",
							"stsroute.createOrReplaceTempView(\"stsroute\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 48
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Reports/LMS/112_RptParcelLevel_OTDBillingReport"
						],
						"outputs": [],
						"execution_count": 49
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"otdbillingreport.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/RPT/otdbillingreport.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the rptparcellevel LMS Table\r\n",
							"rptparcellevel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/RPT/rptparcellevel.parquet', format='parquet')\r\n",
							"rptparcellevel.createOrReplaceTempView(\"rptparcellevel\")\r\n",
							"\r\n",
							"#Create DataFrame for the stparcel LMS Table\r\n",
							"stparcel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stparcel.parquet', format='parquet')\r\n",
							"stparcel.createOrReplaceTempView(\"stparcel\")\r\n",
							"\r\n",
							"#Create DataFrame for the stdeliverypickupcustomer LMS Table\r\n",
							"stdeliverypickupcustomer = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stdeliverypickupcustomer.parquet', format='parquet')\r\n",
							"stdeliverypickupcustomer.createOrReplaceTempView(\"stdeliverypickupcustomer\")\r\n",
							"\r\n",
							"#Create DataFrame for the stwaybill LMS Table\r\n",
							"stwaybill = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stwaybill.parquet', format='parquet')\r\n",
							"stwaybill.createOrReplaceTempView(\"stwaybill\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stlocation LMS Table\r\n",
							"stlocation = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stlocation.parquet', format='parquet')\r\n",
							"stlocation.createOrReplaceTempView(\"stlocation\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stlocation LMS Table\r\n",
							"stlocation = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stlocation.parquet', format='parquet')\r\n",
							"stlocation.createOrReplaceTempView(\"stlocation\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbobill_parceltype LMS Table\r\n",
							"dbobill_parceltype = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS//dbobill_parceltype.parquet', format='parquet')\r\n",
							"dbobill_parceltype.createOrReplaceTempView(\"dbobill_parceltype\")\r\n",
							"\r\n",
							"# #Create DataFrame for the storder LMS Table\r\n",
							"storder = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/storder.parquet', format='parquet')\r\n",
							"storder.createOrReplaceTempView(\"storder\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 50
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Reports/LMS/113_RptParcelLevel_EcomBillingReport"
						],
						"outputs": [],
						"execution_count": 51
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"ecombillingreport.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/RPT/ecombillingreport.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the rptparcellevel LMS Table\r\n",
							"rptparcellevel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/RPT/rptparcellevel.parquet', format='parquet')\r\n",
							"rptparcellevel.createOrReplaceTempView(\"rptparcellevel\")\r\n",
							"\r\n",
							"#Create DataFrame for the stconsignment LMS Table\r\n",
							"stconsignment = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stconsignment.parquet', format='parquet')\r\n",
							"stconsignment.createOrReplaceTempView(\"stconsignment\")\r\n",
							"\r\n",
							"#Create DataFrame for the stparcel LMS Table\r\n",
							"stparcel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stparcel.parquet', format='parquet')\r\n",
							"stparcel.createOrReplaceTempView(\"stparcel\")\r\n",
							"\r\n",
							"#Create DataFrame for the sstparcelunion LMS Table\r\n",
							"sstparcelunion = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Semi Structured Data/OPS/sstparcelunion.parquet', format='parquet')\r\n",
							"sstparcelunion.createOrReplaceTempView(\"sstparcelunion\")\r\n",
							"\r\n",
							"#Create DataFrame for the stlmstrack LMS Table\r\n",
							"stlmstrack = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stlmstrack.parquet', format='parquet')\r\n",
							"stlmstrack.createOrReplaceTempView(\"stlmstrack\")"
						],
						"outputs": [],
						"execution_count": 52
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Reports/LMS/114_RptParcelLevel_MrPIBTReport"
						],
						"outputs": [],
						"execution_count": 53
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"mrpibtreport.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/RPT/mrpibtreport.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**<mark>DETAILED REPORTS TMS</mark>**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the rpttriplevel LMS Table\r\n",
							"rpttriplevel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/RPT/rpttriplevel.parquet', format='parquet')\r\n",
							"rpttriplevel.createOrReplaceTempView(\"rpttriplevel\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 54
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Reports/TMS/301_RptTripLevel_linehaulleadtimereport"
						],
						"outputs": [],
						"execution_count": 55
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"linehaulleadtimereport.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/RPT/linehaulleadtimereport.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/00_STSAP_RunOrder')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Finance"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "TESTSparkPoolL",
					"type": "BigDataPoolReference"
				},
				"targetSparkConfiguration": {
					"referenceName": "SynapseApacheSparkConfigv1",
					"type": "SparkConfigurationReference"
				},
				"sessionProperties": {
					"driverMemory": "112g",
					"driverCores": 16,
					"executorMemory": "112g",
					"executorCores": 16,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "07e582c1-7011-421e-b510-7d42efd0d938"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse_Prod/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace-prod/bigDataPools/TESTSparkPoolL",
						"name": "TESTSparkPoolL",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/TESTSparkPoolL",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 16,
						"memory": 112
					},
					"sessionKeepAliveTimeout": 30,
					"targetSparkConfiguration": "SynapseApacheSparkConfigv1"
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"if 'prod' in Environment:\r\n",
							"    StorageAccount = 'citylogisticsstorageprod'\r\n",
							"    StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"    StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"    StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"else:\r\n",
							"    StorageAccount = 'citylogisticsstorage'\r\n",
							"    StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"    StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"    StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import os\r\n",
							"import pandas as pd\r\n",
							"import numpy as np\r\n",
							"from pyspark.sql.types import StructType, StructField, DoubleType, StringType, DecimalType\r\n",
							"from pyspark.sql.functions import col, to_timestamp"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#COA\r\n",
							"#Create DataFrame for the dboOACT SAP Table\r\n",
							"dboOACT = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dbooact.parquet', format='parquet')\r\n",
							"dboOACT.createOrReplaceTempView(\"dboOACT\")"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Finance/01_STChartOfAccounts"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stchartofaccounts.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stchartofaccounts.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Item\r\n",
							"# #Create DataFrame for the dboOITM SAP Table \r\n",
							"dboOITM = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dbooitm.parquet', format='parquet')\r\n",
							"dboOITM.createOrReplaceTempView(\"dboOITM\")\r\n",
							"\r\n",
							"#Item Group\r\n",
							"# #Create DataFrame for the dboOITB SAP Table\r\n",
							"dboOITB = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dbooitb.parquet', format='parquet')\r\n",
							"dboOITB.createOrReplaceTempView(\"dboOITB\")"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Finance/02_STItems"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stitems.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stitems.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Business Partner\r\n",
							"#Create DataFrame for the dboOCRD SAP Table\r\n",
							"dboOCRD = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dboocrd.parquet', format='parquet')\r\n",
							"dboOCRD.createOrReplaceTempView(\"dboOCRD\")"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Finance/03_STBusinessPartners"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stbusinnesspartner.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stbusinnesspartner.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Sales Invoice Header\r\n",
							"#Create DataFrame for the dboOINV SAP Table\r\n",
							"dboOINV = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dbooinv.parquet', format='parquet')\r\n",
							"dboOINV.createOrReplaceTempView(\"dboOINV\")\r\n",
							"\r\n",
							"#Sales Invoice Line\r\n",
							"#Create DataFrame for the dboINV1 SAP Table\r\n",
							"dboINV1 = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dboinv1.parquet', format='parquet')\r\n",
							"dboINV1.createOrReplaceTempView(\"dboINV1\")"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Finance/04_STSalesInvoices"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stsalesinvoice.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stsalesinvoice.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Stocck Revaluation Line\r\n",
							"#Create DataFrame for the dboOMRV SAP Table\r\n",
							"dboOMRV = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dboomrv.parquet', format='parquet')\r\n",
							"dboOMRV.createOrReplaceTempView(\"dboOMRV\")\r\n",
							"\r\n",
							"#Stocck Revaluation Line 1\r\n",
							"#Create DataFrame for the dboMRV1 SAP Table\r\n",
							"dboMRV1 = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dbomrv1.parquet', format='parquet')\r\n",
							"dboMRV1.createOrReplaceTempView(\"dboMRV1\")\r\n",
							"\r\n",
							"#Stocck Revaluation Line 2\r\n",
							"#Create DataFrame for the dboMRV2 SAP Table\r\n",
							"dboMRV2 = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dbomrv2.parquet', format='parquet')\r\n",
							"dboMRV2.createOrReplaceTempView(\"dboMRV2\")"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Finance/05_STStockRevaluations"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"ststockrevaluations.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/ststockrevaluations.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Stock\r\n",
							"#Create DataFrame for the dboOINM \r\n",
							"dboOINM = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dbooinm.parquet', format='parquet')\r\n",
							"dboOINM.createOrReplaceTempView(\"dboOINM\")\r\n",
							"\r\n",
							"#Goods Issue Header\r\n",
							"#Create DataFrame for the dboOIGE \r\n",
							"dboOIGE = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dbooige.parquet', format='parquet')\r\n",
							"dboOIGE.createOrReplaceTempView(\"dboOIGE\")\r\n",
							"\r\n",
							"#Goods Issue Line\r\n",
							"#Create DataFrame for the dboIGE1 SAP LINE Table\r\n",
							"dboIGE1 = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dboige1.parquet', format='parquet')\r\n",
							"dboIGE1.createOrReplaceTempView(\"dboIGE1\")\r\n",
							"\r\n",
							"#Goods Receipt Header\r\n",
							"#Create DataFrame for the dboOIGN \r\n",
							"dboOIGN = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dbooign.parquet', format='parquet')\r\n",
							"dboOIGN.createOrReplaceTempView(\"dboOIGN\")\r\n",
							"\r\n",
							"#Goods Receipt Line\r\n",
							"#Create DataFrame for the dboIGN1 SAP LINE Table\r\n",
							"dboIGN1 = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dboign1.parquet', format='parquet')\r\n",
							"dboIGN1.createOrReplaceTempView(\"dboIGN1\")"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Finance/06_STStockTransactions"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"ststocktransactions.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/ststocktransactions.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Sales Credit Header\r\n",
							"#Create DataFrame for the dboORIN \r\n",
							"dboORIN = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dboorin.parquet', format='parquet')\r\n",
							"dboORIN.createOrReplaceTempView(\"dboORIN\")\r\n",
							"\r\n",
							"#Sales Credit Line\r\n",
							"#Create DataFrame for the dboRIN1 SAP LINE Table\r\n",
							"dboRIN1 = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dborin1.parquet', format='parquet')\r\n",
							"dboRIN1.createOrReplaceTempView(\"dboRIN1\")\r\n",
							"\r\n",
							"#Purchase Invoice Header\r\n",
							"#Create DataFrame for the dboOPCH \r\n",
							"dboOPCH = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dboopch.parquet', format='parquet')\r\n",
							"dboOPCH.createOrReplaceTempView(\"dboOPCH\")\r\n",
							"\r\n",
							"#Purchase Invoice Line\r\n",
							"#Create DataFrame for the dboPCH1 SAP LINE Table\r\n",
							"dboPCH1 = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dbopch1.parquet', format='parquet')\r\n",
							"dboPCH1.createOrReplaceTempView(\"dboPCH1\")\r\n",
							"\r\n",
							"#Purchase Credit Header\r\n",
							"#Create DataFrame for the dboORPC \r\n",
							"dboORPC = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dboorpc.parquet', format='parquet')\r\n",
							"dboORPC.createOrReplaceTempView(\"dboORPC\")\r\n",
							"\r\n",
							"#Purchase Credit Line\r\n",
							"#Create DataFrame for the dboRPC1 SAP LINE Table\r\n",
							"dboRPC1 = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dborpc1.parquet', format='parquet')\r\n",
							"dboRPC1.createOrReplaceTempView(\"dboRPC1\")\r\n",
							"\r\n",
							"#Goods Receipt Note Header\r\n",
							"#Create DataFrame for the dboOPDN \r\n",
							"dboOPDN = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dboopdn.parquet', format='parquet')\r\n",
							"dboOPDN.createOrReplaceTempView(\"dboOPDN\")\r\n",
							"\r\n",
							"#Goods Receipt Note Line\r\n",
							"#Create DataFrame for the dboPDN1 SAP LINE Table\r\n",
							"dboPDN1 = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dbopdn1.parquet', format='parquet')\r\n",
							"dboPDN1.createOrReplaceTempView(\"dboPDN1\")\r\n",
							"\r\n",
							"#Goods Return Header\r\n",
							"#Create DataFrame for the dboORPD \r\n",
							"dboORPD = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dboorpd.parquet', format='parquet')\r\n",
							"dboORPD.createOrReplaceTempView(\"dboORPD\")\r\n",
							"\r\n",
							"#Goods Return Line\r\n",
							"#Create DataFrame for the dboRPD1 SAP LINE Table\r\n",
							"dboRPD1 = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dborpd1.parquet', format='parquet')\r\n",
							"dboRPD1.createOrReplaceTempView(\"dboRPD1\")\r\n",
							"\r\n",
							"#Goods Issue Header\r\n",
							"#Create DataFrame for the dboOIGE \r\n",
							"dboOIGE = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dbooige.parquet', format='parquet')\r\n",
							"dboOIGE.createOrReplaceTempView(\"dboOIGE\")\r\n",
							"\r\n",
							"#Goods Issue Line\r\n",
							"#Create DataFrame for the dboIGE1 SAP LINE Table\r\n",
							"dboIGE1 = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dboige1.parquet', format='parquet')\r\n",
							"dboIGE1.createOrReplaceTempView(\"dboIGE1\")\r\n",
							"\r\n",
							"#Purchase Orders Header\r\n",
							"#Create DataFrame for the dboOPOR \r\n",
							"dboOPOR = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dboopor.parquet', format='parquet')\r\n",
							"dboOPOR.createOrReplaceTempView(\"dboOPOR\")\r\n",
							"\r\n",
							"#Purchase Orders Line\r\n",
							"#Create DataFrame for the dboPOR1 SAP LINE Table\r\n",
							"dboPOR1 = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dbopor1.parquet', format='parquet')\r\n",
							"dboPOR1.createOrReplaceTempView(\"dboPOR1\")"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Finance/07_STDocuments"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stdocuments.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stdocuments.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Journal Header\r\n",
							"#Create DataFrame for the dboOJDT SAP Table\r\n",
							"dboOJDT = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dboojdt.parquet', format='parquet')\r\n",
							"dboOJDT.createOrReplaceTempView(\"dboOJDT\")\r\n",
							"\r\n",
							"#Journal Line\r\n",
							"#Create DataFrame for the dboJDT1 SAP Table\r\n",
							"dboJDT1 = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dbojdt1.parquet', format='parquet')\r\n",
							"dboJDT1.createOrReplaceTempView(\"dboJDT1\")\r\n",
							"\r\n",
							"#Create DataFrame for the stchartofaccounts SAP Table\r\n",
							"stchartofaccounts = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/FINANCE/stchartofaccounts.parquet', format='parquet')\r\n",
							"stchartofaccounts.createOrReplaceTempView(\"stchartofaccounts\")\r\n",
							"\r\n",
							"#Create DataFrame for the stsalesinvoice SAP Table\r\n",
							"stsalesinvoice = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/FINANCE/stsalesinvoice.parquet', format='parquet')\r\n",
							"stsalesinvoice.createOrReplaceTempView(\"stsalesinvoice\")\r\n",
							"\r\n",
							"#Create DataFrame for the stitem SAP Table\r\n",
							"stitems = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/FINANCE/stitems.parquet', format='parquet')\r\n",
							"stitems.createOrReplaceTempView(\"stitem\")"
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Finance/08_STManualJournals"
						],
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stpurchasepricevariance.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stpurchasepricevariance.parquet', mode = \"overwrite\")\r\n",
							"stcostofgoods.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stcostofgoods.parquet', mode = \"overwrite\")\r\n",
							"stsalesinvoiceinventory.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stsalesinvoiceinventory.parquet', mode = \"overwrite\")\r\n",
							"stdiscountjournal.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stdiscountjournal.parquet', mode = \"overwrite\")\r\n",
							"stmanualjournal.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stmanualjournal.parquet', mode = \"overwrite\")\r\n",
							"stdirectposting.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stdirectposting.parquet', mode = \"overwrite\")\r\n",
							"stgeneraljournal.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stgeneraljournal.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\r\n",
							"#Create DataFrame for the stdocuments SAP Table\r\n",
							"stdocuments = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/FINANCE/stdocuments.parquet', format='parquet')\r\n",
							"stdocuments.createOrReplaceTempView(\"stdocuments\")\r\n",
							"\r\n",
							"######################################################################## UNION\r\n",
							"\r\n",
							"#Create DataFrame for the stmanualjournal SAP Table\r\n",
							"stmanualjournal = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/FINANCE/stmanualjournal.parquet', format='parquet')\r\n",
							"stmanualjournal.createOrReplaceTempView(\"stmanualjournal\")\r\n",
							"\r\n",
							"#Create DataFrame for the stdirectposting SAP Table\r\n",
							"stdirectposting = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/FINANCE/stdirectposting.parquet', format='parquet')\r\n",
							"stdirectposting.createOrReplaceTempView(\"stdirectposting\")\r\n",
							"\r\n",
							"######################################################################## UNION\r\n",
							"\r\n",
							"#Create DataFrame for the stgeneraljournal SAP Table\r\n",
							"stgeneraljournal = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/FINANCE/stgeneraljournal.parquet', format='parquet')\r\n",
							"stgeneraljournal.createOrReplaceTempView(\"stgeneraljournal\")\r\n",
							"\r\n",
							"######################################################################## UNION\r\n",
							"\r\n",
							"#Create DataFrame for the ststocktransactions SAP Table\r\n",
							"ststocktransactions = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/FINANCE/ststocktransactions.parquet', format='parquet')\r\n",
							"ststocktransactions.createOrReplaceTempView(\"ststocktransactions\")\r\n",
							"\r\n",
							"#Create DataFrame for the stdiscountjournal SAP Table\r\n",
							"stdiscountjournal = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/FINANCE/stdiscountjournal.parquet', format='parquet')\r\n",
							"stdiscountjournal.createOrReplaceTempView(\"stdiscountjournal\")\r\n",
							"\r\n",
							"#Create DataFrame for the stsalesinvoiceinventory SAP Table\r\n",
							"stsalesinvoiceinventory = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/FINANCE/stsalesinvoiceinventory.parquet', format='parquet')\r\n",
							"stsalesinvoiceinventory.createOrReplaceTempView(\"stsalesinvoiceinventory\")\r\n",
							"\r\n",
							"#Create DataFrame for the ststockrevaluations SAP Table\r\n",
							"ststockrevaluations = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/FINANCE/ststockrevaluations.parquet', format='parquet')\r\n",
							"ststockrevaluations.createOrReplaceTempView(\"ststockrevaluations\")\r\n",
							"\r\n",
							"######################################################################## UNION\r\n",
							"\r\n",
							"#Create DataFrame for the stpurchasepricevariance SAP Table\r\n",
							"stpurchasepricevariance = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/FINANCE/stpurchasepricevariance.parquet', format='parquet')\r\n",
							"stpurchasepricevariance.createOrReplaceTempView(\"stpurchasepricevariance\")\r\n",
							"\r\n",
							"#Create DataFrame for the stcostofgoods SAP Table\r\n",
							"stcostofgoods = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/FINANCE/stcostofgoods.parquet', format='parquet')\r\n",
							"stcostofgoods.createOrReplaceTempView(\"stcostofgoods\")\r\n",
							"\r\n",
							"######################################################################## LEFT JOIN\r\n",
							"\r\n",
							"#Create DataFrame for the stchartofaccounts SAP Table\r\n",
							"stchartofaccounts = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/FINANCE/stchartofaccounts.parquet', format='parquet')\r\n",
							"stchartofaccounts.createOrReplaceTempView(\"stchartofaccounts\")\r\n",
							"\r\n",
							"#Create DataFrame for the stitems SAP Table\r\n",
							"stitems = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/FINANCE/stitems.parquet', format='parquet')\r\n",
							"stitems.createOrReplaceTempView(\"stitems\")"
						],
						"outputs": [],
						"execution_count": 27
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\r\n",
							"exceptions = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Finance and Operations Mapping Sources/SAP_Transactions_Exceptions.csv', format='csv'\r\n",
							", header=True\r\n",
							")"
						],
						"outputs": [],
						"execution_count": 28
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Finance/09_STTransactions"
						],
						"outputs": [],
						"execution_count": 29
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sttransactionsdetail.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/sttransactionsdetail.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 30
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sttransactionssummary.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/sttransactionssummary.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Finance/10_STFinanceDates"
						],
						"outputs": [],
						"execution_count": 32
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stfinancedates.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stfinancedates.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 33
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the sttransactionssummary SAP Table\r\n",
							"sttransactionssummary = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/FINANCE/sttransactionssummary.parquet', format='parquet')\r\n",
							"sttransactionssummary.createOrReplaceTempView(\"sttransactionssummary\")"
						],
						"outputs": [],
						"execution_count": 34
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Finance/11_STDocumentTypes"
						],
						"outputs": [],
						"execution_count": 35
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stdocumentcategories.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stdocumentcategories.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 36
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the stconsignment LMS Table\r\n",
							"stconsignment = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stconsignment.parquet', format='parquet')\r\n",
							"stconsignment.createOrReplaceTempView(\"stconsignment\")\r\n",
							"\r\n",
							"#Create DataFrame for the stlocation LMS Table\r\n",
							"stlocation = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stlocation.parquet', format='parquet')\r\n",
							"stlocation.createOrReplaceTempView(\"stlocation\")\r\n",
							"\r\n",
							"#Create DataFrame for the stzone LMS Table\r\n",
							"stzone = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stzone.parquet', format='parquet')\r\n",
							"stzone.createOrReplaceTempView(\"stzone\")\r\n",
							"\r\n",
							"#Create DataFrame for the stsroute LMS Table\r\n",
							"stsroute = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stsroute.parquet', format='parquet')\r\n",
							"stsroute.createOrReplaceTempView(\"stsroute\")\r\n",
							"\r\n",
							"#Create DataFrame for the stbillcustomer LMS Table\r\n",
							"stbillcustomer = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stbillcustomer.parquet', format='parquet')\r\n",
							"stbillcustomer.createOrReplaceTempView(\"stbillcustomer\")\r\n",
							"\r\n",
							"#Create DataFrame for the stdeliverypickupcustomer LMS Table\r\n",
							"stdeliverypickupcustomer = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stdeliverypickupcustomer.parquet', format='parquet')\r\n",
							"stdeliverypickupcustomer.createOrReplaceTempView(\"stdeliverypickupcustomer\")\r\n",
							"\r\n",
							"#Create DataFrame for the stlmstrack LMS Table\r\n",
							"stlmstrack = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stlmstrack.parquet', format='parquet')\r\n",
							"stlmstrack.createOrReplaceTempView(\"stlmstrack\")\r\n",
							"\r\n",
							"#Create DataFrame for the stloadchild LMS Table\r\n",
							"stloadchild = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stloadchild.parquet', format='parquet')\r\n",
							"stloadchild.createOrReplaceTempView(\"stloadchild\")\r\n",
							"\r\n",
							"#Create DataFrame for the storder LMS Table\r\n",
							"storder = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/storder.parquet', format='parquet')\r\n",
							"storder.createOrReplaceTempView(\"storder\")\r\n",
							"\r\n",
							"#Create DataFrame for the stparcel LMS Table\r\n",
							"stparcel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stparcel.parquet', format='parquet')\r\n",
							"stparcel.createOrReplaceTempView(\"stparcel\")\r\n",
							"\r\n",
							"#Create DataFrame for the sttrip LMS Table\r\n",
							"sttrip = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/sttrip.parquet', format='parquet')\r\n",
							"sttrip.createOrReplaceTempView(\"sttrip\")\r\n",
							"\r\n",
							"#Create DataFrame for the sttrip LMS Table\r\n",
							"sttrip = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/sttrip.parquet', format='parquet')\r\n",
							"sttrip.createOrReplaceTempView(\"sttrip\")\r\n",
							"\r\n",
							"#Create DataFrame for the stbooking LMS Table\r\n",
							"stbooking = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stbooking.parquet', format='parquet')\r\n",
							"stbooking.createOrReplaceTempView(\"stbooking\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbovehiclebasic MD Table\r\n",
							"dbovehiclebasic = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/CLMasterData/dbovehiclebasic.parquet', format='parquet')\r\n",
							"dbovehiclebasic.createOrReplaceTempView(\"dbovehiclebasic\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbopeoplebasic MD Table\r\n",
							"dbopeoplebasic = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/CLMasterData/dbopeoplebasic.parquet', format='parquet')\r\n",
							"dbopeoplebasic.createOrReplaceTempView(\"dbopeoplebasic\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbovehicle LMS Table\r\n",
							"dbovehicle = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbovehicle.parquet', format='parquet')\r\n",
							"dbovehicle.createOrReplaceTempView(\"dbovehicle\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbodriver LMS Table\r\n",
							"dbodriver = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodriver.parquet', format='parquet')\r\n",
							"dbodriver.createOrReplaceTempView(\"dbodriver\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbolhroutes LMS Table\r\n",
							"dbolhroutes = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbolh_routes.parquet', format='parquet')\r\n",
							"dbolhroutes.createOrReplaceTempView(\"dbolhroutes\")\r\n",
							"\r\n",
							"#Create DataFrame for the publicroute LMS Table\r\n",
							"publicroute = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publicroute.parquet', format='parquet')\r\n",
							"publicroute.createOrReplaceTempView(\"publicroute\")\r\n",
							"\r\n",
							"#Create DataFrame for the publicthirdparty TMS Table\r\n",
							"publicthirdparty = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publicthirdparty.parquet', format='parquet')\r\n",
							"publicthirdparty.createOrReplaceTempView(\"publicthirdparty\")\r\n",
							"\r\n",
							"#Create DataFrame for the publiccustomer TMS Table\r\n",
							"publiccustomer = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publiccustomer.parquet', format='parquet')\r\n",
							"publiccustomer.createOrReplaceTempView(\"publiccustomer\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 37
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Variables\r\n",
							"var_File_Path = 'abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Avis_Files/AvisUsage.xlsx'\r\n",
							"var_File_Page = \"AvisUsage\"\r\n",
							"\r\n",
							"# Processing\r\n",
							"excel_file = pd.ExcelFile(var_File_Path)\r\n",
							"excel_file_page = pd.read_excel(excel_file, var_File_Page,  engine='openpyxl')"
						],
						"outputs": [],
						"execution_count": 38
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Finance/12_STActivity"
						],
						"outputs": [],
						"execution_count": 39
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stkmsandlts.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stkmsandlts.parquet', mode = \"overwrite\")\r\n",
							"sttmsactivitydetail.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/sttmsactivitydetail.parquet', mode = \"overwrite\")\r\n",
							"sttmsactivitysummary.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/sttmsactivitysummary.parquet', mode = \"overwrite\")\r\n",
							"stactivitydetail.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stactivitydetail.parquet', mode = \"overwrite\")\r\n",
							"stactivitysummary.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stactivitysummary.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 40
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\r\n",
							"stdepotmap_tmp = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Finance and Operations Mapping Sources/SAP_DepotLocation_Map.csv', format='csv'\r\n",
							", header=True\r\n",
							")"
						],
						"outputs": [],
						"execution_count": 41
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Finance/13_STDepotMap"
						],
						"outputs": [],
						"execution_count": 42
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stdepotmap.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stdepotmap.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 43
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the prpclockinghoursdetail CLMasterData Table\r\n",
							"prpclockinghoursdetail = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/CLMasterData/prpclockinghoursdetail.parquet', format='parquet')\r\n",
							"prpclockinghoursdetail.createOrReplaceTempView(\"prpclockinghoursdetail\")"
						],
						"outputs": [],
						"execution_count": 44
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Finance/14_STPRPClocking"
						],
						"outputs": [],
						"execution_count": 45
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stprpclockinghoursdetail.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stprpclockinghoursdetail.parquet', mode = \"overwrite\")\r\n",
							"stprpclockinghoursdaily.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stprpclockinghoursdaily.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 46
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"SUCCESS\")"
						],
						"outputs": [],
						"execution_count": 47
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/sparkConfigurations/SynapseApacheSparkConfigv1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/00_SorterRunOrder')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Ops/Sorter"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "3296ddc8-a3ef-4913-869a-41b120ccde84"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace/bigDataPools/DevSparkPool",
						"name": "DevSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/DevSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"if 'prod' in Environment:\r\n",
							"    StorageAccount = 'citylogisticsstorageprod'\r\n",
							"    StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"    StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"    StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"else:\r\n",
							"    StorageAccount = 'citylogisticsstorage'\r\n",
							"    StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"    StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"    StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 55
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import pandas as pd\r\n",
							"from pyspark.sql.types import DecimalType\r\n",
							"from pyspark.sql.functions import col"
						],
						"outputs": [],
						"execution_count": 56
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the dboplc_status LMS Table\r\n",
							"dboplc_status = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboplc_status.parquet', format='parquet')\r\n",
							"dboplc_status.createOrReplaceTempView(\"dboplc_status\")\r\n",
							"\r\n",
							"#Create DataFrame for the dboplc_scanpoint LMS Table\r\n",
							"dboplc_scanpoint = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboplc_scanpoint.parquet', format='parquet')\r\n",
							"dboplc_scanpoint.createOrReplaceTempView(\"dboplc_scanpoint\")\r\n",
							"\r\n",
							"#Create DataFrame for the dboplc_divertmapping LMS Table\r\n",
							"dboplc_divertmapping = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboplc_divertmapping.parquet', format='parquet')\r\n",
							"dboplc_divertmapping.createOrReplaceTempView(\"dboplc_divertmapping\")"
						],
						"outputs": [],
						"execution_count": 57
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\r\n",
							"sorterdivertsorting = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Finance and Operations Mapping Sources/Sorter_DivertSorting.csv', format='csv'\r\n",
							", header=True\r\n",
							")"
						],
						"outputs": [],
						"execution_count": 60
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Ops/Sorter/01_STSorterDimensions"
						],
						"outputs": [],
						"execution_count": 61
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stplcdivertsorting.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stplcdivertsorting.parquet', mode = \"overwrite\")\r\n",
							"stplcstatus.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stplcstatus.parquet', mode = \"overwrite\")\r\n",
							"stplcscanpoint.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stplcscanpoint.parquet', mode = \"overwrite\")\r\n",
							"stplcdivertmapping.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stplcdivertmapping.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 62
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"#Create DataFrame for the dboplc_lpnrouting_history_centurion LMS Table\r\n",
							"dboplc_lpnrouting_history_centurion = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboplc_lpnrouting_history_centurion.parquet', format='parquet')\r\n",
							"dboplc_lpnrouting_history_centurion.createOrReplaceTempView(\"dboplc_lpnrouting_history_centurion\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbocustomer LMS Table\r\n",
							"dbocustomer = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbocustomer.parquet', format='parquet')\r\n",
							"dbocustomer.createOrReplaceTempView(\"dbocustomer\")\r\n",
							"\r\n",
							"#Create DataFrame for the stparcel3myoy LMS Table\r\n",
							"stparcel3myoy = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stparcel3myoy.parquet', format='parquet')\r\n",
							"stparcel3myoy.createOrReplaceTempView(\"stparcel3myoy\")\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 48
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\r\n",
							"sorterchutes = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Finance and Operations Mapping Sources/Sorter_Chutes.csv', format='csv'\r\n",
							", header=True\r\n",
							")"
						],
						"outputs": [],
						"execution_count": 44
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\r\n",
							"sortertimestudy = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Finance and Operations Mapping Sources/Sorter_TimeStudy.csv', format='csv'\r\n",
							", header=True\r\n",
							")"
						],
						"outputs": [],
						"execution_count": 45
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\r\n",
							"sorterstaffallocation = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Finance and Operations Mapping Sources/Sorter_StaffAllocation.csv', format='csv'\r\n",
							", header=True\r\n",
							")"
						],
						"outputs": [],
						"execution_count": 46
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Ops/Sorter/02_STPlcLpnRouting"
						],
						"outputs": [],
						"execution_count": 51
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stplclpnrouting.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stplclpnrouting.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 52
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/01_301_RptTripLevel_linehaulleadtimereport')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Reports/TMS"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "dc7ef6b7-8efd-4220-b576-b784b594ac5e"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace/bigDataPools/ApacheSparkPool",
						"name": "ApacheSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/ApacheSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# #Create DataFrame for the rpttriplevel LMS Table\r\n",
							"# rpttriplevel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/RPT/rpttriplevel.parquet', format='parquet')\r\n",
							"# rpttriplevel.createOrReplaceTempView(\"rpttriplevel\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"linehaulleadtimereport\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"\r\n",
							"tripid\r\n",
							",thirdpartyid\r\n",
							",thirdpartyname\r\n",
							",bookingids\r\n",
							",loadids\r\n",
							",noofparcels\r\n",
							",weight\r\n",
							",chargeweight\r\n",
							",volweight\r\n",
							",volumiserweight\r\n",
							",(to_unix_timestamp(dispatchstartdate) - to_unix_timestamp(loadclosedate)) closeloadtostartdispatch\r\n",
							",(to_unix_timestamp(exitdepotgeofence) - to_unix_timestamp(loadclosedate)) closeloadtoexitgeofence\r\n",
							",effectiveloaddurationminutes\r\n",
							",effectiveoffloaddurationminutes\r\n",
							"\r\n",
							"\r\n",
							"FROM rpttriplevel \r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"linehaulleadtimereport = spark.sql(\"SELECT * FROM linehaulleadtimereport\")"
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# linehaulleadtimereport.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/RPT/linehaulleadtimereport.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 25
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/01_DIST_STDimensions')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Ops/Distribution"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "646c2fff-6626-4de3-aa16-368ea1ee36bb"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace/bigDataPools/DevSparkPool",
						"name": "DevSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/DevSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"# #create dataframe for the publicdispatchsegment orv table\r\n",
							"# publicdispatchsegment = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicdispatchsegment.parquet', format='parquet')\r\n",
							"# publicdispatchsegment.createOrReplaceTempView(\"publicdispatchsegment\")\r\n",
							"\r\n",
							"# #create dataframe for the publicorvuser orv table\r\n",
							"# publicorvuser = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicorvuser.parquet', format='parquet')\r\n",
							"# publicorvuser.createOrReplaceTempView(\"publicorvuser\")\r\n",
							"\r\n",
							"# #create dataframe for the publicvehicle orv table\r\n",
							"# publicvehicle = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicvehicle.parquet', format='parquet')\r\n",
							"# publicvehicle.createOrReplaceTempView(\"publicvehicle\")\r\n",
							"\r\n",
							"# #create dataframe for the publicdepot orv table\r\n",
							"# publicdepot = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicdepo.parquet', format='parquet')\r\n",
							"# publicdepot.createOrReplaceTempView(\"publicdepot\")\r\n",
							"\r\n",
							"# #create dataframe for the publiccustomer orv table\r\n",
							"# publiccustomer = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publiccustomer.parquet', format='parquet')\r\n",
							"# publiccustomer.createOrReplaceTempView(\"publiccustomer\")\r\n",
							"\r\n",
							"# #create dataframe for the dbocustomer lms table\r\n",
							"# dbocustomer = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbocustomer.parquet', format='parquet')\r\n",
							"# dbocustomer.createOrReplaceTempView(\"dbocustomer\")\r\n",
							"\r\n",
							"# #create dataframe for the dbozone lms table\r\n",
							"# dbozone = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbozone.parquet', format='parquet')\r\n",
							"# dbozone.createOrReplaceTempView(\"dbozone\")\r\n",
							"\r\n",
							"# #create dataframe for the dbosroute lms table\r\n",
							"# dbosroute = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbosroute.parquet', format='parquet')\r\n",
							"# dbosroute.createOrReplaceTempView(\"dbosroute\")\r\n",
							"\r\n",
							"# #create dataframe for the dbobillzoneroute lms table\r\n",
							"# dbobillzoneroute = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbobill_zoneroute.parquet', format='parquet')\r\n",
							"# dbobillzoneroute.createOrReplaceTempView(\"dbobillzoneroute\")\r\n",
							"\r\n",
							"# #create dataframe for the dbobillzone lms table\r\n",
							"# dbobillzone = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbobill_zone.parquet', format='parquet')\r\n",
							"# dbobillzone.createOrReplaceTempView(\"dbobillzone\")\r\n",
							"\r\n",
							"# #create dataframe for the dbobillbillcustomersr  lms table\r\n",
							"# dbobillbillcustomersr = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbobill_billcustomersr.parquet', format='parquet')\r\n",
							"# dbobillbillcustomersr.createOrReplaceTempView(\"dbobillbillcustomersr\")\r\n",
							"\r\n",
							"# #create dataframe for the dbobillcustomergroups  lms table\r\n",
							"# dbobillcustomergroups  = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbobill_customergroups.parquet', format='parquet')\r\n",
							"# dbobillcustomergroups.createOrReplaceTempView(\"dbobillcustomergroups\")\r\n",
							"\r\n",
							"# #create dataframe for the dbobillcustomergrouptypes  lms table\r\n",
							"# dbobillcustomergrouptypes  = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbobill_customergrouptypes.parquet', format='parquet')\r\n",
							"# dbobillcustomergrouptypes.createOrReplaceTempView(\"dbobillcustomergrouptypes\")\r\n",
							"\r\n",
							"# #create dataframe for the dbolocation  lms table\r\n",
							"# dbolocation  = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbolocation.parquet', format='parquet')\r\n",
							"# dbolocation.createOrReplaceTempView(\"dbolocation\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbodepot LMS Table\r\n",
							"# dbodepot = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/CLMasterData/dbodepot.parquet', format='parquet')\r\n",
							"# dbodepot.createOrReplaceTempView(\"dbodepot\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbovehicle LMS Table\r\n",
							"# dbovehicle = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbovehicle.parquet', format='parquet')\r\n",
							"# dbovehicle.createOrReplaceTempView(\"dbovehicle\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbodb_reasongroup LMS Table\r\n",
							"# dbodb_reasongroup = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodb_reasongroup.parquet', format='parquet')\r\n",
							"# dbodb_reasongroup.createOrReplaceTempView(\"dbodb_reasongroup\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbodb_inservicereason LMS Table\r\n",
							"# dbodb_inservicereason = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodb_inservicereason.parquet', format='parquet')\r\n",
							"# dbodb_inservicereason.createOrReplaceTempView(\"dbodb_inservicereason\")\r\n",
							"\r\n",
							"# #create dataframe for the publicpod orv table\r\n",
							"# publicpod = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicpod.parquet', format='parquet')\r\n",
							"# publicpod.createOrReplaceTempView(\"publicpod\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbodb_reasondetail LMS Table\r\n",
							"# dbodb_reasondetail = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodb_reasondetail.parquet', format='parquet')\r\n",
							"# dbodb_reasondetail.createOrReplaceTempView(\"dbodb_reasondetail\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbodb_delayreason LMS Table\r\n",
							"# dbodb_delayreason = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodb_delayreason.parquet', format='parquet')\r\n",
							"# dbodb_delayreason.createOrReplaceTempView(\"dbodb_delayreason\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbodb_notdbreason LMS Table\r\n",
							"# dbodb_notdbreason = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodb_notdbreason.parquet', format='parquet')\r\n",
							"# dbodb_notdbreason.createOrReplaceTempView(\"dbodb_notdbreason\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbodb_deliverystatus LMS Table\r\n",
							"# dbodb_deliverystatus = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodb_deliverystatus.parquet', format='parquet')\r\n",
							"# dbodb_deliverystatus.createOrReplaceTempView(\"dbodb_deliverystatus\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbocustacc LMS Table\r\n",
							"# dbocustacc = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbocustacc.parquet', format='parquet')\r\n",
							"# dbocustacc.createOrReplaceTempView(\"dbocustacc\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dborouteratetype LMS Table\r\n",
							"# dborouteratetype = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dborouteratetype.parquet', format='parquet')\r\n",
							"# dborouteratetype.createOrReplaceTempView(\"dborouteratetype\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbodriver LMS Table\r\n",
							"# dbodriver = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodriver.parquet', format='parquet')\r\n",
							"# dbodriver.createOrReplaceTempView(\"dbodriver\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publicorvuser LMS Table\r\n",
							"# publicorvuser = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicorvuser.parquet', format='parquet')\r\n",
							"# publicorvuser.createOrReplaceTempView(\"publicorvuser\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbodamagedparcelauditfluteprofiles LMS Table\r\n",
							"# dbodamagedparcelauditfluteprofiles = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodamagedparcelauditfluteprofiles.parquet', format='parquet')\r\n",
							"# dbodamagedparcelauditfluteprofiles.createOrReplaceTempView(\"dbodamagedparcelauditfluteprofiles\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbodamagedparcelaudittapetype LMS Table\r\n",
							"# dbodamagedparcelaudittapetype = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodamagedparcelaudittapetype.parquet', format='parquet')\r\n",
							"# dbodamagedparcelaudittapetype.createOrReplaceTempView(\"dbodamagedparcelaudittapetype\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbodamagedparcelaudittype LMS Table\r\n",
							"# dbodamagedparcelaudittype = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodamagedparcelaudittype.parquet', format='parquet')\r\n",
							"# dbodamagedparcelaudittype.createOrReplaceTempView(\"dbodamagedparcelaudittype\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stdamagedparcelauditfluteprofiles\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"d.id as lms_damagedparcelauditfluteprofiles_id\r\n",
							",d.description as lms_damagedparcelauditfluteprofiles_description\r\n",
							",d.startrange as lms_damagedparcelauditfluteprofiles_startrange\r\n",
							",d.endrange as lms_damagedparcelauditfluteprofiles_endrange\r\n",
							"FROM\r\n",
							"dbodamagedparcelauditfluteprofiles d"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stdamagedparcelauditfluteprofiles = spark.sql(\"SELECT * FROM stdamagedparcelauditfluteprofiles\")"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stdamagedparcelauditfluteprofiles.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stdamagedparcelauditfluteprofiles.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stdamagedparcelaudittapetype\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"d.id as lms_damagedparcelaudittapetype_id\r\n",
							",d.description as lms_damagedparcelaudittapetype_description\r\n",
							"FROM\r\n",
							"dbodamagedparcelaudittapetype d"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stdamagedparcelaudittapetype = spark.sql(\"SELECT * FROM stdamagedparcelaudittapetype\")"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stdamagedparcelaudittapetype.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stdamagedparcelaudittapetype.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stdamagedparcelaudittype\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"d.id as lms_damagedparcelaudittype_id\r\n",
							",d.description as lms_damagedparcelaudittype_description\r\n",
							",d.db_reasondetailid as lms_damagedparcelaudittype_dbreasondetailid\r\n",
							",d.cdate as lms_damagedparcelaudittype_cdate\r\n",
							"FROM\r\n",
							"dbodamagedparcelaudittype d"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stdamagedparcelaudittype = spark.sql(\"SELECT * FROM stdamagedparcelaudittype\")"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stdamagedparcelaudittype.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stdamagedparcelaudittype.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stdriver\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"d.id as lms_driver_id\r\n",
							",d.ianyflag as lms_driver_ianyflag\r\n",
							",d.billcustid as lms_driver_billcustid\r\n",
							",d.password as lms_driver_password\r\n",
							",d.userid as lms_driver_userid\r\n",
							",d.staffid as lms_driver_staffid\r\n",
							",d.branch as lms_driver_branch\r\n",
							",d.passportnum as lms_driver_passportnum\r\n",
							",d.passportcountry as lms_driver_passportcountry\r\n",
							",d.residence as lms_driver_residence\r\n",
							",d.dob as lms_driver_dob\r\n",
							",d.passportexpiry as lms_driver_passportexpiry\r\n",
							",d.activeflag as lms_driver_activeflag\r\n",
							",d.locid as lms_driver_locid\r\n",
							",d.lastupdate as lms_driver_lastupdate\r\n",
							",d.dateengaged as lms_driver_dateengaged\r\n",
							",d.terminationdate as lms_driver_terminationdate\r\n",
							",d.empid as lms_driver_empid\r\n",
							",d.dname as lms_driver_dname\r\n",
							",d.fname as lms_driver_fname\r\n",
							",d.surname as lms_driver_surname\r\n",
							",d.knownasname as lms_driver_knownasname\r\n",
							",d.depotcode as lms_driver_depotcode\r\n",
							",d.jobcategory as lms_driver_jobcategory\r\n",
							",d.jobtitle as lms_driver_jobtitle\r\n",
							",d.employeecode as lms_driver_employeecode\r\n",
							",d.cellno as lms_driver_cellno\r\n",
							",d.gender as lms_driver_gender\r\n",
							",d.employeestatus as lms_driver_employeestatus\r\n",
							",d.nationnality as lms_driver_nationnality\r\n",
							",d.idno as lms_driver_idno\r\n",
							",d.statusflag as lms_driver_statusflag\r\n",
							",d.idpassportno as lms_driver_idpassportno\r\n",
							",d.ownerid as lms_driver_ownerid\r\n",
							",d.islinehauldriver as lms_driver_islinehauldriver\r\n",
							",d.uid as lms_driver_uid\r\n",
							",u.id as orv_publicorvuser_id\r\n",
							"FROM\r\n",
							"dbodriver d\r\n",
							"LEFT JOIN publicorvuser u on d.id = u.lmsid"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stdriver = spark.sql(\"SELECT * FROM stdriver\")"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stdriver.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stdriver.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"strouteratetype\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"\tid as lms_routeratetype_id\r\n",
							"\t,minkg as lms_routeratetype_minkg\r\n",
							"\t,minrate as lms_routeratetype_minrate\r\n",
							"\t,addrate as lms_routeratetype_addrate\r\n",
							"\t,altaddrate as lms_routeratetype_altaddrate\r\n",
							"\t,docfees as lms_routeratetype_docfees\r\n",
							"\t,volumetric as lms_routeratetype_volumetric\r\n",
							"\t,description as lms_routeratetype_description\r\n",
							"\t,fuellevy as lms_routeratetype_fuellevy\r\n",
							"\t,custaccid as lms_routeratetype_custaccid\r\n",
							"\t,billcustid as lms_routeratetype_billcustid\r\n",
							"\t,servicetime as lms_routeratetype_servicetime\r\n",
							"\t,servicedays as lms_routeratetype_servicedays\r\n",
							"\t,broutemasterid as lms_routeratetype_broutemasterid\r\n",
							"\t,cutofftime as lms_routeratetype_cutofftime\r\n",
							"\r\n",
							"FROM dborouteratetype"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"strouteratetype = spark.sql(\"SELECT * FROM strouteratetype\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# strouteratetype.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/strouteratetype.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stcustacc\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    id as lms_custacc_id,\r\n",
							"\tdescription as lms_custacc_description,\r\n",
							"\tbilltype as lms_custacc_billtype,\r\n",
							"\tcustomerid as lms_custacc_customerid,\r\n",
							"\tcref as lms_custacc_cref,\r\n",
							"\tactive as lms_custacc_active\r\n",
							"FROM dbocustacc"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stcustacc = spark.sql(\"SELECT * FROM stcustacc\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stcustacc.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stcustacc.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stpod\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    p.id as orv_pod_id\r\n",
							"    ,p.did as orv_pod_did\r\n",
							"\t,p.name as orv_pod_name\r\n",
							"\t,to_timestamp(p.poddate) as orv_pod_poddate\r\n",
							"\t,p.lat as orv_pod_lat\r\n",
							"\t,p.lng as orv_pod_lng\r\n",
							"\t,p.customerrating as orv_pod_customerrating\r\n",
							"\t,p.customerfeedback as orv_pod_customerfeedback\r\n",
							"FROM publicpod p"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stpod = spark.sql(\"SELECT * FROM stpod\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stpod.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stpod.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stinservicereasons\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    isr.ID as lms_inservicereason_id\r\n",
							"\t,isr.ReasonGroupID as lms_inservicereason_reasongroupid\r\n",
							"\t,rg.Description as lms_reasongroup_description\r\n",
							"    ,isr.Reason as lms_inservicereason_reason\r\n",
							"    ,isr.Valid as lms_inservicereason_valid\r\n",
							"FROM dbodb_inservicereason isr\r\n",
							"LEFT JOIN dbodb_reasongroup rg on rg.id = isr.ReasonGroupID;"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stinservicereasons = spark.sql(\"SELECT * FROM stinservicereasons\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stinservicereasons.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stinservicereasons.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stdispatchsegment\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"\r\n",
							"    d.id as orv_dispatchsegment_id\r\n",
							"    ,d.fromdelid as orv_dispatchsegment_fromdelid\r\n",
							"    ,d.todelid as orv_dispatchsegment_todelid\r\n",
							"    ,d.distance as orv_dispatchsegment_distance\r\n",
							"    ,d.duration as orv_dispatchsegment_duration\r\n",
							"    ,d.encodedpolyline as orv_dispatchsegment_encodedpolyline\r\n",
							"    ,d.did as orv_dispatchsegment_did\r\n",
							"    ,d.tollcost as orv_dispatchsegment_tollcost\r\n",
							"    ,d.fuelcost as orv_dispatchsegment_fuelcost\r\n",
							"    ,d.inscost as orv_dispatchsegment_inscost\r\n",
							"    ,d.labourcost as orv_dispatchsegment_labourcost\r\n",
							"    ,d.maintcost as orv_dispatchsegment_maintcost\r\n",
							"    ,d.cpicost as orv_dispatchsegment_cpicost\r\n",
							"    ,d.vehicleclass as orv_dispatchsegment_vehicleclass\r\n",
							"    ,d.fromlat as orv_dispatchsegment_fromlat\r\n",
							"    ,d.fromlng as orv_dispatchsegment_fromlng\r\n",
							"    ,d.tolat as orv_dispatchsegment_tolat\r\n",
							"    ,d.tolng as orv_dispatchsegment_tolng\r\n",
							"    ,to_timestamp(d.departuredate) as orv_dispatchsegment_departuredate\r\n",
							"    ,to_timestamp(d.arrivaldate) as orv_dispatchsegment_arrivaldate\r\n",
							"    ,to_timestamp(d.debriefeddate) as orv_dispatchsegment_debriefeddate\r\n",
							"    ,d.optimized as orv_dispatchsegment_optimized\r\n",
							"    ,d.segmentorder as orv_dispatchsegment_segmentorder\r\n",
							"    ,d.customized as orv_dispatchsegment_customized\r\n",
							"    ,d.parcelct as orv_dispatchsegment_parcelct\r\n",
							"\r\n",
							"FROM publicdispatchsegment d\r\n",
							"    "
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stdispatchsegment = spark.sql(\"SELECT * FROM stdispatchsegment\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stdispatchsegment.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stdispatchsegment.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"storvuser\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    o.id as orv_orvuser_id\r\n",
							"    ,o.username as orv_orvuser_username\r\n",
							"    ,o.fname as orv_orvuser_fname\r\n",
							"    ,o.lname as orv_orvuser_lname\r\n",
							"    ,o.empcode as orv_orvuser_empcode\r\n",
							"    ,o.aka as orv_orvuser_aka\r\n",
							"    ,o.gender as orv_orvuser_gender\r\n",
							"    ,o.rsaid as orv_orvuser_rsaid\r\n",
							"    ,o.passportno as orv_orvuser_passportno\r\n",
							"    ,o.idpassportno as orv_orvuser_idpassportno\r\n",
							"    ,o.nationality as orv_orvuser_nationality\r\n",
							"    ,to_timestamp(o.termindate) as orv_orvuser_termindate\r\n",
							"    ,o.status as orv_orvuser_status\r\n",
							"    ,o.depot as orv_orvuser_depot\r\n",
							"    ,o.jobcategory as orv_orvuser_jobcategory\r\n",
							"    ,o.lmsid as orv_orvuser_lmsid\r\n",
							"    ,o.vipid as orv_orvuser_vipid\r\n",
							"    ,o.crewlmsid as orv_orvuser_crewlmsid\r\n",
							"    ,o.courierid as orv_orvuser_courierid\r\n",
							"\r\n",
							"    FROM publicorvuser o\r\n",
							"    \r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"storvuser = spark.sql(\"SELECT * FROM storvuser\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# storvuser.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/storvuser.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stvehicle\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    v.id as lms_vehicle_id\r\n",
							"    ,v.FleetNo as lms_vehicle_fleetno\r\n",
							"    ,v.RegNo as lms_vehicle_regno\r\n",
							"    ,v.UID as lms_vehicle_uid\r\n",
							"    ,v2.id as orv_vehicle_id\r\n",
							"    ,v2.tollclass as orv_vehicle_tollclass\r\n",
							"    ,v2.sapcode as orv_vehicle_sapcode\r\n",
							"    ,v2.lmsid as orv_vehicle_lmsid\r\n",
							"\r\n",
							"FROM dbovehicle v\r\n",
							"LEFT JOIN publicvehicle v2 on v2.lmsid = v.id\r\n",
							"\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stvehicle = spark.sql(\"SELECT * FROM stvehicle\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stvehicle.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stvehicle.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stdepotorv\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    d.id as orv_depot_id\r\n",
							"    ,d.name as orv_depot_name\r\n",
							"    ,d.depcode as orv_depot_depcode\r\n",
							"    ,d.address as orv_depot_address\r\n",
							"    ,d.suburb as orv_depot_suburb\r\n",
							"    ,d.city as orv_depot_city\r\n",
							"    ,d.zipcode as orv_depot_zipcode\r\n",
							"    ,d.region as orv_depot_region\r\n",
							"    ,d.lat as orv_depot_lat\r\n",
							"    ,d.lng as orv_depot_lng\r\n",
							"    ,d.placeid as orv_depot_placeid\r\n",
							"    ,d.w3w as orv_depot_w3w\r\n",
							"    ,to_timestamp(d.insdate) as orv_depot_insdate\r\n",
							"    ,to_timestamp(d.update) as orv_depot_update\r\n",
							"    ,d.lmsid as orv_depot_lmsid\r\n",
							"    ,d.country as orv_depot_country\r\n",
							"    ,d.status as orv_depot_status\r\n",
							"\r\n",
							"FROM publicdepot d\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stdepotorv = spark.sql(\"SELECT * FROM stdepotorv\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stdepotorv.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stdepotorv.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stdepot\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"\r\n",
							"    d.id as md_depot_id\r\n",
							"    ,d.itemlabel as md_depot_itemlabel\r\n",
							"    ,d.itemvalue as md_depot_itemvalue\r\n",
							"    ,d.depotname as md_depot_depotname\r\n",
							"    ,d.parentdepotcode as md_depot_parentdepotcode\r\n",
							"    ,d.division as md_depot_division\r\n",
							"    ,d.address as md_depot_address\r\n",
							"    ,d.latitude as md_depot_latitude\r\n",
							"    ,d.longitude as md_depot_longitude\r\n",
							"    ,d.sapdepreciationaccountcode as md_depot_sapdepreciationaccountcode\r\n",
							"    ,d.mfrauthoriser as md_depot_mfrauthoriser\r\n",
							"    ,d.mfrapplicationflag as md_depot_mfrapplicationflag\r\n",
							"    ,d.activeflag as md_depot_activeflag\r\n",
							"    ,d.operationsmanager as md_depot_operationsmanager\r\n",
							"    ,d.branchmanager as md_depot_branchmanager\r\n",
							"    ,d.fleetmanager as md_depot_fleetmanager\r\n",
							"\r\n",
							"\r\n",
							"FROM dbodepot d\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stdepot = spark.sql(\"SELECT * FROM stdepot\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stdepot.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stdepot.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stbillcustomer \r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    lc.id as lms_customer_id\r\n",
							"    ,lc.ActiveFlag as lms_customer_activeflag\r\n",
							"    ,lc.Adres1 as lms_customer_adres1\r\n",
							"    ,lc.Adres2 as lms_customer_adres2\r\n",
							"    ,lc.Appointment as lms_customer_appointment\r\n",
							"    ,lc.AreaDesc as lms_customer_areadesc\r\n",
							"    ,lc.AutoEmailFlag as lms_customer_autoemailflag\r\n",
							"    ,lc.Bill_RouteID as lms_customer_bill_routeid\r\n",
							"    ,lc.Bill_ServiceTypeID as lms_customer_bill_servicetypeid\r\n",
							"    ,lc.Bill_ZoneRouteID as lms_customer_bill_zonerouteid\r\n",
							"    ,lc.brouteid as lms_customer_brouteid\r\n",
							"    ,lc.BType as lms_customer_btype\r\n",
							"    ,lc.ccDepot as lms_customer_ccdepot\r\n",
							"    ,lc.ccRoute as lms_customer_ccroute\r\n",
							"    ,lc.ChainStoreFlag as lms_customer_chainstoreflag\r\n",
							"    ,lc.CONSPerORD as lms_customer_consperord\r\n",
							"    ,lc.ContactPerson as lms_customer_contactperson\r\n",
							"    ,lc.CountryCode as lms_customer_countrycode\r\n",
							"    ,lc.CRef as lms_customer_cref\r\n",
							"    ,lc.CREF2 as lms_customer_cref2\r\n",
							"    ,lc.CRouteID as lms_customer_crouteid\r\n",
							"    ,lc.CType as lms_customer_ctype\r\n",
							"    ,lc.CustAccID as lms_customer_custaccid\r\n",
							"    ,lc.CustGroupID as lms_customer_custgroupid\r\n",
							"    ,lc.CustMainID as lms_customer_custmainid\r\n",
							"    ,lc.CustRateid as lms_customer_custrateid\r\n",
							"    ,lc.CustRouteGroupID as lms_customer_custroutegroupid\r\n",
							"    ,lc.CustServiceDays as lms_customer_custservicedays\r\n",
							"    ,lc.CustServiceTime as lms_customer_custservicetime\r\n",
							"    ,lc.DC as lms_customer_dc\r\n",
							"    ,lc.DefDelPCode as lms_customer_defdelpcode\r\n",
							"    ,lc.DeliveryDepotID as lms_customer_deliverydepotid\r\n",
							"    ,lc.DeliveryType as lms_customer_deliverytype\r\n",
							"    ,lc.DestinationLocationCode as lms_customer_destinationlocationcode\r\n",
							"    ,lc.DualBillCourier as lms_customer_dualbillcourier\r\n",
							"    ,lc.Email as lms_customer_email\r\n",
							"    ,lc.FaxNo as lms_customer_faxno\r\n",
							"    ,lc.FCType as lms_customer_fctype\r\n",
							"    ,lc.FLevy as lms_customer_flevy\r\n",
							"    ,lc.InCompleteOrderChkFlag as lms_customer_incompleteorderchkflag\r\n",
							"    ,lc.InsertDate as lms_customer_insertdate\r\n",
							"    ,lc.IsBillTo as lms_customer_isbillto\r\n",
							"    ,lc.isBooking as lms_customer_isbooking\r\n",
							"    ,lc.isHighVolume as lms_customer_ishighvolume\r\n",
							"    ,lc.IsManual as lms_customer_ismanual\r\n",
							"    ,lc.LastUpdate as lms_customer_lastupdate\r\n",
							"    ,lc.LocationTypeCode as lms_customer_locationtypecode\r\n",
							"    ,lc.MaxCartons as lms_customer_maxcartons\r\n",
							"    ,lc.Name as lms_customer_name\r\n",
							"    ,lc.NDDID as lms_customer_nddid\r\n",
							"    ,lc.OnHoldID as lms_customer_onholdid\r\n",
							"    ,lc.PCode as lms_customer_pcode\r\n",
							"    ,lc.PCodeID as lms_customer_pcodeid\r\n",
							"    ,lc.ServRouteID as lms_customer_servrouteid\r\n",
							"    ,lc.SRouteID as lms_customer_srouteid\r\n",
							"    ,lc.StoreID as lms_customer_storeid\r\n",
							"    ,lc.Suburb as lms_customer_suburb\r\n",
							"    ,lc.TelNo as lms_customer_telno\r\n",
							"    ,lc.Town as lms_customer_town\r\n",
							"    ,lc.VolFact as lms_customer_volfact\r\n",
							"    ,lc.WType as lms_customer_wtype\r\n",
							"    ,oc.id as orv_customer_id\r\n",
							"    ,oc.name as orv_customer_name\r\n",
							"    ,oc.contact as orv_customer_contact\r\n",
							"    ,oc.email as orv_customer_email\r\n",
							"    ,oc.mobileno as orv_customer_mobileno\r\n",
							"    ,oc.landline as orv_customer_landline\r\n",
							"    ,oc.ctype as orv_customer_ctype\r\n",
							"    ,oc.parent as orv_customer_parent\r\n",
							"    ,to_timestamp(oc.lastupdate) as orv_customer_lastupdate\r\n",
							"    ,oc.lmsid as orv_customer_lmsid\r\n",
							"    ,oc.srouteid as orv_customer_srouteid\r\n",
							"    ,oc.verified as orv_customer_verified\r\n",
							"    ,oc.defaultmaildomain as orv_customer_defaultmaildomain\r\n",
							"    ,oc.autoassignticketid as orv_customer_autoassignticketid\r\n",
							"    ,oc.sendinfodel as orv_customer_sendinfodel\r\n",
							"    ,oc.anonemailcustsupportemail as orv_customer_anonemailcustsupportemail\r\n",
							"    ,oc.anonemailcustsupporttel as orv_customer_anonemailcustsupporttel\r\n",
							"    ,oc.cref as orv_customer_cref\r\n",
							"    ,oc.division as orv_customer_division\r\n",
							"    ,oc.ecomm as orv_customer_ecomm\r\n",
							"    ,oc.preverifiedaddid as orv_customer_preverifiedaddid\r\n",
							"    ,oc.mallname as orv_customer_mallname\r\n",
							"    ,oc.mallid as orv_customer_mallid\r\n",
							"    ,oc.mallflag as orv_customer_mallflag\r\n",
							"    ,oc.malllmsid as orv_customer_malllmsid\r\n",
							"    ,oc.highvolume as orv_customer_highvolume\r\n",
							"    ,oc.custreference as orv_customer_custreference\r\n",
							"    ,oc.unverdate as orv_customer_unverdate\r\n",
							"    ,oc.verdate as orv_customer_verdate\r\n",
							"    ,oc.groupid as orv_customer_groupid\r\n",
							"    ,oc.postalcodecustgroupid as orv_customer_postalcodecustgroupid\r\n",
							"    ,oc.webhookauthcredential as orv_customer_webhookauthcredential\r\n",
							"    ,oc.mergedinto as orv_customer_mergedinto\r\n",
							"    ,oc.cusmergedate as orv_customer_cusmergedate\r\n",
							"    ,oc.customerref as orv_customer_customerref\r\n",
							"    ,oc.routemaint as orv_customer_routemaint\r\n",
							"    ,bcr.id as lms_customer_billcustid\r\n",
							"    ,bcr.billcustid as lms_customer_billcustbillcustid\r\n",
							"    ,bcr.incovalue as lms_customer_billcustincovalue\r\n",
							"    ,bcr.daterangelimit as lms_customer_billcustdaterangelimit\r\n",
							"    ,bcr.status as lms_customer_billcuststatus\r\n",
							"    ,bcr.note as lms_customer_billcustnote\r\n",
							"    ,bcr.custgroupid as lms_customer_billcustcustgroupid\r\n",
							"    ,bcr.activeflag as lms_customer_billcustactiveflag\r\n",
							"    ,bcr.routerateeffectivedate as lms_customer_billcustrouterateeffectivedate\r\n",
							"    ,bcr.ratesaddedflag as lms_customer_billcustratesaddedflag\r\n",
							"    ,bcr.ibtratesflag as lms_customer_billcustibtratesflag\r\n",
							"    ,bcr.slidingscaleflag as lms_customer_billcustslidingscaleflag\r\n",
							"    ,bcr.sapflag as lms_customer_billcustsapflag\r\n",
							"    ,bcr.sapcode as lms_customer_billcustsapcode\r\n",
							"    ,bcr.sapcustname as lms_customer_billcustsapcustname\r\n",
							"    ,bcr.averagepclrateflag as lms_customer_billcustaveragepclrateflag\r\n",
							"    ,bcr.averagepclkg as lms_customer_billcustaveragepclkg\r\n",
							"    ,bcr.billdatestart as lms_customer_billcustbilldatestart\r\n",
							"    ,bcr.itemcode as lms_customer_billcustitemcode\r\n",
							"    ,bcr.servicetypeflag as lms_customer_billcustservicetypeflag\r\n",
							"    ,bcr.returnreverserateflag as lms_customer_billcustreturnreverserateflag\r\n",
							"    ,bcr.minnoofpcls as lms_customer_billcustminnoofpcls\r\n",
							"    ,bcr.productcategoryid as lms_customer_billcustproductcategoryid\r\n",
							"    ,bcr.samedayconsolidation as lms_customer_billcustsamedayconsolidation\r\n",
							"    ,bcr.rategroupid as lms_customer_billcustrategroupid\r\n",
							"    ,bcr.risksurchargeflag as lms_customer_billcustrisksurchargeflag\r\n",
							"    ,bcr.applyavgvolweight as lms_customer_billcustapplyavgvolweight\r\n",
							"    ,bcr.avgvolweight as lms_customer_billcustavgvolweight\r\n",
							"    ,bcr.ecommcustomerflag as lms_customer_billcustecommcustomerflag\r\n",
							"    ,bcr.leadtimestructureid as lms_customer_billcustleadtimestructureid\r\n",
							"    ,bcr.recalcinprogress as lms_customer_billcustrecalcinprogress\r\n",
							"    ,bcr.recalcuserid as lms_customer_billcustrecalcuserid\r\n",
							"    ,bcr.needwebhook as lms_customer_billcustneedwebhook\r\n",
							"    ,bcr.routemaint as lms_customer_billcustroutemaint\r\n",
							"    ,bcr.lhdistrflag as lms_customer_billcustlhdistrflag\r\n",
							"    ,bcg.id as lms_customer_billcustgroupid\r\n",
							"    ,bcg.description as lms_customer_billcustgroupdescription\r\n",
							"    ,bcg.grouptypeid as lms_customer_billcustgroupgrouptypeid\r\n",
							"    ,bcgt.id as lms_customer_billcustgrouptypeid\r\n",
							"    ,bcgt.description as lms_customer_billcustgrouptypedescription\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"FROM dbocustomer lc\r\n",
							"LEFT JOIN publiccustomer oc on oc.lmsid = lc.id\r\n",
							"LEFT JOIN dbobillbillcustomersr bcr on bcr.billcustid = lc.id \r\n",
							"LEFT JOIN dbobillcustomergroups bcg  on bcg.id = bcr.custgroupid\r\n",
							"LEFT JOIN dbobillcustomergrouptypes bcgt on bcgt.id = bcg.grouptypeid\r\n",
							"\r\n",
							"WHERE lc.isbillto = 1"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stbillcustomer = spark.sql(\"SELECT * FROM stbillcustomer\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stbillcustomer.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stbillcustomer.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stdeliverypickupcustomer \r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    lc.id as lms_customer_id\r\n",
							"    ,lc.ActiveFlag as lms_customer_activeflag\r\n",
							"    ,lc.Adres1 as lms_customer_adres1\r\n",
							"    ,lc.Adres2 as lms_customer_adres2\r\n",
							"    ,lc.Appointment as lms_customer_appointment\r\n",
							"    ,lc.AreaDesc as lms_customer_areadesc\r\n",
							"    ,lc.AutoEmailFlag as lms_customer_autoemailflag\r\n",
							"    ,lc.Bill_RouteID as lms_customer_bill_routeid\r\n",
							"    ,lc.Bill_ServiceTypeID as lms_customer_bill_servicetypeid\r\n",
							"    ,lc.Bill_ZoneRouteID as lms_customer_bill_zonerouteid\r\n",
							"    ,lc.brouteid as lms_customer_brouteid\r\n",
							"    ,lc.BType as lms_customer_btype\r\n",
							"    ,lc.ccDepot as lms_customer_ccdepot\r\n",
							"    ,lc.ccRoute as lms_customer_ccroute\r\n",
							"    ,lc.ChainStoreFlag as lms_customer_chainstoreflag\r\n",
							"    ,lc.CONSPerORD as lms_customer_consperord\r\n",
							"    ,lc.ContactPerson as lms_customer_contactperson\r\n",
							"    ,lc.CountryCode as lms_customer_countrycode\r\n",
							"    ,lc.CRef as lms_customer_cref\r\n",
							"    ,lc.CREF2 as lms_customer_cref2\r\n",
							"    ,lc.CRouteID as lms_customer_crouteid\r\n",
							"    ,lc.CType as lms_customer_ctype\r\n",
							"    ,lc.CustAccID as lms_customer_custaccid\r\n",
							"    ,lc.CustGroupID as lms_customer_custgroupid\r\n",
							"    ,lc.CustMainID as lms_customer_custmainid\r\n",
							"    ,lc.CustRateid as lms_customer_custrateid\r\n",
							"    ,lc.CustRouteGroupID as lms_customer_custroutegroupid\r\n",
							"    ,lc.CustServiceDays as lms_customer_custservicedays\r\n",
							"    ,lc.CustServiceTime as lms_customer_custservicetime\r\n",
							"    ,lc.DC as lms_customer_dc\r\n",
							"    ,lc.DefDelPCode as lms_customer_defdelpcode\r\n",
							"    ,lc.DeliveryDepotID as lms_customer_deliverydepotid\r\n",
							"    ,lc.DeliveryType as lms_customer_deliverytype\r\n",
							"    ,lc.DestinationLocationCode as lms_customer_destinationlocationcode\r\n",
							"    ,lc.DualBillCourier as lms_customer_dualbillcourier\r\n",
							"    ,lc.Email as lms_customer_email\r\n",
							"    ,lc.FaxNo as lms_customer_faxno\r\n",
							"    ,lc.FCType as lms_customer_fctype\r\n",
							"    ,lc.FLevy as lms_customer_flevy\r\n",
							"    ,lc.InCompleteOrderChkFlag as lms_customer_incompleteorderchkflag\r\n",
							"    ,lc.InsertDate as lms_customer_insertdate\r\n",
							"    ,lc.IsBillTo as lms_customer_isbillto\r\n",
							"    ,lc.isBooking as lms_customer_isbooking\r\n",
							"    ,lc.isHighVolume as lms_customer_ishighvolume\r\n",
							"    ,lc.IsManual as lms_customer_ismanual\r\n",
							"    ,lc.LastUpdate as lms_customer_lastupdate\r\n",
							"    ,lc.LocationTypeCode as lms_customer_locationtypecode\r\n",
							"    ,lc.MaxCartons as lms_customer_maxcartons\r\n",
							"    ,lc.Name as lms_customer_name\r\n",
							"    ,lc.NDDID as lms_customer_nddid\r\n",
							"    ,lc.OnHoldID as lms_customer_onholdid\r\n",
							"    ,lc.PCode as lms_customer_pcode\r\n",
							"    ,lc.PCodeID as lms_customer_pcodeid\r\n",
							"    ,lc.ServRouteID as lms_customer_servrouteid\r\n",
							"    ,lc.SRouteID as lms_customer_srouteid\r\n",
							"    ,lc.StoreID as lms_customer_storeid\r\n",
							"    ,lc.Suburb as lms_customer_suburb\r\n",
							"    ,lc.TelNo as lms_customer_telno\r\n",
							"    ,lc.Town as lms_customer_town\r\n",
							"    ,lc.VolFact as lms_customer_volfact\r\n",
							"    ,lc.WType as lms_customer_wtype\r\n",
							"    ,oc.id as orv_customer_id\r\n",
							"    ,oc.name as orv_customer_name\r\n",
							"    ,oc.contact as orv_customer_contact\r\n",
							"    ,oc.email as orv_customer_email\r\n",
							"    ,oc.mobileno as orv_customer_mobileno\r\n",
							"    ,oc.landline as orv_customer_landline\r\n",
							"    ,oc.ctype as orv_customer_ctype\r\n",
							"    ,oc.parent as orv_customer_parent\r\n",
							"    ,to_timestamp(oc.lastupdate) as orv_customer_lastupdate\r\n",
							"    ,oc.lmsid as orv_customer_lmsid\r\n",
							"    ,oc.srouteid as orv_customer_srouteid\r\n",
							"    ,oc.verified as orv_customer_verified\r\n",
							"    ,oc.defaultmaildomain as orv_customer_defaultmaildomain\r\n",
							"    ,oc.autoassignticketid as orv_customer_autoassignticketid\r\n",
							"    ,oc.sendinfodel as orv_customer_sendinfodel\r\n",
							"    ,oc.anonemailcustsupportemail as orv_customer_anonemailcustsupportemail\r\n",
							"    ,oc.anonemailcustsupporttel as orv_customer_anonemailcustsupporttel\r\n",
							"    ,oc.cref as orv_customer_cref\r\n",
							"    ,oc.division as orv_customer_division\r\n",
							"    ,oc.ecomm as orv_customer_ecomm\r\n",
							"    ,oc.preverifiedaddid as orv_customer_preverifiedaddid\r\n",
							"    ,oc.mallname as orv_customer_mallname\r\n",
							"    ,oc.mallid as orv_customer_mallid\r\n",
							"    ,oc.mallflag as orv_customer_mallflag\r\n",
							"    ,oc.malllmsid as orv_customer_malllmsid\r\n",
							"    ,oc.highvolume as orv_customer_highvolume\r\n",
							"    ,oc.custreference as orv_customer_custreference\r\n",
							"    ,oc.unverdate as orv_customer_unverdate\r\n",
							"    ,oc.verdate as orv_customer_verdate\r\n",
							"    ,oc.groupid as orv_customer_groupid\r\n",
							"    ,oc.postalcodecustgroupid as orv_customer_postalcodecustgroupid\r\n",
							"    ,oc.webhookauthcredential as orv_customer_webhookauthcredential\r\n",
							"    ,oc.mergedinto as orv_customer_mergedinto\r\n",
							"    ,oc.cusmergedate as orv_customer_cusmergedate\r\n",
							"    ,oc.customerref as orv_customer_customerref\r\n",
							"    ,oc.routemaint as orv_customer_routemaint\r\n",
							"    ,bcr.id as lms_customer_billcustid\r\n",
							"    ,bcr.billcustid as lms_customer_billcustbillcustid\r\n",
							"    ,bcr.incovalue as lms_customer_billcustincovalue\r\n",
							"    ,bcr.daterangelimit as lms_customer_billcustdaterangelimit\r\n",
							"    ,bcr.status as lms_customer_billcuststatus\r\n",
							"    ,bcr.note as lms_customer_billcustnote\r\n",
							"    ,bcr.custgroupid as lms_customer_billcustcustgroupid\r\n",
							"    ,bcr.activeflag as lms_customer_billcustactiveflag\r\n",
							"    ,bcr.routerateeffectivedate as lms_customer_billcustrouterateeffectivedate\r\n",
							"    ,bcr.ratesaddedflag as lms_customer_billcustratesaddedflag\r\n",
							"    ,bcr.ibtratesflag as lms_customer_billcustibtratesflag\r\n",
							"    ,bcr.slidingscaleflag as lms_customer_billcustslidingscaleflag\r\n",
							"    ,bcr.sapflag as lms_customer_billcustsapflag\r\n",
							"    ,bcr.sapcode as lms_customer_billcustsapcode\r\n",
							"    ,bcr.sapcustname as lms_customer_billcustsapcustname\r\n",
							"    ,bcr.averagepclrateflag as lms_customer_billcustaveragepclrateflag\r\n",
							"    ,bcr.averagepclkg as lms_customer_billcustaveragepclkg\r\n",
							"    ,bcr.billdatestart as lms_customer_billcustbilldatestart\r\n",
							"    ,bcr.itemcode as lms_customer_billcustitemcode\r\n",
							"    ,bcr.servicetypeflag as lms_customer_billcustservicetypeflag\r\n",
							"    ,bcr.returnreverserateflag as lms_customer_billcustreturnreverserateflag\r\n",
							"    ,bcr.minnoofpcls as lms_customer_billcustminnoofpcls\r\n",
							"    ,bcr.productcategoryid as lms_customer_billcustproductcategoryid\r\n",
							"    ,bcr.samedayconsolidation as lms_customer_billcustsamedayconsolidation\r\n",
							"    ,bcr.rategroupid as lms_customer_billcustrategroupid\r\n",
							"    ,bcr.risksurchargeflag as lms_customer_billcustrisksurchargeflag\r\n",
							"    ,bcr.applyavgvolweight as lms_customer_billcustapplyavgvolweight\r\n",
							"    ,bcr.avgvolweight as lms_customer_billcustavgvolweight\r\n",
							"    ,bcr.ecommcustomerflag as lms_customer_billcustecommcustomerflag\r\n",
							"    ,bcr.leadtimestructureid as lms_customer_billcustleadtimestructureid\r\n",
							"    ,bcr.recalcinprogress as lms_customer_billcustrecalcinprogress\r\n",
							"    ,bcr.recalcuserid as lms_customer_billcustrecalcuserid\r\n",
							"    ,bcr.needwebhook as lms_customer_billcustneedwebhook\r\n",
							"    ,bcr.routemaint as lms_customer_billcustroutemaint\r\n",
							"    ,bcr.lhdistrflag as lms_customer_billcustlhdistrflag\r\n",
							"    ,bcg.id as lms_customer_billcustgroupid\r\n",
							"    ,bcg.description as lms_customer_billcustgroupdescription\r\n",
							"    ,bcg.grouptypeid as lms_customer_billcustgroupgrouptypeid\r\n",
							"    ,bcgt.id as lms_customer_billcustgrouptypeid\r\n",
							"    ,bcgt.description as lms_customer_billcustgrouptypedescription\r\n",
							"\r\n",
							"\r\n",
							"FROM dbocustomer lc\r\n",
							"LEFT JOIN publiccustomer oc on oc.lmsid = lc.id\r\n",
							"LEFT JOIN dbobillbillcustomersr bcr on bcr.billcustid = lc.id \r\n",
							"LEFT JOIN dbobillcustomergroups bcg  on bcg.id = bcr.custgroupid\r\n",
							"LEFT JOIN dbobillcustomergrouptypes bcgt on bcgt.id = bcg.grouptypeid\r\n",
							"\r\n",
							"WHERE isbillto <> 1\r\n",
							" "
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stdeliverypickupcustomer = spark.sql(\"SELECT * FROM stdeliverypickupcustomer\")"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stdeliverypickupcustomer.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stdeliverypickupcustomer.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stzone \r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    z.Id as lms_zone_id\r\n",
							"    ,z.Code as lms_zone_code\r\n",
							"    ,z.Description as lms_zone_description\r\n",
							"    ,z.ISOCode as lms_zone_isocode\r\n",
							"    ,z.LocID as lms_zone_locid\r\n",
							"    ,z.ZoneMasterID as lms_zone_zonemasterid\r\n",
							"FROM dbozone z\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stzone = spark.sql(\"SELECT * FROM stzone\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stzone.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stzone.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stsroute \r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    sr.ID as lms_sroute_id\r\n",
							"    ,sr.BillCustID as lms_sroute_billcustid\r\n",
							"    ,sr.CourierID as lms_sroute_courierid\r\n",
							"    ,sr.Description as lms_sroute_description\r\n",
							"    ,sr.Code as lms_sroute_code\r\n",
							"    ,sr.id2 as lms_sroute_id2\r\n",
							"    ,sr.SRouteGroupID as lms_sroute_sroutegroupid\r\n",
							"    ,sr.ZoneID as lms_sroute_zoneid\r\n",
							"    ,sr.broutemasterID as lms_sroute_broutemasterid\r\n",
							"    ,sr.LocalFlag as lms_sroute_localflag\r\n",
							"    ,sr.Area as lms_sroute_area\r\n",
							"    ,sr.HighVolume as lms_sroute_highvolume\r\n",
							"FROM dbosroute sr\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stsroute = spark.sql(\"SELECT * FROM stsroute\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stsroute.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stsroute.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stbillzoneroute \r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    bzr.ID as lms_billzoneroute_id\r\n",
							"    ,bzr.Description as lms_billzoneroute_description\r\n",
							"    ,bzr.ZoneID as lms_billzoneroute_zoneid\r\n",
							"    ,bzr.ZRSCode as lms_billzoneroute_zrscode\r\n",
							"    ,bzr.ZoneRouteOrder as lms_billzoneroute_zonerouteorder\r\n",
							"    ,bzr.ZoneType as lms_billzoneroute_zonetype\r\n",
							"    ,bzr.BRouteMasterID as lms_billzoneroute_broutemasterid\r\n",
							"    ,bzr.BrouteCode as lms_billzoneroute_broutecode\r\n",
							"\r\n",
							"FROM dbobillzoneroute bzr\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stbillzoneroute = spark.sql(\"SELECT * FROM stbillzoneroute\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stbillzoneroute.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stbillzoneroute.parquet', mode = \"overwrite\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stbillzone \r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    bz.ID as lms_customer_id\r\n",
							"    ,bz.Description as lms_billzone_description\r\n",
							"    ,bz.ZoneMasterID as lms_billzone_zonemasterid\r\n",
							"    ,bz.CountryID as lms_billzone_countryid\r\n",
							"FROM dbobillzone bz\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stbillzone = spark.sql(\"SELECT * FROM stbillzone\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stbillzone.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stbillzone.parquet', mode = \"overwrite\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stlocation \r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    l.id as lms_location_id\r\n",
							"    ,l.description as lms_location_description\r\n",
							"    ,l.code as lms_location_code\r\n",
							"    ,l.collect as lms_location_collect\r\n",
							"    ,l.pcode as lms_location_pcode\r\n",
							"    ,l.custid as lms_location_custid\r\n",
							"    ,l.retail as lms_location_retail\r\n",
							"    ,l.scale as lms_location_scale\r\n",
							"    ,l.custcode as lms_location_custcode\r\n",
							"    ,l.captureonly as lms_location_captureonly\r\n",
							"    ,l.zoneid as lms_location_zoneid\r\n",
							"    ,l.scantype as lms_location_scantype\r\n",
							"    ,l.billcustid as lms_location_billcustid\r\n",
							"    ,l.latitude as lms_location_latitude\r\n",
							"    ,l.longitude as lms_location_longitude\r\n",
							"    ,l.holdlocid as lms_location_holdlocid\r\n",
							"    ,l.branchmanageruserid as lms_location_branchmanageruserid\r\n",
							"    ,l.fwcostcentre as lms_location_fwcostcentre\r\n",
							"    ,l.fwuserid as lms_location_fwuserid\r\n",
							"    ,l.fwmultibusinessid as lms_location_fwmultibusinessid\r\n",
							"    ,l.glcode as lms_location_glcode\r\n",
							"    ,l.hubcode as lms_location_hubcode\r\n",
							"    ,l.fwapikey as lms_location_fwapikey\r\n",
							"    ,l.geolocationgroupid as lms_location_geolocationgroupid\r\n",
							"    ,l.geolocationflag as lms_location_geolocationflag\r\n",
							"    ,l.orvactive as lms_location_orvactive\r\n",
							"    ,l.cref as lms_location_cref\r\n",
							"    ,l.tempcol as lms_location_tempcol\r\n",
							"    ,l.holdlocparentlocid as lms_location_holdlocparentlocid\r\n",
							"    ,l.floorsize as lms_location_floorsize\r\n",
							"    ,l.isstationidenabled as lms_location_isstationidenabled\r\n",
							"    ,l.activeflag as lms_location_activeflag\r\n",
							"    ,l.masterlocid as lms_location_masterlocid\r\n",
							"    ,l.locationtypeid as lms_location_locationtypeid\r\n",
							"    ,l.locationcode as lms_location_locationcode\r\n",
							"\r\n",
							"\r\n",
							"FROM dbolocation l"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stlocation = spark.sql(\"SELECT * FROM stlocation\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stlocation.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stlocation.parquet', mode = \"overwrite\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stendorsmentreason \r\n",
							"AS\r\n",
							"SELECT \t\r\n",
							"    rd.id as lmds_db_reasondetail_id\r\n",
							"\t,rd.description as lmds_db_reasondetail_description\r\n",
							"\t,rd.db_reasongroupid  as lmds_db_reasondetail_reasongroupid\r\n",
							"\t,rd.orv_type as lmds_db_reasondetail_orv_type\r\n",
							"\t,rd.active as lmds_db_reasondetail_active\r\n",
							"\t,rd.actionid as lmds_db_reasondetail_actionid\r\n",
							"\t,rd.cancelreason as lmds_db_reasondetail_cancelreason\r\n",
							"\t,rd.lmsreasonrule as lmds_db_reasondetail_lmsreasonrule\r\n",
							"\t,rg.description as lmds_db_reasongroup_description\r\n",
							"FROM dbodb_reasondetail rd\r\n",
							"LEFT JOIN dbodb_reasongroup rg on rg.id = rd.db_reasongroupid"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stendorsmentreason = spark.sql(\"SELECT * FROM stendorsmentreason\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stendorsmentreason.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stendorsmentreason.parquet', mode = \"overwrite\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stwaybilldelayreasons\r\n",
							"AS\r\n",
							"SELECT \t\r\n",
							"\tid AS lms_delayreason_id\r\n",
							"\t,description AS lms_delayreason_description\r\n",
							"\r\n",
							"FROM dbodb_delayreason\r\n",
							"WHERE description is not NULL or description <> \"\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stwaybillnotdbreasons\r\n",
							"AS\r\n",
							"SELECT \t\r\n",
							"\tid AS lms_notdbreason_id\r\n",
							"\t,description AS lms_notdbreason_description\r\n",
							"\r\n",
							"FROM dbodb_notdbreason\r\n",
							"WHERE description is not NULL or description <> \"\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stwaybilldeliverystatusreasons\r\n",
							"AS\r\n",
							"SELECT \t\r\n",
							"\tid AS lms_deliverystatus_id\r\n",
							"\t,description AS lms_deliverystatus_description\r\n",
							"\r\n",
							"FROM dbodb_deliverystatus\r\n",
							"WHERE description is not NULL or description <> \"\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stwaybillreasondetails\r\n",
							"AS\r\n",
							"SELECT \t\r\n",
							"\trd.id AS lms_reasondetail_id\r\n",
							"\t,rd.description AS lms_reasondetail_description\r\n",
							"\t,rg.description AS lms_reasondetail_group\r\n",
							"\r\n",
							"FROM dbodb_reasondetail rd\r\n",
							"LEFT JOIN dbodb_reasongroup rg on rg.id = rd.db_reasongroupid\r\n",
							"WHERE rd.description is not NULL or rd.description <> \"\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stwaybillinservicereasons\r\n",
							"AS\r\n",
							"SELECT \t\r\n",
							"\tisr.id AS lms_inservicereason_id\r\n",
							"\t,isr.reason AS lms_inservicereason_description\r\n",
							"\t,rg.description AS lms_inservicereason_group\r\n",
							"\r\n",
							"FROM dbodb_inservicereason isr\r\n",
							"LEFT JOIN dbodb_reasongroup rg on rg.id = isr.reasongroupid\r\n",
							"WHERE isr.reason is not NULL or isr.reason <> \"\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stwaybilldelayreasons = spark.sql(\"SELECT * FROM stwaybilldelayreasons\")\r\n",
							"stwaybillnotdbreasons = spark.sql(\"SELECT * FROM stwaybillnotdbreasons\")\r\n",
							"stwaybilldeliverystatusreasons = spark.sql(\"SELECT * FROM stwaybilldeliverystatusreasons\")\r\n",
							"stwaybillreasondetails = spark.sql(\"SELECT * FROM stwaybillreasondetails\")\r\n",
							"stwaybillinservicereasons = spark.sql(\"SELECT * FROM stwaybillinservicereasons\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stwaybilldelayreasons.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stwaybilldelayreasons.parquet', mode = \"overwrite\")\r\n",
							"# stwaybillnotdbreasons.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stwaybillnotdbreasons.parquet', mode = \"overwrite\")\r\n",
							"# stwaybilldeliverystatusreasons.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stwaybilldeliverystatusreasons.parquet', mode = \"overwrite\")\r\n",
							"# stwaybillreasondetails.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stwaybillreasondetails.parquet', mode = \"overwrite\")\r\n",
							"# stwaybillinservicereasons.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stwaybillinservicereasons.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"SUCCESS\")"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/01_LH_STBooking')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Ops/Linehaul"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "5fc96a87-f0f6-4c06-81ea-9496d0ba2bd6"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace/bigDataPools/DevSparkPool",
						"name": "DevSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/DevSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 47
					},
					{
						"cell_type": "code",
						"source": [
							"# #Create DataFrame for the publicbooking TMS Table\r\n",
							"# publicbooking = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publicbooking.parquet', format='parquet')\r\n",
							"# publicbooking.createOrReplaceTempView(\"publicbooking\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publicnonbooking TMS Table\r\n",
							"# publicnonbooking = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publicnonbooking.parquet', format='parquet')\r\n",
							"# publicnonbooking.createOrReplaceTempView(\"publicnonbooking\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publictrip TMS Table\r\n",
							"# publictrip = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publictrip.parquet', format='parquet')\r\n",
							"# publictrip.createOrReplaceTempView(\"publictrip\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbolhbooking TMS Table\r\n",
							"# dboLHBooking = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbolh_booking.parquet', format='parquet')\r\n",
							"# dboLHBooking.createOrReplaceTempView(\"dboLHBooking\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbolhroutes TMS Table\r\n",
							"# dbolhroutes = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbolh_routes.parquet', format='parquet')\r\n",
							"# dbolhroutes.createOrReplaceTempView(\"dbolhroutes\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbolhbookingspertrip TMS Table\r\n",
							"# dbolhbookingspertrip = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbolh_bookingspertrip.parquet', format='parquet')\r\n",
							"# dbolhbookingspertrip.createOrReplaceTempView(\"dbolhbookingspertrip\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 48
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stbooking_tms\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    b.id as tms_booking_id\r\n",
							"    ,b.allocatedby as tms_booking_allocatedby\r\n",
							"    ,to_timestamp(b.allocateddate) as tms_booking_allocateddate\r\n",
							"    ,to_timestamp(b.assetreleasedate) as tms_booking_assetreleasedate\r\n",
							"    ,b.billingmethod as tms_booking_billingmethod\r\n",
							"    ,b.billingstatus as tms_booking_billingstatus\r\n",
							"    ,null as tms_booking_convertedbookingid\r\n",
							"    ,b.cargoweight as tms_booking_cargoweight\r\n",
							"    ,b.customerid as tms_booking_customerid\r\n",
							"    ,b.childcustomerid as tms_booking_childcustomerid\r\n",
							"    ,b.createdby as tms_booking_insby\r\n",
							"    ,b.createdbyname as tms_booking_insbyname\r\n",
							"    ,b.cref1 as tms_booking_cref1\r\n",
							"    ,b.cref2 as tms_booking_cref2\r\n",
							"    ,b.cubicvolume as tms_booking_cubicvolume\r\n",
							"    ,b.customeragreedrate as tms_booking_customeragreedrate\r\n",
							"    ,b.customerrate as tms_booking_customerrate\r\n",
							"    ,to_timestamp(b.deliverydate) as tms_booking_deliverydate\r\n",
							"    ,CASE \r\n",
							"        WHEN COALESCE(b.directbooking,0) = 1 THEN 1\r\n",
							"        WHEN COALESCE(b.directbooking,0) = 0 and b.tobranchid is null THEN 1\r\n",
							"        ELSE COALESCE(b.directbooking,0)\r\n",
							"     END as tms_booking_directbooking\r\n",
							"    ,to_timestamp(b.dispatchdate) as tms_booking_dispatchdate\r\n",
							"    ,to_timestamp(b.docdate) as tms_booking_docdate\r\n",
							"    ,b.docstatus as tms_booking_docstatus\r\n",
							"    ,b.emailrequester as tms_booking_emailrequester\r\n",
							"    ,b.frombranch as tms_booking_frombranch\r\n",
							"    ,b.frombranchid as tms_booking_frombranchid\r\n",
							"    ,to_timestamp(b.insdate) as tms_booking_insdate\r\n",
							"    ,to_timestamp(b.invoiceddate) as tms_booking_invoiceddate\r\n",
							"    ,b.invoiceid as tms_booking_invoiceid\r\n",
							"    ,b.legacyid as tms_booking_legacyid\r\n",
							"    ,b.local as tms_booking_local\r\n",
							"    ,b.manifests as tms_booking_manifests\r\n",
							"    ,to_timestamp(b.moddate) as tms_booking_moddate\r\n",
							"    ,b.note as tms_booking_note\r\n",
							"    ,b.opsrouteid as tms_booking_opsrouteid\r\n",
							"    ,b.opsroutename as tms_booking_opsroutename\r\n",
							"    ,b.originalpod as tms_booking_originalpod\r\n",
							"    ,to_timestamp(b.pickupdate) as tms_booking_pickupdate\r\n",
							"    ,b.ponumber as tms_booking_ponumber\r\n",
							"    ,b.routeid as tms_booking_routeid\r\n",
							"    ,b.routename as tms_booking_routename\r\n",
							"    ,b.status as tms_booking_status\r\n",
							"    ,b.thirdpartyagreedrate as tms_booking_thirdpartyagreedrate\r\n",
							"    ,b.thirdpartydriverflag as tms_booking_thirdpartydriverflag\r\n",
							"    ,b.thirdpartyid as tms_booking_thirdpartyid\r\n",
							"    ,b.thirdpartyrate as tms_booking_thirdpartyrate\r\n",
							"    ,b.thirdpartyreason as tms_booking_thirdpartyreason\r\n",
							"    ,b.thirdpartytrailerflag as tms_booking_thirdpartytrailerflag\r\n",
							"    ,b.thirdpartyvehicleflag as tms_booking_thirdpartyvehicleflag\r\n",
							"    ,b.tobranch as tms_booking_tobranch\r\n",
							"    ,b.tobranchid as tms_booking_tobranchid\r\n",
							"    ,b.trailersize as tms_booking_trailersize\r\n",
							"    ,b.trailertype as tms_booking_trailertype\r\n",
							"    ,b.transporter as tms_booking_transporter\r\n",
							"    ,b.tripid as tms_booking_tripid\r\n",
							"    ,b.billingrouteid as tms_booking_billingrouteid\r\n",
							"    ,b.billingroutename as tms_booking_billingroutename\r\n",
							"    ,null as tms_booking_reason\r\n",
							"    ,'v2' as tms_booking_source\r\n",
							"    ,'booking' as tms_booking_type\r\n",
							"\r\n",
							"FROM publicbooking b\r\n",
							""
						],
						"outputs": [],
						"execution_count": 49
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stbooking_tms = spark.sql(\"SELECT * FROM stbooking_tms\")"
						],
						"outputs": [],
						"execution_count": 50
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stnonbooking_tms\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    b.id as tms_booking_id\r\n",
							"    ,null as tms_booking_allocatedby\r\n",
							"    ,to_timestamp(t.dispatchdate) as tms_booking_allocateddate\r\n",
							"    ,to_timestamp(b.assetreleasedate) as tms_booking_assetreleasedate\r\n",
							"    ,null as tms_booking_billingmethod\r\n",
							"    ,b.billingstatus as tms_booking_billingstatus\r\n",
							"    ,b.convertedbookingid as tms_booking_convertedbookingid\r\n",
							"    ,null as tms_booking_cargoweight \r\n",
							"    ,null as tms_booking_customerid\r\n",
							"    ,null as tms_booking_childcustomerid\r\n",
							"    ,b.insby as tms_booking_insby\r\n",
							"    ,b.insbyname as tms_booking_insbyname\r\n",
							"    ,null as tms_booking_cref1\r\n",
							"    ,null as tms_booking_cref2\r\n",
							"    ,null as tms_booking_cubicvolume\r\n",
							"    ,null as tms_booking_customeragreedrate\r\n",
							"    ,null as tms_booking_customerrate\r\n",
							"    ,to_timestamp(b.deliverydate) as tms_booking_deliverydate\r\n",
							"    ,0 as tms_booking_directbooking\r\n",
							"    ,to_timestamp(b.dispatchdate) as tms_booking_dispatchdate\r\n",
							"    ,null as tms_booking_docdate\r\n",
							"    ,null as tms_booking_docstatus\r\n",
							"    ,null as tms_booking_emailrequester\r\n",
							"    ,b.frombranch as tms_booking_frombranch\r\n",
							"    ,b.frombranchid as tms_booking_frombranchid\r\n",
							"    ,to_timestamp(b.insdate) as tms_booking_insdate\r\n",
							"    ,null as tms_booking_invoiceddate\r\n",
							"    ,null as tms_booking_invoiceid\r\n",
							"    ,b.legacyid as tms_booking_legacyid\r\n",
							"    ,b.local as tms_booking_local\r\n",
							"    ,null as tms_booking_manifests\r\n",
							"    ,to_timestamp(b.moddate) as tms_booking_moddate\r\n",
							"    ,b.note as tms_booking_note\r\n",
							"    ,b.routeid as tms_booking_opsrouteid\r\n",
							"    ,b.routename as tms_booking_opsroutename\r\n",
							"    ,null as tms_booking_originalpod\r\n",
							"    ,null as tms_booking_pickupdate\r\n",
							"    ,null as tms_booking_ponumber\r\n",
							"    ,null as tms_booking_routeid\r\n",
							"    ,null as tms_booking_routename\r\n",
							"    ,b.status as tms_booking_status\r\n",
							"    ,null as tms_booking_thirdpartyagreedrate\r\n",
							"    ,null as tms_booking_thirdpartydriverflag\r\n",
							"    ,null as tms_booking_thirdpartyid\r\n",
							"    ,null as tms_booking_thirdpartyrate\r\n",
							"    ,null as tms_booking_thirdpartyreason\r\n",
							"    ,null as tms_booking_thirdpartytrailerflag\r\n",
							"    ,null as tms_booking_thirdpartyvehicleflag\r\n",
							"    ,null as tms_booking_tobranch\r\n",
							"    ,null as tms_booking_tobranchid\r\n",
							"    ,b.trailersize as tms_booking_trailersize\r\n",
							"    ,null as tms_booking_trailertype\r\n",
							"    ,null as tms_booking_transporter\r\n",
							"    ,b.tripid as tms_booking_tripid\r\n",
							"    ,b.reason as tms_booking_reason\r\n",
							"    ,null as tms_booking_billingrouteid\r\n",
							"    ,null as tms_booking_billingroutename\r\n",
							"    ,'v2' as tms_booking_source\r\n",
							"    ,'nonbooking' as tms_booking_type\r\n",
							"\r\n",
							"FROM publicnonbooking b\r\n",
							"LEFT JOIN publictrip t on t.id = b.tripid\r\n",
							""
						],
						"outputs": [],
						"execution_count": 51
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stnonbooking_tms = spark.sql(\"SELECT * FROM stnonbooking_tms\")"
						],
						"outputs": [],
						"execution_count": 52
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stbooking_lms\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"\r\n",
							"    (b.id)*-1 as tms_booking_id\r\n",
							"    ,(b.allocationuserid)*-1 as tms_booking_allocatedby\r\n",
							"    ,to_timestamp(b.AllocationDate) as tms_booking_allocateddate\r\n",
							"    ,null as tms_booking_assetreleasedate\r\n",
							"    ,null as tms_booking_billingmethod\r\n",
							"    ,null as tms_booking_billingstatus\r\n",
							"    ,null as tms_booking_convertedbookingid\r\n",
							"    ,b.cargoweight as tms_booking_cargoweight\r\n",
							"    ,(b.primaryaccountid)*-1 as tms_booking_customerid\r\n",
							"    ,(b.secondaryaccountid)*-1 as tms_booking_childcustomerid\r\n",
							"    ,(b.userid)*-1 as tms_booking_insby\r\n",
							"    ,null as tms_booking_insbyname\r\n",
							"    ,b.customerreference1 as tms_booking_cref1\r\n",
							"    ,b.customerreference2 as tms_booking_cref2\r\n",
							"    ,b.cubicvolume as tms_booking_cubicvolume\r\n",
							"    ,null as tms_booking_customeragreedrate\r\n",
							"    ,null as tms_booking_customerrate\r\n",
							"    ,b.deliverydate as tms_booking_deliverydate\r\n",
							"    ,b.isdirect as tms_booking_directbooking\r\n",
							"    ,b.dispatchdate as tms_booking_dispatchdate\r\n",
							"    ,null as tms_booking_docdate\r\n",
							"    ,null as tms_booking_docstatus\r\n",
							"    ,null as tms_booking_emailrequester\r\n",
							"    ,null as tms_booking_frombranch\r\n",
							"    ,null as tms_booking_frombranchid\r\n",
							"    ,b.createddate as tms_booking_insdate\r\n",
							"    ,null as tms_booking_invoiceddate\r\n",
							"    ,null as tms_booking_invoiceid\r\n",
							"    ,null as tms_booking_legacyid\r\n",
							"    ,b.islocal as tms_booking_local\r\n",
							"    ,null as tms_booking_manifests\r\n",
							"    ,null as tms_booking_moddate\r\n",
							"    ,b.notes as tms_booking_note\r\n",
							"    ,(b.routeid)*-1 as tms_booking_opsrouteid\r\n",
							"    ,r.description as tms_booking_opsroutename\r\n",
							"    ,null as tms_booking_originalpod\r\n",
							"    -- ,b.orginalpodreceiveddate as tms_booking_originalpod\r\n",
							"    ,b.pickupdate as tms_booking_pickupdate\r\n",
							"    ,null as tms_booking_ponumber\r\n",
							"    ,null as tms_booking_routeid\r\n",
							"    ,null as tms_booking_routename\r\n",
							"    ,CASE \r\n",
							"        WHEN b.statusid =1  THEN 'created'\r\n",
							"        WHEN b.statusid = 2 THEN 'allocated'\r\n",
							"        WHEN b.statusid in (3,6) THEN 'active'\r\n",
							"        WHEN b.statusid in (4,8) THEN 'completed'\r\n",
							"        WHEN b.statusid in (5,7) THEN 'cancelled'  \r\n",
							"    END as tms_booking_status\r\n",
							"    ,null as tms_booking_thirdpartyagreedrate\r\n",
							"    ,null as tms_booking_thirdpartydriverflag\r\n",
							"    ,(b.3pl_accountid)*-1 as tms_booking_thirdpartyid\r\n",
							"    ,null as tms_booking_thirdpartyrate\r\n",
							"    ,b.3pl_reason as tms_booking_thirdpartyreason\r\n",
							"    ,null as tms_booking_thirdpartytrailerflag\r\n",
							"    ,null as tms_booking_thirdpartyvehicleflag\r\n",
							"    ,null as tms_booking_tobranch\r\n",
							"    ,null as tms_booking_tobranchid\r\n",
							"    ,CASE\t\r\n",
							"        WHEN b.trailersizeid = 1 THEN '12 METER'\r\n",
							"        WHEN b.trailersizeid = 2 THEN '4 METER'\r\n",
							"        WHEN b.trailersizeid = 3 THEN '6 METER'\r\n",
							"        WHEN b.trailersizeid = 4 THEN '18 METER'\r\n",
							"        WHEN b.trailersizeid = 5 THEN '15 METER'\r\n",
							"    END\tas tms_booking_trailersize\r\n",
							"\r\n",
							"    ,CASE\t\r\n",
							"        WHEN b.trailertypeid = 1 THEN 'RIGID'\r\n",
							"        WHEN b.trailertypeid = 2 THEN 'SUPERLINK PANTECH'\r\n",
							"        WHEN b.trailertypeid = 3 THEN 'TRI-AXLE'\r\n",
							"        WHEN b.trailertypeid = 4 THEN 'BELLYPAN'\r\n",
							"        WHEN b.trailertypeid = 5 THEN 'SUPERLINK TAUTLINER'\r\n",
							"        WHEN b.trailertypeid = 6 THEN 'CONTAINER'\r\n",
							"        WHEN b.trailertypeid = 7 THEN 'PANTECH'\r\n",
							"        WHEN b.trailertypeid = 8 THEN 'TAUTLINER'\r\n",
							"    END\tas tms_booking_trailertype\r\n",
							"    ,null as tms_booking_transporter\r\n",
							"    ,(bpt.tripid)*-1 as tms_booking_tripid \r\n",
							"    ,b.nonbookingreasonid as tms_booking_reason\r\n",
							"    ,null as tms_booking_billingrouteid\r\n",
							"    ,null as tms_booking_billingroutename\r\n",
							"    ,'v1' as tms_booking_source\r\n",
							"    ,CASE\r\n",
							"        WHEN  b.bookingtypeid = 1 THEN 'booking'\r\n",
							"        WHEN  b.bookingtypeid = 3 THEN 'nonbooking'\r\n",
							"    END as tms_booking_type\r\n",
							"\r\n",
							"FROM dbolhbooking b\r\n",
							"LEFT JOIN dbolhroutes r on r.id = b.routeid\r\n",
							"LEFT JOIN dbolhbookingspertrip bpt on bpt.bookingid = b.id\r\n",
							"WHERE b.allocationdate > '2020-08-31 23:59:59.999'\r\n",
							"AND b.bookingtypeid in (1,3)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 53
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stbooking_lms = spark.sql(\"SELECT * FROM stbooking_lms\")"
						],
						"outputs": [],
						"execution_count": 54
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stbooking_tmp = stbooking_tms.unionByName(stnonbooking_tms, allowMissingColumns = True)\r\n",
							"stbooking_tmp.createOrReplaceTempView(\"stbooking_tmp\")"
						],
						"outputs": [],
						"execution_count": 55
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stbooking = stbooking_tmp.unionByName(stbooking_lms, allowMissingColumns = True)\r\n",
							"stbooking.createOrReplaceTempView(\"stbooking\")"
						],
						"outputs": [],
						"execution_count": 56
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stbooking.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stbooking.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 57
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"SUCCESS\")"
						],
						"outputs": [],
						"execution_count": 58
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/01_LMSv1v2Recon_ETL')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Ops/LMSv1v2Recon"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "f940b3b9-6db6-475c-b60c-9555f4276dc6"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse_Prod/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace-prod/bigDataPools/PRDSparkPool",
						"name": "PRDSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/PRDSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# from pyspark.sql import functions as F\r\n",
							"# from pyspark.sql import DataFrame"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# #Create DataFrame for the v2customerorder LMSV2 Table\r\n",
							"# v2customerorder = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMSV2/v2customerorder.parquet', format='parquet')\r\n",
							"# columns=v2customerorder.columns\r\n",
							"# for i in range(len(columns)):v2customerorder=v2customerorder.withColumnRenamed(columns[i],'v2'+ columns[i])\r\n",
							"# v2customerorder.createOrReplaceTempView(\"v2customerorder\")\r\n",
							"\r\n",
							"# #Create DataFrame for the v2deliverynote LMSV2 Table\r\n",
							"# v2deliverynote = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMSV2/v2deliverynote.parquet', format='parquet')\r\n",
							"# columns1=v2deliverynote.columns\r\n",
							"# for i in range(len(columns1)):v2deliverynote=v2deliverynote.withColumnRenamed(columns1[i],'v2'+ columns1[i])\r\n",
							"# v2deliverynote.createOrReplaceTempView(\"v2deliverynote\")\r\n",
							"\r\n",
							"# #Create DataFrame for the v2manifest LMSV2 Table\r\n",
							"# v2manifest = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMSV2/v2manifest.parquet', format='parquet')\r\n",
							"# columns2=v2manifest.columns\r\n",
							"# for i in range(len(columns2)):v2manifest=v2manifest.withColumnRenamed(columns2[i],'v2'+ columns2[i])\r\n",
							"# v2manifest.createOrReplaceTempView(\"v2manifest\")\r\n",
							"\r\n",
							"# #Create DataFrame for the v2consignment LMSV2 Table\r\n",
							"# v2consignment = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMSV2/v2consignment.parquet', format='parquet')\r\n",
							"# columns3=v2consignment.columns\r\n",
							"# for i in range(len(columns3)):v2consignment=v2consignment.withColumnRenamed(columns3[i],'v2'+ columns3[i])\r\n",
							"# v2consignment.createOrReplaceTempView(\"v2consignment\")\r\n",
							"\r\n",
							"# #Create DataFrame for the v2parcel LMSV2 Table\r\n",
							"# v2parcel = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMSV2/v2parcel.parquet', format='parquet')\r\n",
							"# columns8=v2parcel.columns\r\n",
							"# for i in range(len(columns8)):v2parcel=v2parcel.withColumnRenamed(columns8[i],'v2'+ columns8[i])\r\n",
							"# v2parcel.createOrReplaceTempView(\"v2parcel\")\r\n",
							"\r\n",
							"# #Create DataFrame for the v2track LMSV2 Table\r\n",
							"# v2track = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMSV2/v2track.parquet', format='parquet')\r\n",
							"# columns9=v2track.columns\r\n",
							"# for i in range(len(columns9)):v2track=v2track.withColumnRenamed(columns9[i],'v2'+ columns9[i])\r\n",
							"# v2track.createOrReplaceTempView(\"v2track\")\r\n",
							"\r\n",
							"# #Create DataFrame for the v2movement LMSV2 Table\r\n",
							"# v2movement = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMSV2/v2movement.parquet', format='parquet')\r\n",
							"# columns10=v2movement.columns\r\n",
							"# for i in range(len(columns10)):v2movement=v2movement.withColumnRenamed(columns10[i],'v2'+ columns10[i])\r\n",
							"# v2movement.createOrReplaceTempView(\"v2movement\")\r\n",
							"\r\n",
							"\r\n",
							"# ######################################################################################################\r\n",
							"\r\n",
							"# #Create DataFrame for the dbowaybill LMS Table\r\n",
							"# dbowaybill = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbowaybill.parquet', format='parquet')\r\n",
							"# columns4=dbowaybill.columns\r\n",
							"# for i in range(len(columns4)):dbowaybill=dbowaybill.withColumnRenamed(columns4[i],'v1'+ columns4[i])\r\n",
							"# dbowaybill.createOrReplaceTempView(\"dbowaybill\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dboloads LMS Table\r\n",
							"# dboloads = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboloads.parquet', format='parquet')\r\n",
							"# columns5=dboloads.columns\r\n",
							"# for i in range(len(columns5)):dboloads=dboloads.withColumnRenamed(columns5[i],'v1'+ columns5[i])\r\n",
							"# dboloads.createOrReplaceTempView(\"dboloads\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dboconsignment LMS Table\r\n",
							"# dboconsignment = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboconsignment.parquet', format='parquet')\r\n",
							"# columns6=dboconsignment.columns\r\n",
							"# for i in range(len(columns6)):dboconsignment=dboconsignment.withColumnRenamed(columns6[i],'v1'+ columns6[i])\r\n",
							"# dboconsignment.createOrReplaceTempView(\"dboconsignment\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dboorder LMS Table\r\n",
							"# dboorder = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboorder.parquet', format='parquet')\r\n",
							"# columns7=dboorder.columns\r\n",
							"# for i in range(len(columns7)):dboorder=dboorder.withColumnRenamed(columns7[i],'v1'+ columns7[i])\r\n",
							"# dboorder.createOrReplaceTempView(\"dboorder\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dboparcel LMS Table\r\n",
							"# dboparcel = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboparcel.parquet', format='parquet')\r\n",
							"# columns11=dboparcel.columns\r\n",
							"# for i in range(len(columns11)):dboparcel=dboparcel.withColumnRenamed(columns11[i],'v1'+ columns11[i])\r\n",
							"# dboparcel.createOrReplaceTempView(\"dboparcel\")\r\n",
							"\r\n",
							"# # Create DataFrame for the sstparcelunion LMS Table\r\n",
							"# sstparcelunion = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Semi Structured Data/OPS/sstparcelunion.parquet', format='parquet')\r\n",
							"# columns11=sstparcelunion.columns\r\n",
							"# for i in range(len(columns11)):sstparcelunion=sstparcelunion.withColumnRenamed(columns11[i],'v1'+ columns11[i])\r\n",
							"# sstparcelunion.createOrReplaceTempView(\"sstparcelunion\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbotrack LMS Table\r\n",
							"# dbotrack = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbotrack.parquet', format='parquet')\r\n",
							"# columns12=dbotrack.columns\r\n",
							"# for i in range(len(columns12)):dbotrack=dbotrack.withColumnRenamed(columns12[i],'v1'+ columns12[i])\r\n",
							"# dbotrack.createOrReplaceTempView(\"dbotrack\")\r\n",
							"\r\n",
							"# # Create DataFrame for the ssttrackunion LMS Table\r\n",
							"# ssttrackunion = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Semi Structured Data/OPS/ssttrackunion.parquet', format='parquet')\r\n",
							"# columns12=ssttrackunion.columns\r\n",
							"# for i in range(len(columns12)):ssttrackunion=ssttrackunion.withColumnRenamed(columns12[i],'v1'+ columns12[i])\r\n",
							"# ssttrackunion.createOrReplaceTempView(\"ssttrackunion\")\r\n",
							"\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"<mark>**APPROACH VERSION 3**</mark>"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"reconparcel\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    p.*\r\n",
							"    ,p2.*\r\n",
							"    ,CASE \r\n",
							"        WHEN p2.v2id IS NULL THEN 1\r\n",
							"        ELSE 0\r\n",
							"    END as notinv2\r\n",
							"FROM sstparcelunion p \r\n",
							"LEFT JOIN v2parcel p2 on p2.v2legacyid = p.v1id\r\n",
							"WHERE  COALESCE(p.v1acceptancedate,p.v1handoverdate)>='2023-01-01 00:00:00' AND COALESCE(p.v1acceptancedate,p.v1handoverdate)<='2023-01-31 23:59:59'\r\n",
							"AND p.v1orderid IS NOT NULL"
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"reconparcel = spark.sql(\"SELECT * FROM reconparcel\")"
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"reconparcel_baseparcelid\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"     v1id\r\n",
							"FROM reconparcel"
						],
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"reconparcel_basewaybillid\r\n",
							"AS\r\n",
							"SELECT DISTINCT\r\n",
							"    v1waybillid\r\n",
							"\r\n",
							"FROM reconparcel"
						],
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"reconparcel_baseorderid\r\n",
							"AS\r\n",
							"SELECT DISTINCT\r\n",
							"    v1orderid\r\n",
							"FROM reconparcel"
						],
						"outputs": [],
						"execution_count": 27
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"reconparcel_baseconsignid\r\n",
							"AS\r\n",
							"SELECT DISTINCT\r\n",
							"    v1consignid\r\n",
							"FROM reconparcel"
						],
						"outputs": [],
						"execution_count": 28
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"reconwaybill\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"\r\n",
							" w.*\r\n",
							",dn.*\r\n",
							",CASE \r\n",
							"    WHEN dn.v2id IS NULL THEN 1\r\n",
							"    ELSE 0\r\n",
							"END as notinv2\r\n",
							"\r\n",
							"FROM reconparcel_basewaybillid b\r\n",
							"LEFT JOIN dbowaybill w on w.v1id = b.v1waybillid\r\n",
							"LEFT JOIN v2deliverynote dn on dn.v2legacyid = w.v1id\r\n",
							"-- WHERE w.v1date>='2023-01-01'"
						],
						"outputs": [],
						"execution_count": 29
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"reconwaybill = spark.sql(\"SELECT * FROM reconwaybill\")"
						],
						"outputs": [],
						"execution_count": 30
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"reconorder\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"\r\n",
							"o.*\r\n",
							",co.*\r\n",
							",CASE \r\n",
							"    WHEN co.v2id IS NULL THEN 1\r\n",
							"    ELSE 0\r\n",
							"END as notinv2\r\n",
							"\r\n",
							"FROM reconparcel_baseorderid b \r\n",
							"LEFT JOIN dboorder o on o.v1id = b.v1orderid\r\n",
							"LEFT JOIN v2customerorder co on co.v2legacyid = o.v1id\r\n",
							"-- WHERE  o.v1hodate>='2023-01-01'"
						],
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"reconorder = spark.sql(\"SELECT * FROM reconorder\")"
						],
						"outputs": [],
						"execution_count": 32
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"reconconsignment\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"\r\n",
							"c.*\r\n",
							",co.*\r\n",
							",CASE \r\n",
							"    WHEN co.v2id IS NULL THEN 1\r\n",
							"    ELSE 0\r\n",
							"END as notinv2\r\n",
							"\r\n",
							"FROM reconparcel_baseconsignid b \r\n",
							"LEFT JOIN dboconsignment c on c.v1id = b.v1consignid\r\n",
							"LEFT JOIN v2consignment co on co.v2legacyid = c.v1id\r\n",
							"-- WHERE  c.v1cdate>='2023-01-01'"
						],
						"outputs": [],
						"execution_count": 33
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"reconconsignment = spark.sql(\"SELECT * FROM reconconsignment\")"
						],
						"outputs": [],
						"execution_count": 34
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"recontrack\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    t1.*\r\n",
							"    ,t2.*\r\n",
							"    ,CASE \r\n",
							"        WHEN t2.v2id IS NULL THEN 1\r\n",
							"        ELSE 0\r\n",
							"    END as notinv2\r\n",
							"\r\n",
							"FROM reconparcel_baseparcelid b \r\n",
							"LEFT JOIN ssttrackunion t1 on t1.v1parcelid = b.v1id\r\n",
							"LEFT JOIN v2track t2 on t2.v2legacyid = t1.v1id\r\n",
							"-- WHERE t1.v1opendt >='2023-01-01'"
						],
						"outputs": [],
						"execution_count": 35
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"recontrack = spark.sql(\"SELECT  * FROM recontrack\")"
						],
						"outputs": [],
						"execution_count": 36
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"recontrack_base\r\n",
							"AS\r\n",
							"SELECT DISTINCT\r\n",
							"     v1loadid\r\n",
							"FROM recontrack"
						],
						"outputs": [],
						"execution_count": 37
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"recontrack_base = spark.sql(\"SELECT  * FROM recontrack_base\")"
						],
						"outputs": [],
						"execution_count": 38
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"reconloads\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"\r\n",
							"l.*\r\n",
							",m.*\r\n",
							",CASE \r\n",
							"    WHEN m.v2id IS NULL THEN 1\r\n",
							"    ELSE 0\r\n",
							"END as notinv2\r\n",
							"\r\n",
							"FROM recontrack_base b\r\n",
							"LEFT JOIN dboloads l on l.v1id = b.v1loadid\r\n",
							"LEFT JOIN v2manifest m on m.v2legacyid = l.v1id\r\n",
							"-- WHERE l.v1ldate >='2023-01-01'"
						],
						"outputs": [],
						"execution_count": 39
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"reconloads = spark.sql(\"SELECT * FROM reconloads\")"
						],
						"outputs": [],
						"execution_count": 40
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"reconmovement\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    * \r\n",
							"FROM v2movement"
						],
						"outputs": [],
						"execution_count": 41
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"reconmovement = spark.sql(\"SELECT  * FROM reconmovement\")"
						],
						"outputs": [],
						"execution_count": 42
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"<mark>**APPROACH VERSION 2**</mark>"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# -- %%sql\r\n",
							"# -- CREATE OR REPLACE TEMP VIEW\r\n",
							"# -- reconwaybill\r\n",
							"# -- AS\r\n",
							"# -- SELECT \r\n",
							"\r\n",
							"# -- w.*\r\n",
							"# -- ,dn.*\r\n",
							"# -- ,CASE \r\n",
							"# --     WHEN dn.v2id IS NULL THEN 1\r\n",
							"# --     ELSE 0\r\n",
							"# -- END as notinv2\r\n",
							"\r\n",
							"# -- FROM dbowaybill w\r\n",
							"# -- LEFT JOIN v2deliverynote dn on dn.v2legacyid = w.v1id\r\n",
							"# -- WHERE w.v1date>='2023-01-01'\r\n",
							""
						],
						"outputs": [],
						"execution_count": 43
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# reconwaybill = spark.sql(\"SELECT * FROM reconwaybill\")"
						],
						"outputs": [],
						"execution_count": 44
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# -- %%sql\r\n",
							"# -- CREATE OR REPLACE TEMP VIEW\r\n",
							"# -- reconorder\r\n",
							"# -- AS\r\n",
							"# -- SELECT \r\n",
							"\r\n",
							"# -- o.*\r\n",
							"# -- ,co.*\r\n",
							"# -- ,CASE \r\n",
							"# --     WHEN co.v2id IS NULL THEN 1\r\n",
							"# --     ELSE 0\r\n",
							"# -- END as notinv2\r\n",
							"\r\n",
							"# -- FROM dboorder o\r\n",
							"# -- LEFT JOIN v2customerorder co on co.v2legacyid = o.v1id\r\n",
							"# -- WHERE  o.v1hodate>='2023-01-01'\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 45
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# reconorder = spark.sql(\"SELECT * FROM reconorder\")"
						],
						"outputs": [],
						"execution_count": 46
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# -- %%sql\r\n",
							"# -- CREATE OR REPLACE TEMP VIEW\r\n",
							"# -- reconconsignment\r\n",
							"# -- AS\r\n",
							"# -- SELECT \r\n",
							"\r\n",
							"# -- c.*\r\n",
							"# -- ,co.*\r\n",
							"# -- ,CASE \r\n",
							"# --     WHEN co.v2id IS NULL THEN 1\r\n",
							"# --     ELSE 0\r\n",
							"# -- END as notinv2\r\n",
							"\r\n",
							"# -- FROM dboconsignment c \r\n",
							"# -- LEFT JOIN v2consignment co on co.v2legacyid = c.v1id\r\n",
							"# -- WHERE  c.v1cdate>='2023-01-01'"
						],
						"outputs": [],
						"execution_count": 47
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# reconconsignment = spark.sql(\"SELECT * FROM reconconsignment\")"
						],
						"outputs": [],
						"execution_count": 48
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# -- %%sql\r\n",
							"# -- CREATE OR REPLACE TEMP VIEW\r\n",
							"# -- reconparcel\r\n",
							"# -- AS\r\n",
							"# -- SELECT \r\n",
							"# --     p.*\r\n",
							"# --     ,p2.*\r\n",
							"# --     ,CASE \r\n",
							"# --         WHEN p2.v2id IS NULL THEN 1\r\n",
							"# --         ELSE 0\r\n",
							"# --     END as notinv2\r\n",
							"# -- FROM sstparcelunion p \r\n",
							"# -- LEFT JOIN v2parcel p2 on p2.v2legacyid = p.v1id\r\n",
							"# -- WHERE  COALESCE(p.v1acceptancedate,p.v1handoverdate)>='2023-01-01'"
						],
						"outputs": [],
						"execution_count": 49
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# reconparcel = spark.sql(\"SELECT * FROM reconparcel\")"
						],
						"outputs": [],
						"execution_count": 50
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# -- %%sql\r\n",
							"# -- CREATE OR REPLACE TEMP VIEW\r\n",
							"# -- reconloads\r\n",
							"# -- AS\r\n",
							"# -- SELECT \r\n",
							"\r\n",
							"# -- l.*\r\n",
							"# -- ,m.*\r\n",
							"# -- ,CASE \r\n",
							"# --     WHEN m.v2id IS NULL THEN 1\r\n",
							"# --     ELSE 0\r\n",
							"# -- END as notinv2\r\n",
							"\r\n",
							"# -- FROM dboloads l \r\n",
							"# -- LEFT JOIN v2manifest m on m.v2legacyid = l.v1id\r\n",
							"# -- WHERE l.v1ldate >='2023-01-01'"
						],
						"outputs": [],
						"execution_count": 51
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# reconloads = spark.sql(\"SELECT * FROM reconloads\")"
						],
						"outputs": [],
						"execution_count": 52
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# -- %%sql\r\n",
							"# -- CREATE OR REPLACE TEMP VIEW\r\n",
							"# -- recontrack\r\n",
							"# -- AS\r\n",
							"# -- SELECT \r\n",
							"# --     t1.*\r\n",
							"# --     ,t2.*\r\n",
							"# --     ,CASE \r\n",
							"# --         WHEN t2.v2id IS NULL THEN 1\r\n",
							"# --         ELSE 0\r\n",
							"# --     END as notinv2\r\n",
							"\r\n",
							"# -- FROM ssttrackunion t1 \r\n",
							"# -- LEFT JOIN v2track t2 on t2.v2legacyid = t1.v1id\r\n",
							"# -- WHERE t1.v1opendt >='2023-01-01'"
						],
						"outputs": [],
						"execution_count": 53
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# recontrack = spark.sql(\"SELECT  * FROM recontrack\")"
						],
						"outputs": [],
						"execution_count": 54
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# -- %%sql\r\n",
							"# -- CREATE OR REPLACE TEMP VIEW\r\n",
							"# -- reconmovement\r\n",
							"# -- AS\r\n",
							"# -- SELECT \r\n",
							"# --     * \r\n",
							"# -- FROM v2movement"
						],
						"outputs": [],
						"execution_count": 55
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# reconmovement = spark.sql(\"SELECT  * FROM reconmovement\")"
						],
						"outputs": [],
						"execution_count": 56
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"<mark>**APPROACH VERSION 1**</mark>"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**BASE PARCEL**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# -- %%sql\r\n",
							"# -- CREATE OR REPLACE TEMP VIEW\r\n",
							"# -- reconparcel_base\r\n",
							"# -- AS\r\n",
							"# -- SELECT \r\n",
							"# --      v1id\r\n",
							"# --     ,v1consignid\r\n",
							"# --     ,v1waybillid\r\n",
							"# --     ,v1orderid\r\n",
							"# -- FROM sstparcelunion p\r\n",
							"# -- WHERE  COALESCE(p.v1acceptancedate,p.v1handoverdate)>='2023-01-01'"
						],
						"outputs": [],
						"execution_count": 57
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# reconparcel_base = spark.sql(\"SELECT * FROM reconparcel_base\")"
						],
						"outputs": [],
						"execution_count": 58
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**WAYBILL**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# -- %%sql\r\n",
							"# -- CREATE OR REPLACE TEMP VIEW\r\n",
							"# -- reconwaybill_tmp1\r\n",
							"# -- AS\r\n",
							"# -- SELECT w.v1id\r\n",
							"# -- FROM dbowaybill w\r\n",
							"# -- WHERE  w.v1date>='2023-01-01'\r\n",
							"\r\n",
							"# -- UNION \r\n",
							"\r\n",
							"# -- SELECT DISTINCT p.v1waybillid as v1id\r\n",
							"# -- FROM reconparcel_base p\r\n",
							""
						],
						"outputs": [],
						"execution_count": 59
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# reconwaybill_tmp1 = spark.sql(\"SELECT DISTINCT * FROM reconwaybill_tmp1\")"
						],
						"outputs": [],
						"execution_count": 60
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# -- %%sql\r\n",
							"# -- CREATE OR REPLACE TEMP VIEW\r\n",
							"# -- reconwaybill\r\n",
							"# -- AS\r\n",
							"# -- SELECT \r\n",
							"\r\n",
							"# -- w.*\r\n",
							"# -- ,dn.*\r\n",
							"# -- ,CASE \r\n",
							"# --     WHEN dn.v2id IS NULL THEN 1\r\n",
							"# --     ELSE 0\r\n",
							"# -- END as notinv2\r\n",
							"\r\n",
							"# -- FROM reconwaybill_tmp1 t\r\n",
							"# -- LEFT JOIN dbowaybill w on w.v1id = t.v1id\r\n",
							"# -- LEFT JOIN v2deliverynote dn on dn.v2legacyid = w.v1id\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 61
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# reconwaybill = spark.sql(\"SELECT * FROM reconwaybill\")"
						],
						"outputs": [],
						"execution_count": 62
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**ORDER**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# -- %%sql\r\n",
							"# -- CREATE OR REPLACE TEMP VIEW\r\n",
							"# -- reconorder_tmp1\r\n",
							"# -- AS\r\n",
							"# -- SELECT o.v1id\r\n",
							"# -- FROM dboorder o\r\n",
							"# -- WHERE  o.v1hodate>='2023-01-01'\r\n",
							"\r\n",
							"# -- UNION \r\n",
							"\r\n",
							"# -- SELECT DISTINCT p.v1orderid as v1id\r\n",
							"# -- FROM reconparcel_base p\r\n",
							""
						],
						"outputs": [],
						"execution_count": 63
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# reconorder_tmp1 = spark.sql(\"SELECT DISTINCT * FROM reconorder_tmp1\")"
						],
						"outputs": [],
						"execution_count": 64
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# -- %%sql\r\n",
							"# -- CREATE OR REPLACE TEMP VIEW\r\n",
							"# -- reconorder\r\n",
							"# -- AS\r\n",
							"# -- SELECT \r\n",
							"\r\n",
							"# -- o.*\r\n",
							"# -- ,co.*\r\n",
							"# -- ,CASE \r\n",
							"# --     WHEN co.v2id IS NULL THEN 1\r\n",
							"# --     ELSE 0\r\n",
							"# -- END as notinv2\r\n",
							"\r\n",
							"# -- FROM reconorder_tmp1 t\r\n",
							"# -- LEFT JOIN dboorder o on o.v1id = t.v1id\r\n",
							"# -- LEFT JOIN v2customerorder co on co.v2legacyid = o.v1id\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 65
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# reconorder = spark.sql(\"SELECT * FROM reconorder\")"
						],
						"outputs": [],
						"execution_count": 66
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**CONSIGNMENT**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# -- %%sql\r\n",
							"# -- CREATE OR REPLACE TEMP VIEW\r\n",
							"# -- reconconsignment_tmp1\r\n",
							"# -- AS\r\n",
							"# -- SELECT c.v1id\r\n",
							"# -- FROM dboconsignment c\r\n",
							"# -- WHERE  c.v1cdate>='2023-01-01'\r\n",
							"\r\n",
							"# -- UNION \r\n",
							"\r\n",
							"# -- SELECT DISTINCT p.v1consignid as v1id\r\n",
							"# -- FROM reconparcel_base p\r\n",
							""
						],
						"outputs": [],
						"execution_count": 67
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# reconconsignment_tmp1 = spark.sql(\"SELECT DISTINCT * FROM reconconsignment_tmp1\")"
						],
						"outputs": [],
						"execution_count": 68
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# -- %%sql\r\n",
							"# -- CREATE OR REPLACE TEMP VIEW\r\n",
							"# -- reconconsignment\r\n",
							"# -- AS\r\n",
							"# -- SELECT \r\n",
							"\r\n",
							"# -- c.*\r\n",
							"# -- ,co.*\r\n",
							"# -- ,CASE \r\n",
							"# --     WHEN co.v2id IS NULL THEN 1\r\n",
							"# --     ELSE 0\r\n",
							"# -- END as notinv2\r\n",
							"\r\n",
							"# -- FROM reconconsignment_tmp1 t\r\n",
							"# -- LEFT JOIN dboconsignment c on c.v1id = t. v1id\r\n",
							"# -- LEFT JOIN v2consignment co on co.v2legacyid = c.v1id\r\n",
							"\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 69
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# reconconsignment = spark.sql(\"SELECT * FROM reconconsignment\")"
						],
						"outputs": [],
						"execution_count": 70
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**PARCEL FINAL**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# -- %%sql\r\n",
							"# -- CREATE OR REPLACE TEMP VIEW\r\n",
							"# -- reconparcel_tmp1\r\n",
							"# -- AS\r\n",
							"# -- SELECT p.v1id\r\n",
							"# -- FROM reconwaybill w\r\n",
							"# -- LEFT JOIN sstparcelunion p on p.v1waybillid = w.v1id \r\n",
							"\r\n",
							"\r\n",
							"# -- UNION \r\n",
							"\r\n",
							"# -- SELECT p.v1id\r\n",
							"# -- FROM reconconsignment c\r\n",
							"# -- LEFT JOIN sstparcelunion p on p.v1consignid = c.v1id \r\n",
							"\r\n",
							"# -- UNION \r\n",
							"\r\n",
							"# -- SELECT p.v1id\r\n",
							"# -- FROM reconorder o\r\n",
							"# -- LEFT JOIN sstparcelunion p on p.v1orderid = o.v1id "
						],
						"outputs": [],
						"execution_count": 71
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# reconparcel_tmp1 = spark.sql(\"SELECT DISTINCT * FROM reconparcel_tmp1 WHERE v1id is not NULL\")"
						],
						"outputs": [],
						"execution_count": 72
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# -- %%sql\r\n",
							"# -- CREATE OR REPLACE TEMP VIEW\r\n",
							"# -- reconparcel\r\n",
							"# -- AS\r\n",
							"# -- SELECT \r\n",
							"# --     p.*\r\n",
							"# --     ,p2.*\r\n",
							"# --     ,CASE \r\n",
							"# --         WHEN p2.v2id IS NULL THEN 1\r\n",
							"# --         ELSE 0\r\n",
							"# --     END as notinv2\r\n",
							"# -- FROM reconparcel_tmp1 t\r\n",
							"# -- LEFT JOIN sstparcelunion p on p.v1id = t.v1id\r\n",
							"# -- LEFT JOIN v2parcel p2 on p2.v2legacyid = p.v1id\r\n",
							""
						],
						"outputs": [],
						"execution_count": 73
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# reconparcel = spark.sql(\"SELECT * FROM reconparcel\")"
						],
						"outputs": [],
						"execution_count": 74
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**TRACK**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# -- %%sql\r\n",
							"# -- CREATE OR REPLACE TEMP VIEW\r\n",
							"# -- recontrack_tmp1\r\n",
							"# -- AS\r\n",
							"# -- SELECT \r\n",
							"# --      t.v1id\r\n",
							"# --     ,t.v1loadid\r\n",
							"# -- FROM ssttrackunion t\r\n",
							"# -- WHERE t.v1opendt >='2023-01-01'\r\n",
							"\r\n",
							"# -- UNION \r\n",
							"\r\n",
							"# -- SELECT \r\n",
							"# --      t.v1id\r\n",
							"# --     ,t.v1loadid\r\n",
							"# -- FROM reconparcel p\r\n",
							"# -- LEFT JOIN ssttrackunion t on t.v1parcelid = p.v1id"
						],
						"outputs": [],
						"execution_count": 75
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# recontrack_tmp1 = spark.sql(\"SELECT DISTINCT * FROM recontrack_tmp1\")"
						],
						"outputs": [],
						"execution_count": 76
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# -- %%sql\r\n",
							"# -- CREATE OR REPLACE TEMP VIEW\r\n",
							"# -- reconloads_tmp1\r\n",
							"# -- AS\r\n",
							"# -- SELECT \r\n",
							"# --      l.v1id\r\n",
							"# -- FROM dboloads l\r\n",
							"# -- WHERE l.v1ldate >='2023-01-01'\r\n",
							"\r\n",
							"# -- UNION \r\n",
							"\r\n",
							"# -- SELECT \r\n",
							"# --     l.v1id\r\n",
							"# -- FROM recontrack_tmp1 t\r\n",
							"# -- LEFT JOIN dboloads l on l.v1id = t.v1loadid"
						],
						"outputs": [],
						"execution_count": 77
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# reconloads_tmp1 = spark.sql(\"SELECT DISTINCT * FROM reconloads_tmp1\")"
						],
						"outputs": [],
						"execution_count": 78
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# -- %%sql\r\n",
							"# -- CREATE OR REPLACE TEMP VIEW\r\n",
							"# -- reconloads\r\n",
							"# -- AS\r\n",
							"# -- SELECT \r\n",
							"\r\n",
							"# -- l.*\r\n",
							"# -- ,m.*\r\n",
							"# -- ,CASE \r\n",
							"# --     WHEN m.v2id IS NULL THEN 1\r\n",
							"# --     ELSE 0\r\n",
							"# -- END as notinv2\r\n",
							"\r\n",
							"# -- FROM reconloads_tmp1 t\r\n",
							"# -- LEFT JOIN dboloads l on l.v1id = t.v1id\r\n",
							"# -- LEFT JOIN v2manifest m on m.v2legacyid = l.v1id"
						],
						"outputs": [],
						"execution_count": 79
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# reconloads = spark.sql(\"SELECT * FROM reconloads\")"
						],
						"outputs": [],
						"execution_count": 80
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# -- %%sql\r\n",
							"# -- CREATE OR REPLACE TEMP VIEW\r\n",
							"# -- recontrack_tmp2\r\n",
							"# -- AS\r\n",
							"# -- SELECT \r\n",
							"# --     t.v1id\r\n",
							"\r\n",
							"# -- FROM recontrack_tmp1 t\r\n",
							"\r\n",
							"# -- UNION \r\n",
							"\r\n",
							"# -- SELECT \r\n",
							"# --     t.v1id\r\n",
							"\r\n",
							"# -- FROM reconloads l\r\n",
							"# -- LEFT JOIN ssttrackunion t on t.v1loadid = l.v1id"
						],
						"outputs": [],
						"execution_count": 81
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# recontrack_tmp2 = spark.sql(\"SELECT DISTINCT * FROM recontrack_tmp2\")"
						],
						"outputs": [],
						"execution_count": 82
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# -- %%sql\r\n",
							"# -- CREATE OR REPLACE TEMP VIEW\r\n",
							"# -- recontrack\r\n",
							"# -- AS\r\n",
							"# -- SELECT \r\n",
							"# --     t2.*\r\n",
							"# --     ,t3.*\r\n",
							"# --     ,CASE \r\n",
							"# --         WHEN t3.v2id IS NULL THEN 1\r\n",
							"# --         ELSE 0\r\n",
							"# --     END as notinv2\r\n",
							"\r\n",
							"# -- FROM recontrack_tmp2 t1\r\n",
							"# -- LEFT JOIN ssttrackunion t2 on t2.v1id = t1.v1id\r\n",
							"# -- LEFT JOIN v2track t3 on t3.v2legacyid = t1.v1id"
						],
						"outputs": [],
						"execution_count": 83
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# recontrack = spark.sql(\"SELECT  * FROM recontrack\")"
						],
						"outputs": [],
						"execution_count": 84
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**MOVEMENT**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# -- %%sql\r\n",
							"# -- CREATE OR REPLACE TEMP VIEW\r\n",
							"# -- reconmovement\r\n",
							"# -- AS\r\n",
							"# -- SELECT \r\n",
							"# --     * \r\n",
							"# -- FROM v2movement"
						],
						"outputs": [],
						"execution_count": 85
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# reconmovement = spark.sql(\"SELECT  * FROM reconmovement\")"
						],
						"outputs": [],
						"execution_count": 86
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# reconparcel.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/LMSV2/reconparcel.parquet', mode = \"overwrite\")\r\n",
							"# reconwaybill.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/LMSV2/reconwaybill.parquet', mode = \"overwrite\")\r\n",
							"# reconorder.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/LMSV2/reconorder.parquet', mode = \"overwrite\")\r\n",
							"# reconconsignment.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/LMSV2/reconconsignment.parquet', mode = \"overwrite\")\r\n",
							"# reconloads.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/LMSV2/reconloads.parquet', mode = \"overwrite\")\r\n",
							"# recontrack.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/LMSV2/recontrack.parquet', mode = \"overwrite\")\r\n",
							"# reconmovement.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/LMSV2/reconmovement.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 87
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/01_STChartOfAccounts')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Finance"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "b4357899-bd1a-4c44-a5a8-bdb27b829bdf"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse_Prod/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace-prod/bigDataPools/PRDSparkPool",
						"name": "PRDSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/PRDSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# #Create DataFrame for the dboOACT SAP Table\r\n",
							"# dboOACT = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dboOACT.parquet', format='parquet')\r\n",
							"# dboOACT.createOrReplaceTempView(\"dboOACT\")"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import col\r\n",
							"import pyspark.sql.functions as F\r\n",
							"x = dboOACT.withColumn('trimacctnames', F.trim('acctname'))\r\n",
							"y = x.withColumn('acctnamesplitlength', F.length('trimacctnames') - F.length(F.substring_index('trimacctnames', \"(\", -1)))\r\n",
							"z = y.withColumn('accttype', F.col('trimacctnames').substr(F.lit(0), col('acctnamesplitlength') - F.lit(2)))\r\n",
							""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"z.createOrReplaceTempView(\"dboOACT\")"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stchartofaccounts_step1\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"\r\n",
							"    oact.acctcode as sap_account_acctcode \r\n",
							"    ,oact.acctname as sap_account_acctname \r\n",
							"    ,oact.accttype as sap_account_accttype \r\n",
							"    ,oact.segment_0  as sap_account_segment_0 \r\n",
							"    ,oact.segment_1  as sap_account_segment_1 \r\n",
							"    ,oact.segment_2  as sap_account_segment_2 \r\n",
							"    ,oact.groupmask as sap_account_groupmask\r\n",
							"    ,oact.fathernum as sap_account_fathernum\r\n",
							"    ,oact.frozenfor as sap_account_frozenfor\r\n",
							"    ,oact.postable as sap_account_postable\r\n",
							"\r\n",
							"FROM dboOACT oact\r\n",
							""
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stchartofaccounts\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"     s1.*\r\n",
							"    ,Upper(s2.acctname) as sap_account_level5name \r\n",
							"\r\n",
							"    ,Upper(s3.acctname) as sap_account_level4name \r\n",
							"    ,s3.fathernum as sap_account_fathernum2\r\n",
							"    \r\n",
							"    ,Upper(s4.acctname) as sap_account_level3name \r\n",
							"    ,s4.fathernum as sap_account_fathernum3\r\n",
							"\r\n",
							"    ,Upper(s5.acctname) as sap_account_level2name \r\n",
							"    ,s5.fathernum as sap_account_fathernum4\r\n",
							"\r\n",
							"    ,Upper(s6.acctname) as sap_account_level1name \r\n",
							"    ,s6.fathernum as sap_account_fathernum5\r\n",
							"   \r\n",
							"\r\n",
							"FROM stchartofaccounts_step1 s1\r\n",
							"LEFT JOIN dboOACT s2 on s2.acctcode = s1.sap_account_acctcode and s2.fathernum = s1.sap_account_fathernum and s2.levels = 5\r\n",
							"LEFT JOIN dboOACT s3 on s3.acctcode = s2.fathernum and s3.levels = 4\r\n",
							"LEFT JOIN dboOACT s4 on s4.acctcode = s3.fathernum and s4.levels = 3\r\n",
							"LEFT JOIN dboOACT s5 on s5.acctcode = s4.fathernum and s5.levels = 2\r\n",
							"LEFT JOIN dboOACT s6 on s6.acctcode = s5.fathernum and s6.levels = 1\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stchartofaccounts = spark.sql(\"SELECT * FROM stchartofaccounts\")"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stchartofaccounts.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stchartofaccounts.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"SUCCESS\")"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/01_STSorterDimensions')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Ops/Sorter"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "8e9cfd78-1ab7-4bb8-aaa9-a653c975dc5a"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace/bigDataPools/DevSparkPool",
						"name": "DevSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/DevSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# #Create DataFrame for the dboplc_status LMS Table\r\n",
							"# dboplc_status = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboplc_status.parquet', format='parquet')\r\n",
							"# dboplc_status.createOrReplaceTempView(\"dboplc_status\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dboplc_scanpoint LMS Table\r\n",
							"# dboplc_scanpoint = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboplc_scanpoint.parquet', format='parquet')\r\n",
							"# dboplc_scanpoint.createOrReplaceTempView(\"dboplc_scanpoint\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dboplc_divertmapping LMS Table\r\n",
							"# dboplc_divertmapping = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboplc_divertmapping.parquet', format='parquet')\r\n",
							"# dboplc_divertmapping.createOrReplaceTempView(\"dboplc_divertmapping\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# import pandas as pd\r\n",
							"# from pyspark.sql.types import DecimalType\r\n",
							"# from pyspark.sql.functions import col"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# %%pyspark\r\n",
							"# sorterdivertsorting = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Finance and Operations Mapping Sources/Sorter_DivertSorting.csv', format='csv'\r\n",
							"# , header=True\r\n",
							"# )"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sorterdivertsorting = sorterdivertsorting.withColumn('divertseq', col('divertseq').cast('int'))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sorterdivertsorting.createOrReplaceTempView(\"sorterdivertsorting\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stplcdivertsorting\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"     divertdescription as excel_sorterdivertsorting_divertdescription\r\n",
							"    ,divertside as excel_sorterdivertsorting_divertside\t\r\n",
							"    ,divertseq as excel_sorterdivertsorting_divertseq\r\n",
							"\r\n",
							"FROM sorterdivertsorting"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stplcdivertsorting = spark.sql(\"SELECT * FROM stplcdivertsorting\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stplcdivertsorting.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stplcdivertsorting.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stplcstatus\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    id as lms_plcstatus_id\r\n",
							"    ,description as lms_plcstatus_description\r\n",
							"FROM dboplc_status"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stplcstatus = spark.sql(\"SELECT * FROM stplcstatus\")"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stplcstatus.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stplcstatus.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stplcscanpoint\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"     id as lms_plcscanpoint_id\r\n",
							"    ,description as lms_plcscanpoint_description\r\n",
							"FROM dboplc_scanpoint"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stplcscanpoint = spark.sql(\"SELECT * FROM stplcscanpoint\")"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stplcscanpoint.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stplcscanpoint.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stplcdivertmapping\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"     id as lms_plcdivertmapping_id\r\n",
							"    ,autosystemid as lms_plcdivertmapping_autosystemid\r\n",
							"    ,plcdivertid as lms_plcdivertmapping_plcdivertid\r\n",
							"    ,cldivertid as lms_plcdivertmapping_cldivertid\r\n",
							"    ,divertdescription as lms_plcdivertmapping_divertdescription\r\n",
							"FROM dboplc_divertmapping"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stplcdivertmapping = spark.sql(\"SELECT * FROM stplcdivertmapping\")"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stplcdivertmapping.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stplcdivertmapping.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"SUCCESS\")"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/01_STSorterHistory')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Ops/Sorter"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "fa83569d-bb78-4b69-a2c0-3e7500f84cd3"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace/bigDataPools/DevSparkPool",
						"name": "DevSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/DevSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# #Create DataFrame for the dboplc_lpnrouting_history LMS Table\r\n",
							"# dboplc_lpnrouting_history = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboplc_lpnrouting_history.parquet', format='parquet')\r\n",
							"# dboplc_lpnrouting_history.createOrReplaceTempView(\"dboplc_lpnrouting_history\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stparcel LMS Table\r\n",
							"# stparcel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stparcel.parquet', format='parquet')\r\n",
							"# stparcel.createOrReplaceTempView(\"stparcel\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbocustomer LMS Table\r\n",
							"# dbocustomer = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbocustomer.parquet', format='parquet')\r\n",
							"# dbocustomer.createOrReplaceTempView(\"dbocustomer\")\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/02_DIST_STLMSTrack')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Ops/Distribution"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "bb130737-c58e-47e3-b584-81a0e3ab3402"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse_Prod/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace-prod/bigDataPools/PRDSparkPool",
						"name": "PRDSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/PRDSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 41
					},
					{
						"cell_type": "code",
						"source": [
							"# #Create DataFrame for the ssttrackunion LMS Table\r\n",
							"# ssttrackunion = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Semi Structured Data/OPS/ssttrackunion.parquet', format='parquet')\r\n",
							"# ssttrackunion.createOrReplaceTempView(\"ssttrackunion\")\r\n",
							"\r\n",
							"# #Create DataFrame for the sstwaybillsperparcel LMS Table\r\n",
							"# sstwaybillsperparcel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Semi Structured Data/OPS/sstwaybillsperparcel.parquet', format='parquet')\r\n",
							"# sstwaybillsperparcel.createOrReplaceTempView(\"sstwaybillsperparcel\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbolocation LMS Table\r\n",
							"# dbolocation = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbolocation.parquet', format='parquet')\r\n",
							"# dbolocation.createOrReplaceTempView(\"dbolocation\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dboloads LMS Table\r\n",
							"# dboloads = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboloads.parquet', format='parquet')\r\n",
							"# dboloads.createOrReplaceTempView(\"dboloads\")\r\n",
							"\r\n",
							"\r\n",
							"# #Create DataFrame for the sstparcelunion LMS Table\r\n",
							"# sstparcelunion = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Semi Structured Data/OPS/sstparcelunion.parquet', format='parquet')\r\n",
							"# sstparcelunion.createOrReplaceTempView(\"sstparcelunion\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 42
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# from pyspark.sql import Window\r\n",
							"# from pyspark.sql.functions import lag    "
						],
						"outputs": [],
						"execution_count": 43
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"windowSpec  = Window.partitionBy(\"parcelid\").orderBy(\"parcelid\",\"id\")\r\n",
							"ssttrackunion = ssttrackunion.withColumn(\"lag\",lag(\"parcelid\",1).over(windowSpec))"
						],
						"outputs": [],
						"execution_count": 44
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"ssttrackunion.createOrReplaceTempView(\"ssttrackunion\")"
						],
						"outputs": [],
						"execution_count": 45
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stlmstrack\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"\r\n",
							"t.ID as lms_track_id,\r\n",
							"t.LoadID as lms_track_loadid,\r\n",
							"t.ParcelID as lms_track_parcelid,\r\n",
							"t.LoadID||'_'||t.ParcelID as lms_track_waybillsperparcelkey,\r\n",
							"t.fromlocid as lms_track_fromlocid,\r\n",
							"t.tolocid as lms_track_tolocid,\r\n",
							"t.TrackTypeID as lms_track_tracktypeid,\r\n",
							"t.TrackTypeID2 as lms_track_tracktypeid2,\r\n",
							"t.OpenDt as lms_track_opendt,\r\n",
							"t.CloseDt as lms_track_closedt,\r\n",
							"t.specdel as lms_track_specdel,\r\n",
							"COALESCE(l.parentloadid,l.id) as lms_track_loadparenetloadid,\r\n",
							"p.weight as lms_track_parcelweight,\r\n",
							"p.chargeweight as lms_track_parcelchargeweight,\r\n",
							"p.volweight as lms_track_parcelvolweight,\r\n",
							"p.totcharge as lms_track_parceltotcharge,\r\n",
							"p.billcustid as lms_track_orderbillcustid,\r\n",
							"p.pickupcustid as lms_track_orderpickupcustid,\r\n",
							"p.delivercustid as lms_track_orderdelivercustid,\r\n",
							"p.consignid as lms_track_parcelconsignid,\r\n",
							"p.orderid as lms_track_parcelorderid,\r\n",
							"CASE \r\n",
							"    WHEN t.ParcelID <> t.lag THEN 1\r\n",
							"    ELSE  0\r\n",
							"END as lms_track_lastmilebranchflag,\r\n",
							"w.lms_waybillsperparcel_waybillid as lms_track_waybillid,\r\n",
							"w.lms_waybillsperparcel_rdate as lms_track_rdate,\r\n",
							"w.lms_waybillsperparcel_returnedflag as lms_track_returnedflag,\r\n",
							"w.lms_waybillsperparcel_dooruserid as lms_track_dooruserid,\r\n",
							"w.lms_waybillsperparcel_doordeliverydate as lms_track_doordeliverydate,\r\n",
							"w.lms_waybillsperparcel_stationid as lms_track_stationid,\r\n",
							"w.lms_waybillsperparcel_doorreasondetailid as lms_track_doorreasondetailid,\r\n",
							"w.lms_waybillsperparcel_doorcomment as lms_track_doorcomment,\r\n",
							"w.lms_waybillsperparcel_debriefuserid as lms_track_debriefuserid,\r\n",
							"w.lms_waybillsperparcel_debriefdeliverydate as lms_track_debriefdeliverydate,\r\n",
							"w.lms_waybillsperparcel_debriefcomment as lms_track_debriefcomment,\r\n",
							"w.lms_waybillsperparcel_debriefreasondetailid as lms_track_debriefreasondetailid,\r\n",
							"w.lms_waybillsperparcel_reasonupdates as lms_track_reasonupdates,\r\n",
							"w.lms_waybillsperparcel_callprogid as lms_track_callprogid\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"FROM ssttrackunion t\r\n",
							"LEFT JOIN sstwaybillsperparcel w on w.lms_waybillsperparcel_trackkey = t.LoadID||'_'||t.ParcelID\r\n",
							"LEFT JOIN sstparcelunion p on p.id = t.parcelid\r\n",
							"LEFT JOIN dboloads l on l.id = t.loadid"
						],
						"outputs": [],
						"execution_count": 46
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stlmstrack = spark.sql(\"SELECT * FROM stlmstrack\")"
						],
						"outputs": [],
						"execution_count": 47
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stlmstrack.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stlmstrack.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 63
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stvsmparcel_tmp1\r\n",
							"AS\r\n",
							"SELECT DISTINCT\r\n",
							"     t.lms_track_parcelid\r\n",
							"FROM stlmstrack t"
						],
						"outputs": [],
						"execution_count": 49
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stvsmparcel_tmp1 = spark.sql(\"SELECT * FROM stvsmparcel_tmp1\")"
						],
						"outputs": [],
						"execution_count": 50
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stvsmparcel_tmp2\r\n",
							"AS\r\n",
							"SELECT DISTINCT\r\n",
							"     t.lms_track_parcelid\r\n",
							"    ,max(t.lms_track_closedt) as lms_track_enddate\r\n",
							"\r\n",
							"FROM stlmstrack t\r\n",
							"WHERE t.lms_track_tracktypeid = 2\r\n",
							"GROUP BY t.lms_track_parcelid"
						],
						"outputs": [],
						"execution_count": 51
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stvsmparcel_tmp2 = spark.sql(\"SELECT * FROM stvsmparcel_tmp2\")"
						],
						"outputs": [],
						"execution_count": 52
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stvsmparcel_tmp3\r\n",
							"AS\r\n",
							"SELECT DISTINCT\r\n",
							"     t.lms_track_parcelid\r\n",
							"    ,max(t.lms_track_opendt) as lms_track_floortimeend\r\n",
							"\r\n",
							"FROM stlmstrack t\r\n",
							"WHERE t.lms_track_tracktypeid = 6\r\n",
							"GROUP BY t.lms_track_parcelid\r\n",
							""
						],
						"outputs": [],
						"execution_count": 53
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stvsmparcel_tmp3 = spark.sql(\"SELECT * FROM stvsmparcel_tmp3\")"
						],
						"outputs": [],
						"execution_count": 54
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stvsmparcel_tmp4\r\n",
							"AS\r\n",
							"SELECT DISTINCT\r\n",
							"     t.lms_track_parcelid\r\n",
							"    ,max(t.lms_track_opendt) as lms_track_opendt\r\n",
							"\r\n",
							"FROM stlmstrack t\r\n",
							"WHERE t.lms_track_closedt IS NULL and t.lms_track_tracktypeid = 6\r\n",
							"GROUP BY t.lms_track_parcelid\r\n",
							""
						],
						"outputs": [],
						"execution_count": 55
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stvsmparcel_tmp4 = spark.sql(\"SELECT * FROM stvsmparcel_tmp4\")"
						],
						"outputs": [],
						"execution_count": 56
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stvsmparcel_tmp5\r\n",
							"AS\r\n",
							"SELECT DISTINCT\r\n",
							"     t.lms_track_parcelid\r\n",
							"    ,t.lms_track_opendt\r\n",
							"    ,t2.lms_track_fromlocid\r\n",
							"\r\n",
							"FROM  stvsmparcel_tmp4 t\r\n",
							"LEFT JOIN stlmstrack t2 on t2.lms_track_parcelid = t.lms_track_parcelid AND t2.lms_track_opendt = t.lms_track_opendt\r\n",
							"WHERE t2.lms_track_closedt IS NULL and t2.lms_track_tracktypeid = 6\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 57
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stvsmparcel_tmp5 = spark.sql(\"SELECT * FROM stvsmparcel_tmp5\")"
						],
						"outputs": [],
						"execution_count": 58
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stvsmparcel\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    t1.lms_track_parcelid\r\n",
							"    ,t2.lms_track_enddate\r\n",
							"    ,t3.lms_track_floortimeend\r\n",
							"    ,t5.lms_track_fromlocid\r\n",
							"FROM stvsmparcel_tmp1 t1 \r\n",
							"LEFT JOIN stvsmparcel_tmp2 t2 ON t2.lms_track_parcelid =t1.lms_track_parcelid \r\n",
							"LEFT JOIN stvsmparcel_tmp3 t3 ON t3.lms_track_parcelid =t1.lms_track_parcelid \r\n",
							"LEFT JOIN stvsmparcel_tmp5 t5 ON t5.lms_track_parcelid =t1.lms_track_parcelid \r\n",
							""
						],
						"outputs": [],
						"execution_count": 59
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stvsmparcel = spark.sql(\"SELECT * FROM stvsmparcel\")"
						],
						"outputs": [],
						"execution_count": 60
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stvsmparcel.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stvsmparcel.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 64
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"SUCCESS\")"
						],
						"outputs": [],
						"execution_count": 62
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/02_LH_STDriverPayItem')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Ops/Linehaul"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "cff0d97e-186a-4336-a063-fc14ca9a1a07"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace/bigDataPools/DevSparkPool",
						"name": "DevSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/DevSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 42
					},
					{
						"cell_type": "code",
						"source": [
							"# #Create DataFrame for the publicdriverpayitem TMS Table\r\n",
							"# publicdriverpayitem = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publicdriverpayitem.parquet', format='parquet')\r\n",
							"# publicdriverpayitem.createOrReplaceTempView(\"publicdriverpayitem\")"
						],
						"outputs": [],
						"execution_count": 43
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stdriverpayitem\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"\r\n",
							"    d.id as tms_driverpayitem_id\r\n",
							"    ,d.driverid as tms_driverpayitem_driverid\r\n",
							"    ,d.tripid as tms_driverpayitem_tripid\r\n",
							"    ,d.triprate as tms_driverpayitem_triprate\r\n",
							"    ,d.mealrate as tms_driverpayitem_mealrate\r\n",
							"    ,d.sleepoverrate as tms_driverpayitem_sleepoverrate\r\n",
							"    ,d.airtimerate as tms_driverpayitem_airtimerate\r\n",
							"    ,d.linkrate as tms_driverpayitem_linkrate\r\n",
							"    ,d.totalrate as tms_driverpayitem_totalrate\r\n",
							"    ,d.routeid as tms_driverpayitem_routeid\r\n",
							"    ,d.routename as tms_driverpayitem_routename\r\n",
							"    ,to_timestamp(d.insdate) as tms_driverpayitem_insdate\r\n",
							"    ,to_timestamp(d.moddate) as tms_driverpayitem_moddate\r\n",
							"    ,d.horse as tms_driverpayitem_horse\r\n",
							"    ,d.trailers as tms_driverpayitem_trailers\r\n",
							"    ,d.linkused as tms_driverpayitem_linkused\r\n",
							"    ,d.manualadjustment as tms_driverpayitem_manualadjustment\r\n",
							"    ,d.editby as tms_driverpayitem_editby\r\n",
							"    ,d.drivername as tms_driverpayitem_drivername\r\n",
							"    ,to_timestamp(d.dispatchdate) as tms_driverpayitem_dispatchdate\r\n",
							"    ,d.bookings as tms_driverpayitem_bookings\r\n",
							"    ,d.employeecode as tms_driverpayitem_employeecode\r\n",
							"    ,d.originatedfrom as tms_driverpayitem_originatedfrom\r\n",
							"    ,d.reportid as tms_driverpayitem_reportid\r\n",
							"\r\n",
							"FROM publicdriverpayitem d\r\n",
							"WHERE d.dispatchdate >0\r\n",
							""
						],
						"outputs": [],
						"execution_count": 44
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stdriverpayitem = spark.sql(\"SELECT * FROM stdriverpayitem\")"
						],
						"outputs": [],
						"execution_count": 45
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stdriverpayitem.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stdriverpayitem.parquet', mode = \"overwrite\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 46
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"SUCCESS\")"
						],
						"outputs": [],
						"execution_count": 47
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/02_STItems')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Finance"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "f24c8088-6258-4f62-9528-5d649e76c45f"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse_Prod/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace-prod/bigDataPools/PRDSparkPool",
						"name": "PRDSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/PRDSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# #Create DataFrame for the dboOITM SAP Table ITEM\r\n",
							"# dboOITM = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dboOITM.parquet', format='parquet')\r\n",
							"# dboOITM.createOrReplaceTempView(\"dboOITM\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dboOITB SAP Table ITEM GROUP\r\n",
							"# dboOITB = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dboOITB.parquet', format='parquet')\r\n",
							"# dboOITB.createOrReplaceTempView(\"dboOITB\")"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stitems\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    oitb.itmsgrpcod as sap_itemgroup_itmsgrpcod\r\n",
							"    ,oitb.itmsgrpnam as sap_itemgroup_itmsgrpnam\r\n",
							"    ,oitm.itemcode as sap_item_itemcode\r\n",
							"    ,oitm.itemname as sap_item_itemname\r\n",
							"    ,oitm.cstgrpcode as sap_item_cstgrpcode\r\n",
							"    ,oitm.vatgourpsa as sap_item_vatgourpsa\r\n",
							"    ,oitm.codebars as sap_item_codebars\r\n",
							"    ,oitm.prchseitem as sap_item_prchseitem\r\n",
							"    ,oitm.sellitem as sap_item_sellitem\r\n",
							"    ,oitm.cardcode as sap_item_cardcode\r\n",
							"    ,oitm.suppcatnum as sap_item_suppcatnum\r\n",
							"    ,oitm.invntitem as sap_item_invntitem\r\n",
							"    ,oitm.u_boy_tb_0 as sap_item_u_boy_tb_0\r\n",
							"    ,oitm.u_svctype as sap_item_u_svctype\r\n",
							"    ,oitm.u_depot as sap_item_u_depot\r\n",
							"\r\n",
							"FROM dboOITM oitm\r\n",
							"LEFT JOIN dboOITB oitb on oitb.itmsgrpcod = oitm.itmsgrpcod\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stitems = spark.sql(\"SELECT * FROM stitems\")"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stitems.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stitems.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"SUCCESS\")"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/02_STPlcLpnRouting')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Ops/Sorter"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "630962b0-f8bd-4a01-a517-525b3dec8849"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace/bigDataPools/DevSparkPool",
						"name": "DevSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/DevSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# #Create DataFrame for the dboplc_lpnrouting_history_centurion LMS Table\r\n",
							"# dboplc_lpnrouting_history_centurion = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboplc_lpnrouting_history_centurion.parquet', format='parquet')\r\n",
							"# dboplc_lpnrouting_history_centurion.createOrReplaceTempView(\"dboplc_lpnrouting_history_centurion\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbocustomer LMS Table\r\n",
							"# dbocustomer = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbocustomer.parquet', format='parquet')\r\n",
							"# dbocustomer.createOrReplaceTempView(\"dbocustomer\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stparcel3myoy LMS Table\r\n",
							"# stparcel3myoy = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stparcel3myoy.parquet', format='parquet')\r\n",
							"# stparcel3myoy.createOrReplaceTempView(\"stparcel3myoy\")\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# import pandas as pd\r\n",
							"# from pyspark.sql.types import DecimalType\r\n",
							"# from pyspark.sql.functions import col"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# %%pyspark\r\n",
							"# sorterchutes = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Finance and Operations Mapping Sources/Sorter_Chutes.csv', format='csv'\r\n",
							"# , header=True\r\n",
							"# )"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# %%pyspark\r\n",
							"# sortertimestudy = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Finance and Operations Mapping Sources/Sorter_TimeStudy.csv', format='csv'\r\n",
							"# , header=True\r\n",
							"# )"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# %%pyspark\r\n",
							"# sorterstaffallocation = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Finance and Operations Mapping Sources/Sorter_StaffAllocation.csv', format='csv'\r\n",
							"# , header=True\r\n",
							"# )"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sorterchutes = sorterchutes.withColumn('times', col('times').cast(DecimalType(38,18)))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sortertimestudy = sortertimestudy.withColumn('averageminsperparcel', col('averageminsperparcel').cast(DecimalType(38,18)))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sorterchutes.createOrReplaceTempView(\"sorterchutes\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sortertimestudy.createOrReplaceTempView(\"sortertimestudy\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sorterstaffallocation.createOrReplaceTempView(\"sorterstaffallocation\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stplclpnrouting_tmp1\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"     id as lms_plclpnrouting_id\r\n",
							"    ,autosystemid as lms_plclpnrouting_autosystemid\r\n",
							"    ,lpn as lms_plclpnrouting_lpn\r\n",
							"    ,divertid as lms_plclpnrouting_divertid\r\n",
							"    ,scanpoint as lms_plclpnrouting_scanpoint\r\n",
							"    ,status as lms_plclpnrouting_status\r\n",
							"    ,datereceived as lms_plclpnrouting_datereceived\r\n",
							"    ,dateupdated as lms_plclpnrouting_dateupdated\r\n",
							"    ,plcdivertid as lms_plclpnrouting_plcdivertid\r\n",
							"    ,cldivertid as lms_plclpnrouting_cldivertid\r\n",
							"    ,divertdescription as lms_plclpnrouting_divertdescription\r\n",
							"    -- ,elapsed_ms as lms_plclpnrouting_elapsed_ms\r\n",
							"    -- ,elapsed1_ms as lms_plclpnrouting_elapsed1_ms\r\n",
							"    -- ,elapsed2_ms as lms_plclpnrouting_elapsed2_ms\r\n",
							"    -- ,elapsed3_ms as lms_plclpnrouting_elapsed3_ms\r\n",
							"    -- ,elapsed4_ms as lms_plclpnrouting_elapsed4_ms\r\n",
							"    -- ,elapsed5_ms as lms_plclpnrouting_elapsed5_ms\r\n",
							"    -- ,elapsed6_ms as lms_plclpnrouting_elapsed6_ms\r\n",
							"FROM dboplc_lpnrouting_history_centurion\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stplclpnrouting_tmp1 = spark.sql(\"SELECT * FROM stplclpnrouting_tmp1\")"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stplclpnrouting\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"     lpn.*\r\n",
							"    ,lms_parcel_id\r\n",
							"    ,p.lms_parcel_orderid\r\n",
							"    ,p.lms_parcel_consignid\r\n",
							"    ,p.lms_parcel_waybillid\r\n",
							"    ,p.lms_parcel_pdate\r\n",
							"    ,p.lms_parcel_orderbillcustid\r\n",
							"    ,p.lms_parcel_orderdelivercustid\r\n",
							"    ,Upper(c.location) as excel_sorterchutes_location\r\n",
							"    ,ts.averageminsperparcel as excel_sortertimestudy_averageminsperparcel\r\n",
							"    ,sa.generalworkerallocation  as excel_sorterstaffallocation_generalworkerallocation\r\n",
							"\r\n",
							"FROM stplclpnrouting_tmp1 lpn\r\n",
							"INNER JOIN stparcel3myoy p on p.lms_parcel_barcode = lpn.lms_plclpnrouting_lpn\r\n",
							"LEFT JOIN dbocustomer bc on bc.id = p.lms_parcel_orderbillcustid\r\n",
							"LEFT JOIN dbocustomer dc on dc.id = p.lms_parcel_orderdelivercustid\r\n",
							"LEFT JOIN sorterchutes c on upper(c.combine) = upper(lms_plclpnrouting_divertdescription||' '||bc.name||' '||dc.name)\r\n",
							"LEFT JOIN sortertimestudy ts on upper(ts.element) = upper(c.location)\r\n",
							"LEFT JOIN sorterstaffallocation sa on upper(sa.divertdescription) = upper(lpn.lms_plclpnrouting_divertdescription)"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stplclpnrouting = spark.sql(\"SELECT * FROM stplclpnrouting\")"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stplclpnrouting.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stplclpnrouting.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"SUCCESS\")"
						],
						"outputs": [],
						"execution_count": 15
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/03_DIST_STParcel')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "1.Need to add a field for \"finalparcelweight\" applying the customer Billing matrix principle",
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Ops/Distribution"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "TESTSparkPoolL",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "112g",
					"driverCores": 16,
					"executorMemory": "112g",
					"executorCores": 16,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "4782a47a-fc8e-4e08-9142-e362bf34b8ba"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse_Prod/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace-prod/bigDataPools/TESTSparkPoolL",
						"name": "TESTSparkPoolL",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/TESTSparkPoolL",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 16,
						"memory": 112
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 44
					},
					{
						"cell_type": "code",
						"source": [
							"# #Create DataFrame for the sstparcelunion LMS Table\r\n",
							"# sstparcelunion = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Semi Structured Data/OPS/sstparcelunion.parquet', format='parquet')\r\n",
							"# sstparcelunion.createOrReplaceTempView(\"sstparcelunion\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dboorder LMS Table\r\n",
							"# dboorder = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboorder.parquet', format='parquet')\r\n",
							"# dboorder.createOrReplaceTempView(\"dboorder\")\r\n",
							"\r\n",
							"# #Create DataFrame for the sstconsignmentunion LMS Table\r\n",
							"# sstconsignmentunion = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Semi Structured Data/OPS/sstconsignmentunion.parquet', format='parquet')\r\n",
							"# sstconsignmentunion.createOrReplaceTempView(\"sstconsignmentunion\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbowaybill LMS Table\r\n",
							"# dbowaybill = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbowaybill.parquet', format='parquet')\r\n",
							"# dbowaybill.createOrReplaceTempView(\"dbowaybill\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dboparceldetail LMS Table\r\n",
							"# dboparceldetail = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboparceldetail.parquet', format='parquet')\r\n",
							"# dboparceldetail.createOrReplaceTempView(\"dboparceldetail\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publicparcel LMS Table\r\n",
							"# publicparcel = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicparcel.parquet', format='parquet')\r\n",
							"# publicparcel.createOrReplaceTempView(\"publicparcel\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publicdelivery ORV Table\r\n",
							"# publicdelivery = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicdelivery.parquet', format='parquet')\r\n",
							"# publicdelivery.createOrReplaceTempView(\"publicdelivery\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stvsmparcel LMS Table\r\n",
							"# stvsmparcel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stvsmparcel.parquet', format='parquet')\r\n",
							"# stvsmparcel.createOrReplaceTempView(\"stvsmparcel\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dboweekendholiday LMS Table\r\n",
							"# dboweekendholiday = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboweekendholiday.parquet', format='parquet')\r\n",
							"# dboweekendholiday.createOrReplaceTempView(\"dboweekendholiday\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbocustacc LMS Table\r\n",
							"# dbocustacc = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbocustacc.parquet', format='parquet')\r\n",
							"# dbocustacc.createOrReplaceTempView(\"dbocustacc\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stdeliverypickupcustomer LMS Table\r\n",
							"# stdeliverypickupcustomer = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Structured Data/OPS/stdeliverypickupcustomer.parquet', format='parquet')\r\n",
							"# stdeliverypickupcustomer.createOrReplaceTempView(\"stdeliverypickupcustomer\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stsroute LMS Table\r\n",
							"# stsroute = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Structured Data/OPS/stsroute.parquet', format='parquet')\r\n",
							"# stsroute.createOrReplaceTempView(\"stsroute\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stzone LMS Table\r\n",
							"# stzone = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Structured Data/OPS/stzone.parquet', format='parquet')\r\n",
							"# stzone.createOrReplaceTempView(\"stzone\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stlocation LMS Table\r\n",
							"# stlocation = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Structured Data/OPS/stlocation.parquet', format='parquet')\r\n",
							"# stlocation.createOrReplaceTempView(\"stlocation\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 45
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stparcel_tmp\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    p.id \r\n",
							"    ,p.lmsid \r\n",
							"    ,p.did \r\n",
							"    ,to_timestamp(p.scandate) as orv_parcel_scandate\r\n",
							"    ,p.scanstatus as orv_parcel_scanstatus\r\n",
							"\t,p.scanmode as orv_parcel_scanmode\r\n",
							"\t,p.claimid as orv_parcel_claimid\r\n",
							"\t,p.courierid as orv_parcel_courierid\r\n",
							"    ,to_timestamp(p.moddate) as orv_parcel_moddate\r\n",
							"    ,d.waybillid\r\n",
							"\r\n",
							"FROM publicparcel p\r\n",
							"LEFT JOIN publicdelivery d ON d.did = p.id\r\n",
							"WHERE d.mode ='deliver' and D.skipreason != 'Fix wrong data manipulation by the company'\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 46
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stparcel_tmp2\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    p.id as lms_parcel_id\r\n",
							"    ,p.OrderID as lms_parcel_orderid\r\n",
							"    ,p.ConsignID as lms_parcel_consignid\r\n",
							"    ,p.WaybillID as lms_parcel_waybillid\r\n",
							"    ,p.LocID as lms_parcel_locid\r\n",
							"    ,p.TotCharge as lms_parcel_totcharge\r\n",
							"    ,p.DIMMS as lms_parcel_dimms\r\n",
							"    ,p.NoOfPcls as lms_parcel_noofpcls\r\n",
							"    ,p.PL as lms_parcel_pl\r\n",
							"    ,p.PW as lms_parcel_pw\r\n",
							"    ,p.PH as lms_parcel_ph\r\n",
							"    ,coalesce(p.acceptanceDate,p.handoverdate) as lms_parcel_pdate\r\n",
							"    ,p.AcceptanceDate as lms_parcel_acceptancedate\r\n",
							"    ,p.AcceptanceDate2 as lms_parcel_acceptancedate2\r\n",
							"    ,p.HandOverDate as lms_parcel_handoverdate\r\n",
							"    ,p.Barcode as lms_parcel_barcode\r\n",
							"    ,p.VolWeight as lms_parcel_volweight\r\n",
							"    ,p.Weight as lms_parcel_weight\r\n",
							"    ,p.ChargeWeight as lms_parcel_chargeweight\r\n",
							"    ,CASE \r\n",
							"        WHEN (p.ChargeWeight = 0 OR p.ChargeWeight is null) THEN GREATEST((p.weight),(p.VolWeight))\r\n",
							"        ELSE p.ChargeWeight \r\n",
							"    END as lms_parcel_pweight\r\n",
							"    ,p.PType as lms_parcel_ptype\r\n",
							"    ,p.FloorStatus as lms_parcel_floorstatus\r\n",
							"    ,p.ClaimID as lms_parcel_claimid\r\n",
							"    ,p.DateVolumised as lms_parcel_datevolumised\r\n",
							"    ,p.VolumiserWeight as lms_parcel_volumiserweight\r\n",
							"    ,p.VolumiserLength as lms_parcel_volumiserlength\r\n",
							"    ,p.VolumiserHeight as lms_parcel_volumiserheight\r\n",
							"    ,p.VolumiserWidth as lms_parcel_volumiserwidth\r\n",
							"    ,p.Volumiserid as lms_parcel_volumiserid\r\n",
							"    ,p.LastUpdate as lms_parcel_lastupdate\r\n",
							"    ,p.Description as lms_parcel_description\r\n",
							"    ,p.OldWBID as lms_parcel_oldwbid\r\n",
							"    ,p.disFlag as lms_parcel_disflag\r\n",
							"    ,p.customerbarcode as lms_parcel_customerbarcode\r\n",
							"    ,p.Cref as lms_parcel_cref\r\n",
							"    ,o.HODate as lms_parcel_orderhodate\r\n",
							"    ,o.PickUpCustID as lms_parcel_orderpickupcustid\r\n",
							"    ,o.DeliverCustID as lms_parcel_orderdelivercustid\r\n",
							"    ,o.BillCustID as lms_parcel_orderbillcustid\r\n",
							"    ,o.specdel as lms_parcel_orderspecdel\r\n",
							"    ,o.custaccid as lms_parcel_ordercustaccid\r\n",
							"    ,o.custref as lms_parcel_ordercustref\r\n",
							"    ,o.corderno as lms_parcel_ordercorderno\r\n",
							"    ,ca.description as lms_parcel_custaccdescription\r\n",
							"\t,ca.Cref as lms_parcel_custacccref\r\n",
							"    ,c.CDate as lms_parcel_consignmentcdate\r\n",
							"    ,c.DeliverBy as lms_parcel_consignmentdeliverby\r\n",
							"    ,c.Cref as lms_parcel_consignmentcref\r\n",
							"    ,w.PODDate  as lms_parcel_waybillpoddate \r\n",
							"    ,w.Date as lms_parcel_waybilldate\r\n",
							"    ,w.cref as lms_parcel_waybillcref\r\n",
							"    ,pd.id as lms_parcel_parceldetailid\r\n",
							"    ,pd.InServiceStart as lms_parcel_parceldetailinservicestart\r\n",
							"    ,pd.DeliverBy as lms_parcel_parceldetaildeliverby\r\n",
							"    ,pd.ServiceDays as lms_parcel_parceldetailservicedays\r\n",
							"    ,pd.ServiceTime as lms_parcel_parceldetailservicetime\r\n",
							"    ,pd.Cutofftime as lms_parcel_parceldetailcutofftime\r\n",
							"    ,pd.Appointment as lms_parcel_parceldetailappointment\r\n",
							"    ,pd.NDDID as lms_parcel_parceldetailnddid\r\n",
							"    ,pd.CustOnholdStart as lms_parcel_parceldetailcustonholdstart\r\n",
							"    ,pd.CustOnholdEnd as lms_parcel_parceldetailcustonholdend\r\n",
							"    ,pd.SplitInitial as lms_parcel_parceldetailsplitinitial\r\n",
							"    ,pd.WeekendHolidays as lms_parcel_parceldetailweekendholidays\r\n",
							"    ,pd.Inbound as lms_parcel_parceldetailinbound\r\n",
							"    ,pd.CourierID as lms_parcel_parceldetailcourierid\r\n",
							"    ,pd.ServiceID as lms_parcel_parceldetailserviceid\r\n",
							"    ,pd.CustAccID as lms_parcel_parceldetailcustaccid\r\n",
							"    ,pd.BrouteMasterID as lms_parcel_parceldetailbroutemasterid\r\n",
							"    ,pd.BrouteMasterFromID as lms_parcel_parceldetailbroutemasterfromid\r\n",
							"    ,pd.BrouteMasterToID as lms_parcel_parceldetailbroutemastertoid\r\n",
							"    ,pd.LeadTimeID as lms_parcel_parceldetailleadtimeid\r\n",
							"    ,CASE \r\n",
							"        WHEN pd.id is null and (coalesce(w.PODDate,CURRENT_TIMESTAMP) > c.DeliverBy) then 0 \r\n",
							"        WHEN coalesce(w.PODDate,CURRENT_TIMESTAMP) > pd.DeliverBy then 0 \r\n",
							"        ELSE 1 \r\n",
							"    END as lms_parcel_inserviceflag\r\n",
							"   ,CASE \r\n",
							"        -- WHEN pd.id is null then (date(coalesce(c.cdate,coalesce(p.acceptanceDate,p.handoverdate)) + interval '86399 seconds'))\r\n",
							"        -- ELSE date((pd.InServiceStart) + interval '86399 seconds')\r\n",
							"        WHEN pd.id is null then to_timestamp((cast(to_date(coalesce(c.cdate,coalesce(p.acceptanceDate,p.handoverdate)),'yyyy-MM-dd') as string)||' 23:59:59.997'))\r\n",
							"        ELSE to_timestamp((cast(to_date(pd.InServiceStart,'yyyy-MM-dd') as string)||' 23:59:59.997'))\r\n",
							"\r\n",
							"    END AS lms_parcel_actualleaddaysstartdate\r\n",
							"    ,coalesce(w.PODDate,CURRENT_TIMESTAMP) AS lms_parcel_actualleaddaysenddate\r\n",
							"    ,CASE\r\n",
							"        WHEN pd.id is null then ((to_unix_timestamp(coalesce(w.PODDate,CURRENT_TIMESTAMP))) - (to_unix_timestamp(date(coalesce(c.cdate,coalesce(p.acceptanceDate,p.handoverdate))) + interval '86399 seconds')))/86400 \r\n",
							"        ELSE ((to_unix_timestamp(coalesce(w.PODDate,CURRENT_TIMESTAMP))) - (to_unix_timestamp(pd.InServiceStart)))/86400 \r\n",
							"    END as lms_parcel_actualleaddaysinclwe\r\n",
							"    ,pt.orv_parcel_scandate\r\n",
							"    ,pt.orv_parcel_scanstatus\r\n",
							"\t,pt.orv_parcel_scanmode\r\n",
							"\t,pt.orv_parcel_claimid\r\n",
							"\t,pt.orv_parcel_courierid\r\n",
							"    ,pt.orv_parcel_moddate\r\n",
							"    ,CASE\r\n",
							"        WHEN pd.id is null then to_timestamp((cast(to_date(c.DeliverBy,'yyyy-MM-dd') as string)||' 23:59:59.997'))\r\n",
							"        ELSE to_timestamp((cast(to_date(pd.DeliverBy,'yyyy-MM-dd') as string)||' 23:59:59.997'))\r\n",
							"    END as lms_parcel_parceldetaildeliverbyroundup\r\n",
							"    ,CASE\r\n",
							"        WHEN pd.id is null then (to_unix_timestamp(coalesce(w.PODDate,CURRENT_TIMESTAMP)) - to_unix_timestamp(to_timestamp((cast(to_date(c.DeliverBy,'yyyy-MM-dd') as string)||' 23:59:59.997')))) \r\n",
							"        ELSE (to_unix_timestamp(coalesce(w.PODDate,CURRENT_TIMESTAMP)) - to_unix_timestamp(to_timestamp((cast(to_date(pd.DeliverBy,'yyyy-MM-dd') as string)||' 23:59:59.997')))) \r\n",
							"    END as parcelagedifferenceinseconds\r\n",
							"    ,l.lms_location_id as lms_location_consignmentlocid \r\n",
							"    ,l.lms_location_description as lms_location_consignmentlocdescription\r\n",
							"    \r\n",
							"FROM sstparcelunion p\r\n",
							"LEFT JOIN stparcel_tmp pt on pt.lmsid = p.id AND pt.waybillid = p.waybillid\r\n",
							"LEFT JOIN dboorder o on p.orderid = o.id\r\n",
							"LEFT JOIN sstconsignmentunion c on p.consignid = c.id\r\n",
							"LEFT JOIN dbowaybill w on p.waybillid = w.ID\r\n",
							"LEFT JOIN dboparceldetail pd on p.id = pd.parcelid\r\n",
							"LEFT JOIN dbocustacc ca on ca.id = o.custaccid\r\n",
							"LEFT JOIN stdeliverypickupcustomer dc on dc.lms_customer_id = o.delivercustid\r\n",
							"LEFT JOIN stsroute s on s.lms_sroute_id = dc.lms_customer_srouteid\r\n",
							"LEFT JOIN stzone z on z.lms_zone_id = s.lms_sroute_zoneid\r\n",
							"LEFT JOIN stlocation l on l.lms_location_id = z.lms_zone_locid \r\n",
							"\r\n",
							"WHERE  coalesce(p.acceptanceDate,p.handoverdate) IS NOT NULL\r\n",
							"AND p.locid NOT IN (163,110,213,27)\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 47
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stparcel_tmp3\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    p.lms_parcel_id\r\n",
							"    ,count(holiday)*86400 as holidays\r\n",
							"FROM stparcel_tmp2 p\r\n",
							"INNER JOIN dboweekendholiday h on (date(h.DateHoliday) >=  date(lms_parcel_actualleaddaysstartdate) and date(h.DateHoliday) <= date(lms_parcel_actualleaddaysenddate))\r\n",
							"GROUP BY p.lms_parcel_id\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 48
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stparcel_tmp4\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    p.*\r\n",
							"    ,(to_unix_timestamp(p.lms_parcel_actualleaddaysenddate) - to_unix_timestamp(p.lms_parcel_actualleaddaysstartdate) - coalesce(p1.holidays,0))/86400 as lms_parcel_actualleaddays\r\n",
							"FROM stparcel_tmp2 p\r\n",
							"LEFT JOIN stparcel_tmp3 p1 on p1.lms_parcel_id = p.lms_parcel_id"
						],
						"outputs": [],
						"execution_count": 49
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stparcel_tmp5\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    *\r\n",
							"    ,CASE \r\n",
							"        WHEN lms_parcel_parceldetaildeliverby is null THEN 'Not Consigned'\r\n",
							"        WHEN lms_parcel_waybillpoddate is null THEN 'Not Delivered'\r\n",
							"        WHEN (parcelagedifferenceinseconds/3600) < 12 THEN '< 12 hrs'\r\n",
							"        WHEN (parcelagedifferenceinseconds/3600) < 24 THEN '< 24 hrs'\r\n",
							"        WHEN (parcelagedifferenceinseconds/3600) < 48 THEN '< 2 days'\r\n",
							"        WHEN (parcelagedifferenceinseconds/3600) < 72 THEN '< 3 days'\r\n",
							"        WHEN (parcelagedifferenceinseconds/3600) < 96 THEN '< 4 days'\r\n",
							"        WHEN (parcelagedifferenceinseconds/3600) < 120 THEN '< 5 days'\r\n",
							"        WHEN (parcelagedifferenceinseconds/3600) < 144 THEN '< 6 days'\r\n",
							"        WHEN (parcelagedifferenceinseconds/3600) < 168 THEN '< 7 days'\r\n",
							"        WHEN (parcelagedifferenceinseconds/3600) > 168 THEN '> 7 days'\r\n",
							"        ELSE NULL\r\n",
							"    END as lms_parcel_age\r\n",
							"\r\n",
							"FROM stparcel_tmp4\r\n",
							""
						],
						"outputs": [],
						"execution_count": 50
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stparcel_tmp6\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"     p.*\r\n",
							"    ,v.lms_track_enddate\r\n",
							"    ,v.lms_track_floortimeend\r\n",
							"    ,v.lms_track_fromlocid\r\n",
							"    ,(to_unix_timestamp(p.lms_parcel_consignmentcdate)-to_unix_timestamp(p.lms_parcel_pdate)) as lms_parcel_pdatetocdate\r\n",
							"    ,(to_unix_timestamp(v.lms_track_enddate)-to_unix_timestamp(p.lms_parcel_consignmentcdate)) as lms_parcel_cdatetoenddate\r\n",
							"    ,(to_unix_timestamp(v.lms_track_enddate)-to_unix_timestamp(p.lms_parcel_pdate)) as lms_parcel_startdatetoenddate\r\n",
							"    ,(to_unix_timestamp(v.lms_track_floortimeend)-to_unix_timestamp(v.lms_track_enddate)) as lms_parcel_floortimeedndtoenddate\r\n",
							"    ,(to_unix_timestamp(p.lms_parcel_waybillpoddate)-to_unix_timestamp(v.lms_track_floortimeend)) as lms_parcel_deliveryduration\r\n",
							"    \r\n",
							"  \r\n",
							"FROM stparcel_tmp5 p\r\n",
							"LEFT JOIN stvsmparcel v ON v.lms_track_parcelid = p.lms_parcel_id"
						],
						"outputs": [],
						"execution_count": 51
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stparcelsplitconsign_1\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"p.lms_parcel_orderid\r\n",
							",p.lms_parcel_consignid\r\n",
							"FROM\r\n",
							"stparcel_tmp6 p\r\n",
							"\r\n",
							"GROUP BY\r\n",
							"p.lms_parcel_orderid,\r\n",
							"p.lms_parcel_consignid\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 52
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stparcelsplitconsign_2\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"p.lms_parcel_orderid\r\n",
							",CASE WHEN COUNT(*) > 1 THEN 1 ELSE 0 END AS lms_parcel_splitconsignmentflag\r\n",
							"\r\n",
							"\r\n",
							"FROM\r\n",
							"stparcelsplitconsign_1 p\r\n",
							"\r\n",
							"GROUP BY\r\n",
							"p.lms_parcel_orderid\r\n",
							""
						],
						"outputs": [],
						"execution_count": 53
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stparcelsplitdeliver_1\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"p.lms_parcel_orderid\r\n",
							",p.lms_parcel_waybillid\r\n",
							"FROM\r\n",
							"stparcel_tmp6 p\r\n",
							"\r\n",
							"GROUP BY\r\n",
							"p.lms_parcel_orderid\r\n",
							",p.lms_parcel_waybillid"
						],
						"outputs": [],
						"execution_count": 54
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stparcelsplitdeliver_2\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"p.lms_parcel_orderid\r\n",
							",CASE WHEN COUNT(*) > 1 THEN 1 ELSE 0 END AS lms_parcel_splitdeliveredflag\r\n",
							"\r\n",
							"FROM\r\n",
							"stparcelsplitdeliver_1 p\r\n",
							"\r\n",
							"GROUP BY\r\n",
							"p.lms_parcel_orderid"
						],
						"outputs": [],
						"execution_count": 55
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stparcel\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"p.*\r\n",
							",c.lms_parcel_splitconsignmentflag\r\n",
							",d.lms_parcel_splitdeliveredflag\r\n",
							"FROM\r\n",
							"stparcel_tmp6 p\r\n",
							"\r\n",
							"LEFT JOIN stparcelsplitconsign_2 c ON p.lms_parcel_orderid = c.lms_parcel_orderid\r\n",
							"LEFT JOIN stparcelsplitdeliver_2 d ON p.lms_parcel_orderid = d.lms_parcel_orderid\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 56
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stparcel = spark.sql(\"SELECT * FROM stparcel\")"
						],
						"outputs": [],
						"execution_count": 57
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stparcel.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stparcel.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 58
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"SUCCESS\")"
						],
						"outputs": [],
						"execution_count": 59
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/03_LH_STFinanceData')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Ops/Linehaul"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "71de3fcd-10b2-42f5-8b92-1dab8c16aa0e"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace/bigDataPools/DevSparkPool",
						"name": "DevSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/DevSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"source": [
							"# #Create DataFrame for the publicfinancedata TMS Table\r\n",
							"# publicfinancedata = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publicfinancedata.parquet', format='parquet')\r\n",
							"# publicfinancedata.createOrReplaceTempView(\"publicfinancedata\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stfinancedata\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    f.id as tms_financedata_id\r\n",
							"    ,f.bookingid as tms_financedata_bookingid\r\n",
							"    ,to_timestamp(f.insdate) as tms_financedata_insdate\r\n",
							"    ,f.peaksurchagrebuy as tms_financedata_peaksurchagrebuy\r\n",
							"    ,f.peaksurchagresell as tms_financedata_peaksurchagresell\r\n",
							"    ,f.manualentryroadhaulsell as tms_financedata_manualentryroadhaulsell\r\n",
							"    ,f.manualentryahocsell as tms_financedata_manualentryahocsell\r\n",
							"    ,f.systemfuellevy as tms_financedata_systemfuellevy\r\n",
							"    ,f.systemratefuellevy as tms_financedata_systemratefuellevy\r\n",
							"    ,f.systemratefuelexlevy as tms_financedata_systemratefuelexlevy\r\n",
							"    ,f.systemratefuelinclevy as tms_financedata_systemratefuelinclevy\r\n",
							"    ,f.systemsellrate as tms_financedata_systemsellrate\r\n",
							"    ,f.totalsellrate as tms_financedata_totalsellrate\r\n",
							"    ,to_timestamp(f.moddate) as tms_financedata_moddate\r\n",
							"    ,f.totalbuyrate as tms_financedata_totalbuyrate\r\n",
							"    ,f.rateprofileid as tms_financedata_rateprofileid\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"FROM publicfinancedata f\r\n",
							""
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stfinancedata = spark.sql(\"SELECT * FROM stfinancedata\")"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stfinancedata.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stfinancedata.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"SUCCESS\")"
						],
						"outputs": [],
						"execution_count": 19
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/03_STBusinessPartners')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Finance"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "630acbb2-df76-454b-9cd2-bf4104b52a73"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse_Prod/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace-prod/bigDataPools/PRDSparkPool",
						"name": "PRDSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/PRDSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"# #Create DataFrame for the dboOCRD SAP Table\r\n",
							"# dboOCRD = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dboOCRD.parquet', format='parquet')\r\n",
							"# dboOCRD.createOrReplaceTempView(\"dboOCRD\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stbusinnesspartner\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    ocrd.cardcode as sap_businesspartners_cardcode\r\n",
							"    ,ocrd.cardname  as sap_businesspartners_cardname \r\n",
							"    ,ocrd.cardtype as sap_businesspartners_cardtype\r\n",
							"    ,ocrd.groupcode as sap_businesspartners_groupcode\r\n",
							"    ,ocrd.cmpprivate as sap_businesspartners_cmpprivate\r\n",
							"    ,ocrd.groupnum as sap_businesspartners_groupnum\r\n",
							"    ,ocrd.creditline as sap_businesspartners_creditline\r\n",
							"    ,ocrd.debtline as sap_businesspartners_debtline\r\n",
							"    ,ocrd.discount as sap_businesspartners_discount\r\n",
							"    ,ocrd.vatstatus as sap_businesspartners_vatstatus\r\n",
							"    ,ocrd.lictradnum as sap_businesspartners_lictradnum\r\n",
							"    ,ocrd.city as sap_businesspartners_city\r\n",
							"    ,ocrd.u_boy_tb_0 as sap_businesspartners_u_boy_tb_0\r\n",
							"    ,ocrd.u_creditapp as sap_businesspartners_u_creditapp\r\n",
							"    ,ocrd.u_creditref as sap_businesspartners_u_creditref\r\n",
							"    ,ocrd.u_creditcontract as sap_businesspartners_u_creditcontract\r\n",
							"    ,ocrd.u_bee as sap_businesspartners_u_bee\r\n",
							"    ,ocrd.u_depotmanager as sap_businesspartners_u_depotmanager\r\n",
							"    ,ocrd.u_branchmanager as sap_businesspartners_u_branchmanager\r\n",
							"    ,ocrd.u_petty_cash as sap_businesspartners_u_petty_cash\r\n",
							"\r\n",
							"FROM dboOCRD ocrd\r\n",
							""
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stbusinnesspartner = spark.sql(\"SELECT * FROM stbusinnesspartner\")"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stbusinnesspartner.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stbusinnesspartner.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"SUCCESS\")"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/04_DIST_STOrder')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Ops/Distribution"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "5df0ee0c-5ea2-42d3-9b27-9189ebae64fe"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace/bigDataPools/DevSparkPool",
						"name": "DevSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/DevSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"# #Create DataFrame for the dboorder LMS Table\r\n",
							"# dboorder = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboorder.parquet', format='parquet')\r\n",
							"# dboorder.createOrReplaceTempView(\"dboorder\")\r\n",
							"\r\n",
							"# #Create DataFrame for the sstparcelbyorder LMS Table\r\n",
							"# sstparcelbyorder = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Semi Structured Data/OPS/sstparcelbyorder.parquet', format='parquet')\r\n",
							"# sstparcelbyorder.createOrReplaceTempView(\"sstparcelbyorder\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbowaybill LMS Table\r\n",
							"# dbowaybill = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbowaybill.parquet', format='parquet')\r\n",
							"# dbowaybill.createOrReplaceTempView(\"dbowaybill\")\r\n",
							"\r\n",
							"# #Create DataFrame for the sstparcelunion LMS \r\n",
							"# sstparcelunion = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Semi Structured Data/OPS/sstparcelunion.parquet', format='parquet')\r\n",
							"# sstparcelunion.createOrReplaceTempView(\"sstparcelunion\")\r\n",
							"\r\n",
							"# #Create DataFrame for the sstconsignmentunion LMS Table\r\n",
							"# sstconsignmentunion = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Semi Structured Data/OPS/sstconsignmentunion.parquet', format='parquet')\r\n",
							"# sstconsignmentunion.createOrReplaceTempView(\"sstconsignmentunion\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dboweekendholiday LMS Table\r\n",
							"# dboweekendholiday = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboweekendholiday.parquet', format='parquet')\r\n",
							"# dboweekendholiday.createOrReplaceTempView(\"dboweekendholiday\")"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"storder_tmp1\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    o.id as lms_order_id\r\n",
							"    ,o.COrderNo as lms_order_corderno\r\n",
							"    ,o.CustRef as lms_order_custref\r\n",
							"    ,o.PickUpCustID as lms_order_pickupcustid\r\n",
							"    ,o.DeliverCustID as lms_order_delivercustid\r\n",
							"    ,o.BillCustID as lms_order_billcustid\r\n",
							"    ,o.Weight as lms_order_weight\r\n",
							"    ,o.OValue as lms_order_ovalue\r\n",
							"    ,o.HODate as lms_order_hodate\r\n",
							"    ,o.CService as lms_order_cservice\r\n",
							"    ,o.CourierID as lms_order_courierid\r\n",
							"    ,o.CustService as lms_order_custservice\r\n",
							"    ,o.UserID as lms_order_userid\r\n",
							"    ,o.Appointment as lms_order_appointment\r\n",
							"    ,o.DeliverBy as lms_order_deliverby\r\n",
							"    ,o.DConfirmed as lms_order_dconfirmed\r\n",
							"    ,o.DComment as lms_order_dcomment\r\n",
							"    ,o.DRefNo as lms_order_drefno\r\n",
							"    ,o.DueDate as lms_order_duedate\r\n",
							"    ,o.BatchNo as lms_order_batchno\r\n",
							"    ,o.CustAccID as lms_order_custaccid\r\n",
							"    ,o.Airfreightflag as lms_order_airfreightflag\r\n",
							"    ,o.SpecDel as lms_order_specdel\r\n",
							"    ,o.Quote as lms_order_quote\r\n",
							"    ,o.QuoteWeight as lms_order_quoteweight\r\n",
							"    ,o.QuoteFuelLevy as lms_order_quotefuellevy\r\n",
							"    ,o.QuoteOtherSurcharges as lms_order_quoteothersurcharges\r\n",
							"    ,o.QuoteDocFees as lms_order_quotedocfees\r\n",
							"    ,o.Bill_VehicleType as lms_order_billvehicletype\r\n",
							"\r\n",
							"    ,pbo.lms_parcelbyorder_aggrweight\r\n",
							"    ,pbo.lms_parcelbyorder_aggrchargeweight\r\n",
							"    ,pbo.lms_parcelbyorder_aggrvolweight\r\n",
							"    ,pbo.lms_parcelbyorder_aggrvolumiserweight\r\n",
							"    ,pbo.lms_parcelbyorder_aggrmaxweight\r\n",
							"    ,pbo.lms_parcelbyorder_aggrnoparcels\r\n",
							"    ,pbo.lms_parcelbyorder_aggrweightexclspecdel\r\n",
							"    ,pbo.lms_parcelbyorder_aggrchargeweightexclspecdel\r\n",
							"    ,pbo.lms_parcelbyorder_aggrvolweightexclspecdel\r\n",
							"    ,pbo.lms_parcelbyorder_aggrvolumiserweightexclspecdel\r\n",
							"    ,pbo.lms_parcelbyorder_aggrmaxweightexclspecdel\r\n",
							"    ,pbo.lms_parcelbyorder_aggrnoparcelsexclspecdel\r\n",
							"\r\n",
							"FROM dboorder o\r\n",
							"LEFT JOIN sstparcelbyorder pbo on pbo.lms_parcelbyorder_orderid =  o.id\r\n",
							""
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"storder_tmp2\r\n",
							"AS\r\n",
							"SELECT DISTINCT\r\n",
							"\r\n",
							"p.orderid,\r\n",
							"max(w.poddate) as lms_order_maxpoddate,\r\n",
							"min(c.deliverby) as lms_order_mindeliverby\r\n",
							"\r\n",
							"FROM  sstparcelunion p\r\n",
							"LEFT JOIN dbowaybill w on w.id = p.waybillid\r\n",
							"LEFT JOIN sstconsignmentunion c on c.id = p.consignid\r\n",
							"WHERE p.orderid is not null and p.consignid is not null\r\n",
							"GROUP BY p.orderid\r\n",
							""
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"storder_tmp3\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    o.*\r\n",
							"    ,o2.lms_order_maxpoddate\r\n",
							"    ,CASE \r\n",
							"\t    WHEN (coalesce(o2.lms_order_maxpoddate,CURRENT_TIMESTAMP)) <= (date(o2.lms_order_mindeliverby) + interval '86399 seconds') THEN 1\r\n",
							"\t    ELSE 0\r\n",
							"    END lms_order_inserviceflag\r\n",
							"    ,to_timestamp((cast(to_date(o.lms_order_hodate,'yyyy-MM-dd') as string)||' 23:59:59.999')) as  lms_order_actualleaddaysstartdate\r\n",
							"    ,coalesce(o2.lms_order_maxpoddate,CURRENT_TIMESTAMP) AS lms_order_actualleaddaysenddate\r\n",
							"    ,(to_unix_timestamp(coalesce(o2.lms_order_maxpoddate,CURRENT_TIMESTAMP)) - (to_unix_timestamp(to_timestamp((cast(to_date(o.lms_order_hodate,'yyyy-MM-dd') as string)||' 23:59:59.999')))))/86400 as lms_order_actualleaddaysinclwe\r\n",
							"    ,to_timestamp((cast(to_date(o.lms_order_hodate,'yyyy-MM-dd') as string)||' 23:59:59.999')) lms_order_hodateroundup\r\n",
							"    ,(to_unix_timestamp(coalesce(o2.lms_order_maxpoddate,CURRENT_TIMESTAMP)) - to_unix_timestamp(to_timestamp((cast(to_date(o.lms_order_hodate,'yyyy-MM-dd') as string)||' 23:59:59.999')))) as orderagedifferenceinseconds\r\n",
							"\r\n",
							"FROM storder_tmp1 o\r\n",
							"LEFT JOIN storder_tmp2 o2 ON o2.orderid = o.lms_order_id"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"storder_tmp4\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    o.lms_order_id\r\n",
							"    ,sum(holiday)*86400 as holidays\r\n",
							"FROM storder_tmp3 o\r\n",
							"INNER JOIN dboweekendholiday h on (date(h.DateHoliday) >=  date(lms_order_actualleaddaysstartdate) and date(h.DateHoliday) <= date(lms_order_actualleaddaysenddate))\r\n",
							"GROUP BY o.lms_order_id"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"storder_tmp5\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    o.*\r\n",
							"    ,(to_unix_timestamp(o.lms_order_actualleaddaysenddate) - to_unix_timestamp(o.lms_order_actualleaddaysstartdate) - coalesce(o1.holidays,0))/86400  as lms_order_actualleaddays\r\n",
							"FROM storder_tmp3 o\r\n",
							"LEFT JOIN storder_tmp4 o1 on o1.lms_order_id = o.lms_order_id"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"storder\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    *\r\n",
							"    ,CASE \r\n",
							"        WHEN lms_order_hodate is null THEN 'Not Ordered'\r\n",
							"        WHEN lms_order_maxpoddate is null THEN 'Not Delivered'\r\n",
							"        WHEN (orderagedifferenceinseconds/3600) < 12 THEN '< 12 hrs'\r\n",
							"        WHEN (orderagedifferenceinseconds/3600) < 24 THEN '< 24 hrs'\r\n",
							"        WHEN (orderagedifferenceinseconds/3600) < 48 THEN '< 2 days'\r\n",
							"        WHEN (orderagedifferenceinseconds/3600) < 72 THEN '< 3 days'\r\n",
							"        WHEN (orderagedifferenceinseconds/3600) < 96 THEN '< 4 days'\r\n",
							"        WHEN (orderagedifferenceinseconds/3600) < 120 THEN '< 5 days'\r\n",
							"        WHEN (orderagedifferenceinseconds/3600) < 144 THEN '< 6 days'\r\n",
							"        WHEN (orderagedifferenceinseconds/3600) < 168 THEN '< 7 days'\r\n",
							"        WHEN (orderagedifferenceinseconds/3600) > 168 THEN '> 7 days'\r\n",
							"        ELSE NULL\r\n",
							"    END as lms_order_age\r\n",
							"\r\n",
							"FROM storder_tmp5\r\n",
							""
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"storder = spark.sql(\"SELECT * FROM storder\")"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# storder.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/storder.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"SUCCESS\")"
						],
						"outputs": [],
						"execution_count": 11
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/04_LH_STInstruction')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Ops/Linehaul"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "39d09e0e-aaa7-4b5b-aa95-d28bf896430e"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace/bigDataPools/DevSparkPool",
						"name": "DevSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/DevSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 28
					},
					{
						"cell_type": "code",
						"source": [
							"# #Create DataFrame for the publicInstruction TMS Table\r\n",
							"# publicInstruction = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publicinstruction.parquet', format='parquet')\r\n",
							"# publicInstruction.createOrReplaceTempView(\"publicInstruction\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dboLHInstruction LMS Table\r\n",
							"# dboLHInstruction = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbolh_instruction.parquet', format='parquet')\r\n",
							"# dboLHInstruction.createOrReplaceTempView(\"dboLHInstruction\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publicLHInstruction LMS Table\r\n",
							"# publicLHInstruction = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publiclhdelivery.parquet', format='parquet')\r\n",
							"# publicLHInstruction.createOrReplaceTempView(\"publicLHInstruction\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stbooking TMSLMS combine Table\r\n",
							"# stbooking = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stbooking.parquet', format='parquet')\r\n",
							"# stbooking.createOrReplaceTempView(\"stbooking\")"
						],
						"outputs": [],
						"execution_count": 29
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stinstruction_tms\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    i.id as tms_instruction_id\r\n",
							"    ,i.bookingid as tms_instruction_bookingid\r\n",
							"    ,i.addressid as tms_instruction_addressid\r\n",
							"    ,i.instrtype as tms_instruction_instrtype\r\n",
							"    ,i.onroute as tms_instruction_onroute\r\n",
							"    ,i.onrouteid as tms_instruction_onrouteid\r\n",
							"    ,i.sort as tms_instruction_sort\r\n",
							"    ,i.onroutestatus as tms_instruction_onroutestatus\r\n",
							"    ,i.note as tms_instruction_note\r\n",
							"    ,i.citybranchid as tms_instruction_citybranchid\r\n",
							"    ,i.citybranchname as tms_instruction_citybranchname\r\n",
							"    ,to_timestamp(i.insdate) as tms_instruction_insdate\r\n",
							"    ,to_timestamp(i.moddate) as tms_instruction_moddate\r\n",
							"    ,i.insby as tms_instruction_insby\r\n",
							"    ,CASE \r\n",
							"        WHEN i.instrtype = \"pickup\" THEN tms_booking_pickupdate\r\n",
							"        WHEN i.instrtype = \"delivery\" THEN tms_booking_deliverydate\r\n",
							"    END as tms_instruction_bookingpickupdeliverydate\r\n",
							"    ,CASE \r\n",
							"        WHEN i.duedate < 0 THEN NULL \r\n",
							"        ELSE to_timestamp(i.duedate) \r\n",
							"    END as tms_instruction_duedate\r\n",
							"    ,to_timestamp(i.actualdate) as tms_instruction_actualdate\r\n",
							"\r\n",
							"\r\n",
							"FROM publicinstruction i\r\n",
							"LEFT JOIN stbooking b on b.tms_booking_id = i.bookingid\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 30
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stinstruction_tms = spark.sql(\"SELECT * FROM stinstruction_tms\")"
						],
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stinstruction_lms \r\n",
							"AS\r\n",
							"\r\n",
							"SELECT    \r\n",
							"    CAST(CONCAT('-',CAST(i.id as string)) as int) as tms_instruction_id\r\n",
							"    ,CAST(CONCAT('-',CAST(i.bookingid as string)) as int) as tms_instruction_bookingid\r\n",
							"    ,CAST(CONCAT('-',CAST(i.addressid as string)) as int) as tms_instruction_addressid\r\n",
							"    ,CASE\r\n",
							"        WHEN i.instructiontypeid = 1 THEN 'pickup'\r\n",
							"        ELSE 'delivery'    \r\n",
							"    END as tms_instruction_instrtype\r\n",
							"    ,null as tms_instruction_onroute\r\n",
							"    ,null as tms_instruction_onrouteid\r\n",
							"    ,i.sequence as tms_instruction_sort\r\n",
							"    ,null as tms_instruction_onroutestatus\r\n",
							"    ,null as tms_instruction_note\r\n",
							"    ,null as tms_instruction_citybranchid\r\n",
							"    ,null as tms_instruction_citybranchname\r\n",
							"    ,i.createddate as tms_instruction_insdate\r\n",
							"    ,i.createddate as tms_instruction_moddate\r\n",
							"    ,CAST(CONCAT('-',CAST(i.userid as string)) as int) as tms_instruction_insby\r\n",
							"    ,CASE \r\n",
							"        WHEN i.instructiontypeid = 1 THEN tms_booking_pickupdate\r\n",
							"        ELSE tms_booking_deliverydate\r\n",
							"    END as tms_instruction_bookingpickupdeliverydate\r\n",
							"\r\n",
							"FROM dbolhinstruction i\r\n",
							"LEFT JOIN stbooking b on b.tms_booking_id = CAST(CONCAT('-',CAST(i.bookingid as string)) as int)"
						],
						"outputs": [],
						"execution_count": 32
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stinstruction_lms = spark.sql(\"SELECT * FROM stinstruction_lms\")"
						],
						"outputs": [],
						"execution_count": 33
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stinstruction_tmp = stinstruction_lms.unionByName(stinstruction_tms, allowMissingColumns = True)\r\n",
							"stinstruction_tmp.createOrReplaceTempView(\"stinstruction_tmp\")"
						],
						"outputs": [],
						"execution_count": 34
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stinstruction_tmp2\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    si.tms_instruction_id\r\n",
							"    ,si.tms_instruction_bookingid\r\n",
							"    ,si.tms_instruction_addressid\r\n",
							"    ,si.tms_instruction_instrtype\r\n",
							"    ,si.tms_instruction_onroute\r\n",
							"    ,si.tms_instruction_onrouteid\r\n",
							"    ,si.tms_instruction_sort\r\n",
							"    ,si.tms_instruction_onroutestatus\r\n",
							"    ,si.tms_instruction_note\r\n",
							"    ,si.tms_instruction_citybranchid\r\n",
							"    ,si.tms_instruction_citybranchname\r\n",
							"    ,si.tms_instruction_insdate\r\n",
							"    ,si.tms_instruction_moddate\r\n",
							"    ,si.tms_instruction_insby\r\n",
							"    ,si.tms_instruction_duedate\r\n",
							"    ,si.tms_instruction_actualdate\r\n",
							"    ,to_timestamp(si.tms_instruction_bookingpickupdeliverydate) as tms_instruction_bookingpickupdeliverydate\r\n",
							"    ,i.id as orv_lhdelivery_id\r\n",
							"    ,i.did as orv_lhdelivery_did\r\n",
							"    ,i.lat as orv_lhdelivery_lat\r\n",
							"    ,i.lng as orv_lhdelivery_lng\r\n",
							"    ,to_timestamp(i.actualeta) as orv_lhdelivery_actualeta\r\n",
							"    ,to_timestamp(i.eta) as orv_lhdelivery_eta\r\n",
							"    ,to_timestamp(i.tsgeofenceenter) as orv_lhdelivery_tsgeofenceenter\r\n",
							"    ,to_timestamp(i.tsdocuments) as orv_lhdelivery_tsdocuments\r\n",
							"    ,i.customerref1 as orv_lhdelivery_customerref1\r\n",
							"    ,i.customerref2 as orv_lhdelivery_customerref2\r\n",
							"    ,i.trailersize as orv_lhdelivery_trailersize\r\n",
							"    ,i.cargoweight as orv_lhdelivery_cargoweight\r\n",
							"    ,i.cubicvolume as orv_lhdelivery_cubicvolume\r\n",
							"    ,i.geocodingstatus as orv_lhdelivery_geocodingstatus\r\n",
							"    ,i.status as orv_lhdelivery_status\r\n",
							"    ,i.optimizedorder as orv_lhdelivery_optimizedorder\r\n",
							"    ,i.triporder as orv_lhdelivery_triporder\r\n",
							"    ,i.sequence as orv_lhdelivery_sequence\r\n",
							"    ,i.bookingid as orv_lhdelivery_bookingid\r\n",
							"    ,i.grv as orv_lhdelivery_grv\r\n",
							"    ,i.chepslip as orv_lhdelivery_chepslip\r\n",
							"    ,i.type as orv_lhdelivery_type\r\n",
							"    ,to_timestamp(i.insdate) as orv_lhdelivery_insdate\r\n",
							"    ,i.isdirect as orv_lhdelivery_isdirect\r\n",
							"    ,i.notes as orv_lhdelivery_notes\r\n",
							"    ,i.contactperson as orv_lhdelivery_contactperson\r\n",
							"    ,i.contactnumber as orv_lhdelivery_contactnumber\r\n",
							"    ,i.trailerfleetcode as orv_lhdelivery_trailerfleetcode\r\n",
							"    ,i.primarycustomerid as orv_lhdelivery_primarycustomerid\r\n",
							"    ,i.pcustomername as orv_lhdelivery_pcustomername\r\n",
							"    ,i.pcustomeraccountnumber as orv_lhdelivery_pcustomeraccountnumber\r\n",
							"    ,i.pcustomerrevenuecode as orv_lhdelivery_pcustomerrevenuecode\r\n",
							"    ,i.secondarycustomerid as orv_lhdelivery_secondarycustomerid\r\n",
							"    ,i.scustomername as orv_lhdelivery_scustomername\r\n",
							"    ,i.scustomeraccountnumber as orv_lhdelivery_scustomeraccountnumber\r\n",
							"    ,i.scustomerrevenuecode as orv_lhdelivery_scustomerrevenuecode\r\n",
							"    ,to_timestamp(i.actiondate) as orv_lhdelivery_actiondate\r\n",
							"    ,to_timestamp(i.tsgeofenceexit) as orv_lhdelivery_tsgeofenceexit\r\n",
							"    ,i.pctmsid as orv_lhdelivery_pctmsid\r\n",
							"    ,i.sctmsid as orv_lhdelivery_sctmsid\r\n",
							"    ,i.customorder as orv_lhdelivery_customorder\r\n",
							"    ,to_timestamp(i.skipdate) as orv_lhdelivery_skipdate\r\n",
							"    ,i.skipreason as orv_lhdelivery_skipreason\r\n",
							"    ,i.outofgeofencereason as orv_lhdelivery_outofgeofencereason\r\n",
							"    ,to_timestamp(i.tsarrivedcustomer) as orv_lhdelivery_tsarrivedcustomer\r\n",
							"    ,to_timestamp(i.tsdroppedoff) as orv_lhdelivery_tsdroppedoff\r\n",
							"    ,to_timestamp(i.tsstartoffloading) as orv_lhdelivery_tsstartoffloading\r\n",
							"    ,to_timestamp(i.tsstartloading) as orv_lhdelivery_tsstartloading\r\n",
							"    ,to_timestamp(i.tsloaded) as orv_lhdelivery_tsloaded\r\n",
							"    ,to_timestamp(i.tsfinishedoffloading) as orv_lhdelivery_tsfinishedoffloading\r\n",
							"    ,to_timestamp(i.tsdocumnetshanddovedriver) as orv_lhdelivery_tsdocumnetshanddovedriver\r\n",
							"    ,to_timestamp(i.tsleavingcustomer) as orv_lhdelivery_tsleavingcustomer\r\n",
							"    ,i.actionsubtype as orv_lhdelivery_actionsubtype\r\n",
							"    ,to_timestamp(i.tsstart) as orv_lhdelivery_tsstart\r\n",
							"    ,i.signame as orv_lhdelivery_signame\r\n",
							"    ,i.signcomment as orv_lhdelivery_signcomment\r\n",
							"    ,to_timestamp(i.signdate) as orv_lhdelivery_signdate\r\n",
							"    ,to_timestamp(i.tsendtate) as orv_lhdelivery_tsendtate\r\n",
							"    ,to_timestamp(i.tsentergeofenceapp) as orv_lhdelivery_tsentergeofenceapp\r\n",
							"    ,to_timestamp(i.tsexitgeofenceapp) as orv_lhdelivery_tsexitgeofenceapp\r\n",
							"    ,i.tmsid as orv_lhdelivery_tmsid\r\n",
							"    ,i.subtype as orv_lhdelivery_subtype\r\n",
							"    ,i.needbotrailerver as orv_lhdelivery_needbotrailerver\r\n",
							"    ,i.secondtrailerfleetcode as orv_lhdelivery_secondtrailerfleetcode\r\n",
							"    ,i.trailerverlat as orv_lhdelivery_trailerverlat\r\n",
							"    ,i.trailerverlng as orv_lhdelivery_trailerverlng\r\n",
							"    ,i.tstrailerver as orv_lhdelivery_tstrailerver\r\n",
							"    ,i.podlat as orv_lhdelivery_podlat\r\n",
							"    ,i.podlng as orv_lhdelivery_podlng\r\n",
							"    ,i.skiplat as orv_lhdelivery_skiplat\r\n",
							"    ,i.skiplng as orv_lhdelivery_skiplng\r\n",
							"    ,to_timestamp(i.uncanceldate) as orv_lhdelivery_uncanceldate\r\n",
							"    --,i.citybranchname as orv_lhdelivery_citybranchname\r\n",
							"\r\n",
							"FROM stinstruction_tmp si\r\n",
							"LEFT JOIN publiclhinstruction i on i.tmsid = si.tms_instruction_id\r\n",
							""
						],
						"outputs": [],
						"execution_count": 35
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stinstruction_tmp2 = spark.sql(\"SELECT * FROM stinstruction_tmp2\")"
						],
						"outputs": [],
						"execution_count": 36
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stinstruction\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"*\r\n",
							",(to_unix_timestamp(orv_lhdelivery_tsleavingcustomer)-to_unix_timestamp(orv_lhdelivery_tsarrivedcustomer)) as tms_instruction_inputpickupduration\r\n",
							",(to_unix_timestamp(orv_lhdelivery_tsloaded)-to_unix_timestamp(orv_lhdelivery_tsstartloading)) as tms_instruction_loadingduration\r\n",
							",(to_unix_timestamp(orv_lhdelivery_tsgeofenceexit)-to_unix_timestamp(orv_lhdelivery_tsgeofenceenter)) as tms_instruction_geopickupduration\r\n",
							",(to_unix_timestamp(orv_lhdelivery_tsendtate)-to_unix_timestamp(tms_instruction_insdate)) as tms_instruction_actionpickupduration\r\n",
							",CASE \r\n",
							"    WHEN orv_lhdelivery_tsarrivedcustomer is null THEN 'unknown'\r\n",
							"    WHEN orv_lhdelivery_tsarrivedcustomer < tms_instruction_bookingpickupdeliverydate THEN 'inservice'\r\n",
							"    ELSE 'outofservice'\r\n",
							"END tms_instruction_driverinputservice        \r\n",
							",CASE\r\n",
							"    WHEN orv_lhdelivery_tsentergeofenceapp is null THEN 'unknown'\r\n",
							"    WHEN orv_lhdelivery_tsentergeofenceapp < tms_instruction_bookingpickupdeliverydate THEN 'inservice'\r\n",
							"    ELSE 'outofservice'   \r\n",
							"END tms_instruction_geofenceservice\r\n",
							"\r\n",
							"FROM stinstruction_tmp2\r\n",
							""
						],
						"outputs": [],
						"execution_count": 37
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stinstruction = spark.sql(\"SELECT * FROM stinstruction\")"
						],
						"outputs": [],
						"execution_count": 38
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stinstruction.write.parquet('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Structured Data/OPS/stinstruction.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 39
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"SUCCESS\")"
						],
						"outputs": [],
						"execution_count": 40
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/04_STSalesInvoices')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Finance"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "5aa9e80b-ecaa-48ca-81bc-28d767b5733f"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse_Prod/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace-prod/bigDataPools/PRDSparkPool",
						"name": "PRDSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/PRDSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							"# #Create DataFrame for the dboOINV SAP Table\r\n",
							"# dboOINV = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dboOINV.parquet', format='parquet')\r\n",
							"# dboOINV.createOrReplaceTempView(\"dboOINV\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dboINV1 SAP Table\r\n",
							"# dboINV1 = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dboINV1.parquet', format='parquet')\r\n",
							"# dboINV1.createOrReplaceTempView(\"dboINV1\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stsalesinvoice\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"\r\n",
							"    oinv.docentry as sap_salesinvoiceheader_docentry\r\n",
							"    ,oinv.docnum as sap_salesinvoiceheader_docnum\r\n",
							"    ,oinv.doctype as sap_salesinvoiceheader_doctype\r\n",
							"    ,oinv.canceled as sap_salesinvoiceheader_canceled\r\n",
							"    ,oinv.objtype as sap_salesinvoiceheader_objtype\r\n",
							"    ,oinv.docdate as sap_salesinvoiceheader_docdate\r\n",
							"    ,oinv.docduedate as sap_salesinvoiceheader_docduedate\r\n",
							"    ,oinv.cardcode as sap_salesinvoiceheader_cardcode\r\n",
							"    ,oinv.cardname as sap_salesinvoiceheader_cardname\r\n",
							"    ,oinv.numatcard as sap_salesinvoiceheader_numatcard\r\n",
							"    ,oinv.discprcnt as sap_salesinvoiceheader_discprcnt\r\n",
							"    ,oinv.vatsum as sap_salesinvoiceheader_vatsum\r\n",
							"    ,oinv.discsum as sap_salesinvoiceheader_discsum\r\n",
							"    ,oinv.doctotal as sap_salesinvoiceheader_doctotal\r\n",
							"    ,oinv.comments as sap_salesinvoiceheader_comments\r\n",
							"    ,oinv.transid as sap_salesinvoiceheader_transid\r\n",
							"    ,oinv.usersign as sap_salesinvoiceheader_usersign\r\n",
							"    ,oinv.u_depot as sap_salesinvoiceheader_u_depot\r\n",
							"    ,oinv.u_usercode as sap_salesinvoiceheader_u_usercode\r\n",
							"    ,oinv.u_expense_depot as sap_salesinvoiceheader_u_expense_depot\r\n",
							"    ,oinv.u_transaction_date as sap_salesinvoiceheader_u_transaction_date\r\n",
							"    ,oinv.rounddif as sap_salesinvoiceheader_rounddif\r\n",
							"    ,oinv.rounding as sap_salesinvoiceheader_rounding\r\n",
							"    ,inv1.linenum as sap_salesinvoiceline_linenum\r\n",
							"    ,inv1.linestatus as sap_salesinvoiceline_linestatus\r\n",
							"    ,inv1.itemcode as sap_salesinvoiceline_itemcode\r\n",
							"    ,inv1.dscription as sap_salesinvoiceline_dscription\r\n",
							"    ,inv1.quantity as sap_salesinvoiceline_quantity\r\n",
							"    ,inv1.openqty as sap_salesinvoiceline_openqty\r\n",
							"    ,inv1.price as sap_salesinvoiceline_price\r\n",
							"    ,inv1.rate as sap_salesinvoiceline_rate\r\n",
							"    ,inv1.discprcnt as sap_salesinvoiceline_discprcnt\r\n",
							"    ,inv1.linetotal as sap_salesinvoiceline_linetotal\r\n",
							"    ,inv1.opensum as sap_salesinvoiceline_opensum\r\n",
							"    ,inv1.pricebefdi as sap_salesinvoiceline_pricebefdi\r\n",
							"    ,inv1.docdate as sap_salesinvoiceline_docdate\r\n",
							"    ,inv1.project as sap_salesinvoiceline_project\r\n",
							"    ,inv1.vatprcnt as sap_salesinvoiceline_vatprcnt\r\n",
							"    ,inv1.volume as sap_salesinvoiceline_volume\r\n",
							"    ,inv1.vatgroup as sap_salesinvoiceline_vatgroup\r\n",
							"    ,inv1.vatsum as sap_salesinvoiceline_vatsum\r\n",
							"    ,inv1.grssprofit as sap_salesinvoiceline_grssprofit\r\n",
							"    ,inv1.u_reason as sap_salesinvoiceline_u_reason\r\n",
							"    ,inv1.u_lane as sap_salesinvoiceline_u_lane\r\n",
							"    ,inv1.u_expenseitem as sap_salesinvoiceline_u_expenseitem\r\n",
							"    ,inv1.u_fueldate as sap_salesinvoiceline_u_fueldate\r\n",
							"    ,inv1.acctcode as sap_salesinvoiceline_acctcode\r\n",
							"\r\n",
							"\r\n",
							"FROM dboOINV oinv\r\n",
							"LEFT JOIN  dboINV1 inv1 on inv1.docentry = oinv.docentry"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stsalesinvoice = spark.sql(\"SELECT * FROM stsalesinvoice\")"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stsalesinvoice.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stsalesinvoice.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"SUCCESS\")"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/05_DIST_STConsignment')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Ops/Distribution"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "69aecf45-b9af-4d21-a0f1-7d2b9d71ac9b"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace/bigDataPools/DevSparkPool",
						"name": "DevSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/DevSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"source": [
							"# #create dataframe for the sstconsignemntunion lms table\r\n",
							"# sstconsignemntunion = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Semi Structured Data/OPS/sstconsignmentunion.parquet/', format='parquet')\r\n",
							"# sstconsignemntunion.createOrReplaceTempView(\"sstconsignemntunion\")\r\n",
							"\r\n",
							"# #create dataframe for the sstparcelbyconsignment lms table\r\n",
							"# sstparcelbyconsignment = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Semi Structured Data/OPS/sstparcelbyconsignment.parquet', format='parquet')\r\n",
							"# sstparcelbyconsignment.createOrReplaceTempView(\"sstparcelbyconsignment\")\r\n",
							"\r\n",
							"# #create dataframe for the dbobill_consignmentr lms table\r\n",
							"# dbobill_consignmentr = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbobill_consignmentr.parquet', format='parquet')\r\n",
							"# dbobill_consignmentr.createOrReplaceTempView(\"dbobill_consignmentr\")\r\n",
							"\r\n",
							"# #create dataframe for the dbobill_billcustomersr lms table\r\n",
							"# dbobill_billcustomersr = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbobill_billcustomersr.parquet', format='parquet')\r\n",
							"# dbobill_billcustomersr.createOrReplaceTempView(\"dbobill_billcustomersr\")\r\n",
							"\r\n",
							"# #create dataframe for the stparcel lms table\r\n",
							"# stparcel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stparcel.parquet', format='parquet')\r\n",
							"# stparcel.createOrReplaceTempView(\"stparcel\")\r\n",
							"\r\n",
							"# #create dataframe for the dbowaybill lms table\r\n",
							"# dbowaybill = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbowaybill.parquet', format='parquet')\r\n",
							"# dbowaybill.createOrReplaceTempView(\"dbowaybill\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dboweekendholiday LMS Table\r\n",
							"# dboweekendholiday = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboweekendholiday.parquet', format='parquet')\r\n",
							"# dboweekendholiday.createOrReplaceTempView(\"dboweekendholiday\")\r\n",
							"\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import *\r\n",
							"from pyspark.sql.window import Window"
						],
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"partition_on_consign = Window.partitionBy(\"ConsignID\").orderBy(col(\"ConsignID\").asc()).orderBy(col(\"id\").desc())\r\n",
							"dbobill_consignmentr = dbobill_consignmentr.withColumn(\"rn\",row_number().over(partition_on_consign))\r\n",
							"dbobill_consignmentr.createOrReplaceTempView(\"dbobill_consignmentr\")"
						],
						"outputs": [],
						"execution_count": 27
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"\r\n",
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stconsignment_tmp1\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"\r\n",
							"    c.id as lms_consignment_id\r\n",
							"    ,c.CRef as lms_consignment_cref\r\n",
							"    ,c.CDate as lms_consignment_cdate\r\n",
							"    ,c.UserID as lms_consignment_userid\r\n",
							"    ,c.Appointment as lms_consignment_appointment\r\n",
							"    ,c.DeliverBy as lms_consignment_deliverby\r\n",
							"    ,c.DeliverByOriginal as lms_consignment_deliverbyoriginal\r\n",
							"    ,c.CustAccID as lms_consignment_custaccid\r\n",
							"    ,c.BillCust as lms_consignment_billcust\r\n",
							"    ,c.DeliverCustID as lms_consignment_delivercustid\r\n",
							"    ,c.InServiceReasonID as lms_consignment_inservicereasonid\r\n",
							"    ,c.InServiceComment as lms_consignment_inservicecomment\r\n",
							"    ,c.LastUpdate as lms_consignment_lastupdate\r\n",
							"    ,c.CollectID as lms_consignment_collectid\r\n",
							"    ,c.DeliverTypeID as lms_consignment_delivertypeid\r\n",
							"    ,c.BillTypeID as lms_consignment_billtypeid\r\n",
							"    ,c.LoadID as lms_consignment_loadid\r\n",
							"    ,c.TotalCharge as lms_consignment_totalcharge\r\n",
							"    ,c.Charge as lms_consignment_charge\r\n",
							"    ,c.DocFees as lms_consignment_docfees\r\n",
							"    ,c.FuelLevy as lms_consignment_fuellevy\r\n",
							"    ,c.TotOtherSurcharges as lms_consignment_totothersurcharges\r\n",
							"    ,pbc.lms_parcelbyconsignment_aggrweight\r\n",
							"    ,pbc.lms_parcelbyconsignment_aggrchargeweight\r\n",
							"    ,pbc.lms_parcelbyconsignment_aggrvolweight\r\n",
							"    ,pbc.lms_parcelbyconsignment_aggrvolumiserweight\r\n",
							"    ,pbc.lms_parcelbyconsignment_aggrmaxweight\r\n",
							"    ,pbc.lms_parcelbyconsignment_aggrnoparcels\r\n",
							"    ,pbc.lms_parcelbyconsignment_aggrweightexclspecdel\r\n",
							"    ,pbc.lms_parcelbyconsignment_aggrchargeweightexclspecdel\r\n",
							"    ,pbc.lms_parcelbyconsignment_aggrvolweightexclspecdel\r\n",
							"    ,pbc.lms_parcelbyconsignment_aggrvolumiserweightexclspecdel\r\n",
							"    ,pbc.lms_parcelbyconsignment_aggrmaxweightexclspecdel\r\n",
							"    ,pbc.lms_parcelbyconsignment_aggrnoparcelsexclspecdel\r\n",
							"    ,bcr.id as lms_billconsignmentr_id\r\n",
							"    ,bcr.ConsignID as lms_billconsignmentr_consignid\r\n",
							"    ,bcr.UpdateDate as lms_billconsignmentr_updatedate\r\n",
							"    ,bcr.PickUpRouteID as lms_billconsignmentr_pickuprouteid\r\n",
							"    ,bcr.DeliverRouteID as lms_billconsignmentr_deliverrouteid\r\n",
							"    ,bcr.ChargeWeight as lms_billconsignmentr_chargeweight\r\n",
							"    ,bcr.NoOfParcels as lms_billconsignmentr_noofparcels\r\n",
							"    ,bcr.TripDistance as lms_billconsignmentr_tripdistance\r\n",
							"    ,bcr.ConsignmentValue as lms_billconsignmentr_consignmentvalue\r\n",
							"    ,bcr.BillCust as lms_billconsignmentr_billcust\r\n",
							"    ,bcr.RedFlag as lms_billconsignmentr_redflag\r\n",
							"    ,bcr.UpdInd as lms_billconsignmentr_updind\r\n",
							"    ,bcr.ServiceTypeID as lms_billconsignmentr_servicetypeid\r\n",
							"    ,bcr.RouteMissingInd as lms_billconsignmentr_routemissingind\r\n",
							"    ,bcr.ChargesCalcInd as lms_billconsignmentr_chargescalcind\r\n",
							"    ,bcr.CDate1 as lms_billconsignmentr_cdate1\r\n",
							"    ,bcr.InvalidPCode as lms_billconsignmentr_invalidpcode\r\n",
							"    ,bcr.ProgramNo as lms_billconsignmentr_programno\r\n",
							"    ,bcr.TotRate as lms_billconsignmentr_totrate\r\n",
							"    ,bcr.RouteCharge as lms_billconsignmentr_routecharge\r\n",
							"    ,bcr.MinWeight as lms_billconsignmentr_minweight\r\n",
							"    ,bcr.MinRate as lms_billconsignmentr_minrate\r\n",
							"    ,bcr.COrderNo as lms_billconsignmentr_corderno\r\n",
							"    ,bcr.CustRef as lms_billconsignmentr_custref\r\n",
							"    ,bcr.ManualUpdateFlag as lms_billconsignmentr_manualupdateflag\r\n",
							"    ,bcr.PrevBillCustID as lms_billconsignmentr_prevbillcustid\r\n",
							"    ,bcr.SpecDelVehicleID as lms_billconsignmentr_specdelvehicleid\r\n",
							"    ,bcr.SpecDelMissingRouteFlag as lms_billconsignmentr_specdelmissingrouteflag\r\n",
							"    ,bcr.OverBorderFlag as lms_billconsignmentr_overborderflag\r\n",
							"    ,bcr.SameDeliverID as lms_billconsignmentr_samedeliverid\r\n",
							"    ,bcr.CombineRouteCharge as lms_billconsignmentr_combineroutecharge\r\n",
							"    ,bcr.WaybillID as lms_billconsignmentr_waybillid\r\n",
							"    ,bcr.RouteLineCount as lms_billconsignmentr_routelinecount\r\n",
							"    ,bcr.SameDelCount as lms_billconsignmentr_samedelcount\r\n",
							"    ,bcr.AWeight as lms_billconsignmentr_aweight\r\n",
							"    ,bcr.VWeight as lms_billconsignmentr_vweight\r\n",
							"    ,bcr.CustRef3 as lms_billconsignmentr_custref3\r\n",
							"    ,bcr.Units as lms_billconsignmentr_units\r\n",
							"    ,bcr.TotSurcharges as lms_billconsignmentr_totsurcharges\r\n",
							"    ,bcr.NoOfPallets as lms_billconsignmentr_noofpallets\r\n",
							"    ,bcr.InvalidPUDel as lms_billconsignmentr_invalidpudel\r\n",
							"    ,bcr.SharikaFlag as lms_billconsignmentr_sharikaflag\r\n",
							"    ,bcr.SapID as lms_billconsignmentr_sapid\r\n",
							"    ,bcr.TotFuelSurcharge as lms_billconsignmentr_totfuelsurcharge\r\n",
							"    ,bcr.TotDocSurcharge as lms_billconsignmentr_totdocsurcharge\r\n",
							"    ,bcr.TotOtherSurcharge as lms_billconsignmentr_totothersurcharge\r\n",
							"    ,bcr.SpecDel as lms_billconsignmentr_specdel\r\n",
							"    ,bcr.QuoteFlag as lms_billconsignmentr_quoteflag\r\n",
							"    ,bcr.BillPeriod as lms_billconsignmentr_billperiod\r\n",
							"    ,bcr.BillWeekR as lms_billconsignmentr_billweekr\r\n",
							"    ,bcr.BillPeriodFlag as lms_billconsignmentr_billperiodflag\r\n",
							"    ,bcr.SurhargeFlag as lms_billconsignmentr_surhargeflag\r\n",
							"    ,bcr.RecalcFlag as lms_billconsignmentr_recalcflag\r\n",
							"    ,bcr.QuoteMissingFlag as lms_billconsignmentr_quotemissingflag\r\n",
							"    ,bcr.DeliverTypeID as lms_billconsignmentr_delivertypeid\r\n",
							"    ,bcr.TotCovidSurcharge as lms_billconsignmentr_totcovidsurcharge\r\n",
							"    ,bcc.sapcode as lms_billbillcustomersr_sapcode\r\n",
							"    -- ,l.lms_location_id as lms_consignment_tlocationid\r\n",
							"    -- ,l.lms_location_description as lms_consignment_tlocation\r\n",
							"    -- ,CASE\r\n",
							"    --     WHEN coalesce(bcr.samedelcount,0) > 1 THEN coalesce(bcr.CombineRouteCharge, 0)\r\n",
							"    --     ELSE coalesce(bcr.RouteCharge, 0)\r\n",
							"    -- END \r\n",
							"    -- + coalesce(bcr.TotFuelSurcharge, 0) \r\n",
							"    -- + coalesce(bcr.TotDocSurcharge, 0) \r\n",
							"    -- + coalesce(bcr.TotOtherSurcharge, 0) \r\n",
							"    -- + coalesce(bcr.TotCovidSurcharge, 0) as lms_billconsignmentr_aggrtotalcharge\r\n",
							"    -- ,coalesce(bcr.RouteCharge, 0) as lms_billconsignmentr_aggrstandardcharge\r\n",
							"    -- ,CASE\r\n",
							"    --     WHEN (coalesce(bcr.samedelcount,0) > 1)THEN coalesce(bcr.CombineRouteCharge, 0)\r\n",
							"    --     ELSE 0\r\n",
							"    -- END lms_billconsignmentr_aggrsdconsolidatedcharge\r\n",
							"    -- ,coalesce(bcr.TotFuelSurcharge, 0) as lms_billconsignmentr_aggrfuelsurcharge\r\n",
							"    -- ,coalesce(bcr.TotDocSurcharge, 0) as lms_billconsignmentr_aggrdocsurcharge\r\n",
							"    -- ,coalesce(bcr.TotOtherSurcharge, 0) as lms_billconsignmentr_aggrothersurcharge\r\n",
							"    -- ,coalesce(bcr.TotCovidSurcharge, 0) as lms_billconsignmentr_aggrcovidsurcharge\r\n",
							"\r\n",
							"FROM\r\n",
							"sstconsignemntunion c\r\n",
							"LEFT JOIN sstparcelbyconsignment pbc on c.id= pbc.lms_parcelbyconsignment_consignid\r\n",
							"LEFT JOIN dbobill_consignmentr bcr on c.id= bcr.consignid and  bcr.rn = 1\r\n",
							"LEFT JOIN dbobill_billcustomersr bcc on bcc.billcustid = bcr.billcust\r\n",
							"-- LEFT JOIN stparcel_tmp p on p.lms_parcel_consignid = c.id\r\n",
							"-- LEFT JOIN stdeliverypickupcustomer dpc on dpc.lms_customer_id = p.lms_parcel_orderdelivercustid\r\n",
							"-- LEFT JOIN stsroute sr on sr.lms_sroute_id = dpc.lms_customer_srouteid\r\n",
							"-- LEFT JOIN stzone z on z.lms_zone_id = sr.lms_sroute_zoneid\r\n",
							"-- LEFT JOIN stlocation l on l.lms_location_id = z.lms_zone_locid\r\n",
							""
						],
						"outputs": [],
						"execution_count": 28
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stconsignment_tmp2\r\n",
							"AS\r\n",
							"SELECT DISTINCT\r\n",
							"\r\n",
							"p.lms_parcel_consignid,\r\n",
							"max(w.poddate) as lms_consignment_maxpoddate\r\n",
							"\r\n",
							"FROM  stparcel p\r\n",
							"LEFT JOIN dbowaybill w on w.id = p.lms_parcel_waybillid\r\n",
							"WHERE p.lms_parcel_consignid is not null\r\n",
							"GROUP BY p.lms_parcel_consignid"
						],
						"outputs": [],
						"execution_count": 29
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"\r\n",
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stconsignment_tmp3\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    c.*\r\n",
							"    ,c2.lms_consignment_maxpoddate\r\n",
							"    ,CASE \r\n",
							"\t    WHEN (coalesce(c2.lms_consignment_maxpoddate,CURRENT_TIMESTAMP)) <= (date(c.lms_consignment_deliverby) + interval '86399 seconds') THEN 1\r\n",
							"\t    ELSE 0\r\n",
							"    END lms_consignment_inserviceflag\r\n",
							"    ,to_timestamp((cast(to_date(c.lms_consignment_cdate,'yyyy-MM-dd') as string)||' 23:59:59.999')) as  lms_consignment_actualleaddaysstartdate\r\n",
							"    ,coalesce(c2.lms_consignment_maxpoddate,CURRENT_TIMESTAMP) AS lms_consignment_actualleaddaysenddate\r\n",
							"    ,(to_unix_timestamp(coalesce(c2.lms_consignment_maxpoddate,CURRENT_TIMESTAMP)) - (to_unix_timestamp(to_timestamp((cast(to_date(c.lms_consignment_cdate,'yyyy-MM-dd') as string)||' 23:59:59.999')))))/86400 as lms_consignment_actualleaddaysinclwe\r\n",
							"    ,to_timestamp((cast(to_date(c.lms_consignment_cdate,'yyyy-MM-dd') as string)||' 23:59:59.999')) lms_consignment_cdateroundup\r\n",
							"    ,(to_unix_timestamp(coalesce(c2.lms_consignment_maxpoddate,CURRENT_TIMESTAMP)) - to_unix_timestamp(to_timestamp((cast(to_date(c.lms_consignment_cdate,'yyyy-MM-dd') as string)||' 23:59:59.999')))) as consignagedifferenceinseconds\r\n",
							"FROM stconsignment_tmp1 c\r\n",
							"LEFT JOIN stconsignment_Tmp2 c2 ON c2.lms_parcel_consignid = c.lms_consignment_id"
						],
						"outputs": [],
						"execution_count": 30
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stconsignment_tmp4\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    c.lms_consignment_id\r\n",
							"    ,sum(holiday)*86400 as holidays\r\n",
							"FROM stconsignment_tmp3 c\r\n",
							"INNER JOIN dboweekendholiday h on (date(h.DateHoliday) >=  date(lms_consignment_actualleaddaysstartdate) and date(h.DateHoliday) <= date(lms_consignment_actualleaddaysenddate))\r\n",
							"GROUP BY c.lms_consignment_id"
						],
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stconsignment_tmp5\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    c.*\r\n",
							"    ,(to_unix_timestamp(c.lms_consignment_actualleaddaysenddate) - to_unix_timestamp(c.lms_consignment_actualleaddaysstartdate) - coalesce(c1.holidays,0))/86400  as lms_consignment_actualleaddays\r\n",
							"FROM stconsignment_tmp3 c\r\n",
							"LEFT JOIN stconsignment_tmp4 c1 on c1.lms_consignment_id = c.lms_consignment_id"
						],
						"outputs": [],
						"execution_count": 32
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stconsignment\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    *\r\n",
							"    ,CASE \r\n",
							"        WHEN lms_consignment_cdate is null THEN 'Not Consigned'\r\n",
							"        WHEN lms_consignment_maxpoddate is null THEN 'Not Delivered'\r\n",
							"        WHEN (consignagedifferenceinseconds/3600) < 12 THEN '< 12 hrs'\r\n",
							"        WHEN (consignagedifferenceinseconds/3600) < 24 THEN '< 24 hrs'\r\n",
							"        WHEN (consignagedifferenceinseconds/3600) < 48 THEN '< 2 days'\r\n",
							"        WHEN (consignagedifferenceinseconds/3600) < 72 THEN '< 3 days'\r\n",
							"        WHEN (consignagedifferenceinseconds/3600) < 96 THEN '< 4 days'\r\n",
							"        WHEN (consignagedifferenceinseconds/3600) < 120 THEN '< 5 days'\r\n",
							"        WHEN (consignagedifferenceinseconds/3600) < 144 THEN '< 6 days'\r\n",
							"        WHEN (consignagedifferenceinseconds/3600) < 168 THEN '< 7 days'\r\n",
							"        WHEN (consignagedifferenceinseconds/3600) > 168 THEN '> 7 days'\r\n",
							"        ELSE NULL\r\n",
							"    END as lms_consignment_age\r\n",
							"\r\n",
							"FROM stconsignment_tmp5\r\n",
							""
						],
						"outputs": [],
						"execution_count": 33
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stconsignment = spark.sql(\"SELECT * FROM stconsignment\")"
						],
						"outputs": [],
						"execution_count": 34
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stconsignment.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stconsignment.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 36
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"SUCCESS\")"
						],
						"outputs": [],
						"execution_count": 37
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/05_LH_STInvoice')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Ops/Linehaul"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "0728c68c-fa2c-439f-b103-89890d772558"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse_Prod/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace-prod/bigDataPools/PRDSparkPool",
						"name": "PRDSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						}
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# #Create DataFrame for the publicinvoice TMS Table\r\n",
							"# publicinvoice = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publicinvoice.parquet', format='parquet')\r\n",
							"# publicinvoice.createOrReplaceTempView(\"publicinvoice\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publicinvoiceitem TMS Table\r\n",
							"# publicinvoiceitem = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publicinvoiceitem.parquet', format='parquet')\r\n",
							"# publicinvoiceitem.createOrReplaceTempView(\"publicinvoiceitem\")"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stinvoice\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    i.id as tms_invoice_id\r\n",
							"    ,i.customerid as tms_invoice_customerid\r\n",
							"    ,to_timestamp(i.invoicedate) as tms_invoice_invoicedate\r\n",
							"    ,i.reference as tms_invoice_reference\r\n",
							"    ,i.status as tms_invoice_status\r\n",
							"    ,i.insby as tms_invoice_insby\r\n",
							"    ,to_timestamp(i.insdate) as tms_invoice_insdate\r\n",
							"    ,i.total as tms_invoice_total\r\n",
							"    ,i.vat as tms_invoice_vat\r\n",
							"    ,i.invoicenum as tms_invoice_invoicenum\r\n",
							"    ,i.opaqueid as tms_invoice_opaqueid\r\n",
							"    ,i.sapstatus as tms_invoice_sapstatus\r\n",
							"    ,i.sapid as tms_invoice_sapid\r\n",
							"    ,to_timestamp(i.sendsapdate) as tms_invoice_sendsapdate\r\n",
							"    ,i.legacyid as tms_invoice_legacyid\r\n",
							"    ,i.customercode as tms_invoice_customercode\r\n",
							"    ,i.revennuecode as tms_invoice_revennuecode\r\n",
							"    ,i.depotcode  as tms_invoice_depotcode \r\n",
							"    ,i2.id as tms_invoiceitem_id\r\n",
							"    ,i2.invoiceid as tms_invoiceitem_invoiceid\r\n",
							"    ,i2.bookingid as tms_invoiceitem_bookingid\r\n",
							"    ,i2.amount as tms_invoiceitem_amount\r\n",
							"    ,i2.vat as tms_invoiceitem_vat\r\n",
							"    ,i2.linetotal as tms_invoiceitem_linetotal\r\n",
							"    ,to_timestamp(i2.insdate) as tms_invoiceitem_insdate\r\n",
							"    ,to_timestamp(i2.moddate) as tms_invoiceitem_moddate\r\n",
							"    ,i2.insby as tms_invoiceitem_insby\r\n",
							"    ,i2.linenum as tms_invoiceitem_linenum\r\n",
							"    ,i2.legacyid as tms_invoiceitem_legacyid\r\n",
							"    ,i2.sapid as tms_invoiceitem_sapid\r\n",
							"    ,i2.vatable as tms_invoiceitem_vatable\r\n",
							"    ,i2.routeid as tms_invoiceitem_routeid\r\n",
							"    ,i2.routename as tms_invoiceitem_routename\r\n",
							"FROM publicinvoice i\r\n",
							"LEFT JOIN publicinvoiceitem i2 on i2.invoiceid = i.id"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stinvoice = spark.sql(\"SELECT * FROM stinvoice\")"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stinvoice.write.parquet('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Structured Data/OPS/stinvoice.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"SUCCESS\")"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/05_STStockRevaluations')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Finance"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "2c81efad-8eec-46ed-b11b-37db9147fb8b"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse_Prod/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace-prod/bigDataPools/PRDSparkPool",
						"name": "PRDSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/PRDSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"source": [
							"# #Create DataFrame for the dboOMRV SAP Table\r\n",
							"# dboOMRV = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dboOMRV.parquet', format='parquet')\r\n",
							"# dboOMRV.createOrReplaceTempView(\"dboOMRV\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dboMRV1 SAP Table\r\n",
							"# dboMRV1 = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dboMRV1.parquet', format='parquet')\r\n",
							"# dboMRV1.createOrReplaceTempView(\"dboMRV1\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dboMRV2 SAP Table\r\n",
							"# dboMRV2 = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dboMRV2.parquet', format='parquet')\r\n",
							"# dboMRV2.createOrReplaceTempView(\"dboMRV2\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"ststockrevaluations\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"     omrv.docentry as sap_stockrevaluationsheader_docentry\r\n",
							"     ,omrv.transid as sap_stockrevaluationsheader_transid\r\n",
							"     ,omrv.docnum as sap_stockrevaluationsheader_docnum\r\n",
							"     ,omrv.docdate as sap_stockrevaluationsheader_postingdate\r\n",
							"     ,omrv.comments as sap_stockrevaluationsheader_comments\r\n",
							"     ,omrv.objtype as sap_stockrevaluationsheader_objtype\r\n",
							"     -- ,omrv.usersign as sap_stockrevaluationsheader_usersign\r\n",
							"     ,mrv1.linenum as sap_stockrevaluationsline_linenum\r\n",
							"     ,mrv1.dscription as sap_stockrevaluationsline_dscription\r\n",
							"     -- ,mrv1.objtype as sap_stockrevaluationsline_objtype\r\n",
							"     ,mrv1.itemcode as sap_stockrevaluationsline_itemcode\r\n",
							"     ,Right(Left(mrv1.itemcode,5),3) as sap_stockrevaluationsline_subdepotcode\r\n",
							"     ,mrv2.rtostock as sap_stockrevaluationsline_linetotal\r\n",
							"     ,mrv1.rincmacct as sap_stockrevaluationsline_acctcode\r\n",
							"     --,mrv2.baseline as sap_stockrevaluationsline_baseline\r\n",
							"     ,'stockrevaluation' as sap_stockrevaluationsline_doccat\r\n",
							"\r\n",
							"FROM dboOMRV omrv\r\n",
							"LEFT JOIN dboMRV1 mrv1 on mrv1.docentry = omrv.docentry\r\n",
							"LEFT JOIN dboMRV2 mrv2 on mrv2.docentry = omrv.docentry and mrv2.baseline = mrv1.linenum\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"ststockrevaluations = spark.sql(\"SELECT * FROM ststockrevaluations\")"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# ststockrevaluations.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/ststockrevaluations.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"SUCCESS\")"
						],
						"outputs": [],
						"execution_count": 12
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/06_DIST_STWaybill')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Ops/Distribution"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "70f93a23-5150-4104-a44c-c178ac4b2554"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace/bigDataPools/DevSparkPool",
						"name": "DevSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/DevSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"source": [
							"# #Create DataFrame for the dbowaybill LMS Table\r\n",
							"# dbowaybill = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbowaybill.parquet', format='parquet')\r\n",
							"# dbowaybill.createOrReplaceTempView(\"dbowaybill\")\r\n",
							"\r\n",
							"# #Create DataFrame for the sstparcelbywaybill LMS Table\r\n",
							"# sstparcelbywaybill = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Semi Structured Data/OPS/sstparcelbywaybill.parquet', format='parquet')\r\n",
							"# sstparcelbywaybill.createOrReplaceTempView(\"sstparcelbywaybill\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publicdelivery ORV Table\r\n",
							"# publicdelivery = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicdelivery.parquet', format='parquet')\r\n",
							"# publicdelivery.createOrReplaceTempView(\"publicdelivery\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publicdispatch ORV Table\r\n",
							"# publicdispatch = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicdispatch.parquet', format='parquet')\r\n",
							"# publicdispatch.createOrReplaceTempView(\"publicdispatch\")\r\n",
							"\r\n",
							"# #create dataframe for the publicpod orv table\r\n",
							"# publicpod = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicpod.parquet', format='parquet')\r\n",
							"# publicpod.createOrReplaceTempView(\"publicpod\")\r\n",
							"\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# from pyspark.sql.functions import *\r\n",
							"# from pyspark.sql.window import Window\r\n",
							"# from pyspark.sql.types import IntegralType"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"partitionfirst=Window.partitionBy(\"did\").orderBy(col(\"did\").asc())"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"pod_tmp = publicpod.withColumn(\"row_num\",row_number().over(partitionfirst))"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"pod_tmp.createOrReplaceTempView(\"pod_tmp\")"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"dbowaybill_tmp\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"     wb.*\r\n",
							"    ,dis.id as dispatchid\r\n",
							"FROM dbowaybill wb\r\n",
							"LEFT JOIN publicdispatch dis on dis.lmsid = wb.loadid\r\n",
							"\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"dbowaybill_tmp = spark.sql(\"SELECT * FROM dbowaybill_tmp\")"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stwaybill_tmp\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"\r\n",
							"    w.ID as lms_waybill_id\r\n",
							"    ,w.date as lms_waybill_date\r\n",
							"    ,w.CustomerID as lms_waybill_customerid\r\n",
							"    ,w.BService as lms_waybill_bservice\r\n",
							"    ,w.CService as lms_waybill_cservice\r\n",
							"    ,w.OriginID as lms_waybill_originid\r\n",
							"    ,w.DefUser as lms_waybill_defuser\r\n",
							"    ,w.Distance as lms_waybill_distance\r\n",
							"    ,w.CourierID as lms_waybill_courierid\r\n",
							"    ,w.CourierWBNo as lms_waybill_courierwbno\r\n",
							"    ,w.BillType as lms_waybill_billtype\r\n",
							"    ,w.CapDate as lms_waybill_capdate\r\n",
							"    ,w.OrderType as lms_waybill_ordertype\r\n",
							"    ,w.BillCust as lms_waybill_billcust\r\n",
							"    ,w.PickupCustID as lms_waybill_pickupcustid\r\n",
							"    ,w.DeliverCustID as lms_waybill_delivercustid\r\n",
							"    ,w.BillTo as lms_waybill_billto\r\n",
							"    ,w.Claimable as lms_waybill_claimable\r\n",
							"    ,w.Override as lms_waybill_override\r\n",
							"    ,w.VehicleCapacity as lms_waybill_vehiclecapacity\r\n",
							"    ,w.PODDate as lms_waybill_poddate\r\n",
							"    ,w.Signee as lms_waybill_signee\r\n",
							"    ,w.GRVNo as lms_waybill_grvno\r\n",
							"    ,w.Endorsements as lms_waybill_endorsements\r\n",
							"    ,w.LoadID as lms_waybill_loadid\r\n",
							"    ,w.RouteID as lms_waybill_routeid\r\n",
							"    ,w.mrpFlag as lms_waybill_mrpflag\r\n",
							"    ,w.Printed as lms_waybill_printed\r\n",
							"    ,w.SReasonID as lms_waybill_sreasonid\r\n",
							"    ,w.HasPrinted as lms_waybill_hasprinted\r\n",
							"    ,w.Site as lms_waybill_site\r\n",
							"    ,w.DeliveryNo as lms_waybill_deliveryno\r\n",
							"    ,w.CustAccID as lms_waybill_custaccid\r\n",
							"    ,w.Scanned as lms_waybill_scanned\r\n",
							"    ,w.UserID as lms_waybill_userid\r\n",
							"    ,w.Autoprinted as lms_waybill_autoprinted\r\n",
							"    ,w.cref as lms_waybill_cref\r\n",
							"    ,w.UShort as lms_waybill_ushort\r\n",
							"    ,w.Delays as lms_waybill_delays\r\n",
							"    ,w.DeBriefed as lms_waybill_debriefed\r\n",
							"    ,w.DeBriefDt as lms_waybill_debriefdt\r\n",
							"    ,w.DBCtnsRet as lms_waybill_dbctnsret\r\n",
							"    ,w.Uploaded as lms_waybill_uploaded\r\n",
							"    ,w.ServiceExclude as lms_waybill_serviceexclude\r\n",
							"    ,w.Failed as lms_waybill_failed\r\n",
							"    ,w.DelayReasonID as lms_waybill_delayreasonid\r\n",
							"    ,w.NotDebrievReasonID as lms_waybill_notdebrievreasonid\r\n",
							"    ,w.DeliveryStatID as lms_waybill_deliverystatid\r\n",
							"    ,w.ReasonGroupID as lms_waybill_reasongroupid\r\n",
							"    ,w.eMailCust as lms_waybill_emailcust\r\n",
							"    ,w.MobileIssueReportedID as lms_waybill_mobileissuereportedid\r\n",
							"    ,w.isMobile as lms_waybill_ismobile\r\n",
							"    ,w.CustComment as lms_waybill_custcomment\r\n",
							"    ,w.CustFeedback as lms_waybill_custfeedback\r\n",
							"    ,w.ReasonDetailID as lms_waybill_reasondetailid\r\n",
							"    ,w.MobileComments as lms_waybill_mobilecomments\r\n",
							"    ,w.InServiceReasonID as lms_waybill_inservicereasonid\r\n",
							"    ,w.NotDebrievCom as lms_waybill_notdebrievcom\r\n",
							"    ,w.LastUpdate as lms_waybill_lastupdate\r\n",
							"    ,w.Dropsequence as lms_waybill_dropsequence\r\n",
							"    ,w.Noofparcelssreturned as lms_waybill_noofparcelssreturned\r\n",
							"    ,pbw.lms_parcelbywaybill_aggrweight\r\n",
							"    ,pbw.lms_parcelbywaybill_aggrchargeweight\r\n",
							"    ,pbw.lms_parcelbywaybill_aggrvolweight\r\n",
							"    ,pbw.lms_parcelbywaybill_aggrvolumiserweight\r\n",
							"    ,pbw.lms_parcelbywaybill_aggrmaxweight\r\n",
							"    ,pbw.lms_parcelbywaybill_aggrnoparcels\r\n",
							"    ,pbw.lms_parcelbywaybill_aggrweightexclspecdel\r\n",
							"    ,pbw.lms_parcelbywaybill_aggrchargeweightexclspecdel\r\n",
							"    ,pbw.lms_parcelbywaybill_aggrvolweightexclspecdel\r\n",
							"    ,pbw.lms_parcelbywaybill_aggrvolumiserweightexclspecdel\r\n",
							"    ,pbw.lms_parcelbywaybill_aggrmaxweightexclspecdel\r\n",
							"    ,pbw.lms_parcelbywaybill_aggrnoparcelsexclspecdel\r\n",
							"    ,d.id as orv_delivery_id\r\n",
							"    ,d.did as orv_delivery_did\r\n",
							"    ,d.waybillid as orv_delivery_waybillid\r\n",
							"    ,d.address as orv_delivery_address\r\n",
							"    ,d.town as orv_delivery_town\r\n",
							"    ,d.province as orv_delivery_province\r\n",
							"    ,d.country as orv_delivery_country\r\n",
							"    ,d.lat as orv_delivery_lat\r\n",
							"    ,d.lng as orv_delivery_lng\r\n",
							"    ,d.placeid as orv_delivery_placeid\r\n",
							"    ,d.what3words as orv_delivery_what3words\r\n",
							"    ,d.zipcode as orv_delivery_zipcode\r\n",
							"    ,to_timestamp(d.eta) as orv_delivery_eta\r\n",
							"    ,to_timestamp(d.actualeta) as orv_delivery_actualeta\r\n",
							"    ,to_timestamp(d.deliverydate) as orv_delivery_deliverydate\r\n",
							"    ,to_timestamp(d.deliverby) as orv_delivery_deliverby\r\n",
							"    ,d.deliveryorder as orv_delivery_deliveryorder\r\n",
							"    ,to_timestamp(d.tsgeofenceenter) as orv_delivery_tsgeofenceenter\r\n",
							"    ,to_timestamp(d.tsscanningstart) as orv_delivery_tsscanningstart\r\n",
							"    ,to_timestamp(d.tsscanningstop) as orv_delivery_tsscanningstop\r\n",
							"    ,to_timestamp(d.tspodsignature) as orv_delivery_tspodsignature\r\n",
							"    ,d.aid as orv_delivery_aid\r\n",
							"    ,d.numprcls as orv_delivery_numprcls\r\n",
							"    ,d.geocodingstatus as orv_delivery_geocodingstatus\r\n",
							"    ,to_timestamp(d.skipdate) as orv_delivery_skipdate\r\n",
							"    ,d.skipreason as orv_delivery_skipreason\r\n",
							"    ,d.status as orv_delivery_status\r\n",
							"    ,d.partialmatch as orv_delivery_partialmatch\r\n",
							"    ,d.approximated as orv_delivery_approximated\r\n",
							"    ,d.lmsorder as orv_delivery_lmsorder\r\n",
							"    ,d.optimizedorder as orv_delivery_optimizedorder\r\n",
							"    ,d.seal as orv_delivery_seal\r\n",
							"    ,d.customerinvoice as orv_delivery_customerinvoice\r\n",
							"    ,d.customerdn as orv_delivery_customerdn\r\n",
							"    ,d.grv as orv_delivery_grv\r\n",
							"    ,d.chepslip as orv_delivery_chepslip\r\n",
							"    ,d.cleandelivery as orv_delivery_cleandelivery\r\n",
							"    ,d.trackingcode as orv_delivery_trackingcode\r\n",
							"    ,d.mode as orv_delivery_mode\r\n",
							"    ,d.customorder as orv_delivery_customorder\r\n",
							"    ,d.verified as orv_delivery_verified\r\n",
							"    ,d.altered as orv_delivery_altered\r\n",
							"    ,d.orderid as orv_delivery_orderid\r\n",
							"    ,d.mergeid as orv_delivery_mergeid\r\n",
							"    ,d.orderref as orv_delivery_orderref\r\n",
							"    ,d.drivernote as orv_delivery_drivernote\r\n",
							"    ,d.guid as orv_delivery_guid\r\n",
							"    ,d.timedefinite as orv_delivery_timedefinite\r\n",
							"    ,d.orvcode as orv_delivery_orvcode\r\n",
							"    ,d.vaid as orv_delivery_vaid\r\n",
							"    ,to_timestamp(d.delegatedate) as orv_delivery_delegatedate\r\n",
							"    ,d.delegatemode as orv_delivery_delegatemode\r\n",
							"    ,d.debriefmode as orv_delivery_debriefmode\r\n",
							"    ,d.debriefed as orv_delivery_debriefed\r\n",
							"    ,to_timestamp(d.insdate) as orv_delivery_insdate\r\n",
							"    ,d.reviseddebrief as orv_delivery_reviseddebrief\r\n",
							"    ,d.lmsdebriefed as orv_delivery_lmsdebriefed\r\n",
							"    ,to_timestamp(d.uncanceldate) as orv_delivery_uncanceldate\r\n",
							"    ,to_timestamp(d.podlaterdate) as orv_delivery_podlaterdate\r\n",
							"    ,to_timestamp(d.podmanualdate) as orv_delivery_podmanualdate\r\n",
							"    ,d.manualdebriefreason as orv_delivery_manualdebriefreason\r\n",
							"    ,d.submanualdebriefreason as orv_delivery_submanualdebriefreason\r\n",
							"    ,d.outofgeofencereason as orv_delivery_outofgeofencereason\r\n",
							"    ,to_timestamp(d.tsentergeofenceapp) as orv_delivery_tsentergeofenceapp\r\n",
							"    ,to_timestamp(d.tsexitgeofenceapp) as orv_delivery_tsexitgeofenceapp\r\n",
							"    ,d.comebacklater as orv_delivery_comebacklater\r\n",
							"    ,d.uncancelusername as orv_delivery_uncancelusername\r\n",
							"    ,d.cref as orv_delivery_cref\r\n",
							"    ,d.division as orv_delivery_division\r\n",
							"    ,d.descriptionofgoods as orv_delivery_descriptionofgoods\r\n",
							"    ,d.skiplng as orv_delivery_skiplng\r\n",
							"    ,d.skiplat as orv_delivery_skiplat\r\n",
							"    ,d.courierid as orv_delivery_courierid\r\n",
							"    ,d.posreason as orv_delivery_posreason\r\n",
							"    ,to_timestamp(d.moddate) as orv_delivery_moddate\r\n",
							"    ,d.mallid as orv_delivery_mallid\r\n",
							"    ,d.mallgid as orv_delivery_mallgid\r\n",
							"    ,d.lmsidskipreason as orv_delivery_lmsidskipreason\r\n",
							"    ,d.deleteforimport as orv_delivery_deleteforimport\r\n",
							"    ,d.podoutofgeofence as orv_delivery_podoutofgeofence\r\n",
							"    ,d.originalcustomerid as orv_delivery_originalcustomerid\r\n",
							"    ,p.id as orv_pod_id\r\n",
							"\t,p.name as orv_pod_name\r\n",
							"\t,to_timestamp(p.poddate) as orv_pod_poddate\r\n",
							"\t,p.lat as orv_pod_lat\r\n",
							"\t,p.lng as orv_pod_lng\r\n",
							"\t,p.customerrating as orv_pod_customerrating\r\n",
							"\t,p.customerfeedback as orv_pod_customerfeedback\r\n",
							"    --,2.00 * 3961.00 * asin(sqrt((sin(radians((p.lat - d.lat) / 2.00))) ^ 2.00 + cos(radians(d.lat)) * cos(radians(p.lat)) * (sin(radians((p.lng - d.lng ) / 2.00))) ^ 2.00))*1.60934 as orv_delivery_poddistancefromdeliverypoint\r\n",
							"    ,(to_unix_timestamp(w.DeBriefDt) - d.deliverydate)/86400 as orv_delivery_datediffpoddeldate\r\n",
							"    ,(COALESCE(d.tsexitgeofenceapp,d.tspodsignature) - COALESCE(d.tsgeofenceenter,d.tsentergeofenceapp))/60 as orv_delivery_deliveryenterexitdiff\r\n",
							"\r\n",
							"FROM dbowaybill_tmp w\r\n",
							"LEFT JOIN sstparcelbywaybill pbw on pbw.lms_parcelbywaybill_waybillid =  w.id\r\n",
							"LEFT JOIN publicdelivery d on d.waybillid =  w.id and d.did = w.dispatchid\r\n",
							"LEFT JOIN pod_tmp p on p.did = d.id and row_num = 1\r\n",
							""
						],
						"outputs": [],
						"execution_count": 28
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stwaybill_tmp = spark.sql(\"SELECT * FROM stwaybill_tmp\")"
						],
						"outputs": [],
						"execution_count": 29
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stwaybill = stwaybill_tmp.withColumn(\"a\", pow(sin(radians (col(\"orv_pod_lat\") - col(\"orv_delivery_lat\")) / 2), 2) + cos(radians (\"orv_delivery_lat\")) * cos(radians (col(\"orv_pod_lat\"))) * pow(sin(radians (col(\"orv_pod_lng\") - col(\"orv_delivery_lng\")) / 2), 2)).withColumn(\"orv_delivery_poddistancefromdeliverypoint\", atan2(sqrt(col(\"a\")), sqrt(-col(\"a\") + 1)) * 2 * 6371)"
						],
						"outputs": [],
						"execution_count": 30
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stwaybill.createOrReplaceTempView(\"stwaybill\")"
						],
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stwaybill.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stwaybill.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 33
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"SUCCESS\")"
						],
						"outputs": [],
						"execution_count": 67
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/06_LH_STLinehaulDimensions')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Ops/Linehaul"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "28466094-46b8-46eb-981e-6236931b2b94"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace/bigDataPools/DevSparkPool",
						"name": "DevSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/DevSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"source": [
							"# #Create DataFrame for the publiccustomer TMS Table\r\n",
							"# publiccustomer = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publiccustomer.parquet', format='parquet')\r\n",
							"# publiccustomer.createOrReplaceTempView(\"publiccustomer\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publicthirdparty TMS Table\r\n",
							"# publicthirdparty = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publicthirdparty.parquet', format='parquet')\r\n",
							"# publicthirdparty.createOrReplaceTempView(\"publicthirdparty\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publicroute TMS Table\r\n",
							"# publicroute = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publicroute.parquet', format='parquet')\r\n",
							"# publicroute.createOrReplaceTempView(\"publicroute\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbolh_routes TMS Table\r\n",
							"# dbolh_routes = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbolh_routes.parquet', format='parquet')\r\n",
							"# dbolh_routes.createOrReplaceTempView(\"dbolh_routes\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publicaddress TMS Table\r\n",
							"# publicaddress = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publicaddress.parquet', format='parquet')\r\n",
							"# publicaddress.createOrReplaceTempView(\"publicaddress\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publicfuelcard TMS Table\r\n",
							"# publicfuelcard = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publicfuelcard.parquet', format='parquet')\r\n",
							"# publicfuelcard.createOrReplaceTempView(\"publicfuelcard\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publicfuelzone TMS Table\r\n",
							"# publicfuelzone = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publicfuelzone.parquet', format='parquet')\r\n",
							"# publicfuelzone.createOrReplaceTempView(\"publicfuelzone\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publicfuelzonehistory TMS Table\r\n",
							"# publicfuelzonehistory = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publicfuelzonehistory.parquet', format='parquet')\r\n",
							"# publicfuelzonehistory.createOrReplaceTempView(\"publicfuelzonehistory\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publicratecomponent TMS Table\r\n",
							"# publicratecomponent = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publicratecomponent.parquet', format='parquet')\r\n",
							"# publicratecomponent.createOrReplaceTempView(\"publicratecomponent\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publicrateprofile TMS Table\r\n",
							"# publicrateprofile = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publicrateprofile.parquet', format='parquet')\r\n",
							"# publicrateprofile.createOrReplaceTempView(\"publicrateprofile\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publicselist TMS Table\r\n",
							"# publicselist = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publicselist.parquet', format='parquet')\r\n",
							"# publicselist.createOrReplaceTempView(\"publicselist\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publicselistitem TMS Table\r\n",
							"# publicselistitem = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publicselistitem.parquet', format='parquet')\r\n",
							"# publicselistitem.createOrReplaceTempView(\"publicselistitem\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publicdriverpay TMS Table\r\n",
							"# publicdriverpay = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publicdriverpay.parquet', format='parquet')\r\n",
							"# publicdriverpay.createOrReplaceTempView(\"publicdriverpay\")\r\n",
							"\r\n",
							"# #Create DataFrame for the ssttrackunion LMS Table\r\n",
							"# ssttrackunion = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Semi Structured Data/OPS/ssttrackunion.parquet', format='parquet')\r\n",
							"# ssttrackunion.createOrReplaceTempView(\"ssttrackunion\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dboloads LMS Table\r\n",
							"# dboloads = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboloads.parquet', format='parquet')\r\n",
							"# dboloads.createOrReplaceTempView(\"dboloads\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publicdispatchactionlog LMS Table\r\n",
							"# publicdispatchactionlog = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicdispatchactionlog.parquet', format='parquet')\r\n",
							"# publicdispatchactionlog.createOrReplaceTempView(\"publicdispatchactionlog\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publiclmsdata LMS Table\r\n",
							"# publiclmsdata = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publiclmsdata.parquet', format='parquet')\r\n",
							"# publiclmsdata.createOrReplaceTempView(\"publiclmsdata\")\r\n",
							"\r\n",
							"# #create dataframe for the dbocustomer lms table\r\n",
							"# dbocustomer = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbocustomer.parquet', format='parquet')\r\n",
							"# dbocustomer.createOrReplaceTempView(\"dbocustomer\")\r\n",
							"\r\n",
							"# #create dataframe for the publicbooking lms table\r\n",
							"# publicbooking = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publicbooking.parquet', format='parquet')\r\n",
							"# publicbooking.createOrReplaceTempView(\"publicbooking\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# from pyspark.sql.functions import *\r\n",
							"# from pyspark.sql.window import Window"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"partitionfirst=Window.partitionBy(\"LoadID\").orderBy(col(\"ID\").asc())"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"partitionlast=Window.partitionBy(\"LoadID\").orderBy(col(\"ID\").desc())"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"ssttrackunion = ssttrackunion.withColumn(\"lms_track_firstscan\",row_number().over(partitionfirst))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"ssttrackunion.createOrReplaceTempView(\"ssttrackunion\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"ssttrackunion = ssttrackunion.withColumn(\"lms_track_lastscan\",row_number().over(partitionlast))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"ssttrackunion.createOrReplaceTempView(\"ssttrackunion\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stlmstrackfirstscan\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"\r\n",
							"LoadID as lms_track_loadid\r\n",
							",OpenDt as lms_track_firstscan\r\n",
							",l.masterloadid as lms_loads_masterloadid\r\n",
							"\r\n",
							"FROM ssttrackunion t\r\n",
							"LEFT JOIN dboloads l on l.id = t.LoadID\r\n",
							"WHERE t.TrackTypeID = 2\r\n",
							"AND lms_track_firstscan = 1\r\n",
							"AND l.masterloadid IS NOT NULL\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stlmstrackfirstscan = spark.sql(\"SELECT * FROM stlmstrackfirstscan\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stlmstracklastscan\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"\r\n",
							"LoadID as lms_track_loadid\r\n",
							", OpenDt as lms_track_lastscan\r\n",
							"\r\n",
							"FROM ssttrackunion t\r\n",
							"LEFT JOIN dboloads l on l.id = t.LoadID\r\n",
							"WHERE t.TrackTypeID = 2\r\n",
							"AND lms_track_lastscan = 1\r\n",
							"AND l.masterloadid IS NOT NULL\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stlmstracklastscan = spark.sql(\"SELECT * FROM stlmstracklastscan\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stlmstrackscantime\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"\r\n",
							"t1.lms_track_loadid\r\n",
							",t1.lms_loads_masterloadid\r\n",
							",t1.lms_track_firstscan\r\n",
							",t2.lms_track_lastscan\r\n",
							",(to_unix_timestamp(t2.lms_track_lastscan)-to_unix_timestamp(t1.lms_track_firstscan)) as lms_track_loadtime\r\n",
							"\r\n",
							"FROM stlmstrackfirstscan t1\r\n",
							"LEFT JOIN stlmstracklastscan t2 on t2.lms_track_loadid = t1.lms_track_loadid"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stlmstrackscantime = spark.sql(\"SELECT * FROM stlmstrackscantime\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stlmstrackscantime.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stlmstrackscantime.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stdriverpay\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    d.id as tms_driverpay_id\r\n",
							"    ,d.routeid as tms_driverpay_routeid\r\n",
							"    ,d.triprate as tms_driverpay_triprate\r\n",
							"    ,d.mealrate as tms_driverpay_mealrate\r\n",
							"    ,d.linkrate as tms_driverpay_linkrate\r\n",
							"    ,d.sleepoverrate as tms_driverpay_sleepoverrate\r\n",
							"    ,d.numsleepover as tms_driverpay_numsleepover\r\n",
							"    ,d.nummeal as tms_driverpay_nummeal\r\n",
							"    ,to_timestamp(d.validfromdate) as tms_driverpay_validfromdate\r\n",
							"    ,to_timestamp(d.validtodate) as tms_driverpay_validtodate\r\n",
							"    ,to_timestamp(d.insdate) as tms_driverpay_insdate\r\n",
							"    ,to_timestamp(d.moddate) as tms_driverpay_moddate\r\n",
							"    ,d.insby as tms_driverpay_insby\r\n",
							"    ,d.airtimerate as tms_driverpay_airtimerate\r\n",
							"    ,d.status as tms_driverpay_status\r\n",
							"    ,d.legacyid as tms_driverpay_legacyid\r\n",
							"FROM publicdriverpay d\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 51
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stdriverpay = spark.sql(\"SELECT * FROM stdriverpay\")"
						],
						"outputs": [],
						"execution_count": 52
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stdriverpay.write.parquet('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Structured Data/OPS/stdriverpay.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 53
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stfuelcard\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"\r\n",
							"    f.id as tms_fuelcard_id\r\n",
							"    ,f.eid as tms_fuelcard_eid\r\n",
							"    ,f.entity as tms_fuelcard_entity\r\n",
							"    ,f.baseprice as tms_fuelcard_baseprice\r\n",
							"    ,f.routeid as tms_fuelcard_routeid\r\n",
							"    ,f.operatingpercentage as tms_fuelcard_operatingpercentage\r\n",
							"    ,to_timestamp(f.insdate) as tms_fuelcard_insdate\r\n",
							"    ,to_timestamp(f.activefromdate) as tms_fuelcard_activefromdate\r\n",
							"    ,to_timestamp(f.activetodate) as tms_fuelcard_activetodate\r\n",
							"    ,to_timestamp(f.moddate) as tms_fuelcard_moddate\r\n",
							"    ,f.status as tms_fuelcard_status\r\n",
							"    ,f.insby as tms_fuelcard_insby\r\n",
							"    ,f.zoneid as tms_fuelcard_zoneid\r\n",
							"    ,f.legacyid as tms_fuelcard_legacyid\r\n",
							"\r\n",
							"FROM publicfuelcard f"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\r\n",
							"stfuelcard = spark.sql(\"SELECT * FROM stfuelcard\")"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\r\n",
							"# stfuelcard.write.parquet('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Structured Data/OPS/stfuelcard.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stfuelzone\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"\r\n",
							"    f.id as tms_fuelzone_id\r\n",
							"    ,f.zonename as tms_fuelzone_zonename\r\n",
							"    ,f.rulingprice as tms_fuelzone_rulingprice\r\n",
							"    ,to_timestamp(f.insdate) as tms_fuelzone_insdate\r\n",
							"    ,to_timestamp(f.moddate) as tms_fuelzone_moddate\r\n",
							"    ,f.insby as tms_fuelzone_insby\r\n",
							"    ,f.lastrulingprice as tms_fuelzone_lastrulingprice\r\n",
							"    ,f.legacyid as tms_fuelzone_legacyid\r\n",
							"\r\n",
							"FROM publicfuelzone f\r\n",
							""
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\r\n",
							"stfuelzone = spark.sql(\"SELECT * FROM stfuelzone\")"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\r\n",
							"# stfuelzone.write.parquet('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Structured Data/OPS/stfuelzone.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stfuelzonehistory\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"\r\n",
							"    f.id as tms_fuelzonehistory_id\r\n",
							"    ,f.zoneid as tms_fuelzonehistory_zoneid\r\n",
							"    ,f.rulingprice as tms_fuelzonehistory_rulingprice\r\n",
							"    ,f.validfromdate as tms_fuelzonehistory_validfromdate\r\n",
							"    ,f.validtodate as tms_fuelzonehistory_validtodate\r\n",
							"    ,f.editby as tms_fuelzonehistory_editby\r\n",
							"    ,to_timestamp(f.insdate) as tms_fuelzonehistory_insdate\r\n",
							"    ,to_timestamp(f.moddate) as tms_fuelzonehistory_moddate\r\n",
							"\r\n",
							"\r\n",
							"FROM publicfuelzonehistory f\r\n",
							""
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stfuelzonehistory = spark.sql(\"SELECT * FROM stfuelzonehistory\")"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stfuelzonehistory.write.parquet('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Structured Data/OPS/stfuelzonehistory.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stratecomponent\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"\r\n",
							"    r.id as tms_ratecomponent_id\r\n",
							"    ,r.profileid as tms_ratecomponent_profileid\r\n",
							"    ,r.ratetype as tms_ratecomponent_ratetype\r\n",
							"    ,r.amount as tms_ratecomponent_amount\r\n",
							"    ,r.applyfuellevy as tms_ratecomponent_applyfuellevy\r\n",
							"    ,r.note as tms_ratecomponent_note\r\n",
							"    ,to_timestamp(r.insdate) as tms_ratecomponent_insdate\r\n",
							"    ,to_timestamp(r.moddate) as tms_ratecomponent_moddate\r\n",
							"    ,r.insby as tms_ratecomponent_insby\r\n",
							"    ,r.legacyid as tms_ratecomponent_legacyid\r\n",
							"\r\n",
							"\r\n",
							"FROM publicratecomponent r"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stratecomponent = spark.sql(\"SELECT * FROM stratecomponent\")"
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stratecomponent.write.parquet('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Structured Data/OPS/stratecomponent.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"strateprofile\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"\r\n",
							"    r.id as tms_rateprofile_id\r\n",
							"    ,r.eid as tms_rateprofile_eid\r\n",
							"    ,r.entity as tms_rateprofile_entity\r\n",
							"    ,r.trailersize as tms_rateprofile_trailersize\r\n",
							"    ,r.routeid as tms_rateprofile_routeid\r\n",
							"    ,r.profiletype as tms_rateprofile_profiletype\r\n",
							"    ,to_timestamp(r.fromdate) as tms_rateprofile_fromdate\r\n",
							"    ,to_timestamp(r.todate) as tms_rateprofile_todate\r\n",
							"    ,r.insby as tms_rateprofile_insby\r\n",
							"    ,r.status as tms_rateprofile_status\r\n",
							"    ,to_timestamp(r.insdate) as tms_rateprofile_insdate\r\n",
							"    ,to_timestamp(r.moddate) as tms_rateprofile_moddate\r\n",
							"    ,r.note as tms_rateprofile_note\r\n",
							"    ,r.legacyid as tms_rateprofile_legacyid\r\n",
							"    ,r.usedfuelprice as tms_rateprofile_usedfuelprice\r\n",
							"\r\n",
							"FROM publicrateprofile r\r\n",
							""
						],
						"outputs": [],
						"execution_count": 27
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"strateprofile = spark.sql(\"SELECT * FROM strateprofile\")"
						],
						"outputs": [],
						"execution_count": 28
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# strateprofile.write.parquet('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Structured Data/OPS/strateprofile.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 29
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"sttmslist\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"\r\n",
							"    s.id as tms_list_id\r\n",
							"    ,s.slug as tms_list_slug\r\n",
							"    ,s.valuetype as tms_list_valuetype\r\n",
							"    ,s.descr as tms_list_descr\r\n",
							"    ,to_timestamp(s.insdate) as tms_list_insdate\r\n",
							"    ,to_timestamp(s.moddate) as tms_list_moddate\r\n",
							"    ,s.refcols as tms_list_refcols\r\n",
							"    ,s.editable  as tms_list_editable \r\n",
							"    ,s2.id as tms_listitem_id\r\n",
							"    ,s2.lid as tms_listitem_lid\r\n",
							"    ,s2.itemlabel as tms_listitem_itemlabel\r\n",
							"    ,s2.itemvalue as tms_listitem_itemvalue\r\n",
							"    ,s2.sort as tms_listitem_sort\r\n",
							"    ,s2.mdid as tms_listitem_mdid\r\n",
							"\r\n",
							"FROM publicselist s\r\n",
							"LEFT JOIN publicselistitem s2 on s2.lid = s.id\r\n",
							""
						],
						"outputs": [],
						"execution_count": 41
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sttmslist = spark.sql(\"SELECT * FROM sttmslist\")"
						],
						"outputs": [],
						"execution_count": 42
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# sttmslist.write.parquet('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Structured Data/OPS/sttmslist.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 43
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stprimarycustomer\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    c.id as tms_customer_id\r\n",
							"    ,c.status as tms_customer_status\r\n",
							"    ,c.name as tms_customer_name\r\n",
							"    ,c.parentid as tms_customer_parentid\r\n",
							"    ,c.accountnumber as tms_customer_accountnumber\r\n",
							"    ,c.revenuecode as tms_customer_revenuecode\r\n",
							"    ,to_timestamp(c.insdate) as tms_customer_insdate\r\n",
							"    ,to_timestamp(c.moddate) as tms_customer_moddate\r\n",
							"    ,c.insby as tms_customer_insby\r\n",
							"    ,c.opaqueid as tms_customer_opaqueid\r\n",
							"    ,c.email as tms_customer_email\r\n",
							"    ,c.areacode as tms_customer_areacode\r\n",
							"    ,c.mobilenum as tms_customer_mobilenum\r\n",
							"    ,c.legacyid as tms_customer_legacyid\r\n",
							"    ,c.billingmethod as tms_customer_billingmethod\r\n",
							"    ,c.adjustmetric as tms_customer_adjustmetric\r\n",
							"    ,c.adjustvalue as tms_customer_adjustvalue\r\n",
							"    ,c.adjustnegativeallowed as tms_customer_adjustnegativeallowed\r\n",
							"    ,c.adjustvalueusingprevmonth as tms_customer_adjustvalueusingprevmonth\r\n",
							"    ,c.excludefromnotinvoicedreport as tms_customer_excludefromnotinvoicedreport\r\n",
							"    ,c.logictype as tms_customer_logictype\r\n",
							"    ,c.minliability as tms_customer_minliability\r\n",
							" \r\n",
							"FROM publiccustomer c\r\n",
							"WHERE c.parentid is null\r\n",
							""
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stprimarycustomer = spark.sql(\"SELECT * FROM stprimarycustomer\")"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stprimarycustomer.write.parquet('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Structured Data/OPS/stprimarycustomer.parquet', mode = \"overwrite\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stsecondarycustomer\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    c.id as tms_customer_id\r\n",
							"    ,c.status as tms_customer_status\r\n",
							"    ,c.name as tms_customer_name\r\n",
							"    ,c.parentid as tms_customer_parentid\r\n",
							"    ,c.accountnumber as tms_customer_accountnumber\r\n",
							"    ,c.revenuecode as tms_customer_revenuecode\r\n",
							"    ,to_timestamp(c.insdate) as tms_customer_insdate\r\n",
							"    ,to_timestamp(c.moddate) as tms_customer_moddate\r\n",
							"    ,c.insby as tms_customer_insby\r\n",
							"    ,c.opaqueid as tms_customer_opaqueid\r\n",
							"    ,c.email as tms_customer_email\r\n",
							"    ,c.areacode as tms_customer_areacode\r\n",
							"    ,c.mobilenum as tms_customer_mobilenum\r\n",
							"    ,c.legacyid as tms_customer_legacyid\r\n",
							"    ,c.billingmethod as tms_customer_billingmethod\r\n",
							"    ,c.adjustmetric as tms_customer_adjustmetric\r\n",
							"    ,c.adjustvalue as tms_customer_adjustvalue\r\n",
							"    ,c.adjustnegativeallowed as tms_customer_adjustnegativeallowed\r\n",
							"    ,c.adjustvalueusingprevmonth as tms_customer_adjustvalueusingprevmonth\r\n",
							"    ,c.excludefromnotinvoicedreport as tms_customer_excludefromnotinvoicedreport\r\n",
							"    ,c.logictype as tms_customer_logictype\r\n",
							"    ,c.minliability as tms_customer_minliability\r\n",
							" \r\n",
							"FROM publiccustomer c\r\n",
							"WHERE c.parentid is not null\r\n",
							""
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stsecondarycustomer = spark.sql(\"SELECT * FROM stsecondarycustomer\")"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stsecondarycustomer.write.parquet('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Structured Data/OPS/stsecondarycustomer.parquet', mode = \"overwrite\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stthirdparty\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    t.id as tms_thirdparty_id\r\n",
							"    ,t.name as tms_thirdparty_name\r\n",
							"    ,t.areacode as tms_thirdparty_areacode\r\n",
							"    ,t.telnumber as tms_thirdparty_telnumber\r\n",
							"    ,t.mobareacode as tms_thirdparty_mobareacode\r\n",
							"    ,t.mobnumer as tms_thirdparty_mobnumer\r\n",
							"    ,t.contact as tms_thirdparty_contact\r\n",
							"    ,t.address as tms_thirdparty_address\r\n",
							"    ,t.streetno as tms_thirdparty_streetno\r\n",
							"    ,t.zipcode as tms_thirdparty_zipcode\r\n",
							"    ,t.city as tms_thirdparty_city\r\n",
							"    ,t.province as tms_thirdparty_province\r\n",
							"    ,t.status as tms_thirdparty_status\r\n",
							"    ,t.mdvoid as tms_thirdparty_mdvoid\r\n",
							"    ,t.registration as tms_thirdparty_registration\r\n",
							"    ,t.email as tms_thirdparty_email\r\n",
							"    ,t.accnumber as tms_thirdparty_accnumber\r\n",
							"    ,to_timestamp(t.insdate) as tms_thirdparty_insdate\r\n",
							"    ,to_timestamp(t.moddate) as tms_thirdparty_moddate\r\n",
							"    ,t.gitstatus as tms_thirdparty_gitstatus\r\n",
							"    ,t.legacyid as tms_thirdparty_legacyid\r\n",
							"    ,t.lat as tms_thirdparty_lat\r\n",
							"    ,t.lng as tms_thirdparty_lng\r\n",
							"    ,t.note as tms_thirdparty_note\r\n",
							"    ,t.revenuecode as tms_thirdparty_revenuecode\r\n",
							"    ,t.country as tms_thirdparty_country\r\n",
							"\r\n",
							"FROM publicthirdparty t"
						],
						"outputs": [],
						"execution_count": 27
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stthirdparty = spark.sql(\"SELECT * FROM stthirdparty\")"
						],
						"outputs": [],
						"execution_count": 28
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stthirdparty.write.parquet('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Structured Data/OPS/stthirdparty.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 29
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stparentroute\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    r.id as tms_parentroute_id\r\n",
							"    ,r.name as tms_parentroute_name\r\n",
							"    ,r.slug as tms_parentroute_slug\r\n",
							"    ,r.parentroute as tms_parentroute_parentroute\r\n",
							"    ,to_timestamp(r.insdate) as tms_parentroute_insdate\r\n",
							"    ,to_timestamp(r.moddate) as tms_parentroute_moddate\r\n",
							"    ,r.insby as tms_parentroute_insby\r\n",
							"    ,r.status as tms_parentroute_status\r\n",
							"    ,r.crossborder as tms_parentroute_crossborder\r\n",
							"    ,r.roundtripdays as tms_parentroute_roundtripdays\r\n",
							"    ,r.distance as tms_parentroute_distance\r\n",
							"    ,r.duration as tms_parentroute_duration\r\n",
							"    ,r.numofuse as tms_parentroute_numofuse\r\n",
							"    ,r.legacyid as tms_parentroute_legacyid\r\n",
							"    ,r.local as tms_parentroute_local\r\n",
							"\r\n",
							"FROM publicroute r\r\n",
							"WHERE r.parentroute is null\r\n",
							""
						],
						"outputs": [],
						"execution_count": 36
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stparentroute = spark.sql(\"SELECT * FROM stparentroute\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stparentroute.write.parquet('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Structured Data/OPS/stparentroute.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 38
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stopsroute_tmp1\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    r.id as tms_opsroute_id\r\n",
							"    ,r.name as tms_opsroute_name\r\n",
							"    ,r.slug as tms_opsroute_slug\r\n",
							"    ,r.parentroute as tms_opsroute_parentroute\r\n",
							"    ,to_timestamp(r.insdate) as tms_opsroute_insdate\r\n",
							"    ,to_timestamp(r.moddate) as tms_opsroute_moddate\r\n",
							"    ,r.insby as tms_opsroute_insby\r\n",
							"    ,r.status as tms_opsroute_status\r\n",
							"    ,r.crossborder as tms_opsroute_crossborder\r\n",
							"    ,r.roundtripdays as tms_opsroute_roundtripdays\r\n",
							"    ,r.distance as tms_opsroute_distance\r\n",
							"    ,r.duration as tms_opsroute_duration\r\n",
							"    ,r.numofuse as tms_opsroute_numofuse\r\n",
							"    ,r.legacyid as tms_opsroute_legacyid\r\n",
							"    ,r.local as tms_opsroute_local\r\n",
							"\r\n",
							"FROM publicroute r\r\n",
							"WHERE r.parentroute is not null\r\n",
							""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stopsroute_tmp1 = spark.sql(\"SELECT * FROM stopsroute_tmp1\")"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stopsroute_tmp2\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    (r.id)*-1 as tms_opsroute_id\r\n",
							"    ,upper(r.description) as tms_opsroute_name\r\n",
							"\r\n",
							"FROM dbolh_routes r\r\n",
							""
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stopsroute_tmp2 = spark.sql(\"SELECT * FROM stopsroute_tmp2\")"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stopsroute = stopsroute_tmp1.unionByName(stopsroute_tmp2, allowMissingColumns = True)\r\n",
							"stopsroute.createOrReplaceTempView(\"stopsroute\")"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stopsroute = spark.sql(\"SELECT * FROM stopsroute\")"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stopsroute.write.parquet('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Structured Data/OPS/stopsroute.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stdispatchactionlog\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"\r\n",
							"    id AS orv_dispatchactionlog_id,\r\n",
							"\tlat AS orv_dispatchactionlog_lat ,\r\n",
							"\tlng AS orv_dispatchactionlog_lng,\r\n",
							"\treason AS orv_dispatchactionlog_reason,\r\n",
							"\tmode AS orv_dispatchactionlog_mode,\r\n",
							"\tto_timestamp(insdate) AS orv_dispatchactionlog_insdate,\r\n",
							"\tdid AS orv_dispatchactionlog_did,\r\n",
							"\taction  AS orv_dispatchactionlog_action,\r\n",
							"\tuid  AS orv_dispatchactionlog_uid\r\n",
							"\r\n",
							"\t\r\n",
							"FROM publicdispatchactionlog"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stdispatchactionlog = spark.sql(\"SELECT * FROM stdispatchactionlog\")"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stdispatchactionlog.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stdispatchactionlog.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stlmsparcelweights\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"     b.tripid AS tms_lmsparcelweights_tripid\r\n",
							"    ,l.bookingid AS tms_lmsparcelweights_bookingid\r\n",
							"    ,l.loadid AS tms_lmsparcelweights_loadid\r\n",
							"    ,t.parcelid AS tms_lmsparcelweights_parcelid\r\n",
							"    ,t.weight AS tms_lmsparcelweights_weight\r\n",
							"    ,t.chargeweight AS tms_lmsparcelweights_chargeweight\r\n",
							"    ,t.volweight AS tms_lmsparcelweights_volweight\r\n",
							"    ,t.volumiserweight AS tms_lmsparcelweights_volumiserweight\r\n",
							"    ,c.name AS tms_lmsparcelweights_billbustomer\r\n",
							"\r\n",
							"FROM publiclmsdata l\r\n",
							"LEFT JOIN publicbooking b ON b.id = l.bookingid\r\n",
							"LEFT JOIN ssttrackunion t ON t.loadid = l.loadid\r\n",
							"LEFT JOIN dbocustomer c ON c.id = t.BillCustID"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stlmsparcelweights = spark.sql(\"SELECT * FROM stlmsparcelweights\")"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stlmsparcelweights.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stlmsparcelweights.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stlmsbookingweights_tmp\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"\r\n",
							"tms_lmsparcelweights_tripid as tms_lmsbookingweights_tripid\r\n",
							",tms_lmsparcelweights_bookingid as tms_lmsbookingweights_bookingid\r\n",
							",tms_lmsparcelweights_loadid as tms_lmsbookingweights_loadid\r\n",
							",tms_lmsparcelweights_billbustomer as tms_lmsbookingweights_billbustomer\r\n",
							",Sum(1) as tms_lmsbookingweights_noofparcels\r\n",
							",Sum(tms_lmsparcelweights_weight) as tms_lmsbookingweights_weight\r\n",
							",Sum(tms_lmsparcelweights_chargeweight) as tms_lmsbookingweights_chargeweight\r\n",
							",Sum(tms_lmsparcelweights_volweight) as tms_lmsbookingweights_volweight\r\n",
							",Sum(tms_lmsparcelweights_volumiserweight) as tms_lmsbookingweights_volumiserweight\r\n",
							"\r\n",
							"\r\n",
							"FROM stlmsparcelweights \r\n",
							"GROUP BY \r\n",
							"tms_lmsparcelweights_tripid\r\n",
							",tms_lmsparcelweights_bookingid\r\n",
							",tms_lmsparcelweights_loadid\r\n",
							",tms_lmsparcelweights_billbustomer\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 30
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stlmsbookingweights_tmp = spark.sql(\"SELECT * FROM stlmsbookingweights_tmp\")"
						],
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stlmsbookingweights\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"     t.tms_lmsbookingweights_tripid\r\n",
							"    ,l.bookingid AS tms_lmsbookingweights_bookingid\r\n",
							"    ,l.loadid AS tms_lmsbookingweights_loadid\r\n",
							"    -- ,l.numprcls AS tms_lmsbookingweights_numprcls\r\n",
							"    -- ,l.totalweight AS tms_lmsbookingweights_totalweight\r\n",
							"    ,t.tms_lmsbookingweights_noofparcels\r\n",
							"    ,t.tms_lmsbookingweights_weight\r\n",
							"    ,t.tms_lmsbookingweights_chargeweight\r\n",
							"    ,t.tms_lmsbookingweights_volweight\r\n",
							"    ,t.tms_lmsbookingweights_volumiserweight\r\n",
							"    ,t.tms_lmsbookingweights_billbustomer\r\n",
							"\r\n",
							"\r\n",
							"FROM publiclmsdata l\r\n",
							"LEFT JOIN stlmsbookingweights_tmp t ON t.tms_lmsbookingweights_bookingid||'|'||t.tms_lmsbookingweights_loadid = l.bookingid||'|'||l.loadid\r\n",
							""
						],
						"outputs": [],
						"execution_count": 36
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stlmsbookingweights = spark.sql(\"SELECT * FROM stlmsbookingweights\")"
						],
						"outputs": [],
						"execution_count": 37
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stlmsbookingweights.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stlmsbookingweights.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 38
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"SUCCESS\")"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/06_STStockTransactions')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Finance"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "773ad6be-f59b-4fb2-b7ad-20a5b1b8684e"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse_Prod/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace-prod/bigDataPools/PRDSparkPool",
						"name": "PRDSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/PRDSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"source": [
							"# #Stock\r\n",
							"# #Create DataFrame for the dboOINM SAP HEADER Table\r\n",
							"# dboOINM = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dboOINM.parquet', format='parquet')\r\n",
							"# dboOINM.createOrReplaceTempView(\"dboOINM\")\r\n",
							"\r\n",
							"# #Goods Issue\r\n",
							"# #Create DataFrame for the dboOIGE SAP HEADER Table\r\n",
							"# dboOIGE = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dboOIGE.parquet', format='parquet')\r\n",
							"# dboOIGE.createOrReplaceTempView(\"dboOIGE\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dboIGE1 SAP LINE Table\r\n",
							"# dboIGE1 = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dboIGE1.parquet', format='parquet')\r\n",
							"# dboIGE1.createOrReplaceTempView(\"dboIGE1\")\r\n",
							"\r\n",
							"# #Goods Receipt\r\n",
							"# #Create DataFrame for the dboOIGN SAP HEADER Table\r\n",
							"# dboOIGN = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dboOIGN.parquet', format='parquet')\r\n",
							"# dboOIGN.createOrReplaceTempView(\"dboOIGN\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dboIGN1 SAP LINE Table\r\n",
							"# dboIGN1 = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dboIGN1.parquet', format='parquet')\r\n",
							"# dboIGN1.createOrReplaceTempView(\"dboIGN1\")"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"ststocktransactions_gr\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"\r\n",
							"    oinm.transnum as sap_stocktransactions_transid\r\n",
							"    ,oinm.transtype as sap_stocktransactions_objtype\r\n",
							"    ,'I' as sap_stocktransactions_doctype\r\n",
							"    ,oinm.createdby as sap_stocktransactions_createdby\r\n",
							"    ,CAST(TRIM(oinm.base_ref) AS INT) as sap_stocktransactions_docnum\r\n",
							"    ,oinm.doclinenum as sap_stocktransactions_doclinenum\r\n",
							"    ,oinm.docdate as sap_stocktransactions_postingdate\r\n",
							"    ,oinm.docduedate as sap_stocktransactions_duedate\r\n",
							"    ,oinm.cardcode as sap_stocktransactions_cardcode\r\n",
							"    ,oinm.cardname as sap_stocktransactions_cardname\r\n",
							"    ,oinm.itemcode as sap_stocktransactions_itemcode\r\n",
							"    ,oinm.dscription as sap_stocktransactions_dscription\r\n",
							"    ,oinm.vendornum as sap_stocktransactions_vendornum\r\n",
							"    ,oinm.warehouse as sap_stocktransactions_warehouse\r\n",
							"    ,oinm.prjcode as sap_stocktransactions_project\r\n",
							"    ,oinm.calcprice as sap_stocktransactions_calcprice\r\n",
							"    ,oinm.transvalue as sap_stocktransactions_linetotal\r\n",
							"    ,oinm.invntact as sap_stocktransactions_invntact\r\n",
							"    -- ,oinm.doffdecacc as sap_stocktransactions_doffdecacc\r\n",
							"    ,oinm.ioffincacc as sap_stocktransactions_acctcode\r\n",
							"    ,sum(oinm.inqty) as sap_stocktransactions_inqty\r\n",
							"    ,sum(oinm.outqty) as sap_stocktransactions_outqty\r\n",
							"    ,(sum(oinm.inqty)-sum(oinm.outqty)) as sap_stocktransactions_qty\r\n",
							"\r\n",
							"\r\n",
							"FROM dboOINM oinm\r\n",
							"WHERE oinm.transtype = 59\r\n",
							"GROUP BY\r\n",
							"    oinm.transnum \r\n",
							"    ,oinm.transtype\r\n",
							"    ,oinm.createdby\r\n",
							"    ,oinm.base_ref \r\n",
							"    ,oinm.doclinenum\r\n",
							"    ,oinm.docdate \r\n",
							"    ,oinm.docduedate\r\n",
							"    ,oinm.cardcode \r\n",
							"    ,oinm.cardname \r\n",
							"    ,oinm.itemcode \r\n",
							"    ,oinm.dscription\r\n",
							"    ,oinm.vendornum\r\n",
							"    ,oinm.warehouse \r\n",
							"    ,oinm.prjcode \r\n",
							"    ,oinm.calcprice \r\n",
							"    ,oinm.transvalue\r\n",
							"    ,oinm.invntact\r\n",
							"    ,oinm.doffdecacc\r\n",
							"    ,oinm.ioffincacc\r\n",
							""
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"ststocktransactions_gr = spark.sql(\"SELECT * FROM ststocktransactions_gr\")"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stgoodsreceipt_tmp2\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    oign.docnum\r\n",
							"    ,(oign.discsum*-1)/COUNT(ign1.linenum) as discsum\r\n",
							"\r\n",
							"FROM dboOIGN oign\r\n",
							"LEFT JOIN dboIGN1 ign1 on ign1.docentry = oign.docentry\r\n",
							"GROUP BY oign.docnum, oign.discsum"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stgoodsreceipt_tmp2 = spark.sql(\"SELECT * FROM stgoodsreceipt_tmp2\")"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stgoodsreceipt_tmp\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"\r\n",
							"    oign.docentry as sap_goodsreceiptheader_docentry\r\n",
							"    ,oign.docnum as sap_goodsreceiptheader_docnum\r\n",
							"    ,oign.doctype as sap_goodsreceiptheader_doctype\r\n",
							"    -- ,oign.canceled as sap_goodsreceiptheader_canceled\r\n",
							"    ,oign.objtype as sap_goodsreceiptheader_objtype\r\n",
							"    ,oign.docdate as sap_goodsreceiptheader_postingdate\r\n",
							"    ,oign.docduedate as sap_goodsreceiptheader_duedate\r\n",
							"    ,oign.cardcode as sap_goodsreceiptheader_cardcode\r\n",
							"    -- ,oign.cardname as sap_goodsreceiptheader_cardname\r\n",
							"    ,oign.numatcard as sap_goodsreceiptheader_cardref\r\n",
							"    ,oign.discprcnt as sap_goodsreceiptheader_discprcnt\r\n",
							"    ,oign.discsum as sap_goodsreceiptheader_discsum\r\n",
							"    ,oign.doctotal as sap_goodsreceiptheader_doctotal\r\n",
							"    ,oign.comments as sap_goodsreceiptheader_comments\r\n",
							"    ,oign.transid as sap_goodsreceiptheader_transid\r\n",
							"    -- ,oign.usersign as sap_goodsreceiptheader_usersign\r\n",
							"    -- ,oign.u_depot as sap_goodsreceiptheader_u_depot\r\n",
							"    -- ,oign.u_usercode as sap_goodsreceiptheader_u_usercode\r\n",
							"    ,oign.u_expense_depot as sap_goodsreceiptheader_u_expense_depot\r\n",
							"    -- ,oign.u_transaction_date as sap_goodsreceiptheader_u_transaction_date\r\n",
							"    -- ,oign.rounddif as sap_goodsreceiptheader_rounddif\r\n",
							"    -- ,oign.rounding as sap_goodsreceiptheader_rounding\r\n",
							"    -- ,ign1.docentry as sap_goodsreceiptline_docentry\r\n",
							"    ,ign1.linenum as sap_goodsreceiptline_linenum\r\n",
							"    ,ign1.linestatus as sap_goodsreceiptline_linestatus\r\n",
							"    ,ign1.itemcode as sap_goodsreceiptline_itemcode\r\n",
							"    ,ign1.dscription as sap_goodsreceiptline_dscription\r\n",
							"    ,ign1.quantity as sap_goodsreceiptline_quantity\r\n",
							"    ,ign1.openqty as sap_goodsreceiptline_openqty\r\n",
							"    ,ign1.price as sap_goodsreceiptline_price\r\n",
							"    ,ign1.rate as sap_goodsreceiptline_rate\r\n",
							"    ,ign1.discprcnt as sap_goodsreceiptline_discprcnt\r\n",
							"    ,ign1.linetotal as sap_goodsreceiptline_linetotal\r\n",
							"    ,ign1.opensum as sap_goodsreceiptline_opensum\r\n",
							"    ,ign1.pricebefdi as sap_goodsreceiptline_pricebefdi\r\n",
							"    ,ign1.docdate as sap_goodsreceiptline_docdate\r\n",
							"    ,ign1.project as sap_goodsreceiptline_project\r\n",
							"    ,ign1.vatprcnt as sap_goodsreceiptline_vatprcnt\r\n",
							"    -- ,ign1.volume as sap_goodsreceiptline_volume\r\n",
							"    ,ign1.vatgroup as sap_goodsreceiptline_vatgroup\r\n",
							"    ,ign1.vatsum as sap_goodsreceiptline_vatamount\r\n",
							"    -- ,ign1.grssprofit as sap_goodsreceiptline_grssprofit\r\n",
							"    ,ign1.u_reason as sap_goodsreceiptline_u_reason\r\n",
							"    -- ,ign1.u_lane as sap_goodsreceiptline_u_lane\r\n",
							"    ,ign1.u_expenseitem as sap_goodsreceiptline_u_expenseitem\r\n",
							"    ,ign1.u_fueldate as sap_goodsreceiptline_u_fueldate\r\n",
							"    ,ign1.acctcode as sap_goodsreceiptline_acctcode\r\n",
							"    ,'goodsreceipt' as sap_goodsreceiptline_doccat\r\n",
							"    ,Right(LEFT(ign1.u_expenseitem,5),3) as sap_goodsreceiptline_subdepotcode\r\n",
							"    ,t.discsum as sap_goodsreceiptline_discsum\r\n",
							"\r\n",
							"FROM dboOIGN oign\r\n",
							"LEFT JOIN dboIGN1 ign1 on ign1.docentry = oign.docentry\r\n",
							"LEFT JOIN stgoodsreceipt_tmp2 t on t.docnum = oign.docnum\r\n",
							""
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stgoodsreceipt_tmp = spark.sql(\"SELECT * FROM stgoodsreceipt_tmp\")"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"ststocktransactions_gr2\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    gr.sap_goodsreceiptheader_docentry as sap_stocktransactionsheader_docentry\r\n",
							"    ,sgr.sap_stocktransactions_docnum as sap_stocktransactionsheader_docnum\r\n",
							"    ,sgr.sap_stocktransactions_doctype as sap_stocktransactionsheader_doctype\r\n",
							"    -- ,gr.sap_goodsreceiptheader_canceled as sap_stocktransactionsheader_canceled\r\n",
							"    ,sgr.sap_stocktransactions_objtype as sap_stocktransactionsheader_objtype\r\n",
							"    ,sgr.sap_stocktransactions_postingdate as sap_stocktransactionsheader_postingdate\r\n",
							"    ,sgr.sap_stocktransactions_duedate as sap_stocktransactionsheader_duedate\r\n",
							"    ,sgr.sap_stocktransactions_cardcode as sap_stocktransactionsheader_cardcode\r\n",
							"    -- ,sgr.sap_stocktransactions_cardname as sap_stocktransactionsheader_cardname\r\n",
							"    ,gr.sap_goodsreceiptheader_cardref as sap_stocktransactionsheader_cardref\r\n",
							"    ,gr.sap_goodsreceiptheader_discprcnt as sap_stocktransactionsheader_discprcnt\r\n",
							"    ,gr.sap_goodsreceiptheader_discsum as sap_stocktransactionsheader_discsum\r\n",
							"    ,gr.sap_goodsreceiptheader_doctotal as sap_stocktransactionsheader_doctotal\r\n",
							"    ,gr.sap_goodsreceiptheader_comments as sap_stocktransactionsheader_comments\r\n",
							"    ,sgr.sap_stocktransactions_transid as sap_stocktransactionsheader_transid\r\n",
							"    -- ,gr.sap_goodsreceiptheader_usersign as sap_stocktransactionsheader_usersign\r\n",
							"    -- ,gr.sap_goodsreceiptheader_u_depot as sap_stocktransactionsheader_u_depot\r\n",
							"    -- ,gr.sap_goodsreceiptheader_u_usercode as sap_stocktransactionsheader_u_usercode\r\n",
							"    ,gr.sap_goodsreceiptheader_u_expense_depot as sap_stocktransactionsheader_u_expense_depot\r\n",
							"    -- ,gr.sap_goodsreceiptheader_u_transaction_date as sap_stocktransactionsheader_u_transaction_date\r\n",
							"    -- ,gr.sap_goodsreceiptheader_rounddif as sap_stocktransactionsheader_rounddif\r\n",
							"    -- ,gr.sap_goodsreceiptheader_rounding as sap_stocktransactionsheader_rounding\r\n",
							"    -- ,gr.sap_goodsreceiptline_docentry as sap_stocktransactionsline_docentry\r\n",
							"    ,gr.sap_goodsreceiptline_linenum as sap_stocktransactionsline_linenum\r\n",
							"    ,gr.sap_goodsreceiptline_linestatus as sap_stocktransactionsline_linestatus\r\n",
							"    ,sgr.sap_stocktransactions_itemcode as sap_stocktransactionsline_itemcode\r\n",
							"    ,sgr.sap_stocktransactions_dscription as sap_stocktransactionsline_dscription\r\n",
							"    ,gr.sap_goodsreceiptline_quantity as sap_stocktransactionsline_quantity\r\n",
							"    ,gr.sap_goodsreceiptline_openqty as sap_stocktransactionsline_openqty\r\n",
							"    ,gr.sap_goodsreceiptline_price as sap_stocktransactionsline_price\r\n",
							"    ,gr.sap_goodsreceiptline_rate as sap_stocktransactionsline_rate\r\n",
							"    ,gr.sap_goodsreceiptline_discprcnt as sap_stocktransactionsline_discprcnt\r\n",
							"    ,sgr.sap_stocktransactions_linetotal as sap_stocktransactionsline_linetotal\r\n",
							"    ,gr.sap_goodsreceiptline_opensum as sap_stocktransactionsline_opensum\r\n",
							"    ,gr.sap_goodsreceiptline_pricebefdi as sap_stocktransactionsline_pricebefdi\r\n",
							"    ,gr.sap_goodsreceiptline_docdate as sap_stocktransactionsline_docdate\r\n",
							"    ,sgr.sap_stocktransactions_project as sap_stocktransactionsline_project\r\n",
							"    ,gr.sap_goodsreceiptline_vatprcnt as sap_stocktransactionsline_vatprcnt\r\n",
							"    -- ,gr.sap_goodsreceiptline_volume as sap_stocktransactionsline_volume\r\n",
							"    ,gr.sap_goodsreceiptline_vatgroup as sap_stocktransactionsline_vatgroup\r\n",
							"    ,gr.sap_goodsreceiptline_vatamount as sap_stocktransactionsline_vatamount\r\n",
							"    -- ,gr.sap_goodsreceiptline_grssprofit as sap_stocktransactionsline_grssprofit\r\n",
							"    ,gr.sap_goodsreceiptline_u_reason as sap_stocktransactionsline_u_reason\r\n",
							"    -- ,gr.sap_goodsreceiptline_u_lane as sap_stocktransactionsline_u_lane\r\n",
							"    ,gr.sap_goodsreceiptline_u_expenseitem as sap_stocktransactionsline_u_expenseitem\r\n",
							"    ,gr.sap_goodsreceiptline_u_fueldate as sap_stocktransactionsline_u_fueldate\r\n",
							"    ,sgr.sap_stocktransactions_acctcode as sap_stocktransactionsline_acctcode\r\n",
							"    ,gr.sap_goodsreceiptline_doccat as sap_stocktransactionsline_doccat\r\n",
							"    ,gr.sap_goodsreceiptline_subdepotcode as sap_stocktransactionsline_subdepotcode\r\n",
							"    ,gr.sap_goodsreceiptline_discsum as sap_stocktransactionsline_discsum\r\n",
							"\r\n",
							"FROM ststocktransactions_gr sgr\r\n",
							"LEFT JOIN stgoodsreceipt_tmp gr on gr.sap_goodsreceiptheader_docnum = sgr.sap_stocktransactions_docnum\r\n",
							"AND gr.sap_goodsreceiptline_linenum = sgr.sap_stocktransactions_doclinenum\r\n",
							""
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"ststocktransactions_gr2 = spark.sql(\"SELECT * FROM ststocktransactions_gr2\")"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"ststocktransactions_gi\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"\r\n",
							"    oinm.transnum as sap_stocktransactions_transid\r\n",
							"    ,oinm.transtype as sap_stocktransactions_objtype\r\n",
							"    ,'I' as sap_stocktransactions_doctype\r\n",
							"    ,oinm.createdby as sap_stocktransactions_createdby\r\n",
							"    ,CAST(TRIM(oinm.base_ref) AS INT) as sap_stocktransactions_docnum\r\n",
							"    ,oinm.doclinenum as sap_stocktransactions_doclinenum\r\n",
							"    ,oinm.docdate as sap_stocktransactions_postingdate\r\n",
							"    ,oinm.docduedate as sap_stocktransactions_duedate\r\n",
							"    ,oinm.cardcode as sap_stocktransactions_cardcode\r\n",
							"    ,oinm.cardname as sap_stocktransactions_cardname\r\n",
							"    ,oinm.itemcode as sap_stocktransactions_itemcode\r\n",
							"    ,oinm.dscription as sap_stocktransactions_dscription\r\n",
							"    ,oinm.vendornum as sap_stocktransactions_vendornum\r\n",
							"    ,oinm.warehouse as sap_stocktransactions_warehouse\r\n",
							"    ,oinm.prjcode as sap_stocktransactions_project\r\n",
							"    ,oinm.calcprice as sap_stocktransactions_calcprice\r\n",
							"    ,oinm.transvalue as sap_stocktransactions_linetotal\r\n",
							"    ,oinm.invntact as sap_stocktransactions_invntact\r\n",
							"    ,oinm.doffdecacc as sap_stocktransactions_acctcode\r\n",
							"    -- ,oinm.ioffincacc as sap_stocktransactions_ioffincacc\r\n",
							"    ,sum(oinm.inqty) as sap_stocktransactions_inqty\r\n",
							"    ,sum(oinm.outqty) as sap_stocktransactions_outqty\r\n",
							"    ,(sum(oinm.outqty)-sum(oinm.inqty)) as sap_stocktransactions_qty\r\n",
							"\r\n",
							"\r\n",
							"FROM dboOINM oinm\r\n",
							"WHERE oinm.transtype = 60\r\n",
							"GROUP BY\r\n",
							"    oinm.transnum \r\n",
							"    ,oinm.transtype\r\n",
							"    ,oinm.createdby\r\n",
							"    ,oinm.base_ref \r\n",
							"    ,oinm.doclinenum\r\n",
							"    ,oinm.docdate \r\n",
							"    ,oinm.docduedate\r\n",
							"    ,oinm.cardcode \r\n",
							"    ,oinm.cardname \r\n",
							"    ,oinm.itemcode \r\n",
							"    ,oinm.dscription\r\n",
							"    ,oinm.vendornum\r\n",
							"    ,oinm.warehouse \r\n",
							"    ,oinm.prjcode \r\n",
							"    ,oinm.calcprice \r\n",
							"    ,oinm.transvalue\r\n",
							"    ,oinm.invntact\r\n",
							"    ,oinm.doffdecacc\r\n",
							"    ,oinm.ioffincacc\r\n",
							"\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"ststocktransactions_gi = spark.sql(\"SELECT * FROM ststocktransactions_gi\")"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stgoodsissue_tmp2\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    oige.docnum\r\n",
							"    ,(oige.discsum)/COUNT(ige1.linenum) as discsum\r\n",
							"FROM dboOIGE oige\r\n",
							"LEFT JOIN dboIGE1 ige1 on ige1.docentry = oige.docentry\r\n",
							"GROUP BY oige.docnum, oige.discsum"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stgoodsissue_tmp2 = spark.sql(\"SELECT * FROM stgoodsissue_tmp2\")"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stgoodsissue_tmp\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    oige.docentry as sap_goodsissueheader_docentry\r\n",
							"    ,oige.docnum as sap_goodsissueheader_docnum\r\n",
							"    ,oige.doctype as sap_goodsissueheader_doctype\r\n",
							"    -- ,oige.canceled as sap_goodsissueheader_canceled\r\n",
							"    ,oige.objtype as sap_goodsissueheader_objtype\r\n",
							"    ,oige.docdate as sap_goodsissueheader_postingdate\r\n",
							"    ,oige.docduedate as sap_goodsissueheader_duedate\r\n",
							"    ,oige.cardcode as sap_goodsissueheader_cardcode\r\n",
							"    -- ,oige.cardname as sap_goodsissueheader_cardname\r\n",
							"    ,oige.numatcard as sap_goodsissueheader_cardref\r\n",
							"    ,oige.discprcnt as sap_goodsissueheader_discprcnt\r\n",
							"    ,oige.discsum as sap_goodsissueheader_discsum\r\n",
							"    ,oige.doctotal as sap_goodsissueheader_doctotal\r\n",
							"    ,oige.comments as sap_goodsissueheader_comments\r\n",
							"    ,oige.transid as sap_goodsissueheader_transid\r\n",
							"    -- ,oige.usersign as sap_goodsissueheader_usersign\r\n",
							"    -- ,oige.u_depot as sap_goodsissueheader_u_depot\r\n",
							"    -- ,oige.u_usercode as sap_goodsissueheader_u_usercode\r\n",
							"    ,oige.u_expense_depot as sap_goodsissueheader_u_expense_depot\r\n",
							"    -- ,oige.u_transaction_date as sap_goodsissueheader_u_transaction_date\r\n",
							"    -- ,oige.rounddif as sap_goodsissueheader_rounddif\r\n",
							"    -- ,oige.rounding as sap_goodsissueheader_rounding\r\n",
							"    -- ,ige1.docentry as sap_goodsissueline_docentry\r\n",
							"    ,ige1.linenum as sap_goodsissueline_linenum\r\n",
							"    ,ige1.linestatus as sap_goodsissueline_linestatus\r\n",
							"    ,ige1.itemcode as sap_goodsissueline_itemcode\r\n",
							"    ,ige1.dscription as sap_goodsissueline_dscription\r\n",
							"    ,ige1.quantity as sap_goodsissueline_quantity\r\n",
							"    ,ige1.openqty as sap_goodsissueline_openqty\r\n",
							"    ,ige1.price as sap_goodsissueline_price\r\n",
							"    ,ige1.rate as sap_goodsissueline_rate\r\n",
							"    ,ige1.discprcnt as sap_goodsissueline_discprcnt\r\n",
							"    ,ige1.linetotal as sap_goodsissueline_linetotal\r\n",
							"    ,ige1.opensum as sap_goodsissueline_opensum\r\n",
							"    ,ige1.pricebefdi as sap_goodsissueline_pricebefdi\r\n",
							"    ,ige1.docdate as sap_goodsissueline_docdate\r\n",
							"    ,ige1.project as sap_goodsissueline_project\r\n",
							"    ,ige1.vatprcnt as sap_goodsissueline_vatprcnt\r\n",
							"    -- ,ige1.volume as sap_goodsissueline_volume\r\n",
							"    ,ige1.vatgroup as sap_goodsissueline_vatgroup\r\n",
							"    ,ige1.vatsum as sap_goodsissueline_vatamount\r\n",
							"    -- ,ige1.grssprofit as sap_goodsissueline_grssprofit\r\n",
							"    ,ige1.u_reason as sap_goodsissueline_u_reason\r\n",
							"    -- ,ige1.u_lane as sap_goodsissueline_u_lane\r\n",
							"    ,ige1.u_expenseitem as sap_goodsissueline_u_expenseitem\r\n",
							"    ,ige1.u_fueldate as sap_goodsissueline_u_fueldate\r\n",
							"    ,ige1.acctcode as sap_goodsissueline_acctcode\r\n",
							"    ,'goodsissue' as sap_goodsissueline_doccat\r\n",
							"    ,Right(LEFT(ige1.u_expenseitem,5),3) as sap_goodsissueline_subdepotcode\r\n",
							"    ,t.discsum as sap_goodsissueline_discsum\r\n",
							"\r\n",
							"\r\n",
							"FROM dboOIGE oige\r\n",
							"LEFT JOIN dboIGE1 ige1 on ige1.docentry = oige.docentry\r\n",
							"LEFT JOIN stgoodsissue_tmp2 t on t.docnum = oige.docnum\r\n",
							""
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stgoodsissue_tmp = spark.sql(\"SELECT * FROM stgoodsissue_tmp\")"
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"ststocktransactions_gi2\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    gi.sap_goodsissueheader_docentry as sap_stocktransactionsheader_docentry\r\n",
							"    ,sgi.sap_stocktransactions_docnum as sap_stocktransactionsheader_docnum\r\n",
							"    ,gi.sap_goodsissueheader_doctype as sap_stocktransactionsheader_doctype\r\n",
							"    -- ,gi.sap_goodsissueheader_canceled as sap_stocktransactionsheader_canceled\r\n",
							"    ,sgi.sap_stocktransactions_objtype as sap_stocktransactionsheader_objtype\r\n",
							"    ,sgi.sap_stocktransactions_postingdate as sap_stocktransactionsheader_postingdate\r\n",
							"    ,sgi.sap_stocktransactions_duedate as sap_stocktransactionsheader_duedate\r\n",
							"    ,sgi.sap_stocktransactions_cardcode as sap_stocktransactionsheader_cardcode\r\n",
							"    -- ,sgi.sap_stocktransactions_cardname as sap_stocktransactionsheader_cardname\r\n",
							"    ,gi.sap_goodsissueheader_cardref as sap_stocktransactionsheader_cardref\r\n",
							"    ,gi.sap_goodsissueheader_discprcnt as sap_stocktransactionsheader_discprcnt\r\n",
							"    ,gi.sap_goodsissueheader_discsum as sap_stocktransactionsheader_discsum\r\n",
							"    ,gi.sap_goodsissueheader_doctotal as sap_stocktransactionsheader_doctotal\r\n",
							"    ,gi.sap_goodsissueheader_comments as sap_stocktransactionsheader_comments\r\n",
							"    ,sgi.sap_stocktransactions_transid as sap_stocktransactionsheader_transid\r\n",
							"    -- ,gi.sap_goodsissueheader_usersign as sap_stocktransactionsheader_usersign\r\n",
							"    -- ,gi.sap_goodsissueheader_u_depot as sap_stocktransactionsheader_u_depot\r\n",
							"    -- ,gi.sap_goodsissueheader_u_usercode as sap_stocktransactionsheader_u_usercode\r\n",
							"    ,gi.sap_goodsissueheader_u_expense_depot as sap_stocktransactionsheader_u_expense_depot\r\n",
							"    -- ,gi.sap_goodsissueheader_u_transaction_date as sap_stocktransactionsheader_u_transaction_date\r\n",
							"    -- ,gi.sap_goodsissueheader_rounddif as sap_stocktransactionsheader_rounddif\r\n",
							"    -- ,gi.sap_goodsissueheader_rounding as sap_stocktransactionsheader_rounding\r\n",
							"    -- ,gi.sap_goodsissueline_docentry as sap_stocktransactionsline_docentry\r\n",
							"    ,gi.sap_goodsissueline_linenum as sap_stocktransactionsline_linenum\r\n",
							"    ,gi.sap_goodsissueline_linestatus as sap_stocktransactionsline_linestatus\r\n",
							"    ,sgi.sap_stocktransactions_itemcode as sap_stocktransactionsline_itemcode\r\n",
							"    ,sgi.sap_stocktransactions_dscription as sap_stocktransactionsline_dscription\r\n",
							"    ,gi.sap_goodsissueline_quantity as sap_stocktransactionsline_quantity\r\n",
							"    ,gi.sap_goodsissueline_openqty as sap_stocktransactionsline_openqty\r\n",
							"    ,gi.sap_goodsissueline_price as sap_stocktransactionsline_price\r\n",
							"    ,gi.sap_goodsissueline_rate as sap_stocktransactionsline_rate\r\n",
							"    ,gi.sap_goodsissueline_discprcnt as sap_stocktransactionsline_discprcnt\r\n",
							"    ,sgi.sap_stocktransactions_linetotal as sap_stocktransactionsline_linetotal\r\n",
							"    ,gi.sap_goodsissueline_opensum as sap_stocktransactionsline_opensum\r\n",
							"    ,gi.sap_goodsissueline_pricebefdi as sap_stocktransactionsline_pricebefdi\r\n",
							"    ,gi.sap_goodsissueline_docdate as sap_stocktransactionsline_docdate\r\n",
							"    ,sgi.sap_stocktransactions_project as sap_stocktransactionsline_project\r\n",
							"    ,gi.sap_goodsissueline_vatprcnt as sap_stocktransactionsline_vatprcnt\r\n",
							"    -- ,gi.sap_goodsissueline_volume as sap_stocktransactionsline_volume\r\n",
							"    ,gi.sap_goodsissueline_vatgroup as sap_stocktransactionsline_vatgroup\r\n",
							"    ,gi.sap_goodsissueline_vatamount as sap_stocktransactionsline_vatamount\r\n",
							"    -- ,gi.sap_goodsissueline_grssprofit as sap_stocktransactionsline_grssprofit\r\n",
							"    ,gi.sap_goodsissueline_u_reason as sap_stocktransactionsline_u_reason\r\n",
							"    -- ,gi.sap_goodsissueline_u_lane as sap_stocktransactionsline_u_lane\r\n",
							"    ,gi.sap_goodsissueline_u_expenseitem as sap_stocktransactionsline_u_expenseitem\r\n",
							"    ,gi.sap_goodsissueline_u_fueldate as sap_stocktransactionsline_u_fueldate\r\n",
							"    ,sgi.sap_stocktransactions_acctcode as sap_stocktransactionsline_acctcode\r\n",
							"    ,gi.sap_goodsissueline_doccat as sap_stocktransactionsline_doccat\r\n",
							"    ,gi.sap_goodsissueline_subdepotcode as sap_stocktransactionsline_subdepotcode\r\n",
							"    ,gi.sap_goodsissueline_discsum as sap_stocktransactionsline_discsum\r\n",
							"\r\n",
							"FROM ststocktransactions_gi sgi\r\n",
							"LEFT JOIN stgoodsissue_tmp gi on gi.sap_goodsissueheader_docnum = sgi.sap_stocktransactions_docnum\r\n",
							"AND gi.sap_goodsissueline_linenum = sgi.sap_stocktransactions_doclinenum\r\n",
							""
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"ststocktransactions_gi2 = spark.sql(\"SELECT * FROM ststocktransactions_gi2\")"
						],
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stocktransactions = ststocktransactions_gi2.unionByName(ststocktransactions_gr2, allowMissingColumns = True)\r\n",
							"stocktransactions.createOrReplaceTempView(\"stocktransactions\")"
						],
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"ststocktransactions\r\n",
							"AS\r\n",
							"SELECT * \r\n",
							"FROM stocktransactions"
						],
						"outputs": [],
						"execution_count": 27
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"ststocktransactions = spark.sql(\"SELECT * FROM ststocktransactions\")"
						],
						"outputs": [],
						"execution_count": 28
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# ststocktransactions.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/ststocktransactions.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 29
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"SUCCESS\")"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/07_DIST_STLoadChild')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Ops/Distribution"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "b0d69996-36a9-4591-96c4-41a2c03f51c6"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace/bigDataPools/DevSparkPool",
						"name": "DevSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/DevSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"# #Create DataFrame for the dboloads LMS Table\r\n",
							"# dboloads = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboloads.parquet', format='parquet')\r\n",
							"# dboloads.createOrReplaceTempView(\"dboloads\")\r\n",
							"\r\n",
							"# #Create DataFrame for the sstparcelbyload LMS Table\r\n",
							"# sstparcelbyload = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Semi Structured Data/OPS/sstparcelbyload.parquet', format='parquet')\r\n",
							"# sstparcelbyload.createOrReplaceTempView(\"sstparcelbyload\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stloadchild\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    l.id as lms_loadchild_id\r\n",
							"    ,l.routeid as lms_loadchild_routeid\r\n",
							"    ,l.driverid as lms_loadchild_driverid\r\n",
							"    ,l.sealno as lms_loadchild_sealno\r\n",
							"    ,l.vehicleid as lms_loadchild_vehicleid\r\n",
							"    ,l.ttypeid as lms_loadchild_ttypeid\r\n",
							"    ,l.userid as lms_loadchild_userid\r\n",
							"    ,l.ldate as lms_loadchild_ldate\r\n",
							"    ,l.courierid as lms_loadchild_courierid\r\n",
							"    ,l.site as lms_loadchild_site\r\n",
							"    ,l.fromlocid as lms_loadchild_fromlocid\r\n",
							"    ,l.tolocid as lms_loadchild_tolocid\r\n",
							"    ,l.cdate as lms_loadchild_cdate\r\n",
							"    ,l.trailer1 as lms_loadchild_trailer1\r\n",
							"    ,l.trailer2 as lms_loadchild_trailer2\r\n",
							"    ,l.cref as lms_loadchild_cref\r\n",
							"    ,l.debriefed as lms_loadchild_debriefed\r\n",
							"    ,l.mrpflag as lms_loadchild_mrpflag\r\n",
							"    ,l.mrpcflag as lms_loadchild_mrpcflag\r\n",
							"    ,l.finalflag as lms_loadchild_finalflag\r\n",
							"    ,l.alidaflag as lms_loadchild_alidaflag\r\n",
							"    ,l.puibts as lms_loadchild_puibts\r\n",
							"    ,l.pucols as lms_loadchild_pucols\r\n",
							"    ,l.planningref as lms_loadchild_planningref\r\n",
							"    ,l.interfacedate as lms_loadchild_interfacedate\r\n",
							"    ,l.nosealsret as lms_loadchild_nosealsret\r\n",
							"    ,l.excludeopsi as lms_loadchild_excludeopsi\r\n",
							"    ,l.dccref as lms_loadchild_dccref\r\n",
							"    ,l.srouteid as lms_loadchild_srouteid\r\n",
							"    ,l.fuellitres as lms_loadchild_fuellitres\r\n",
							"    ,l.outsourcedcrew as lms_loadchild_outsourcedcrew\r\n",
							"    ,l.masterloadid as lms_loadchild_masterloadid\r\n",
							"    ,l.securitygatedespatched as lms_loadchild_securitygatedespatched\r\n",
							"    ,l.securitygateuserid as lms_loadchild_securitygateuserid\r\n",
							"    ,l.securitygatearrived as lms_loadchild_securitygatearrived\r\n",
							"    ,l.securitygatearriveduserid as lms_loadchild_securitygatearriveduserid\r\n",
							"    ,l.lastupdate as lms_loadchild_lastupdate\r\n",
							"    ,l.bayno as lms_loadchild_bayno\r\n",
							"    ,l.costs as lms_loadchild_costs\r\n",
							"    ,l.estdeparturedatetime as lms_loadchild_estdeparturedatetime\r\n",
							"    ,l.totalweight as lms_loadchild_totalweight\r\n",
							"    ,l.ovrignore as lms_loadchild_ovrignore\r\n",
							"    ,l.childloadid as lms_loadchild_childloadid\r\n",
							"    ,l.orvcode as lms_loadchild_orvcode\r\n",
							"    ,l.companyid as lms_loadchild_companyid\r\n",
							"    ,l.editloaddate as lms_loadchild_editloaddate\r\n",
							"    ,l.reopeneddate as lms_loadchild_reopeneddate\r\n",
							"    ,l.reopeneduserid as lms_loadchild_reopeneduserid\r\n",
							"    ,l.novehicle as lms_loadchild_novehicle\r\n",
							"    ,l.mobilefeedbackreasonid as lms_loadchild_mobilefeedbackreasonid\r\n",
							"    ,l.direct as lms_loadchild_direct\r\n",
							"    ,l.noofpcls as lms_loadchild_noofpcls\r\n",
							"    ,l.debriefdt as lms_loadchild_debriefdt\r\n",
							"    ,l.httpcode as lms_loadchild_httpcode\r\n",
							"    ,l.parentloadid as lms_loadchild_parentloadid\r\n",
							"    ,l.mrpobversion as lms_loadchild_mrpobversion\r\n",
							"    ,l.EffectiveLoadDurationMinutes as lms_loadchild_effectiveloaddurationminutes\r\n",
							"    ,l.EffectiveOffLoadDurationMinutes as lms_loadchild_effectiveoffloaddurationminutes\r\n",
							"    ,pbl.lms_parcelbyload_loadid as lms_loadchild_loadid\r\n",
							"    ,pbl.lms_parcelbyload_aggrweight as lms_loadchild_aggrweight\r\n",
							"    ,pbl.lms_parcelbyload_aggrchargeweight as lms_loadchild_aggrchargeweight\r\n",
							"    ,pbl.lms_parcelbyload_aggrvolweight as lms_loadchild_aggrvolweight\r\n",
							"    ,pbl.lms_parcelbyload_aggrvolumiserweight as lms_loadchild_aggrvolumiserweight\r\n",
							"    ,pbl.lms_parcelbyload_aggrmaxweight as lms_loadchild_aggrmaxweight\r\n",
							"    ,pbl.lms_parcelbyload_aggrnoparcels as lms_loadchild_aggrnoparcels\r\n",
							"    ,pbl.lms_parcelbyload_aggrweight as lms_loadchild_aggrweightexclspecdel\r\n",
							"    ,pbl.lms_parcelbyload_aggrchargeweight as lms_loadchild_aggrchargeweightexclspecdel\r\n",
							"    ,pbl.lms_parcelbyload_aggrvolweight as lms_loadchild_aggrvolweightexclspecdel\r\n",
							"    ,pbl.lms_parcelbyload_aggrvolumiserweight as lms_loadchild_aggrvolumiserweightexclspecdel\r\n",
							"    ,pbl.lms_parcelbyload_aggrmaxweight as lms_loadchild_aggrmaxweightexclspecdel\r\n",
							"    ,pbl.lms_parcelbyload_aggrnoparcels as lms_loadchild_aggrnoparcelsexclspecdel\r\n",
							"    -- Not added to SOR table definition as I do not want this in the DB table\r\n",
							"    ,l.vehodo as lms_loadchild_vehodo\r\n",
							"    ,l.vehodo2 as lms_loadchild_vehodo2\r\n",
							"\r\n",
							"FROM dboloads l\r\n",
							"LEFT JOIN sstparcelbyload pbl on pbl.lms_parcelbyload_loadid = l.id"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stloadchild = spark.sql(\"SELECT * FROM stloadchild\")"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stloadchild.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stloadchild.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"SUCCESS\")"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/07_LH_STTMSTrack')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Ops/Linehaul"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "73cc84ae-d501-43c3-a6ca-f318b4b94f3d"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse_Prod/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace-prod/bigDataPools/PRDSparkPool",
						"name": "PRDSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						}
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# #Create DataFrame for the publictrack TMS Table\r\n",
							"# publictrack = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publictrack.parquet', format='parquet')\r\n",
							"# publictrack.createOrReplaceTempView(\"publictrack\")"
						],
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"sttmstrack\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"\r\n",
							"    t.id as tms_track_id\r\n",
							"    ,t.tripid as tms_track_tripid\r\n",
							"    ,t.source as tms_track_source\r\n",
							"    ,to_timestamp(t.trackdate) as tms_track_trackdate\r\n",
							"    ,t.tracktype as tms_track_tracktype\r\n",
							"    ,t.userid as tms_track_userid\r\n",
							"    ,t.lat as tms_track_lat\r\n",
							"    ,t.lng as tms_track_lng\r\n",
							"    ,t.address as tms_track_address\r\n",
							"    ,t.note as tms_track_note\r\n",
							"\r\n",
							"FROM publictrack t\r\n",
							""
						],
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sttmstrack = spark.sql(\"SELECT * FROM sttmstrack\")"
						],
						"outputs": [],
						"execution_count": 32
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# sttmstrack.write.parquet('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Structured Data/OPS/sttmstrack.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 33
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"SUCCESS\")"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/07_STDocuments')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Finance"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "7518f9a5-349e-4d9b-af4b-13f4213d52b4"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse_Prod/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace-prod/bigDataPools/PRDSparkPool",
						"name": "PRDSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/PRDSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 114
					},
					{
						"cell_type": "code",
						"source": [
							"# #Sales Invoice\r\n",
							"# #Create DataFrame for the dboOINV SAP HEADER Table\r\n",
							"# dboOINV = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dboOINV.parquet', format='parquet')\r\n",
							"# dboOINV.createOrReplaceTempView(\"dboOINV\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dboINV1 SAP LINE Table\r\n",
							"# dboINV1 = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dboINV1.parquet', format='parquet')\r\n",
							"# dboINV1.createOrReplaceTempView(\"dboINV1\")\r\n",
							"\r\n",
							"# #Sales Credit\r\n",
							"# #Create DataFrame for the dboORIN SAP HEADER Table\r\n",
							"# dboORIN = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dboORIN.parquet', format='parquet')\r\n",
							"# dboORIN.createOrReplaceTempView(\"dboORIN\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dboRIN1 SAP LINE Table\r\n",
							"# dboRIN1 = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dboRIN1.parquet', format='parquet')\r\n",
							"# dboRIN1.createOrReplaceTempView(\"dboRIN1\")\r\n",
							"\r\n",
							"# #Purchase Invoice\r\n",
							"# #Create DataFrame for the dboOPCH SAP HEADER Table\r\n",
							"# dboOPCH = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dboOPCH.parquet', format='parquet')\r\n",
							"# dboOPCH.createOrReplaceTempView(\"dboOPCH\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dboPCH1 SAP LINE Table\r\n",
							"# dboPCH1 = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dboPCH1.parquet', format='parquet')\r\n",
							"# dboPCH1.createOrReplaceTempView(\"dboPCH1\")\r\n",
							"\r\n",
							"# #Purchase Credit\r\n",
							"# #Create DataFrame for the dboORPC SAP HEADER Table\r\n",
							"# dboORPC = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dboORPC.parquet', format='parquet')\r\n",
							"# dboORPC.createOrReplaceTempView(\"dboORPC\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dboRPC1 SAP LINE Table\r\n",
							"# dboRPC1 = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dboRPC1.parquet', format='parquet')\r\n",
							"# dboRPC1.createOrReplaceTempView(\"dboRPC1\")\r\n",
							"\r\n",
							"# #Goods Receipt Note\r\n",
							"# #Create DataFrame for the dboOPDN SAP HEADER Table\r\n",
							"# dboOPDN = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dboOPDN.parquet', format='parquet')\r\n",
							"# dboOPDN.createOrReplaceTempView(\"dboOPDN\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dboPDN1 SAP LINE Table\r\n",
							"# dboPDN1 = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dboPDN1.parquet', format='parquet')\r\n",
							"# dboPDN1.createOrReplaceTempView(\"dboPDN1\")\r\n",
							"\r\n",
							"# #Goods Return\r\n",
							"# #Create DataFrame for the dboORPD SAP HEADER Table\r\n",
							"# dboORPD = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dboORPD.parquet', format='parquet')\r\n",
							"# dboORPD.createOrReplaceTempView(\"dboORPD\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dboRPD1 SAP LINE Table\r\n",
							"# dboRPD1 = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dboRPD1.parquet', format='parquet')\r\n",
							"# dboRPD1.createOrReplaceTempView(\"dboRPD1\")\r\n",
							"\r\n",
							"# #Goods Issue\r\n",
							"# #Create DataFrame for the dboOIGE SAP HEADER Table\r\n",
							"# dboOIGE = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dboOIGE.parquet', format='parquet')\r\n",
							"# dboOIGE.createOrReplaceTempView(\"dboOIGE\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dboIGE1 SAP LINE Table\r\n",
							"# dboIGE1 = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dboIGE1.parquet', format='parquet')\r\n",
							"# dboIGE1.createOrReplaceTempView(\"dboIGE1\")\r\n",
							"\r\n",
							"# #Purchase Orders\r\n",
							"# #Create DataFrame for the dboOPOR SAP HEADER Table\r\n",
							"# dboOPOR = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dboOPOR.parquet', format='parquet')\r\n",
							"# dboOPOR.createOrReplaceTempView(\"dboOPOR\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dboPOR1 SAP LINE Table\r\n",
							"# dboPOR1 = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dboPOR1.parquet', format='parquet')\r\n",
							"# dboPOR1.createOrReplaceTempView(\"dboPOR1\")"
						],
						"outputs": [],
						"execution_count": 115
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"--NEGETIVE VALUES\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stpurchaseorder\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    '' as sap_purchaseorderline_polookupid\r\n",
							"    ,opor.docentry as sap_purchaseorderheader_docentry\r\n",
							"    ,opor.docnum as sap_purchaseorderheader_docnum\r\n",
							"    ,opor.doctype as sap_purchaseorderheader_doctype\r\n",
							"    -- ,opor.canceled as sap_purchaseorderheader_canceled\r\n",
							"    ,opor.objtype as sap_purchaseorderheader_objtype\r\n",
							"    ,opor.docdate as sap_purchaseorderheader_postingdate\r\n",
							"    ,opor.docduedate as sap_purchaseorderheader_duedate\r\n",
							"    ,opor.cardcode as sap_purchaseorderheader_cardcode\r\n",
							"    -- ,opor.cardname as sap_purchaseorderheader_cardname\r\n",
							"    ,opor.numatcard as sap_purchaseorderheader_cardref\r\n",
							"    ,opor.discprcnt as sap_purchaseorderheader_discprcnt\r\n",
							"    ,(opor.discsum *-1) as sap_purchaseorderheader_discsum\r\n",
							"    ,(opor.doctotal*-1) as sap_purchaseorderheader_doctotal\r\n",
							"    ,opor.comments as sap_purchaseorderheader_comments\r\n",
							"    ,opor.transid as sap_purchaseorderheader_transid\r\n",
							"    -- ,opor.usersign as sap_purchaseorderheader_usersign\r\n",
							"    -- ,opor.u_depot as sap_purchaseorderheader_u_depot\r\n",
							"    -- ,opor.u_usercode as sap_purchaseorderheader_u_usercode\r\n",
							"    ,opor.u_expense_depot as sap_purchaseorderheader_u_expense_depot\r\n",
							"    -- ,opor.u_transaction_date as sap_purchaseorderheader_u_transaction_date\r\n",
							"    -- ,(opor.rounddif*-1) as sap_purchaseorderheader_rounddif\r\n",
							"    -- ,opor.rounding as sap_purchaseorderheader_rounding\r\n",
							"    -- ,por1.docentry as sap_purchaseorderline_docentry\r\n",
							"    ,por1.linenum as sap_purchaseorderline_linenum\r\n",
							"    ,por1.linestatus as sap_purchaseorderline_linestatus\r\n",
							"    ,por1.itemcode as sap_purchaseorderline_itemcode\r\n",
							"    ,por1.dscription as sap_purchaseorderline_dscription\r\n",
							"    ,(por1.quantity*-1) as sap_purchaseorderline_quantity\r\n",
							"    ,(por1.openqty*-1) as sap_purchaseorderline_openqty\r\n",
							"    ,por1.price as sap_purchaseorderline_price\r\n",
							"    ,(por1.rate*-1) as sap_purchaseorderline_rate\r\n",
							"    ,por1.discprcnt as sap_purchaseorderline_discprcnt\r\n",
							"    ,(por1.linetotal*-1) as sap_purchaseorderline_linetotal\r\n",
							"    ,(por1.opensum*-1) as sap_purchaseorderline_opensum\r\n",
							"    ,por1.pricebefdi as sap_purchaseorderline_pricebefdi\r\n",
							"    ,por1.docdate as sap_purchaseorderline_docdate\r\n",
							"    ,por1.project as sap_purchaseorderline_project\r\n",
							"    ,por1.vatprcnt as sap_purchaseorderline_vatprcnt\r\n",
							"    -- ,(por1.volume*-1) as sap_purchaseorderline_volume\r\n",
							"    ,por1.vatgroup as sap_purchaseorderline_vatgroup\r\n",
							"    ,(por1.vatsum*-1) as sap_purchaseorderline_vatamount\r\n",
							"    -- ,(por1.grssprofit*-1) as sap_purchaseorderline_grssprofit\r\n",
							"    ,por1.u_reason as sap_purchaseorderline_u_reason\r\n",
							"    -- ,por1.u_lane as sap_purchaseorderline_u_lane\r\n",
							"    ,por1.u_expenseitem as sap_purchaseorderline_u_expenseitem\r\n",
							"    ,por1.u_fueldate as sap_purchaseorderline_u_fueldate\r\n",
							"    ,por1.acctcode as sap_purchaseorderline_acctcode\r\n",
							"    ,'purchaseorder' as sap_purchaseorderline_doccat\r\n",
							"    ,RIGHT(LEFT(por1.itemcode,5),3) as sap_purchaseorderline_subdepotcode\r\n",
							"    ,por1.u_rate as sap_purchaseorderline_u_rate\r\n",
							"    ,por1.u_ir_number as sap_purchaseorderline_u_ir_number\r\n",
							"    ,0 as sap_purchaseorderline_discsum\r\n",
							"\r\n",
							"\r\n",
							"FROM dboOPOR opor\r\n",
							"LEFT JOIN dboPOR1 por1 on por1.docentry = opor.docentry\r\n",
							"WHERE opor.canceled = 'N'"
						],
						"outputs": [],
						"execution_count": 116
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stpurchaseorder = spark.sql(\"SELECT * FROM stpurchaseorder\")\r\n",
							"stpurchaseorder.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stpurchaseorder.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 117
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stsalesinvoice_tmp\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    oinv.docnum\r\n",
							"    ,(oinv.discsum*-1)/COUNT(inv1.linenum) as discsum\r\n",
							"\r\n",
							"FROM dboOINV oinv\r\n",
							"LEFT JOIN dboINV1 inv1 on inv1.docentry = oinv.docentry\r\n",
							"\r\n",
							"GROUP BY oinv.docnum, oinv.discsum\r\n",
							""
						],
						"outputs": [],
						"execution_count": 118
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stsalesinvoice_tmp = spark.sql(\"SELECT * FROM stsalesinvoice_tmp\")"
						],
						"outputs": [],
						"execution_count": 119
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stsalesinvoice\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    '' as sap_salesinvoiceline_polookupid\r\n",
							"    ,oinv.docentry as sap_salesinvoiceheader_docentry\r\n",
							"    ,oinv.docnum as sap_salesinvoiceheader_docnum\r\n",
							"    ,oinv.doctype as sap_salesinvoiceheader_doctype\r\n",
							"    -- ,oinv.canceled as sap_salesinvoiceheader_canceled\r\n",
							"    ,oinv.objtype as sap_salesinvoiceheader_objtype\r\n",
							"    ,oinv.docdate as sap_salesinvoiceheader_postingdate\r\n",
							"    ,oinv.docduedate as sap_salesinvoiceheader_duedate\r\n",
							"    ,oinv.cardcode as sap_salesinvoiceheader_cardcode\r\n",
							"    -- ,oinv.cardname as sap_salesinvoiceheader_cardname\r\n",
							"    ,oinv.numatcard as sap_salesinvoiceheader_cardref\r\n",
							"    ,oinv.discprcnt as sap_salesinvoiceheader_discprcnt\r\n",
							"    ,oinv.discsum as sap_salesinvoiceheader_discsum\r\n",
							"    ,oinv.doctotal as sap_salesinvoiceheader_doctotal\r\n",
							"    ,oinv.comments as sap_salesinvoiceheader_comments\r\n",
							"    ,oinv.transid as sap_salesinvoiceheader_transid\r\n",
							"    -- ,oinv.usersign as sap_salesinvoiceheader_usersign\r\n",
							"    -- ,oinv.u_depot as sap_salesinvoiceheader_u_depot\r\n",
							"    -- ,oinv.u_usercode as sap_salesinvoiceheader_u_usercode\r\n",
							"    ,oinv.u_expense_depot as sap_salesinvoiceheader_u_expense_depot\r\n",
							"    -- ,oinv.u_transaction_date as sap_salesinvoiceheader_u_transaction_date\r\n",
							"    -- ,oinv.rounddif as sap_salesinvoiceheader_rounddif\r\n",
							"    -- ,oinv.rounding as sap_salesinvoiceheader_rounding\r\n",
							"    -- ,inv1.docentry as sap_salesinvoiceline_docentry\r\n",
							"    ,inv1.linenum as sap_salesinvoiceline_linenum\r\n",
							"    ,inv1.linestatus as sap_salesinvoiceline_linestatus\r\n",
							"    ,inv1.itemcode as sap_salesinvoiceline_itemcode\r\n",
							"    ,inv1.dscription as sap_salesinvoiceline_dscription\r\n",
							"    ,inv1.quantity as sap_salesinvoiceline_quantity\r\n",
							"    ,inv1.openqty as sap_salesinvoiceline_openqty\r\n",
							"    ,inv1.price as sap_salesinvoiceline_price\r\n",
							"    ,inv1.rate as sap_salesinvoiceline_rate\r\n",
							"    ,inv1.discprcnt as sap_salesinvoiceline_discprcnt\r\n",
							"    ,inv1.linetotal as sap_salesinvoiceline_linetotal\r\n",
							"    ,inv1.opensum as sap_salesinvoiceline_opensum\r\n",
							"    ,inv1.pricebefdi as sap_salesinvoiceline_pricebefdi\r\n",
							"    ,inv1.docdate as sap_salesinvoiceline_docdate\r\n",
							"    ,inv1.project as sap_salesinvoiceline_project\r\n",
							"    ,inv1.vatprcnt as sap_salesinvoiceline_vatprcnt\r\n",
							"    -- ,inv1.volume as sap_salesinvoiceline_volume\r\n",
							"    ,inv1.vatgroup as sap_salesinvoiceline_vatgroup\r\n",
							"    ,inv1.vatsum as sap_salesinvoiceline_vatamount\r\n",
							"    -- ,inv1.grssprofit as sap_salesinvoiceline_grssprofit\r\n",
							"    ,inv1.u_reason as sap_salesinvoiceline_u_reason\r\n",
							"    -- ,inv1.u_lane as sap_salesinvoiceline_u_lane\r\n",
							"    ,inv1.u_expenseitem as sap_salesinvoiceline_u_expenseitem\r\n",
							"    ,inv1.u_fueldate as sap_salesinvoiceline_u_fueldate\r\n",
							"    ,inv1.acctcode as sap_salesinvoiceline_acctcode\r\n",
							"    ,'salesinvoice' as sap_salesinvoiceline_doccat\r\n",
							"    ,RIGHT(LEFT(inv1.itemcode,5),3) as sap_salesinvoiceline_subdepotcode\r\n",
							"    ,inv1.u_rate as sap_salesinvoiceline_u_rate\r\n",
							"    ,inv1.u_ir_number as sap_salesinvoiceline_u_ir_number\r\n",
							"    ,t.discsum as sap_salesinvoiceline_discsum\r\n",
							"\r\n",
							"FROM dboOINV oinv\r\n",
							"LEFT JOIN dboINV1 inv1 on inv1.docentry = oinv.docentry\r\n",
							"LEFT JOIN stsalesinvoice_tmp t on t.docnum = oinv.docnum\r\n",
							"WHERE oinv.canceled = 'N'"
						],
						"outputs": [],
						"execution_count": 120
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stsalesinvoice = spark.sql(\"SELECT * FROM stsalesinvoice\")\r\n",
							"stsalesinvoice.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stsalesinvoice.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 121
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stsalescredit_tmp\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    orin.docnum\r\n",
							"    ,(orin.discsum)/COUNT(rin1.linenum) as discsum\r\n",
							"\r\n",
							"FROM dboORIN orin\r\n",
							"LEFT JOIN dboRIN1 rin1 on rin1.docentry = orin.docentry\r\n",
							"\r\n",
							"GROUP BY orin.docnum, orin.discsum"
						],
						"outputs": [],
						"execution_count": 122
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stsalescredit_tmp = spark.sql(\"SELECT * FROM stsalescredit_tmp\")"
						],
						"outputs": [],
						"execution_count": 123
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"--NEGETIVE VALUES\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stsalescredit\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    '' as sap_salescreditline_polookupid\r\n",
							"    ,orin.docentry as sap_salescreditheader_docentry\r\n",
							"    ,orin.docnum as sap_salescreditheader_docnum\r\n",
							"    ,orin.doctype as sap_salescreditheader_doctype\r\n",
							"    -- ,orin.canceled as sap_salescreditheader_canceled\r\n",
							"    ,orin.objtype as sap_salescreditheader_objtype\r\n",
							"    ,orin.docdate as sap_salescreditheader_postingdate\r\n",
							"    ,orin.docduedate as sap_salescreditheader_duedate\r\n",
							"    ,orin.cardcode as sap_salescreditheader_cardcode\r\n",
							"    -- ,orin.cardname as sap_salescreditheader_cardname\r\n",
							"    ,orin.numatcard as sap_salescreditheader_cardref\r\n",
							"    ,orin.discprcnt as sap_salescreditheader_discprcnt\r\n",
							"    ,(orin.discsum*-1) as sap_salescreditheader_discsum\r\n",
							"    ,(orin.doctotal*-1) as sap_salescreditheader_doctotal\r\n",
							"    ,orin.comments as sap_salescreditheader_comments\r\n",
							"    ,orin.transid as sap_salescreditheader_transid\r\n",
							"    -- ,orin.usersign as sap_salescreditheader_usersign\r\n",
							"    -- ,orin.u_depot as sap_salescreditheader_u_depot\r\n",
							"    -- ,orin.u_usercode as sap_salescreditheader_u_usercode\r\n",
							"    ,orin.u_expense_depot as sap_salescreditheader_u_expense_depot\r\n",
							"    -- ,orin.u_transaction_date as sap_salescreditheader_u_transaction_date\r\n",
							"    -- ,orin.rounddif as sap_salescreditheader_rounddif\r\n",
							"    -- ,orin.rounding as sap_salescreditheader_rounding\r\n",
							"    -- ,rin1.docentry as sap_salescreditline_docentry\r\n",
							"    ,rin1.linenum as sap_salescreditline_linenum\r\n",
							"    ,rin1.linestatus as sap_salescreditline_linestatus\r\n",
							"    ,rin1.itemcode as sap_salescreditline_itemcode\r\n",
							"    ,rin1.dscription as sap_salescreditline_dscription\r\n",
							"    ,(rin1.quantity*-1) as sap_salescreditline_quantity\r\n",
							"    ,(rin1.openqty*-1) as sap_salescreditline_openqty\r\n",
							"    ,rin1.price as sap_salescreditline_price\r\n",
							"    ,(rin1.rate*-1) as sap_salescreditline_rate\r\n",
							"    ,rin1.discprcnt as sap_salescreditline_discprcnt\r\n",
							"    ,(rin1.linetotal*-1) as sap_salescreditline_linetotal\r\n",
							"    ,(rin1.opensum*-1) as sap_salescreditline_opensum\r\n",
							"    ,rin1.pricebefdi as sap_salescreditline_pricebefdi\r\n",
							"    ,rin1.docdate as sap_salescreditline_docdate\r\n",
							"    ,rin1.project as sap_salescreditline_project\r\n",
							"    ,rin1.vatprcnt as sap_salescreditline_vatprcnt\r\n",
							"    -- ,(rin1.volume*-1) as sap_salescreditline_volume\r\n",
							"    ,rin1.vatgroup as sap_salescreditline_vatgroup\r\n",
							"    ,(rin1.vatsum*-1) as sap_salescreditline_vatamount\r\n",
							"    -- ,(rin1.grssprofit*-1) as sap_salescreditline_grssprofit\r\n",
							"    ,rin1.u_reason as sap_salescreditline_u_reason\r\n",
							"    -- ,rin1.u_lane as sap_salescreditline_u_lane\r\n",
							"    ,rin1.u_expenseitem as sap_salescreditline_u_expenseitem\r\n",
							"    ,rin1.u_fueldate as sap_salescreditline_u_fueldate\r\n",
							"    ,rin1.acctcode as sap_salescreditline_acctcode\r\n",
							"    ,'salescredit' as sap_salescreditline_doccat\r\n",
							"    ,Right(LEFT(rin1.itemcode,5),3) as sap_salescreditline_subdepotcode\r\n",
							"    ,rin1.u_rate as sap_salescreditline_u_rate\r\n",
							"    ,rin1.u_ir_number as sap_salescreditline_u_ir_number\r\n",
							"    ,t.discsum as sap_salescreditline_discsum\r\n",
							"\r\n",
							"FROM dboORIN orin\r\n",
							"LEFT JOIN dboRIN1 rin1 on rin1.docentry = orin.docentry\r\n",
							"LEFT JOIN stsalescredit_tmp t on t.docnum = orin.docnum\r\n",
							"WHERE orin.canceled = 'N'"
						],
						"outputs": [],
						"execution_count": 124
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stsalescredit = spark.sql(\"SELECT * FROM stsalescredit\")\r\n",
							"stsalescredit.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stsalescredit.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 125
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stpurchaseinvoice_tmp\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    opch.docnum\r\n",
							"    ,(opch.discsum)/COUNT(pch1.linenum) as discsum\r\n",
							"\r\n",
							"FROM dboOPCH opch\r\n",
							"LEFT JOIN dboPCH1 pch1 on pch1.docentry = opch.docentry\r\n",
							"GROUP BY opch.docnum, opch.discsum"
						],
						"outputs": [],
						"execution_count": 126
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stpurchaseinvoice_tmp = spark.sql(\"SELECT * FROM stpurchaseinvoice_tmp\")"
						],
						"outputs": [],
						"execution_count": 127
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"--NEGETIVE VALUES\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stpurchaseinvoice\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    pch1.baseline||'_'||pch1.baseref  as sap_purchaseinvoiceline_polookupid\r\n",
							"    ,opch.docentry as sap_purchaseinvoiceheader_docentry\r\n",
							"    ,opch.docnum as sap_purchaseinvoiceheader_docnum\r\n",
							"    ,opch.doctype as sap_purchaseinvoiceheader_doctype\r\n",
							"    -- ,opch.canceled as sap_purchaseinvoiceheader_canceled\r\n",
							"    ,opch.objtype as sap_purchaseinvoiceheader_objtype\r\n",
							"    ,opch.docdate as sap_purchaseinvoiceheader_postingdate\r\n",
							"    ,opch.docduedate as sap_purchaseinvoiceheader_duedate\r\n",
							"    ,opch.cardcode as sap_purchaseinvoiceheader_cardcode\r\n",
							"    -- ,opch.cardname as sap_purchaseinvoiceheader_cardname\r\n",
							"    ,opch.numatcard as sap_purchaseinvoiceheader_cardref\r\n",
							"    ,opch.discprcnt as sap_purchaseinvoiceheader_discprcnt\r\n",
							"    ,(opch.discsum*-1) as sap_purchaseinvoiceheader_discsum\r\n",
							"    ,(opch.doctotal*-1) as sap_purchaseinvoiceheader_doctotal\r\n",
							"    ,opch.comments as sap_purchaseinvoiceheader_comments\r\n",
							"    ,opch.transid as sap_purchaseinvoiceheader_transid\r\n",
							"    -- ,opch.usersign as sap_purchaseinvoiceheader_usersign\r\n",
							"    -- ,opch.u_depot as sap_purchaseinvoiceheader_u_depot\r\n",
							"    -- ,opch.u_usercode as sap_purchaseinvoiceheader_u_usercode\r\n",
							"    ,opch.u_expense_depot as sap_purchaseinvoiceheader_u_expense_depot\r\n",
							"    -- ,opch.u_transaction_date as sap_purchaseinvoiceheader_u_transaction_date\r\n",
							"    -- ,opch.rounddif as sap_purchaseinvoiceheader_rounddif\r\n",
							"    -- ,opch.rounding as sap_purchaseinvoiceheader_rounding\r\n",
							"    -- ,pch1.docentry as sap_purchaseinvoiceline_docentry\r\n",
							"    ,pch1.linenum as sap_purchaseinvoiceline_linenum\r\n",
							"    ,pch1.linestatus as sap_purchaseinvoiceline_linestatus\r\n",
							"    ,pch1.itemcode as sap_purchaseinvoiceline_itemcode\r\n",
							"    ,pch1.dscription as sap_purchaseinvoiceline_dscription\r\n",
							"    ,(pch1.quantity*-1) as sap_purchaseinvoiceline_quantity\r\n",
							"    ,(pch1.openqty*-1) as sap_purchaseinvoiceline_openqty\r\n",
							"    ,pch1.price as sap_purchaseinvoiceline_price\r\n",
							"    ,(pch1.rate*-1) as sap_purchaseinvoiceline_rate\r\n",
							"    ,pch1.discprcnt as sap_purchaseinvoiceline_discprcnt\r\n",
							"    ,(pch1.linetotal*-1) as sap_purchaseinvoiceline_linetotal\r\n",
							"    ,(pch1.opensum*-1) as sap_purchaseinvoiceline_opensum\r\n",
							"    ,pch1.pricebefdi as sap_purchaseinvoiceline_pricebefdi\r\n",
							"    ,pch1.docdate as sap_purchaseinvoiceline_docdate\r\n",
							"    ,pch1.project as sap_purchaseinvoiceline_project\r\n",
							"    ,pch1.vatprcnt as sap_purchaseinvoiceline_vatprcnt\r\n",
							"    -- ,(pch1.volume*-1) as sap_purchaseinvoiceline_volume\r\n",
							"    ,pch1.vatgroup as sap_purchaseinvoiceline_vatgroup\r\n",
							"    ,(pch1.vatsum*-1) as sap_purchaseinvoiceline_vatamount\r\n",
							"    -- ,(pch1.grssprofit*-1) as sap_purchaseinvoiceline_grssprofit\r\n",
							"    ,pch1.u_reason as sap_purchaseinvoiceline_u_reason\r\n",
							"    -- ,pch1.u_lane as sap_purchaseinvoiceline_u_lane\r\n",
							"    ,pch1.u_expenseitem as sap_purchaseinvoiceline_u_expenseitem\r\n",
							"    ,pch1.u_fueldate as sap_purchaseinvoiceline_u_fueldate\r\n",
							"    ,pch1.acctcode as sap_purchaseinvoiceline_acctcode\r\n",
							"    ,'purchaseinvoice' as sap_purchaseinvoiceline_doccat\r\n",
							"    ,Right(LEFT(pch1.itemcode,5),3) as sap_purchaseinvoiceline_subdepotcode\r\n",
							"    ,pch1.u_rate as sap_purchaseinvoiceline_u_rate\r\n",
							"    ,pch1.u_ir_number as sap_purchaseinvoiceline_u_ir_number\r\n",
							"    ,t.discsum as sap_purchaseinvoiceline_discsum\r\n",
							"    \r\n",
							"FROM dboOPCH opch\r\n",
							"LEFT JOIN dboPCH1 pch1 on pch1.docentry = opch.docentry\r\n",
							"LEFT JOIN stpurchaseinvoice_tmp t on t.docnum = opch.docnum\r\n",
							"WHERE opch.canceled = 'N'"
						],
						"outputs": [],
						"execution_count": 128
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stpurchaseinvoice = spark.sql(\"SELECT * FROM stpurchaseinvoice\")\r\n",
							"stpurchaseinvoice.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stpurchaseinvoice.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 129
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stpurchasecredit_tmp\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    orpc.docnum\r\n",
							"    ,(orpc.discsum*-1)/COUNT(rpc1.linenum) as discsum\r\n",
							"\r\n",
							"FROM dboORPC orpc\r\n",
							"LEFT JOIN dboRPC1 rpc1 on rpc1.docentry = orpc.docentry\r\n",
							"GROUP BY orpc.docnum, orpc.discsum"
						],
						"outputs": [],
						"execution_count": 130
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stpurchasecredit_tmp = spark.sql(\"SELECT * FROM stpurchasecredit_tmp\")"
						],
						"outputs": [],
						"execution_count": 131
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stpurchasecredit\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    '' as sap_purchasecreditinvoiceline_polookupid\r\n",
							"    ,orpc.docentry as sap_purchasecreditheader_docentry\r\n",
							"    ,orpc.docnum as sap_purchasecreditheader_docnum\r\n",
							"    ,orpc.doctype as sap_purchasecreditheader_doctype\r\n",
							"    -- ,orpc.canceled as sap_purchasecreditheader_canceled\r\n",
							"    ,orpc.objtype as sap_purchasecreditheader_objtype\r\n",
							"    ,orpc.docdate as sap_purchasecreditheader_postingdate\r\n",
							"    ,orpc.docduedate as sap_purchasecreditheader_duedate\r\n",
							"    ,orpc.cardcode as sap_purchasecreditheader_cardcode\r\n",
							"    -- ,orpc.cardname as sap_purchasecreditheader_cardname\r\n",
							"    ,orpc.numatcard as sap_purchasecreditheader_cardref\r\n",
							"    ,orpc.discprcnt as sap_purchasecreditheader_discprcnt\r\n",
							"    ,orpc.discsum as sap_purchasecreditheader_discsum\r\n",
							"    ,orpc.doctotal as sap_purchasecreditheader_doctotal\r\n",
							"    ,orpc.comments as sap_purchasecreditheader_comments\r\n",
							"    ,orpc.transid as sap_purchasecreditheader_transid\r\n",
							"    -- ,orpc.usersign as sap_purchasecreditheader_usersign\r\n",
							"    -- ,orpc.u_depot as sap_purchasecreditheader_u_depot\r\n",
							"    -- ,orpc.u_usercode as sap_purchasecreditheader_u_usercode\r\n",
							"    ,orpc.u_expense_depot as sap_purchasecreditheader_u_expense_depot\r\n",
							"    -- ,orpc.u_transaction_date as sap_purchasecreditheader_u_transaction_date\r\n",
							"    -- ,orpc.rounddif as sap_purchasecreditheader_rounddif\r\n",
							"    -- ,orpc.rounding as sap_purchasecreditheader_rounding\r\n",
							"    -- ,rpc1.docentry as sap_purchasecreditline_docentry\r\n",
							"    ,rpc1.linenum as sap_purchasecreditline_linenum\r\n",
							"    ,rpc1.linestatus as sap_purchasecreditline_linestatus\r\n",
							"    ,rpc1.itemcode as sap_purchasecreditline_itemcode\r\n",
							"    ,rpc1.dscription as sap_purchasecreditline_dscription\r\n",
							"    ,rpc1.quantity as sap_purchasecreditline_quantity\r\n",
							"    ,rpc1.openqty as sap_purchasecreditline_openqty\r\n",
							"    ,rpc1.price as sap_purchasecreditline_price\r\n",
							"    ,rpc1.rate as sap_purchasecreditline_rate\r\n",
							"    ,rpc1.discprcnt as sap_purchasecreditline_discprcnt\r\n",
							"    ,rpc1.linetotal as sap_purchasecreditline_linetotal\r\n",
							"    ,rpc1.opensum as sap_purchasecreditline_opensum\r\n",
							"    ,rpc1.pricebefdi as sap_purchasecreditline_pricebefdi\r\n",
							"    ,rpc1.docdate as sap_purchasecreditline_docdate\r\n",
							"    ,rpc1.project as sap_purchasecreditline_project\r\n",
							"    ,rpc1.vatprcnt as sap_purchasecreditline_vatprcnt\r\n",
							"    -- ,rpc1.volume as sap_purchasecreditline_volume\r\n",
							"    ,rpc1.vatgroup as sap_purchasecreditline_vatgroup\r\n",
							"    ,rpc1.vatsum as sap_purchasecreditline_vatamount\r\n",
							"    -- ,rpc1.grssprofit as sap_purchasecreditline_grssprofit\r\n",
							"    ,rpc1.u_reason as sap_purchasecreditline_u_reason\r\n",
							"    -- ,rpc1.u_lane as sap_purchasecreditline_u_lane\r\n",
							"    ,rpc1.u_expenseitem as sap_purchasecreditline_u_expenseitem\r\n",
							"    ,rpc1.u_fueldate as sap_purchasecreditline_u_fueldate\r\n",
							"    ,rpc1.acctcode as sap_purchasecreditline_acctcode\r\n",
							"    ,'purchasecredit' as sap_purchasecreditline_doccat\r\n",
							"    ,Right(LEFT(rpc1.itemcode,5),3) as sap_purchasecreditline_subdepotcode\r\n",
							"    ,rpc1.u_rate as sap_purchasecreditline_u_rate\r\n",
							"    ,rpc1.u_ir_number as sap_purchasecreditline_u_ir_number\r\n",
							"    ,t.discsum as sap_purchasecreditline_discsum\r\n",
							"\r\n",
							"FROM dboORPC orpc\r\n",
							"LEFT JOIN dboRPC1 rpc1 on rpc1.docentry = orpc.docentry\r\n",
							"LEFT JOIN stpurchasecredit_tmp t on t.docnum = orpc.docnum\r\n",
							"WHERE orpc.canceled = 'N'"
						],
						"outputs": [],
						"execution_count": 132
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stpurchasecredit = spark.sql(\"SELECT * FROM stpurchasecredit\")\r\n",
							"stpurchasecredit.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stpurchasecredit.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 133
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stgoodsreceiptnote_tmp\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    opdn.docnum\r\n",
							"    ,(opdn.discsum)/COUNT(opd1.linenum) as discsum\r\n",
							"\r\n",
							"FROM dboOPDN opdn\r\n",
							"LEFT JOIN dboPDN1 opd1 on opd1.docentry = opdn.docentry\r\n",
							"GROUP BY opdn.docnum, opdn.discsum"
						],
						"outputs": [],
						"execution_count": 134
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stgoodsreceiptnote_tmp = spark.sql(\"SELECT * FROM stgoodsreceiptnote_tmp\")"
						],
						"outputs": [],
						"execution_count": 135
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"--NEGETIVE VALUES\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stgoodsreceiptnote\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    '' as sap_goodsreceiptnoteline_polookupid\r\n",
							"    ,opdn.docentry as sap_goodsreceiptnoteheader_docentry\r\n",
							"    ,opdn.docnum as sap_goodsreceiptnoteheader_docnum\r\n",
							"    ,opdn.doctype as sap_goodsreceiptnoteheader_doctype\r\n",
							"    -- ,opdn.canceled as sap_goodsreceiptnoteheader_canceled\r\n",
							"    ,opdn.objtype as sap_goodsreceiptnoteheader_objtype\r\n",
							"    ,opdn.docdate as sap_goodsreceiptnoteheader_postingdate\r\n",
							"    ,opdn.docduedate as sap_goodsreceiptnoteheader_duedate\r\n",
							"    ,opdn.cardcode as sap_goodsreceiptnoteheader_cardcode\r\n",
							"    -- ,opdn.cardname as sap_goodsreceiptnoteheader_cardname\r\n",
							"    ,opdn.numatcard as sap_goodsreceiptnoteheader_cardref\r\n",
							"    ,opdn.discprcnt as sap_goodsreceiptnoteheader_discprcnt\r\n",
							"    ,(opdn.discsum*-1) as sap_goodsreceiptnoteheader_discsum\r\n",
							"    ,(opdn.doctotal*-1) as sap_goodsreceiptnoteheader_doctotal\r\n",
							"    ,opdn.comments as sap_goodsreceiptnoteheader_comments\r\n",
							"    ,opdn.transid as sap_goodsreceiptnoteheader_transid\r\n",
							"    -- ,opdn.usersign as sap_goodsreceiptnoteheader_usersign\r\n",
							"    -- ,opdn.u_depot as sap_goodsreceiptnoteheader_u_depot\r\n",
							"    -- ,opdn.u_usercode as sap_goodsreceiptnoteheader_u_usercode\r\n",
							"    ,opdn.u_expense_depot as sap_goodsreceiptnoteheader_u_expense_depot\r\n",
							"    -- ,opdn.u_transaction_date as sap_goodsreceiptnoteheader_u_transaction_date\r\n",
							"    -- ,opdn.rounddif as sap_goodsreceiptnoteheader_rounddif\r\n",
							"    -- ,opdn.rounding as sap_goodsreceiptnoteheader_rounding\r\n",
							"    -- ,pdn1.docentry as sap_goodsreceiptnoteline_docentry\r\n",
							"    ,pdn1.linenum as sap_goodsreceiptnoteline_linenum\r\n",
							"    ,pdn1.linestatus as sap_goodsreceiptnoteline_linestatus\r\n",
							"    ,pdn1.itemcode as sap_goodsreceiptnoteline_itemcode\r\n",
							"    ,pdn1.dscription as sap_goodsreceiptnoteline_dscription\r\n",
							"    ,(pdn1.quantity*-1) as sap_goodsreceiptnoteline_quantity\r\n",
							"    ,(pdn1.openqty*-1) as sap_goodsreceiptnoteline_openqty\r\n",
							"    ,pdn1.price as sap_goodsreceiptnoteline_price\r\n",
							"    ,(pdn1.rate*-1) as sap_goodsreceiptnoteline_rate\r\n",
							"    ,pdn1.discprcnt as sap_goodsreceiptnoteline_discprcnt\r\n",
							"    ,(pdn1.linetotal*-1) as sap_goodsreceiptnoteline_linetotal\r\n",
							"    ,(pdn1.opensum*-1) as sap_goodsreceiptnoteline_opensum\r\n",
							"    ,pdn1.pricebefdi as sap_goodsreceiptnoteline_pricebefdi\r\n",
							"    ,pdn1.docdate as sap_goodsreceiptnoteline_docdate\r\n",
							"    ,pdn1.project as sap_goodsreceiptnoteline_project\r\n",
							"    ,pdn1.vatprcnt as sap_goodsreceiptnoteline_vatprcnt\r\n",
							"    -- ,(pdn1.volume*-1) as sap_goodsreceiptnoteline_volume\r\n",
							"    ,pdn1.vatgroup as sap_goodsreceiptnoteline_vatgroup\r\n",
							"    ,(pdn1.vatsum*-1) as sap_goodsreceiptnoteline_vatamount\r\n",
							"    -- ,(pdn1.grssprofit*-1) as sap_goodsreceiptnoteline_grssprofit\r\n",
							"    ,pdn1.u_reason as sap_goodsreceiptnoteline_u_reason\r\n",
							"    -- ,pdn1.u_lane as sap_goodsreceiptnoteline_u_lane\r\n",
							"    ,pdn1.u_expenseitem as sap_goodsreceiptnoteline_u_expenseitem\r\n",
							"    ,pdn1.u_fueldate as sap_goodsreceiptnoteline_u_fueldate\r\n",
							"    ,pdn1.acctcode as sap_goodsreceiptnoteline_acctcode\r\n",
							"    ,'goodsreceiptnote' as sap_goodsreceiptnoteline_doccat\r\n",
							"    ,Right(LEFT(pdn1.itemcode,5),3) as sap_goodsreceiptnoteline_subdepotcode\r\n",
							"    ,pdn1.u_rate as sap_goodsreceiptnoteline_u_rate\r\n",
							"    ,pdn1.u_ir_number as sap_goodsreceiptnoteline_u_ir_number\r\n",
							"    ,t.discsum as sap_goodsreceiptnoteline_discsum\r\n",
							"\r\n",
							"FROM dboOPDN opdn\r\n",
							"LEFT JOIN dboPDN1 pdn1 on pdn1.docentry = opdn.docentry\r\n",
							"LEFT JOIN stgoodsreceiptnote_tmp t on t.docnum = opdn.docnum\r\n",
							"WHERE opdn.canceled = 'N' \r\n",
							"AND pdn1.linestatus = 'O'"
						],
						"outputs": [],
						"execution_count": 136
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stgoodsreceiptnote = spark.sql(\"SELECT * FROM stgoodsreceiptnote\")\r\n",
							"stgoodsreceiptnote.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stgoodsreceiptnote.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 137
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stpolookup\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    pdn1.linenum||'_'||opdn.docnum as sap_polookup_polookupid\r\n",
							"    ,pdn1.baseref as sap_polookup_baseref\r\n",
							"    ,opdn.docnum as sap_polookup_docnum\r\n",
							"    ,to_date(opdn.docdate,'yyyy-MM-dd') as sap_polookup_docdate\r\n",
							"FROM dboOPDN opdn\r\n",
							"LEFT JOIN dboPDN1 pdn1 on pdn1.docentry = opdn.docentry\r\n",
							""
						],
						"outputs": [],
						"execution_count": 138
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stpolookup = spark.sql(\"SELECT * FROM stpolookup\")\r\n",
							"stpolookup.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stpolookup.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 139
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stgoodsreturn_tmp\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    orpd.docnum\r\n",
							"    ,(orpd.discsum*-1)/COUNT(rpd1.linenum) as discsum\r\n",
							"\r\n",
							"FROM dboORPD orpd\r\n",
							"LEFT JOIN dboRPD1 rpd1 on rpd1.docentry = orpd.docentry\r\n",
							"GROUP BY orpd.docnum, orpd.discsum"
						],
						"outputs": [],
						"execution_count": 140
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stgoodsreturn_tmp = spark.sql(\"SELECT * FROM stgoodsreturn_tmp\")"
						],
						"outputs": [],
						"execution_count": 141
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stgoodsreturn\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    '' as sap_goodsreturnline_polookupid\r\n",
							"    ,orpd.docentry as sap_goodsreturnheader_docentry\r\n",
							"    ,orpd.docnum as sap_goodsreturnheader_docnum\r\n",
							"    ,orpd.doctype as sap_goodsreturnheader_doctype\r\n",
							"    -- ,orpd.canceled as sap_goodsreturnheader_canceled\r\n",
							"    ,orpd.objtype as sap_goodsreturnheader_objtype\r\n",
							"    ,orpd.docdate as sap_goodsreturnheader_postingdate\r\n",
							"    ,orpd.docduedate as sap_goodsreturnheader_duedate\r\n",
							"    ,orpd.cardcode as sap_goodsreturnheader_cardcode\r\n",
							"    -- ,orpd.cardname as sap_goodsreturnheader_cardname\r\n",
							"    ,orpd.numatcard as sap_goodsreturnheader_cardref\r\n",
							"    ,orpd.discprcnt as sap_goodsreturnheader_discprcnt\r\n",
							"    ,orpd.discsum as sap_goodsreturnheader_discsum\r\n",
							"    ,orpd.doctotal as sap_goodsreturnheader_doctotal\r\n",
							"    ,orpd.comments as sap_goodsreturnheader_comments\r\n",
							"    ,orpd.transid as sap_goodsreturnheader_transid\r\n",
							"    -- ,orpd.usersign as sap_goodsreturnheader_usersign\r\n",
							"    -- ,orpd.u_depot as sap_goodsreturnheader_u_depot\r\n",
							"    -- ,orpd.u_usercode as sap_goodsreturnheader_u_usercode\r\n",
							"    ,orpd.u_expense_depot as sap_goodsreturnheader_u_expense_depot\r\n",
							"    -- ,orpd.u_transaction_date as sap_goodsreturnheader_u_transaction_date\r\n",
							"    -- ,orpd.rounddif as sap_goodsreturnheader_rounddif\r\n",
							"    -- ,orpd.rounding as sap_goodsreturnheader_rounding\r\n",
							"    -- ,rpd1.docentry as sap_goodsreturnline_docentry\r\n",
							"    ,rpd1.linenum as sap_goodsreturnline_linenum\r\n",
							"    ,rpd1.linestatus as sap_goodsreturnline_linestatus\r\n",
							"    ,rpd1.itemcode as sap_goodsreturnline_itemcode\r\n",
							"    ,rpd1.dscription as sap_goodsreturnline_dscription\r\n",
							"    ,rpd1.quantity as sap_goodsreturnline_quantity\r\n",
							"    ,rpd1.openqty as sap_goodsreturnline_openqty\r\n",
							"    ,rpd1.price as sap_goodsreturnline_price\r\n",
							"    ,rpd1.rate as sap_goodsreturnline_rate\r\n",
							"    ,rpd1.discprcnt as sap_goodsreturnline_discprcnt\r\n",
							"    ,rpd1.linetotal as sap_goodsreturnline_linetotal\r\n",
							"    ,rpd1.opensum as sap_goodsreturnline_opensum\r\n",
							"    ,rpd1.pricebefdi as sap_goodsreturnline_pricebefdi\r\n",
							"    ,rpd1.docdate as sap_goodsreturnline_docdate\r\n",
							"    ,rpd1.project as sap_goodsreturnline_project\r\n",
							"    ,rpd1.vatprcnt as sap_goodsreturnline_vatprcnt\r\n",
							"    -- ,rpd1.volume as sap_goodsreturnline_volume\r\n",
							"    ,rpd1.vatgroup as sap_goodsreturnline_vatgroup\r\n",
							"    ,rpd1.vatsum as sap_goodsreturnline_vatamount\r\n",
							"    -- ,rpd1.grssprofit as sap_goodsreturnline_grssprofit\r\n",
							"    ,rpd1.u_reason as sap_goodsreturnline_u_reason\r\n",
							"    -- ,rpd1.u_lane as sap_goodsreturnline_u_lane\r\n",
							"    ,rpd1.u_expenseitem as sap_goodsreturnline_u_expenseitem\r\n",
							"    ,rpd1.u_fueldate as sap_goodsreturnline_u_fueldate\r\n",
							"    ,rpd1.acctcode as sap_goodsreturnline_acctcode\r\n",
							"    ,'goodsreturn' as sap_goodsreturnline_doccat\r\n",
							"    ,Right(LEFT(rpd1.itemcode,5),3) as sap_goodsreturnline_subdepotcode\r\n",
							"    ,rpd1.u_rate as sap_goodsreturnline_u_rate\r\n",
							"    ,rpd1.u_ir_number as sap_goodsreturnline_u_ir_number\r\n",
							"    ,t.discsum as sap_goodsreturnline_discsum\r\n",
							"\r\n",
							"\r\n",
							"FROM dboORPD orpd\r\n",
							"LEFT JOIN dboRPD1 rpd1 on rpd1.docentry = orpd.docentry\r\n",
							"LEFT JOIN stgoodsreturn_tmp t on t.docnum = orpd.docnum\r\n",
							"WHERE orpd.canceled = 'N' \r\n",
							""
						],
						"outputs": [],
						"execution_count": 142
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stgoodsreturn = spark.sql(\"SELECT * FROM stgoodsreturn\")\r\n",
							"stgoodsreturn.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stgoodsreturn.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 143
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stdocuments = stsalesinvoice.unionAll(stsalescredit).unionAll(stpurchaseinvoice).unionAll(stpurchasecredit).unionAll(stgoodsreceiptnote).unionAll(stgoodsreturn).unionAll(stpurchaseorder)"
						],
						"outputs": [],
						"execution_count": 144
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stdocuments = stdocuments.withColumnRenamed(\"sap_salesinvoiceline_polookupid\",\"sap_documentsline_polookupid\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceheader_docentry\",\"sap_documentsheader_docentry\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceheader_docnum\",\"sap_documentsheader_docnum\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceheader_doctype\",\"sap_documentsheader_doctype\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceheader_objtype\",\"sap_documentsheader_objtype\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceheader_postingdate\",\"sap_documentsheader_postingdate\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceheader_duedate\",\"sap_documentsheader_duedate\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceheader_cardcode\",\"sap_documentsheader_cardcode\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceheader_cardref\",\"sap_documentsheader_cardref\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceheader_discprcnt\",\"sap_documentsheader_discprcnt\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceheader_discsum\",\"sap_documentsheader_discsum\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceheader_doctotal\",\"sap_documentsheader_doctotal\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceheader_comments\",\"sap_documentsheader_comments\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceheader_transid\",\"sap_documentsheader_transid\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceheader_u_expense_depot\",\"sap_documentsheader_u_expense_depot\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceline_linenum\",\"sap_documentsline_linenum\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceline_linestatus\",\"sap_documentsline_linestatus\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceline_itemcode\",\"sap_documentsline_itemcode\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceline_dscription\",\"sap_documentsline_dscription\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceline_quantity\",\"sap_documentsline_quantity\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceline_openqty\",\"sap_documentsline_openqty\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceline_price\",\"sap_documentsline_price\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceline_rate\",\"sap_documentsline_rate\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceline_discprcnt\",\"sap_documentsline_discprcnt\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceline_linetotal\",\"sap_documentsline_linetotal\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceline_opensum\",\"sap_documentsline_opensum\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceline_pricebefdi\",\"sap_documentsline_pricebefdi\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceline_docdate\",\"sap_documentsline_docdate\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceline_project\",\"sap_documentsline_project\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceline_vatprcnt\",\"sap_documentsline_vatprcnt\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceline_vatgroup\",\"sap_documentsline_vatgroup\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceline_vatamount\",\"sap_documentsline_vatamount\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceline_u_reason\",\"sap_documentsline_u_reason\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceline_u_expenseitem\",\"sap_documentsline_u_expenseitem\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceline_u_fueldate\",\"sap_documentsline_u_fueldate\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceline_acctcode\",\"sap_documentsline_acctcode\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceline_doccat\",\"sap_documentsline_doccat\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceline_subdepotcode\",\"sap_documentsline_subdepotcode\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceline_u_rate\",\"sap_documentsline_u_rate\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceline_u_ir_number\",\"sap_documentsline_u_ir_number\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceline_discsum\",\"sap_documentsline_discsum\")"
						],
						"outputs": [],
						"execution_count": 145
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stdocuments.createOrReplaceTempView(\"stdocuments\")"
						],
						"outputs": [],
						"execution_count": 146
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stdocuments = spark.sql(\"SELECT * FROM stdocuments\")"
						],
						"outputs": [],
						"execution_count": 147
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stdocuments.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stdocuments.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 151
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"SUCCESS\")"
						],
						"outputs": [],
						"execution_count": 149
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/08_DIST_STLoadParent')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Ops/Distribution"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "0130ba52-e8b8-48c9-9353-90bc5417455c"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace/bigDataPools/DevSparkPool",
						"name": "DevSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/DevSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"# #Create DataFrame for the stloadchild LMS Table\r\n",
							"# stloadchild = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stloadchild.parquet', format='parquet')\r\n",
							"# stloadchild.createOrReplaceTempView(\"stloadchild\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stlmstrack LMS Table\r\n",
							"# stlmstrack = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stlmstrack.parquet', format='parquet')\r\n",
							"# stlmstrack.createOrReplaceTempView(\"stlmstrack\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stwaybill LMS Table\r\n",
							"# stwaybill = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stwaybill.parquet', format='parquet')\r\n",
							"# stwaybill.createOrReplaceTempView(\"stwaybill\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stdeliverypickupcustomer LMS Table\r\n",
							"# stdeliverypickupcustomer = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stdeliverypickupcustomer.parquet', format='parquet')\r\n",
							"# stdeliverypickupcustomer.createOrReplaceTempView(\"stdeliverypickupcustomer\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stsroute LMS Table\r\n",
							"# stsroute = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stsroute.parquet', format='parquet')\r\n",
							"# stsroute.createOrReplaceTempView(\"stsroute\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stlmstrack LMS Table\r\n",
							"# stlmstrack = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stlmstrack.parquet', format='parquet')\r\n",
							"# stlmstrack.createOrReplaceTempView(\"stlmstrack\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stwaybill LMS Table\r\n",
							"# stwaybill = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stwaybill.parquet', format='parquet')\r\n",
							"# stwaybill.createOrReplaceTempView(\"stwaybill\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stdeliverypickupcustomer LMS Table\r\n",
							"# stdeliverypickupcustomer = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stdeliverypickupcustomer.parquet', format='parquet')\r\n",
							"# stdeliverypickupcustomer.createOrReplaceTempView(\"stdeliverypickupcustomer\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stsroute LMS Table\r\n",
							"# stsroute = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stsroute.parquet', format='parquet')\r\n",
							"# stsroute.createOrReplaceTempView(\"stsroute\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publicdispatch ORV Table\r\n",
							"# publicdispatch = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicdispatch.parquet', format='parquet')\r\n",
							"# publicdispatch.createOrReplaceTempView(\"publicdispatch\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publicreportdistotmgntdispatchdt ORV Table\r\n",
							"# publicreportdistotmgntdispatchdt = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicreportdistotmgntdispatchdt.parquet', format='parquet')\r\n",
							"# publicreportdistotmgntdispatchdt.createOrReplaceTempView(\"publicreportdistotmgntdispatchdt\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stmdvehicle ORV Table\r\n",
							"# stmdvehicle = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Structured Data/MD/stmdvehicle.parquet', format='parquet')\r\n",
							"# stmdvehicle.createOrReplaceTempView(\"stmdvehicle\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publiccourier ORV Table\r\n",
							"# publiccourier = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publiccourier.parquet', format='parquet')\r\n",
							"# publiccourier.createOrReplaceTempView(\"publiccourier\")\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# from pyspark.sql.functions import *\r\n",
							"# from pyspark.sql.window import Window"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"partitionparent=Window.partitionBy(\"lms_loadchild_parentloadid\").orderBy(col(\"lms_loadchild_id\").desc())\r\n",
							"stloadchild = stloadchild.withColumn(\"rn\",row_number().over(partitionparent))\r\n",
							"stloadchild.createOrReplaceTempView(\"stloadchild\")"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"partitionlmsid=Window.partitionBy(\"lmsid\").orderBy(col(\"lmsid\").desc(),col(\"id\").desc())\r\n",
							"publicdispatch = publicdispatch.withColumn(\"rn\",row_number().over(partitionlmsid))\r\n",
							"publicdispatch.createOrReplaceTempView(\"publicdispatch\")"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stlmstrack_tmp\r\n",
							"AS\r\n",
							"SELECT DISTINCT\r\n",
							"     lms_track_loadparenetloadid\r\n",
							"\r\n",
							"    ,CASE \r\n",
							"        WHEN sum(lms_sroute_localflag) is null THEN 'Linehaul'\r\n",
							"        WHEN sum(lms_sroute_localflag) = 0 THEN 'Outlying'\r\n",
							"        WHEN sum(lms_sroute_localflag)/count(lms_sroute_localflag) = 1 THEN 'Local'\r\n",
							"        WHEN sum(lms_sroute_localflag)/count(lms_sroute_localflag) = 2 THEN 'Cross Border'\r\n",
							"        ELSE 'Mixed'   \r\n",
							"    END as lms_loadchild_type\r\n",
							"\r\n",
							"\r\n",
							"FROM stlmstrack tr \r\n",
							"LEFT JOIN stwaybill w on w.lms_waybill_id = tr.lms_track_waybillid\r\n",
							"LEFT JOIN stdeliverypickupcustomer c on c.lms_customer_id = w.lms_waybill_delivercustid\r\n",
							"LEFT JOIN stsroute s on s.lms_sroute_id = c.lms_customer_srouteid\r\n",
							"\r\n",
							"GROUP BY lms_track_loadparenetloadid"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stlmstrack_tmp = spark.sql(\"SELECT * FROM stlmstrack_tmp\")"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stloadparent_tmp\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    lc.lms_loadchild_parentloadid\r\n",
							"    ,lc2.lms_loadchild_vehicleid\r\n",
							"    ,lc2.lms_loadchild_ldate\r\n",
							"    ,lc2.lms_loadchild_vehodo\r\n",
							"    ,lc2.lms_loadchild_vehodo2\r\n",
							"    ,lc2.lms_loadchild_fromlocid\r\n",
							"    ,lc2.lms_loadchild_tolocid\r\n",
							"    ,lc2.lms_loadchild_ttypeid\r\n",
							"    ,CASE \r\n",
							"        WHEN lc2.lms_loadchild_ttypeid = 17 THEN 'Collect'\r\n",
							"        ELSE COALESCE(tr.lms_loadchild_type,'Unknown')\r\n",
							"    END as lms_loadchild_type\r\n",
							"    ,sum(lc.lms_loadchild_aggrweight) as lms_loadchild_aggrweight\r\n",
							"    ,sum(lc.lms_loadchild_aggrchargeweight) as lms_loadchild_aggrchargeweight\r\n",
							"    ,sum(lc.lms_loadchild_aggrvolweight) as lms_loadchild_aggrvolweight\r\n",
							"    ,sum(lc.lms_loadchild_aggrvolumiserweight) as lms_loadchild_aggrvolumiserweight\r\n",
							"    ,sum(lc.lms_loadchild_aggrmaxweight) as lms_loadchild_aggrmaxweight\r\n",
							"    ,sum(lc.lms_loadchild_aggrnoparcels) as lms_loadchild_aggrnoparcels\r\n",
							"    ,sum(lc.lms_loadchild_aggrweightexclspecdel) as lms_loadchild_aggrweightexclspecdel\r\n",
							"    ,sum(lc.lms_loadchild_aggrchargeweightexclspecdel) as lms_loadchild_aggrchargeweightexclspecdel\r\n",
							"    ,sum(lc.lms_loadchild_aggrvolweightexclspecdel) as lms_loadchild_aggrvolweightexclspecdel\r\n",
							"    ,sum(lc.lms_loadchild_aggrvolumiserweightexclspecdel) as lms_loadchild_aggrvolumiserweightexclspecdel\r\n",
							"    ,sum(lc.lms_loadchild_aggrmaxweightexclspecdel) as lms_loadchild_aggrmaxweightexclspecdel\r\n",
							"    ,sum(lc.lms_loadchild_aggrnoparcelsexclspecdel) as lms_loadchild_aggrnoparcelsexclspecdel\r\n",
							"FROM stloadchild lc \r\n",
							"LEFT JOIN stloadchild lc2 on lc2.lms_loadchild_parentloadid = lc.lms_loadchild_parentloadid and lc2.rn = 1\r\n",
							"LEFT JOIN stlmstrack_tmp tr on tr.lms_track_loadparenetloadid = lc.lms_loadchild_parentloadid\r\n",
							"WHERE lc.lms_loadchild_parentloadid is not null\r\n",
							"GROUP BY \r\n",
							"    lc.lms_loadchild_parentloadid\r\n",
							"    ,lc2.lms_loadchild_vehicleid\r\n",
							"    ,lc2.lms_loadchild_ldate\r\n",
							"    ,lc2.lms_loadchild_vehodo\r\n",
							"    ,lc2.lms_loadchild_vehodo2\r\n",
							"    ,lc2.lms_loadchild_fromlocid\r\n",
							"    ,lc2.lms_loadchild_tolocid\r\n",
							"    ,tr.lms_loadchild_type\r\n",
							"    ,lc2.lms_loadchild_ttypeid\r\n",
							""
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stloadparent_tmp = spark.sql(\"SELECT * FROM stloadparent_tmp\")"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stloadparent\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    t.lms_loadchild_parentloadid as  lms_loadparent_parentloadid \r\n",
							"    ,t.lms_loadchild_vehicleid as lms_loadparent_vehicleid\r\n",
							"    ,t.lms_loadchild_ldate as lms_loadparent_ldate\r\n",
							"    ,t.lms_loadchild_vehodo as  lms_loadparent_vehodo \r\n",
							"    ,t.lms_loadchild_vehodo2 as  lms_loadparent_vehodo2 \r\n",
							"    ,t.lms_loadchild_fromlocid as  lms_loadparent_fromlocid \r\n",
							"    ,t.lms_loadchild_tolocid as  lms_loadparent_tolocid \r\n",
							"    ,t.lms_loadchild_ttypeid as lms_loadparent_ttypeid\r\n",
							"    ,t.lms_loadchild_type as  lms_loadparent_type \r\n",
							"    ,t.lms_loadchild_aggrweight  as  lms_loadparent_aggrweight \r\n",
							"    ,t.lms_loadchild_aggrchargeweight  as  lms_loadparent_aggrchargeweight\r\n",
							"    ,t.lms_loadchild_aggrvolweight  as  lms_loadparent_aggrvolweight\r\n",
							"    ,t.lms_loadchild_aggrvolumiserweight  as  lms_loadparent_aggrvolumiserweight\r\n",
							"    ,t.lms_loadchild_aggrmaxweight  as  lms_loadparent_aggrmaxweight\r\n",
							"    ,t.lms_loadchild_aggrnoparcels  as  lms_loadparent_aggrnoparcels\r\n",
							"    ,t.lms_loadchild_aggrweightexclspecdel  as  lms_loadparent_aggrweightexclspecdel\r\n",
							"    ,t.lms_loadchild_aggrchargeweightexclspecdel  as  lms_loadparent_aggrchargeweightexclspecdel\r\n",
							"    ,t.lms_loadchild_aggrvolweightexclspecdel  as  lms_loadparent_aggrvolweightexclspecdel\r\n",
							"    ,t.lms_loadchild_aggrvolumiserweightexclspecdel  as  lms_loadparent_aggrvolumiserweightexclspecdel\r\n",
							"    ,t.lms_loadchild_aggrmaxweightexclspecdel  as  lms_loadparent_aggrmaxweightexclspecdel\r\n",
							"    ,t.lms_loadchild_aggrnoparcelsexclspecdel  as  lms_loadparent_aggrnoparcelsexclspecdel\r\n",
							"    ,v.md_vehicle_vehicletype\r\n",
							"    ,v.md_vehicle_internalvolume\r\n",
							"    ,d.id as orv_dispatch_id\r\n",
							"    ,d.lmsid as orv_dispatch_lmsid\r\n",
							"    ,d.vid as orv_dispatch_vid\r\n",
							"    ,d.did as orv_dispatch_did\r\n",
							"    ,d.status as orv_dispatch_status\r\n",
							"    ,to_timestamp(d.startdate) as orv_dispatch_startdate\r\n",
							"    ,to_timestamp(d.stopdate) as orv_dispatch_stopdate\r\n",
							"    ,d.vehicletype as orv_dispatch_vehicletype\r\n",
							"    ,d.uid as orv_dispatch_uid\r\n",
							"    ,to_timestamp(d.estdepdate) as orv_dispatch_estdepdate\r\n",
							"    ,d.trailer as orv_dispatch_trailer\r\n",
							"    ,d.trailer2 as orv_dispatch_trailer2\r\n",
							"    ,d.fromlocid as orv_dispatch_fromlocid\r\n",
							"    ,d.tolocid as orv_dispatch_tolocid\r\n",
							"    ,d.site as orv_dispatch_site\r\n",
							"    ,to_timestamp(d.lastupdate) as orv_dispatch_lastupdate\r\n",
							"    ,d.etastatus as orv_dispatch_etastatus\r\n",
							"    ,d.startinglat as orv_dispatch_startinglat\r\n",
							"    ,d.startinglng as orv_dispatch_startinglng\r\n",
							"    ,d.endinglat as orv_dispatch_endinglat\r\n",
							"    ,d.endinglng as orv_dispatch_endinglng\r\n",
							"    ,d.startingfrom as orv_dispatch_startingfrom\r\n",
							"    ,d.goingto as orv_dispatch_goingto\r\n",
							"    ,d.importissue as orv_dispatch_importissue\r\n",
							"    ,to_timestamp(d.insdate) as orv_dispatch_insdate\r\n",
							"    ,to_timestamp(d.estarrdate) as orv_dispatch_estarrdate\r\n",
							"    ,d.duration as orv_dispatch_duration\r\n",
							"    ,d.distance as orv_dispatch_distance\r\n",
							"    ,d.cost as orv_dispatch_cost\r\n",
							"    ,d.stoplat as orv_dispatch_stoplat\r\n",
							"    ,d.stoplng as orv_dispatch_stoplng\r\n",
							"    ,d.stopreason as orv_dispatch_stopreason\r\n",
							"    ,d.startodo as orv_dispatch_startodo\r\n",
							"    ,d.stopodo as orv_dispatch_stopodo\r\n",
							"    ,d.expduration as orv_dispatch_expduration\r\n",
							"    ,d.expdistance as orv_dispatch_expdistance\r\n",
							"    ,d.expcost as orv_dispatch_expcost\r\n",
							"    ,d.collectionid as orv_dispatch_collectionid\r\n",
							"    ,d.crew as orv_dispatch_crew\r\n",
							"    ,d.crewnames as orv_dispatch_crewnames\r\n",
							"    ,d.bocloseuid as orv_dispatch_bocloseuid\r\n",
							"    ,d.routing as orv_dispatch_routing\r\n",
							"    ,d.swapct as orv_dispatch_swapct\r\n",
							"    ,d.debriefed as orv_dispatch_debriefed\r\n",
							"    ,d.debriefnotes as orv_dispatch_debriefnotes\r\n",
							"    ,d.bopin as orv_dispatch_bopin\r\n",
							"    ,d.cpicost as orv_dispatch_cpicost\r\n",
							"    ,d.labourcost as orv_dispatch_labourcost\r\n",
							"    ,d.maintcost as orv_dispatch_maintcost\r\n",
							"    ,d.inscost as orv_dispatch_inscost\r\n",
							"    ,d.tollcost as orv_dispatch_tollcost\r\n",
							"    ,d.fuelcost as orv_dispatch_fuelcost\r\n",
							"    ,d.internal as orv_dispatch_internal\r\n",
							"    ,d.childids as orv_dispatch_childids\r\n",
							"    ,d.guid as orv_dispatch_guid\r\n",
							"    ,d.timedefinite as orv_dispatch_timedefinite\r\n",
							"    ,d.orvcode as orv_dispatch_orvcode\r\n",
							"    ,to_timestamp(d.tsstopdepoenter) as orv_dispatch_tsstopdepoenter\r\n",
							"    ,to_timestamp(d.tsstartdepoexit) as orv_dispatch_tsstartdepoexit\r\n",
							"    ,d.reroutect as orv_dispatch_reroutect\r\n",
							"    ,to_timestamp(d.actualarrdate) as orv_dispatch_actualarrdate\r\n",
							"    ,d.physicalstartlat as orv_dispatch_physicalstartlat\r\n",
							"    ,d.physicalstartlng as orv_dispatch_physicalstartlng\r\n",
							"    ,to_timestamp(d.tsstartdepoexitapp) as orv_dispatch_tsstartdepoexitapp\r\n",
							"    ,to_timestamp(d.tsstopdepoenterapp) as orv_dispatch_tsstopdepoenterapp\r\n",
							"    ,to_timestamp(d.uncanceldate) as orv_dispatch_uncanceldate\r\n",
							"    ,d.uncancelusername as orv_dispatch_uncancelusername\r\n",
							"    ,d.lmsdebrifed as orv_dispatch_lmsdebrifed\r\n",
							"    ,d.reviseddebrief as orv_dispatch_reviseddebrief\r\n",
							"    ,d.lhtransporter as orv_dispatch_lhtransporter\r\n",
							"    ,d.triptype as orv_dispatch_triptype\r\n",
							"    ,d.startreason as orv_dispatch_startreason\r\n",
							"    ,d.route as orv_dispatch_route\r\n",
							"    ,d.tmstraileridentify as orv_dispatch_tmstraileridentify\r\n",
							"    ,d.tmstrailer2identify as orv_dispatch_tmstrailer2identify\r\n",
							"    ,d.tmscontroller as orv_dispatch_tmscontroller\r\n",
							"    ,d.routekey as orv_dispatch_routekey\r\n",
							"    ,d.courierid as orv_dispatch_courierid\r\n",
							"    ,to_timestamp(d.originalstartdate) as orv_dispatch_originalstartdate\r\n",
							"    ,to_timestamp(d.originalextarrival) as orv_dispatch_originalextarrival\r\n",
							"    ,d.recomputeoriginals as orv_dispatch_recomputeoriginals\r\n",
							"    ,d.originalextdistance as orv_dispatch_originalextdistance\r\n",
							"    ,d.originalextduration as orv_dispatch_originalextduration\r\n",
							"    ,to_timestamp(d.moddate) as orv_dispatch_moddate\r\n",
							"    ,d.tmsmode as orv_dispatch_tmsmode\r\n",
							"    ,d.driversignature as orv_dispatch_driversignature\r\n",
							"    ,to_timestamp(x.exitdepot) as orv_dispatch_exitdepot\r\n",
							"\t,to_timestamp(x.enterdepot) as orv_dispatch_enterdepot\r\n",
							"\t,to_timestamp(x.fdeliverydate) as orv_dispatch_fdeliverydate\r\n",
							"\t,to_timestamp(x.fdeltsgeofenceenter) as orv_dispatch_fdeltsgeofenceenter\r\n",
							"\t,to_timestamp(x.fdeltsscanningstart) as orv_dispatch_fdeltsscanningstart\r\n",
							"\t,to_timestamp(x.fdeltsscanningstop) as orv_dispatch_fdeltsscanningstop\r\n",
							"\t,to_timestamp(x.fdeltspodsignature) as orv_dispatch_fdeltspodsignature\r\n",
							"\t,x.fdelnumprcls as orv_dispatch_fdelnumprcls\r\n",
							"\t,to_timestamp(x.deliverydate) as orv_dispatch_deliverydate\r\n",
							"\t,to_timestamp(x.ldeltsgeofenceenter) as orv_dispatch_ldeltsgeofenceenter\r\n",
							"\t,to_timestamp(x.ldeltsscanningstart) as orv_dispatch_ldeltsscanningstart\r\n",
							"\t,to_timestamp(x.ldeltsscanningstop) as orv_dispatch_ldeltsscanningstop\r\n",
							"\t,to_timestamp(x.ldeltspodsignature) as orv_dispatch_ldeltspodsignature\r\n",
							"\t,x.ldelnumprcls as orv_dispatch_ldelnumprcls\r\n",
							"\t,x.fromdelid as orv_dispatch_fromdelid\r\n",
							"\t,x.todelid as orv_dispatch_todelid\r\n",
							"\t,x.distancemeters as orv_dispatch_distancemeters\r\n",
							"\t,(x.durationseconds)/3600 as orv_dispatch_durationhours\r\n",
							"\t,to_timestamp(x.timeduebackatdepot) as orv_dispatch_timeduebackatdepot\r\n",
							"\t,to_timestamp(x.lastcanceleddeltodepotime) as orv_dispatch_lastcanceleddeltodepotime\r\n",
							"    ,(d.stopdate - coalesce(x.timeduebackatdepot,x.lastcanceleddeltodepotime))/3600  orv_dispatch_difstopdatevstimedueback\r\n",
							"    ,CASE \r\n",
							"        WHEN to_date(to_timestamp(d.startdate)) = to_date(to_timestamp(d.stopdate)) THEN 1\r\n",
							"        ELSE 0\r\n",
							"    END AS orv_dispatch_completesameday\r\n",
							"    ,CASE\r\n",
							"        WHEN (d.startdate + 1800) > coalesce(d.tsstartdepoexitapp,d.tsstartdepoexit) THEN 1\r\n",
							"        ELSE 0\r\n",
							"    END AS orv_dispatch_latedepotexitflag\r\n",
							"    ,CASE\r\n",
							"        WHEN (x.timeduebackatdepot + 1800) > coalesce(d.tsstopdepoenter,d.tsstopdepoenterapp) THEN 1\r\n",
							"        ELSE 0\r\n",
							"    END AS orv_dispatch_latedepotenterflag\r\n",
							"    ,CASE\r\n",
							"        WHEN coalesce(d.startdate,coalesce(d.tsstartdepoexitapp,d.tsstartdepoexit)) IS NULL THEN 1\r\n",
							"        ELSE 0\r\n",
							"    END AS orv_dispatch_latedepotexitflagnull\r\n",
							"    ,CASE\r\n",
							"        WHEN coalesce(x.timeduebackatdepot,coalesce(d.tsstopdepoenter,d.tsstopdepoenterapp)) IS NULL THEN 1\r\n",
							"        ELSE 0\r\n",
							"    END AS orv_dispatch_latedepotenterflagnull\r\n",
							"    ,c.cexternal as orv_dispatch_cexternal\r\n",
							"    ,CASE\r\n",
							"        WHEN d.stopdate > (coalesce(d.tsstopdepoenter,d.tsstopdepoenterapp) + 900) THEN 1\r\n",
							"        ELSE 0\r\n",
							"    END AS orv_dispatch_latestopdateflag\r\n",
							"\r\n",
							"FROM stloadparent_tmp t\r\n",
							"LEFT JOIN publicdispatch d on d.lmsid = t.lms_loadchild_parentloadid and d.vehicletype = 6 and d.rn = 1\r\n",
							"LEFT JOIN publicreportdistotmgntdispatchdt x on x.dispatchid = d.id\r\n",
							"LEFT JOIN stmdvehicle v on v.md_vehicle_lmsid = t.lms_loadchild_vehicleid\r\n",
							"LEFT JOIN publiccourier c on d.courierid = c.id\r\n",
							""
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stloadparent = spark.sql(\"SELECT * FROM stloadparent\")"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stloadparent.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stloadparent.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"SUCCESS\")"
						],
						"outputs": [],
						"execution_count": 19
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/08_LH_STTrip')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Ops/Linehaul"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "6d20a796-973b-4a8b-819c-914c9b909913"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace/bigDataPools/DevSparkPool",
						"name": "DevSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/DevSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 55
					},
					{
						"cell_type": "code",
						"source": [
							"# #Create DataFrame for the publictrip TMS Table\r\n",
							"# publictrip = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publictrip.parquet', format='parquet')\r\n",
							"# publictrip.createOrReplaceTempView(\"publictrip\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publicdispatch ORV Table\r\n",
							"# publicdispatch = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicdispatch.parquet', format='parquet')\r\n",
							"# publicdispatch.createOrReplaceTempView(\"publicdispatch\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbolhtrip LMS Table\r\n",
							"# dbolhtrip = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbolh_trip.parquet', format='parquet')\r\n",
							"# dbolhtrip.createOrReplaceTempView(\"dbolhtrip\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbolhtrackstatus LMS Table\r\n",
							"# dbolhtrackstatus = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbolh_trackstatus.parquet', format='parquet')\r\n",
							"# dbolhtrackstatus.createOrReplaceTempView(\"dbolhtrackstatus\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbolhbookingspertrip TMS Table\r\n",
							"# dbolhbookingspertrip = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbolh_bookingspertrip.parquet', format='parquet')\r\n",
							"# dbolhbookingspertrip.createOrReplaceTempView(\"dbolhbookingspertrip\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbolhbooking TMS Table\r\n",
							"# dbolhbooking = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbolh_booking.parquet', format='parquet')\r\n",
							"# dbolhbooking.createOrReplaceTempView(\"dbolhbooking\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbolhtrailersize TMS Table\r\n",
							"# dbolhtrailersize = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbolh_trailersize.parquet', format='parquet')\r\n",
							"# dbolhtrailersize.createOrReplaceTempView(\"dbolhtrailersize\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbolh_3pl TMS Table\r\n",
							"# dbolh_3pl = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbolh_3pl.parquet', format='parquet')\r\n",
							"# dbolh_3pl.createOrReplaceTempView(\"dbolh_3pl\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbovehicle TMS Table\r\n",
							"# dbovehicle = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbovehicle.parquet', format='parquet')\r\n",
							"# dbovehicle.createOrReplaceTempView(\"dbovehicle\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbodriver TMS Table\r\n",
							"# dbodriver = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodriver.parquet', format='parquet')\r\n",
							"# dbodriver.createOrReplaceTempView(\"dbodriver\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbovehiclebasic MD Table\r\n",
							"# dbovehiclebasic = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/CLMasterData/dbovehiclebasic.parquet', format='parquet')\r\n",
							"# dbovehiclebasic.createOrReplaceTempView(\"dbovehiclebasic\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbopeoplebasic MD Table\r\n",
							"# dbopeoplebasic = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/CLMasterData/dbopeoplebasic.parquet', format='parquet')\r\n",
							"# dbopeoplebasic.createOrReplaceTempView(\"dbopeoplebasic\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stbooking TMS Table\r\n",
							"# stbooking = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Structured Data/OPS/stbooking.parquet', format='parquet')\r\n",
							"# stbooking.createOrReplaceTempView(\"stbooking\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publictrack TMS Table\r\n",
							"# publictrack = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publictrack.parquet', format='parquet')\r\n",
							"# publictrack.createOrReplaceTempView(\"publictrack\")"
						],
						"outputs": [],
						"execution_count": 56
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# from pyspark.sql.functions import *\r\n",
							"# from pyspark.sql.window import Window\r\n",
							"# from pyspark.sql.types import IntegralType"
						],
						"outputs": [],
						"execution_count": 57
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"publictrack_tmp\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"       id\r\n",
							"      ,tripid as tms_track_tripid\r\n",
							"      ,tracktype as tms_track_tracktype\r\n",
							"      ,to_timestamp(trackdate) as tms_track_trackdate\r\n",
							"FROM publictrack\r\n",
							"WHERE tracktype = 'Complete'\r\n",
							""
						],
						"outputs": [],
						"execution_count": 58
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"publictrack_tmp = spark.sql(\"SELECT * FROM publictrack_tmp\")"
						],
						"outputs": [],
						"execution_count": 59
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"partitiontripid=Window.partitionBy(\"tms_track_tripid\").orderBy(col(\"tms_track_tripid\").desc(),col(\"id\").desc())\r\n",
							"publictrack_tmp = publictrack_tmp.withColumn(\"rn\",row_number().over(partitiontripid))\r\n",
							"publictrack_tmp.createOrReplaceTempView(\"publictrack_tmp\")"
						],
						"outputs": [],
						"execution_count": 61
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stbooking_tmp\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"     tms_booking_tripid\r\n",
							"    ,sum(tms_booking_directbooking) as tms_booking_directbooking\r\n",
							"FROM stbooking\r\n",
							"GROUP BY tms_booking_tripid"
						],
						"outputs": [],
						"execution_count": 62
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stbooking_tmp = spark.sql(\"SELECT * FROM stbooking_tmp\")"
						],
						"outputs": [],
						"execution_count": 63
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"sttrip_tms\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    lt.id as tms_trip_id\r\n",
							"    ,lt.driverid as tms_trip_driverid\r\n",
							"    ,UPPER(pb.firstname||'_'||pb.lastname) tms_trip_drivername\r\n",
							"    ,lt.vehicleid as tms_trip_vehicleid\r\n",
							"    ,vbv.fleetcode as tms_trip_vehiclefleetcode\r\n",
							"    ,lt.trailerid as tms_trip_trailerid\r\n",
							"    ,vbt1.fleetcode as tms_trip_trailer1fleetcode\r\n",
							"    ,vbt1.vehicletrailersize as tms_trip_trailer1size\r\n",
							"    ,lt.sectrailerid as tms_trip_sectrailerid\r\n",
							"    ,vbt2.fleetcode as tms_trip_trailer2fleetcode\r\n",
							"    ,vbt2.vehicletrailersize as tms_trip_trailer2size\r\n",
							"    ,CAST(COALESCE(CAST(TRIM(LEFT(vbt1.vehicletrailersize,2)) AS int),0)+COALESCE(CAST(TRIM(LEFT(vbt2.vehicletrailersize,2)) AS int),0) AS STRING)||' '||'METER' as tms_trip_trailersize\r\n",
							"    ,lt.status as tms_trip_status\r\n",
							"    ,lt.routeid as tms_trip_routeid\r\n",
							"    ,lt.opsrouteid as tms_trip_opsrouteid\r\n",
							"    ,lt.startfromaddress as tms_trip_startfromaddress\r\n",
							"    ,lt.startfromlat as tms_trip_startfromlat\r\n",
							"    ,lt.startfromlng as tms_trip_startfromlng\r\n",
							"    ,to_timestamp(lt.dispatchdate) as tms_trip_dispatchdate\r\n",
							"    ,lt.stopaddress as tms_trip_stopaddress\r\n",
							"    ,lt.stoplat as tms_trip_stoplat\r\n",
							"    ,lt.stoplng as tms_trip_stoplng\r\n",
							"    ,to_timestamp(lt.insdate) as tms_trip_insdate\r\n",
							"    ,to_timestamp(lt.moddate) as tms_trip_moddate\r\n",
							"    ,lt.insby as tms_trip_insby\r\n",
							"    ,lt.onrouteid as tms_trip_onrouteid\r\n",
							"    ,lt.uuid as tms_trip_uuid\r\n",
							"    ,lt.insbyuser as tms_trip_insbyuser\r\n",
							"    ,lt.thirdpartyid as tms_trip_thirdpartyid\r\n",
							"    ,lt.timehash as tms_trip_timehash\r\n",
							"    ,lt.onroutestatus as tms_trip_onroutestatus\r\n",
							"    ,to_timestamp(lt.lastupdate) as tms_trip_lastupdate\r\n",
							"    ,lt.lastlng as tms_trip_lastlng\r\n",
							"    ,lt.lastlat as tms_trip_lastlat\r\n",
							"    ,lt.controllerid as tms_trip_controllerid\r\n",
							"    ,lt.trackstatus as tms_trip_trackstatus\r\n",
							"    ,lt.lastrackid as tms_trip_lastrackid\r\n",
							"    ,lt.customer as tms_trip_customer\r\n",
							"    ,lt.bookings as tms_trip_bookings\r\n",
							"    ,lt.controller as tms_trip_controller\r\n",
							"    ,lt.transporter as tms_trip_transporter\r\n",
							"    ,lt.legacyid as tms_trip_legacyid\r\n",
							"    ,lt.originatedfrom as tms_trip_originatedfrom\r\n",
							"    ,lt.local as tms_trip_local\r\n",
							"    ,to_timestamp(lt.startdate) as tms_trip_startdate\r\n",
							"    ,to_timestamp(lt.enddate) as tms_trip_enddate\r\n",
							"    ,to_timestamp(lt.actualdeliverydate) as tms_trip_actualdeliverydate\r\n",
							"    ,lt.note as tms_trip_note\r\n",
							"    ,to_timestamp(lt.deliverydate) as tms_trip_deliverydate\r\n",
							"    ,lt.internalexternal as tms_trip_internalexternal\r\n",
							"    ,lt.pickupstartaddress as tms_trip_pickupstartaddress\r\n",
							"    ,to_timestamp(lt.originaletadate) as tms_trip_originaletadate\r\n",
							"    ,to_timestamp(lt.currentetadate) as tms_trip_currentetadate\r\n",
							"    ,to_timestamp(lt.lastetaupdate) as tms_trip_lastetaupdate\r\n",
							"    ,'v2' as tms_trip_source\r\n",
							"\r\n",
							"FROM publictrip lt\r\n",
							"LEFT JOIN dbovehiclebasic vbv on vbv.uid = lt.vehicleid\r\n",
							"LEFT JOIN dbovehiclebasic vbt1 on vbt1.uid = lt.trailerid\r\n",
							"LEFT JOIN dbovehiclebasic vbt2 on vbt2.uid = lt.sectrailerid\r\n",
							"LEFT JOIN dbopeoplebasic pb on pb.uid = lt.driverid\r\n",
							""
						],
						"outputs": [],
						"execution_count": 64
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sttrip_tms = spark.sql(\"SELECT * FROM sttrip_tms\")"
						],
						"outputs": [],
						"execution_count": 65
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stthirdparty_tmp\r\n",
							"AS\r\n",
							"SELECT DISTINCT\r\n",
							"    bpt.tripid\r\n",
							"    ,CASE \r\n",
							"        WHEN lb.id = 65190 THEN 'TANZER TRANSPORT' \r\n",
							"        WHEN lb.id = 70593 THEN 'TAURAS TRANS CC' \r\n",
							"        WHEN lb.id = 72599 THEN 'TRANSSA TRASNPORTERS' \r\n",
							"        ELSE lp.name\r\n",
							"    END as name\r\n",
							"FROM dbolhbooking lb \r\n",
							"LEFT JOIN  dbolhbookingspertrip bpt on bpt.BookingID = lb.ID \r\n",
							"LEFT JOIN  dbolh_3pl lp on lp.id = lb.3PL_ID \r\n",
							"WHERE bpt.tripid>0\r\n",
							"AND lb.allocationdate > '2020-08-31 23:59:59.999'\r\n",
							"AND lb.bookingtypeid in (1,3)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 66
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stthirdparty_tmp = spark.sql(\"SELECT * FROM stthirdparty_tmp\")"
						],
						"outputs": [],
						"execution_count": 67
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"sttrip_lms_tmp0\r\n",
							"AS\r\n",
							"SELECT DISTINCT \r\n",
							"t.id\r\n",
							",t.trailerid\r\n",
							",v.fleetno\r\n",
							",CASE \t\r\n",
							"\tWHEN lts.Description = '4 Meter' THEN 1\r\n",
							"\tWHEN lts.Description = '6 Meter' THEN 2\r\n",
							"\tWHEN lts.Description = '12 Meter' THEN 3\r\n",
							"\tWHEN lts.Description = '15 Meter' THEN 4\r\n",
							"\tWHEN lts.Description = '18 Meter' THEN 5\r\n",
							"\t\r\n",
							"END as Trailersize\r\n",
							",CAST(LEFT(lts.Description,2) AS int) as sumsize\r\n",
							",ROW_NUMBER() OVER (\r\n",
							"          PARTITION BY CAST(LEFT(lts.Description,2) AS int) \r\n",
							"          ORDER BY CAST(LEFT(lts.Description,2) AS int) DESC) rownum\r\n",
							"\r\n",
							"from dboLHTrip t \r\n",
							"LEFT JOIN dbovehicle v on v.id = t.TrailerID\r\n",
							"LEFT JOIN dbolhbookingspertrip lbpt on lbpt.TripID = t.ID \r\n",
							"LEFT JOIN dbolhbooking lb on lb.id = lbpt.BookingID \r\n",
							"LEFT JOIN dbolhtrailersize lts on lts.id =  lb.TrailerSizeID  \r\n",
							"\r\n",
							"WHERE lb.allocationdate > '2020-08-31 23:59:59.999'\r\n",
							"AND lb.bookingtypeid in (1,3)\r\n",
							"AND lbpt.tripid>0\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 68
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sttrip_lms_tmp0 = spark.sql(\"SELECT * FROM sttrip_lms_tmp0\")"
						],
						"outputs": [],
						"execution_count": 69
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"sttrip_lms_tmp1\r\n",
							"AS\r\n",
							"SELECT DISTINCT \r\n",
							"*\r\n",
							"FROM sttrip_lms_tmp0\r\n",
							"WHERE rownum = 1"
						],
						"outputs": [],
						"execution_count": 70
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sttrip_lms_tmp1 = spark.sql(\"SELECT * FROM sttrip_lms_tmp1\")"
						],
						"outputs": [],
						"execution_count": 71
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"sttrip_lms_tmp2\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"DISTINCT\r\n",
							"t.id\r\n",
							",t.TrailerID\r\n",
							",t.FleetNo\r\n",
							",MAX(Trailersize) Trailersize\r\n",
							",CASE \r\n",
							"\tWHEN MAX(Trailersize) = 5 and sum(sumsize) > 18 THEN 18\r\n",
							"\tWHEN MAX(Trailersize) = 4 and sum(sumsize) > 18 THEN 15\r\n",
							"\tELSE SUM(sumsize)\r\n",
							"END as sumsize\r\n",
							"FROM sttrip_lms_tmp1 t\r\n",
							"GROUP BY\r\n",
							"t.id\r\n",
							",t.TrailerID\r\n",
							",t.FleetNo\r\n",
							""
						],
						"outputs": [],
						"execution_count": 72
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sttrip_lms_tmp2 = spark.sql(\"SELECT * FROM sttrip_lms_tmp2\")"
						],
						"outputs": [],
						"execution_count": 73
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"sttrip_lms_tmp3\r\n",
							"AS\r\n",
							"SELECT DISTINCT \r\n",
							"lt.id\r\n",
							",CASE \r\n",
							"\tWHEN lbpt.tripid in (2188,5563,7070,7167,7557,8361,8543,8647,9231,10050,10161,11478,11495,13024,16738,16977,17105,17112,17436,17930,21970,39539,44340,44572,61574,62471) THEN 3\r\n",
							"\tELSE lb.BookingTypeID \r\n",
							"END as BookingTypeID\r\n",
							"\r\n",
							"\r\n",
							"FROM dbolhbookingspertrip lbpt \r\n",
							"LEFT JOIN dboLHTrip lt on lt.ID  = lbpt.tripid \r\n",
							"LEFT JOIN dbolhbooking lb on lb.id = lbpt.bookingid\r\n",
							"WHERE lb.BookingTypeID in (1,3)\r\n",
							"AND lt.id IS NOT NULL\r\n",
							"AND lb.allocationdate > '2020-08-31 23:59:59.999'\r\n",
							"AND lb.bookingtypeid in (1,3)\r\n",
							"AND lbpt.tripid >0"
						],
						"outputs": [],
						"execution_count": 74
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sttrip_lms_tmp3 = spark.sql(\"SELECT * FROM sttrip_lms_tmp3\")"
						],
						"outputs": [],
						"execution_count": 75
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"sttrip_lms_tmp4\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"a.id\r\n",
							",trailerid\r\n",
							",fleetno\r\n",
							",trailersize\r\n",
							",sumsize\r\n",
							",bookingtypeid\r\n",
							"FROM  sttrip_lms_tmp2 a\r\n",
							"LEFT JOIN  sttrip_lms_tmp3 b on b.id = a.id\r\n",
							"ORDER BY sumsize desc\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 76
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sttrip_lms_tmp4 = spark.sql(\"SELECT * FROM sttrip_lms_tmp4\")"
						],
						"outputs": [],
						"execution_count": 77
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"sttrip_lms\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    (lt.id)*-1 as tms_trip_id\r\n",
							"    ,(x.driverid)*-1 as tms_trip_driverid\r\n",
							"    ,UPPER((d.fname||'_'||d.surname)) tms_trip_drivername\r\n",
							"    ,(x.vehicleid)*-1 as tms_trip_vehicleid\r\n",
							"    ,v.fleetno as tms_trip_vehiclefleetcode\r\n",
							"    ,(lt.trailerid)*-1 as tms_trip_trailerid\r\n",
							"    ,t1.fleetno as tms_trip_trailer1fleetcode\r\n",
							"    ,null as tms_trip_trailer1size\r\n",
							"    ,(x.trailer2id)*-1 as tms_trip_sectrailerid\r\n",
							"    ,t2.fleetno as tms_trip_trailer2fleetcode\r\n",
							"    ,null as tms_trip_trailer2size\r\n",
							"    ,CAST(lt.sumsize AS STRING)||' '||'METER' AS tms_trip_trailersize\r\n",
							"    ,ts.description as tms_trip_status\r\n",
							"    ,null as tms_trip_routeid\r\n",
							"    ,(x.routeid)*-1 as tms_trip_opsrouteid\r\n",
							"    ,x.dispatchaddress as tms_trip_startfromaddress\r\n",
							"    ,x.dispatchlongitude as tms_trip_startfromlat\r\n",
							"    ,x.dispatchlatitude as tms_trip_startfromlng\r\n",
							"    ,to_timestamp(coalesce(x.dispatchdate,x.cdate)) as tms_trip_dispatchdate\r\n",
							"    ,x.destinationaddress as tms_trip_stopaddress\r\n",
							"    ,x.destinationlongitude as tms_trip_stoplat\r\n",
							"    ,x.destinationlatitude as tms_trip_stoplng\r\n",
							"    ,null as tms_trip_insdate\r\n",
							"    ,null as tms_trip_moddate\r\n",
							"    ,null as tms_trip_insby\r\n",
							"    ,null as tms_trip_onrouteid\r\n",
							"    ,(x.userid)*-1 as tms_trip_uuid\r\n",
							"    ,null as tms_trip_insbyuser\r\n",
							"    ,null as tms_trip_thirdpartyid\r\n",
							"    ,null as tms_trip_timehash\r\n",
							"    ,null as tms_trip_onroutestatus\r\n",
							"    ,null as tms_trip_lastupdate\r\n",
							"    ,null as tms_trip_lastlng\r\n",
							"    ,null as tms_trip_lastlat\r\n",
							"    ,null as tms_trip_controllerid\r\n",
							"    ,null as tms_trip_trackstatus\r\n",
							"    ,null as tms_trip_lastrackid\r\n",
							"    ,null as tms_trip_customer\r\n",
							"    ,null as tms_trip_bookings\r\n",
							"    ,null as tms_trip_controller\r\n",
							"    ,CASE \r\n",
							"        WHEN lt.id in(68453,66450,61094) THEN 'CITY LINEHAUL'\r\n",
							"        ELSE coalesce(tp.name,'CITY LINEHAUL') \r\n",
							"    END as tms_trip_transporter\r\n",
							"    ,null as tms_trip_legacyid\r\n",
							"    ,null as tms_trip_originatedfrom\r\n",
							"    ,null as tms_trip_local\r\n",
							"    ,null as tms_trip_startdate\r\n",
							"    ,null as tms_trip_enddate\r\n",
							"    ,null as tms_trip_actualdeliverydate\r\n",
							"    ,null as tms_trip_note\r\n",
							"    ,null as tms_trip_deliverydate\r\n",
							"    ,null as tms_trip_internalexternal\r\n",
							"    ,null as tms_trip_originaletadate\r\n",
							"    ,null as tms_trip_currentetadate\r\n",
							"    ,null as tms_trip_lastetaupdate\r\n",
							"    ,null as tms_trip_pickupstartaddress   \r\n",
							"    ,'v1' as tms_trip_source\r\n",
							"FROM sttrip_lms_tmp4 lt\r\n",
							"LEFT JOIN dboLHTrip x on x.id = lt.id\r\n",
							"LEFT JOIN dboLHTrackStatus ts on ts.id = x.tripstatusid\r\n",
							"LEFT JOIN stthirdparty_tmp tp on tp.tripid = lt.id\r\n",
							"-- LEFT JOIN dbolhbookingspertrip bpt on bpt.tripid = lt.id\r\n",
							"-- LEFT JOIN dbolhbooking lb on lb.id = bpt.bookingid\r\n",
							"LEFT JOIN dbovehicle v on v.id = x.vehicleid\r\n",
							"LEFT JOIN dbovehicle t1 on t1.id = x.trailerid\r\n",
							"LEFT JOIN dbovehicle t2 on t1.id = x.trailer2id\r\n",
							"LEFT JOIN dbodriver d on d.id = x.driverid\r\n",
							"\r\n",
							"-- WHERE lb.allocationdate > '2020-08-31 23:59:59.999'\r\n",
							"-- AND lb.bookingtypeid in (1,3)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 78
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sttrip_lms = spark.sql(\"SELECT * FROM sttrip_lms\")"
						],
						"outputs": [],
						"execution_count": 79
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sttrip_tmp = sttrip_tms.unionByName(sttrip_lms, allowMissingColumns = True)\r\n",
							"sttrip_tmp.createOrReplaceTempView(\"sttrip_tmp\")"
						],
						"outputs": [],
						"execution_count": 80
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"sttrip_tmp2\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    st.*\r\n",
							"    ,d.id as orv_dispatch_id\r\n",
							"    ,d.lmsid as orv_dispatch_lmsid\r\n",
							"    ,d.vid as orv_dispatch_vid\r\n",
							"    ,d.did as orv_dispatch_did\r\n",
							"    ,d.status as orv_dispatch_status\r\n",
							"    ,to_timestamp(d.startdate) as orv_dispatch_startdate\r\n",
							"    ,to_timestamp(d.stopdate) as orv_dispatch_stopdate\r\n",
							"    ,d.vehicletype as orv_dispatch_vehicletype\r\n",
							"    ,d.uid as orv_dispatch_uid\r\n",
							"    ,to_timestamp(d.estdepdate) as orv_dispatch_estdepdate\r\n",
							"    ,d.trailer as orv_dispatch_trailer\r\n",
							"    ,d.trailer2 as orv_dispatch_trailer2\r\n",
							"    ,d.fromlocid as orv_dispatch_fromlocid\r\n",
							"    ,d.tolocid as orv_dispatch_tolocid\r\n",
							"    ,d.site as orv_dispatch_site\r\n",
							"    ,to_timestamp(d.lastupdate) as orv_dispatch_lastupdate\r\n",
							"    ,d.etastatus as orv_dispatch_etastatus\r\n",
							"    ,d.startinglat as orv_dispatch_startinglat\r\n",
							"    ,d.startinglng as orv_dispatch_startinglng\r\n",
							"    ,d.endinglat as orv_dispatch_endinglat\r\n",
							"    ,d.endinglng as orv_dispatch_endinglng\r\n",
							"    ,d.startingfrom as orv_dispatch_startingfrom\r\n",
							"    ,d.goingto as orv_dispatch_goingto\r\n",
							"    ,d.importissue as orv_dispatch_importissue\r\n",
							"    ,to_timestamp(d.insdate) as orv_dispatch_insdate\r\n",
							"    ,to_timestamp(d.estarrdate) as orv_dispatch_estarrdate\r\n",
							"    ,d.duration as orv_dispatch_duration\r\n",
							"    ,d.distance as orv_dispatch_distance\r\n",
							"    ,d.cost as orv_dispatch_cost\r\n",
							"    ,d.stoplat as orv_dispatch_stoplat\r\n",
							"    ,d.stoplng as orv_dispatch_stoplng\r\n",
							"    ,d.stopreason as orv_dispatch_stopreason\r\n",
							"    ,d.startodo as orv_dispatch_startodo\r\n",
							"    ,d.stopodo as orv_dispatch_stopodo\r\n",
							"    ,d.expduration as orv_dispatch_expduration\r\n",
							"    ,d.expdistance as orv_dispatch_expdistance\r\n",
							"    ,d.expcost as orv_dispatch_expcost\r\n",
							"    ,d.collectionid as orv_dispatch_collectionid\r\n",
							"    ,d.crew as orv_dispatch_crew\r\n",
							"    ,d.crewnames as orv_dispatch_crewnames\r\n",
							"    ,d.bocloseuid as orv_dispatch_bocloseuid\r\n",
							"    ,d.routing as orv_dispatch_routing\r\n",
							"    ,d.swapct as orv_dispatch_swapct\r\n",
							"    ,d.debriefed as orv_dispatch_debriefed\r\n",
							"    ,d.debriefnotes as orv_dispatch_debriefnotes\r\n",
							"    ,d.bopin as orv_dispatch_bopin\r\n",
							"    ,d.cpicost as orv_dispatch_cpicost\r\n",
							"    ,d.labourcost as orv_dispatch_labourcost\r\n",
							"    ,d.maintcost as orv_dispatch_maintcost\r\n",
							"    ,d.inscost as orv_dispatch_inscost\r\n",
							"    ,d.tollcost as orv_dispatch_tollcost\r\n",
							"    ,d.fuelcost as orv_dispatch_fuelcost\r\n",
							"    ,d.internal as orv_dispatch_internal\r\n",
							"    ,d.childids as orv_dispatch_childids\r\n",
							"    ,d.guid as orv_dispatch_guid\r\n",
							"    ,d.timedefinite as orv_dispatch_timedefinite\r\n",
							"    ,d.orvcode as orv_dispatch_orvcode\r\n",
							"    ,to_timestamp(d.tsstopdepoenter) as orv_dispatch_tsstopdepoenter\r\n",
							"    ,to_timestamp(d.tsstartdepoexit) as orv_dispatch_tsstartdepoexit\r\n",
							"    ,d.reroutect as orv_dispatch_reroutect\r\n",
							"    ,to_timestamp(d.actualarrdate) as orv_dispatch_actualarrdate\r\n",
							"    ,d.physicalstartlat as orv_dispatch_physicalstartlat\r\n",
							"    ,d.physicalstartlng as orv_dispatch_physicalstartlng\r\n",
							"    ,to_timestamp(d.tsstartdepoexitapp) as orv_dispatch_tsstartdepoexitapp\r\n",
							"    ,to_timestamp(d.tsstopdepoenterapp) as orv_dispatch_tsstopdepoenterapp\r\n",
							"    ,to_timestamp(d.uncanceldate) as orv_dispatch_uncanceldate\r\n",
							"    ,d.uncancelusername as orv_dispatch_uncancelusername\r\n",
							"    ,d.lmsdebrifed as orv_dispatch_lmsdebrifed\r\n",
							"    ,d.reviseddebrief as orv_dispatch_reviseddebrief\r\n",
							"    ,d.lhtransporter as orv_dispatch_lhtransporter\r\n",
							"    ,d.triptype as orv_dispatch_triptype\r\n",
							"    ,d.startreason as orv_dispatch_startreason\r\n",
							"    ,d.route as orv_dispatch_route\r\n",
							"    ,d.tmstraileridentify as orv_dispatch_tmstraileridentify\r\n",
							"    ,d.tmstrailer2identify as orv_dispatch_tmstrailer2identify\r\n",
							"    ,d.tmscontroller as orv_dispatch_tmscontroller\r\n",
							"    ,d.routekey as orv_dispatch_routekey\r\n",
							"    ,d.courierid as orv_dispatch_courierid\r\n",
							"    ,to_timestamp(d.originalstartdate) as orv_dispatch_originalstartdate\r\n",
							"    ,to_timestamp(d.originalextarrival) as orv_dispatch_originalextarrival\r\n",
							"    ,d.recomputeoriginals as orv_dispatch_recomputeoriginals\r\n",
							"    ,d.originalextdistance as orv_dispatch_originalextdistance\r\n",
							"    ,d.originalextduration as orv_dispatch_originalextduration\r\n",
							"    ,to_timestamp(d.moddate) as orv_dispatch_moddate\r\n",
							"    ,d.tmsmode as orv_dispatch_tmsmode\r\n",
							"    ,d.driversignature as orv_dispatch_driversignature\r\n",
							"    \r\n",
							"    --trip.deliverydate set on booking allocation using the delivery date on the booking being the last booking allocated it does not update if this changes on the booking y the controller ie its the original deliverydate\r\n",
							"    --trip.startdate set when tracking is started\r\n",
							"    --trip.enddate set when tracking is completed\r\n",
							"    --tms_track_completetrackdate is the track time complete is logged on tms tracking\r\n",
							"\r\n",
							"\r\n",
							"    --tms_trip_appduration only available when ORV is used\r\n",
							"    ,(coalesce(coalesce(d.tsstopdepoenterapp,d.tsstopdepoenter),to_unix_timestamp(st.tms_trip_enddate)) - to_unix_timestamp(st.tms_trip_originaletadate)) as tms_trip_appduration\r\n",
							"   \r\n",
							"    --tms_trip_driverduration only available when ORV is used\r\n",
							"    ,(d.stopdate - d.startdate) as tms_trip_driverduration\r\n",
							"   \r\n",
							"    ,(to_unix_timestamp(st.tms_trip_enddate) - to_unix_timestamp(st.tms_trip_startdate)) as tms_trip_controlroomduration\r\n",
							"    ,CASE \r\n",
							"        WHEN coalesce(coalesce(coalesce(coalesce(d.tsstopdepoenterapp,d.tsstopdepoenter),d.stopdate),to_unix_timestamp(t.tms_track_trackdate)),to_unix_timestamp(st.tms_trip_enddate)) is null THEN 'unknown'\r\n",
							"        WHEN coalesce(coalesce(coalesce(coalesce(d.tsstopdepoenterapp,d.tsstopdepoenter),d.stopdate),to_unix_timestamp(t.tms_track_trackdate)),to_unix_timestamp(st.tms_trip_enddate)) < to_unix_timestamp(st.tms_trip_deliverydate) THEN 'ontime'\r\n",
							"        ELSE 'late'\r\n",
							"    END tms_trip_arrivedontime\r\n",
							"    ,CASE \r\n",
							"        WHEN b.tms_booking_directbooking>0 then 1\r\n",
							"        ELSE 0 \r\n",
							"     END tms_trip_directtrip\r\n",
							"     ,t.tms_track_trackdate as tms_track_completetrackdate\r\n",
							"   \r\n",
							"FROM sttrip_tmp st\r\n",
							"LEFT JOIN publicDispatch d on d.lmsid = st.tms_trip_id and d.vehicletype = 2 and tmsmode = 2\r\n",
							"LEFT JOIN stbooking_tmp b on b.tms_booking_tripid = st.tms_trip_id\r\n",
							"LEFT JOIN publictrack_tmp t on t.tms_track_tripid = st.tms_trip_id and t.rn=1"
						],
						"outputs": [],
						"execution_count": 81
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sttrip_tmp2 = spark.sql(\"SELECT * FROM sttrip_tmp2\")"
						],
						"outputs": [],
						"execution_count": 82
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sttrip = sttrip_tmp2.withColumn(\"a\", pow(sin(radians (col(\"orv_dispatch_physicalstartlat\") - col(\"tms_trip_startfromlat\")) / 2), 2) + cos(radians (\"tms_trip_startfromlat\")) * cos(radians (col(\"orv_dispatch_physicalstartlat\"))) * pow(sin(radians (col(\"orv_dispatch_physicalstartlng\") - col(\"tms_trip_startfromlng\")) / 2), 2)).withColumn(\"orv_dispatch_actualstartfromexpected\", atan2(sqrt(col(\"a\")), sqrt(-col(\"a\") + 1)) * 2 * 6371)"
						],
						"outputs": [],
						"execution_count": 83
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sttrip.createOrReplaceTempView(\"sttrip\")"
						],
						"outputs": [],
						"execution_count": 84
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# sttrip.write.parquet('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Structured Data/OPS/sttrip.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 85
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"SUCCESS\")"
						],
						"outputs": [],
						"execution_count": 86
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/08_STManualJournals')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "MANUAL JOURNAL\nDIRECT POSTINGS\nGENERAL JOURNAL\nDISCOUNT JOURNAL\nSALES INVOICES INVENTORY\nPURCHASE PRICE VARIANCE\nCOGS",
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Finance"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "3e6d5291-32f4-4fc9-9838-592d7c899aff"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse_Prod/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace-prod/bigDataPools/PRDSparkPool",
						"name": "PRDSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/PRDSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"source": [
							"# #Create DataFrame for the dboOJDT SAP Table\r\n",
							"# dboOJDT = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dboojdt.parquet', format='parquet')\r\n",
							"# dboOJDT.createOrReplaceTempView(\"dboOJDT\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dboJDT1 SAP Table\r\n",
							"# dboJDT1 = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/SAP/dbojdt1.parquet', format='parquet')\r\n",
							"# dboJDT1.createOrReplaceTempView(\"dboJDT1\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stchartofaccounts SAP Table\r\n",
							"# stchartofaccounts = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/FINANCE/stchartofaccounts.parquet', format='parquet')\r\n",
							"# stchartofaccounts.createOrReplaceTempView(\"stchartofaccounts\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stsalesinvoice SAP Table\r\n",
							"# stsalesinvoice = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/FINANCE/stsalesinvoice.parquet', format='parquet')\r\n",
							"# stsalesinvoice.createOrReplaceTempView(\"stsalesinvoice\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stitem SAP Table\r\n",
							"# stitem = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/FINANCE/stitem.parquet', format='parquet')\r\n",
							"# stitem.createOrReplaceTempView(\"stitem\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": true,
								"outputs_hidden": true
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# -- %%sql\r\n",
							"# -- -- CREATE OR REPLACE TEMP VIEW\r\n",
							"# -- -- stmanualjournals\r\n",
							"# -- -- AS\r\n",
							"# -- -- SELECT\r\n",
							"# -- --     ojdt.refdate as sap_manualjournalsheader_postingdate\r\n",
							"# -- --     ,ojdt.duedate as sap_manualjournalsheader_duedate\r\n",
							"# -- --     ,ojdt.loctotal as sap_manualjournalsheader_doctotal\r\n",
							"# -- --     ,CASE \r\n",
							"# -- --         WHEN (si.sap_salesinvoiceheader_transid = ojdt.transid \r\n",
							"# -- --             AND (ojdt.transtype = 13 AND coa.sap_account_groupmask IN (5,6,7,8)) \r\n",
							"# -- --             AND si.sap_salesinvoiceheader_canceled = 'N') THEN si.sap_salesinvoiceheader_doctype\r\n",
							"# -- --         ELSE 'M' \r\n",
							"# -- --     END as sap_manualjournalsheader_doctype\r\n",
							"# -- --     ,ojdt.transid as sap_manualjournalsheader_transid\r\n",
							"# -- --     ,jdt1.line_id as sap_manualjournalsline_linenum\r\n",
							"# -- --     ,jdt1.account as sap_manualjournalsline_acctcode\r\n",
							"# -- --     ,ojdt.transtype as sap_manualjournalsheader_objtype\r\n",
							"# -- --     ,jdt1.project as sap_manualjournalsline_project\r\n",
							"# -- --     ,jdt1.vatgroup as sap_manualjournalsline_vatgroup\r\n",
							"# -- --     ,jdt1.vatrate as sap_manualjournalsline_vatrate\r\n",
							"# -- --     ,jdt1.vatamount as sap_manualjournalsline_vatamount\r\n",
							"# -- --     ,jdt1.linememo as sap_manualjournalsline_description\r\n",
							"# -- --     ,jdt1.ref1 as sap_manualjournalsline_jnlref1\r\n",
							"# -- --     ,jdt1.ref2 as sap_manualjournalsline_jnlref2\r\n",
							"# -- --     ,jdt1.u_depot as sap_manualjournalsline_subdepotcode\r\n",
							"# -- --     ,CASE \r\n",
							"# -- --         --PPV\r\n",
							"# -- --         WHEN (jdt1.account = '_SYS00000010296' AND UPPER(LEFT(ojdt.memo, 8)) <> 'YEAR END') THEN sum((jdt1.credit-jdt1.debit))\r\n",
							"# -- --         --COGS\r\n",
							"# -- --         WHEN (jdt1.account = '_SYS00000005261' AND UPPER(LEFT(ojdt.memo, 8)) <> 'YEAR END') THEN sum((jdt1.credit-jdt1.debit))\r\n",
							"# -- --         --SALES INVETORY\r\n",
							"# -- --         WHEN (ojdt.transtype = 13 AND coa.sap_account_groupmask in (5,6,7,8)) THEN sum((jdt1.credit-jdt1.debit))\r\n",
							"# -- --         --DISCOUNT \r\n",
							"# -- --         WHEN (ojdt.transtype = 321) THEN sum((jdt1.credit-jdt1.debit))\r\n",
							"# -- --         --MANUAL JOURNAL    \r\n",
							"# -- --         WHEN (ojdt.transtype in (30,-4) \r\n",
							"# -- --             AND coa.sap_account_groupmask in (5,6,7,8) \r\n",
							"# -- --             AND CONCAT(coa.sap_account_segment_0,coa.sap_account_segment_1,coa.sap_account_segment_2) \r\n",
							"# -- --             NOT IN ('572110ADHDO','511120ADHDO''571170ADHDO')) THEN sum((jdt1.credit-jdt1.debit))\r\n",
							"# -- --         --MANUAL JOURNAL \r\n",
							"# -- --         WHEN (ojdt.transtype = 30 \r\n",
							"# -- --             AND coa.sap_account_groupmask = 4  \r\n",
							"# -- --             AND CONCAT(coa.sap_account_segment_0,coa.sap_account_segment_1,coa.sap_account_segment_2) \r\n",
							"# -- --             NOT IN ('572110ADHDO','511120ADHDO''571170ADHDO')) THEN sum((jdt1.credit-jdt1.debit))\r\n",
							"# -- --         --DIRECT POSTING\r\n",
							"# -- --         WHEN (ojdt.transtype = 46 \r\n",
							"# -- --             AND coa.sap_account_groupmask in (5,6,7,8) \r\n",
							"# -- --             AND CONCAT(coa.sap_account_segment_0,coa.sap_account_segment_1,coa.sap_account_segment_2) \r\n",
							"# -- --             NOT IN ('572110ADHDO')) THEN sum((jdt1.credit-jdt1.debit))\r\n",
							"# -- --         --DIRECT POSTING\r\n",
							"# -- --         WHEN (ojdt.transtype = 24 \r\n",
							"# -- --             AND coa.sap_account_groupmask in (5,6,7,8) \r\n",
							"# -- --             AND CONCAT(coa.sap_account_segment_0,coa.sap_account_segment_1,coa.sap_account_segment_2) \r\n",
							"# -- --             NOT IN ('572110ADHDO')) THEN sum((jdt1.credit-jdt1.debit))\r\n",
							"# -- --         --DIRECT POSTING\r\n",
							"# -- --         WHEN (ojdt.transtype = 46 \r\n",
							"# -- --             AND coa.sap_account_groupmask = 4\r\n",
							"# -- --             AND CONCAT(coa.sap_account_segment_0,coa.sap_account_segment_1,coa.sap_account_segment_2) \r\n",
							"# -- --             NOT IN ('572110ADHDO')) THEN sum((jdt1.credit-jdt1.debit))\r\n",
							"# -- --         --DIRECT POSTING\r\n",
							"# -- --         WHEN (ojdt.transtype = 24 \r\n",
							"# -- --             AND coa.sap_account_groupmask = 4\r\n",
							"# -- --             AND CONCAT(coa.sap_account_segment_0,coa.sap_account_segment_1,coa.sap_account_segment_2) \r\n",
							"# -- --             NOT IN ('572110ADHDO')) THEN sum((jdt1.credit-jdt1.debit))\r\n",
							"# -- --         --JOURNAL\r\n",
							"# -- --         WHEN (coa.sap_account_groupmask IN (1)) THEN sum((jdt1.debit-jdt1.credit))\r\n",
							"# -- --         WHEN (coa.sap_account_groupmask IN (2,3,4,5,6,8)) THEN sum((jdt1.credit-jdt1.debit))\r\n",
							"# -- --         ELSE NULL \r\n",
							"# -- --     END as  sap_manualjournalsline_linetotal\r\n",
							"# -- --     ,CASE \r\n",
							"# -- --         WHEN (jdt1.account = '_SYS00000010296' AND UPPER(LEFT(ojdt.memo, 8)) <> 'YEAR END') THEN 'purchasepricevariance'\r\n",
							"# -- --         WHEN (jdt1.account = '_SYS00000005261' AND UPPER(LEFT(ojdt.memo, 8)) <> 'YEAR END') THEN 'costofgoodssoldjournal'\r\n",
							"# -- --         WHEN (ojdt.transtype = 13 AND coa.sap_account_groupmask in (5,6,7,8)) THEN 'salesinvoiceinventory'\r\n",
							"# -- --         WHEN (ojdt.transtype = 321) THEN 'discountjournal'\r\n",
							"# -- --         WHEN (ojdt.transtype in (30,-4) \r\n",
							"# -- --             AND coa.sap_account_groupmask in (5,6,7,8) \r\n",
							"# -- --             AND CONCAT(coa.sap_account_segment_0,coa.sap_account_segment_1,coa.sap_account_segment_2) \r\n",
							"# -- --             NOT IN ('572110ADHDO','511120ADHDO''571170ADHDO')) THEN 'manualjournal'\r\n",
							"# -- --         WHEN (ojdt.transtype = 30 \r\n",
							"# -- --             AND coa.sap_account_groupmask = 4  \r\n",
							"# -- --             AND CONCAT(coa.sap_account_segment_0,coa.sap_account_segment_1,coa.sap_account_segment_2) \r\n",
							"# -- --             NOT IN ('572110ADHDO','511120ADHDO''571170ADHDO')) THEN 'manualjournal'\r\n",
							"# -- --         WHEN (ojdt.transtype = 46 \r\n",
							"# -- --             AND coa.sap_account_groupmask in (5,6,7,8) \r\n",
							"# -- --             AND CONCAT(coa.sap_account_segment_0,coa.sap_account_segment_1,coa.sap_account_segment_2) \r\n",
							"# -- --             NOT IN ('572110ADHDO')) THEN 'directposting'\r\n",
							"# -- --         WHEN (ojdt.transtype = 24 \r\n",
							"# -- --             AND coa.sap_account_groupmask in (5,6,7,8) \r\n",
							"# -- --             AND CONCAT(coa.sap_account_segment_0,coa.sap_account_segment_1,coa.sap_account_segment_2) \r\n",
							"# -- --             NOT IN ('572110ADHDO')) THEN 'directposting'\r\n",
							"# -- --         WHEN (ojdt.transtype = 46 \r\n",
							"# -- --             AND coa.sap_account_groupmask = 4\r\n",
							"# -- --             AND CONCAT(coa.sap_account_segment_0,coa.sap_account_segment_1,coa.sap_account_segment_2) \r\n",
							"# -- --             NOT IN ('572110ADHDO')) THEN 'directposting'\r\n",
							"# -- --         WHEN (ojdt.transtype = 24 \r\n",
							"# -- --             AND coa.sap_account_groupmask = 4\r\n",
							"# -- --             AND CONCAT(coa.sap_account_segment_0,coa.sap_account_segment_1,coa.sap_account_segment_2) \r\n",
							"# -- --             NOT IN ('572110ADHDO')) THEN 'directposting'\r\n",
							"# -- --         WHEN (coa.sap_account_groupmask IN (1,2,3,4,5,6,8)) THEN 'journal'\r\n",
							"# -- --         ELSE NULL \r\n",
							"# -- --     END as  sap_manualjournalsline_doccat\r\n",
							"# -- --     ,si.sap_salesinvoiceline_itemcode as sap_manualjournalsline_itemcode\r\n",
							"# -- --     ,CASE \r\n",
							"# -- --         WHEN (si.sap_salesinvoiceheader_transid = ojdt.transid \r\n",
							"# -- --             AND (ojdt.transtype = 13 AND coa.sap_account_groupmask IN (5,6,7,8)) \r\n",
							"# -- --             AND si.sap_salesinvoiceheader_canceled = 'N') THEN sap_salesinvoiceheader_cardcode\r\n",
							"# -- --         WHEN (d.sap_documentsheader_transid = ojdt.transid AND coa.sap_account_groupmask IN (1,2,3,4,5,6,8)) THEN d.sap_documentsheader_cardcode\r\n",
							"# -- --         ELSE NULL\r\n",
							"# -- --     END sap_manualjournalsheader_cardcode\r\n",
							"# -- --     ,CASE \r\n",
							"# -- --         WHEN (si.sap_salesinvoiceheader_transid = ojdt.transid \r\n",
							"# -- --             AND (ojdt.transtype = 13 AND coa.sap_account_groupmask IN (5,6,7,8)) \r\n",
							"# -- --             AND si.sap_salesinvoiceheader_canceled = 'N') THEN sap_salesinvoiceheader_numatcard\r\n",
							"# -- --         WHEN (d.sap_documentsheader_transid = ojdt.transid AND coa.sap_account_groupmask IN (1,2,3,4,5,6,8)) THEN d.sap_documentsheader_cardref\r\n",
							"# -- --         ELSE NULL\r\n",
							"# -- --     END sap_manualjournalsheader_cardref\r\n",
							"# -- --     ,i.sap_item_invntitem as sap_manualjournalsline_invntitem\r\n",
							"\r\n",
							" \r\n",
							"# -- --     -- ojdt.transid as sap_manualjournalsheader_transid\r\n",
							"# -- --     -- ,ojdt.transtype as sap_manualjournalsheader_transtype \r\n",
							"# -- --     -- ,ojdt.ref1 as sap_manualjournalsheader_ref1\r\n",
							"# -- --     -- ,ojdt.memo as sap_manualjournalsheader_memo\r\n",
							"# -- --     -- ,ojdt.createdby as sap_manualjournalsheader_createdby\r\n",
							"# -- --     -- ,ojdt.transcode as sap_manualjournalsheader_transcode\r\n",
							"# -- --     -- ,ojdt.project as sap_manualjournalsheader_project\r\n",
							"# -- --     -- ,ojdt.usersign as sap_manualjournalsheader_usersign\r\n",
							"# -- --     -- ,ojdt.objtype as sap_manualjournalsheader_objtype\r\n",
							"# -- --     -- ,jdt1.baseref as sap_manualjournalsline_baseref\r\n",
							"# -- --     -- ,jdt1.debit as sap_manualjournalsline_debit\r\n",
							"# -- --     -- ,jdt1.credit as sap_manualjournalsline_credit\r\n",
							"# -- --     -- ,jdt1.duedate as sap_manualjournalsline_duedate\r\n",
							"# -- --     -- ,jdt1.sourceid as sap_manualjournalsline_sourceid\r\n",
							"# -- --     -- ,jdt1.sourceline as sap_manualjournalsline_sourceline\r\n",
							"# -- --     -- ,jdt1.createdby as sap_manualjournalsline_createdby\r\n",
							"# -- --     -- ,jdt1.transcode as sap_manualjournalsline_transcode\r\n",
							"# -- --     -- ,jdt1.usersign as sap_manualjournalsline_usersign\r\n",
							"# -- --     -- ,jdt1.objtype as sap_manualjournalsline_objtype\r\n",
							"# -- --     -- ,jdt1.vatline as sap_manualjournalsline_vatline\r\n",
							"# -- --     -- ,jdt1.grossvalue as sap_manualjournalsline_grossvalue\r\n",
							"# -- --     -- ,jdt1.linetype as sap_manualjournalsline_linetype\r\n",
							"# -- --     -- ,jdt1.debcred as sap_manualjournalsline_debcred\r\n",
							"# -- --     -- ,coa.sap_account_groupmask\r\n",
							"# -- --     -- ,coa.sap_account_acctname\r\n",
							"# -- --     -- ,coa.sap_account_segment_0\r\n",
							"# -- --     -- ,coa.sap_account_segment_1\r\n",
							"# -- --     -- ,coa.sap_account_segment_2\r\n",
							"# -- --     -- ,i.sap_item_invntitem as sap_manualjournalsline_invntitem\r\n",
							"   \r\n",
							"# -- -- FROM dboOJDT ojdt\r\n",
							"# -- -- LEFT JOIN dboJDT1 jdt1 on jdt1.transid = ojdt.transid\r\n",
							"# -- -- LEFT JOIN stchartofaccounts coa on coa.sap_account_acctcode = jdt1.account\r\n",
							"# -- -- LEFT JOIN stsalesinvoice si on si.sap_salesinvoiceheader_transid = jdt1.transid \r\n",
							"# -- --     AND (ojdt.transtype = 13 AND coa.sap_account_groupmask IN (5,6,7,8)) \r\n",
							"# -- --     AND si.sap_salesinvoiceheader_canceled = 'N'\r\n",
							"# -- -- LEFT JOIN stitem i on i.sap_item_itemcode = si.sap_salesinvoiceline_itemcode \r\n",
							"# -- --     AND (ojdt.transtype = 13 AND coa.sap_account_groupmask IN (5,6,7,8))\r\n",
							"# -- -- LEFT JOIN stdocuments d on d.sap_documentsheader_transid = jdt1.transid \r\n",
							"# -- --     AND coa.sap_account_groupmask IN (1,2,3,4,5,6,8) AND d.sap_documentsline_doccat NOT IN ('journal','purchaseorder')\r\n",
							"# -- -- GROUP BY\r\n",
							"# -- --     ojdt.memo\r\n",
							"# -- --     ,ojdt.transtype\r\n",
							"# -- --     ,ojdt.duedate\r\n",
							"# -- --     ,ojdt.loctotal\r\n",
							"# -- --     ,coa.sap_account_groupmask\r\n",
							"# -- --     ,coa.sap_account_acctname\r\n",
							"# -- --     ,coa.sap_account_segment_0\r\n",
							"# -- --     ,coa.sap_account_segment_1\r\n",
							"# -- --     ,coa.sap_account_segment_2\r\n",
							"# -- --     ,ojdt.refdate\r\n",
							"# -- --     ,ojdt.doctype\r\n",
							"# -- --     ,ojdt.transid\r\n",
							"# -- --     ,jdt1.line_id\r\n",
							"# -- --     ,jdt1.account\r\n",
							"# -- --     ,jdt1.transtype\r\n",
							"# -- --     ,jdt1.refdate\r\n",
							"# -- --     ,jdt1.project\r\n",
							"# -- --     ,jdt1.vatgroup\r\n",
							"# -- --     ,jdt1.vatrate\r\n",
							"# -- --     ,jdt1.vatamount\r\n",
							"# -- --     ,jdt1.linememo\r\n",
							"# -- --     ,jdt1.ref1\r\n",
							"# -- --     ,jdt1.ref2\r\n",
							"# -- --     ,jdt1.baseref\r\n",
							"# -- --     ,jdt1.u_depot\r\n",
							"# -- --     ,si.sap_salesinvoiceline_itemcode\r\n",
							"# -- --     ,si.sap_salesinvoiceheader_doctype\r\n",
							"# -- --     ,si.sap_salesinvoiceheader_transid\r\n",
							"# -- --     ,si.sap_salesinvoiceheader_canceled\r\n",
							"# -- --     ,si.sap_salesinvoiceheader_cardcode\r\n",
							"# -- --     ,si.sap_salesinvoiceheader_numatcard\r\n",
							"# -- --     ,d.sap_documentsheader_transid\r\n",
							"# -- --     ,d.sap_documentsheader_cardcode\r\n",
							"# -- --     ,d.sap_documentsheader_cardref\r\n",
							"# -- --     ,i.sap_item_invntitem\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stmanualjournals_tmp\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"\r\n",
							"    ojdt.refdate as sap_manualjournalsheader_postingdate\r\n",
							"    ,ojdt.duedate as sap_manualjournalsheader_duedate\r\n",
							"    ,ojdt.loctotal as sap_manualjournalsheader_doctotal\r\n",
							"    ,COALESCE(ojdt.doctype,'M') as sap_manualjournalsheader_doctype\r\n",
							"    ,ojdt.transid as sap_manualjournalsheader_transid\r\n",
							"    ,jdt1.line_id as sap_manualjournalsline_linenum\r\n",
							"    ,jdt1.account as sap_manualjournalsline_acctcode\r\n",
							"    ,ojdt.transtype as sap_manualjournalsheader_objtype\r\n",
							"    ,jdt1.project as sap_manualjournalsline_project\r\n",
							"    ,jdt1.vatgroup as sap_manualjournalsline_vatgroup\r\n",
							"    ,jdt1.vatrate as sap_manualjournalsline_vatrate\r\n",
							"    ,jdt1.vatamount as sap_manualjournalsline_vatamount\r\n",
							"    ,jdt1.linememo as sap_manualjournalsline_description\r\n",
							"    ,jdt1.ref1 as sap_manualjournalsline_jnlref1\r\n",
							"    ,jdt1.ref2 as sap_manualjournalsline_jnlref2\r\n",
							"    ,jdt1.u_depot as sap_manualjournalsline_subdepotcode\r\n",
							"    ,jdt1.credit as sap_manualjournalsline_credit --REMOVE IN SUB TABLES\r\n",
							"    ,jdt1.debit as sap_manualjournalsline_debit --REMOVE IN SUB TABLES\r\n",
							"    -- ,as sap_manualjournalsline_linetotal\r\n",
							"    -- ,as  sap_manualjournalsline_doccat\r\n",
							"    -- ,si.sap_salesinvoiceline_itemcode as sap_manualjournalsline_itemcode\r\n",
							"    -- ,as sap_manualjournalsheader_cardcode\r\n",
							"    -- ,as sap_manualjournalsheader_cardref\r\n",
							"    -- ,i.sap_item_invntitem as sap_manualjournalsline_invntitem\r\n",
							"\r\n",
							"    ,ojdt.memo as sap_manualjournalsheader_memo\r\n",
							"    ,coa.sap_account_groupmask\r\n",
							"    ,coa.sap_account_acctname\r\n",
							"    ,coa.sap_account_segment_0\r\n",
							"    ,coa.sap_account_segment_1\r\n",
							"    ,coa.sap_account_segment_2\r\n",
							"    ,coa.sap_account_level1name\r\n",
							"\r\n",
							"FROM dboOJDT ojdt\r\n",
							"LEFT JOIN dboJDT1 jdt1 on jdt1.transid = ojdt.transid\r\n",
							"LEFT JOIN stchartofaccounts coa on coa.sap_account_acctcode = jdt1.account\r\n",
							""
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stmanualjournals_tmp = spark.sql(\"SELECT * FROM stmanualjournals_tmp\")"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stpurchasepricevariance\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    sap_manualjournalsheader_postingdate\r\n",
							"    ,sap_manualjournalsheader_duedate\r\n",
							"    ,sap_manualjournalsheader_doctotal\r\n",
							"    ,sap_manualjournalsheader_doctype\r\n",
							"    ,sap_manualjournalsheader_transid\r\n",
							"    ,sap_manualjournalsheader_memo\r\n",
							"    ,sap_manualjournalsline_linenum\r\n",
							"    ,sap_manualjournalsline_acctcode\r\n",
							"    ,sap_manualjournalsheader_objtype\r\n",
							"    ,sap_manualjournalsline_project\r\n",
							"    ,sap_manualjournalsline_vatgroup\r\n",
							"    ,sap_manualjournalsline_vatrate\r\n",
							"    ,sap_manualjournalsline_vatamount\r\n",
							"    ,sap_manualjournalsline_description\r\n",
							"    ,sap_manualjournalsline_jnlref1\r\n",
							"    ,sap_manualjournalsline_jnlref2\r\n",
							"    ,sap_manualjournalsline_subdepotcode\r\n",
							"    ,SUM(sap_manualjournalsline_credit-sap_manualjournalsline_debit)as sap_manualjournalsline_linetotal\r\n",
							"    ,'purchasepricevariance' as  sap_manualjournalsline_doccat\r\n",
							"\r\n",
							"    -- ,si.sap_salesinvoiceline_itemcode as sap_manualjournalsline_itemcode\r\n",
							"    -- ,as sap_manualjournalsheader_cardcode\r\n",
							"    -- ,as sap_manualjournalsheader_cardref\r\n",
							"    -- ,i.sap_item_invntitem as sap_manualjournalsline_invntitem\r\n",
							"\r\n",
							"FROM stmanualjournals_tmp\r\n",
							"WHERE sap_manualjournalsline_acctcode =  '_SYS00000010296'\r\n",
							"AND UPPER(LEFT(sap_manualjournalsheader_memo, 8)) <> 'YEAR END'\r\n",
							"GROUP BY\r\n",
							"    sap_manualjournalsheader_postingdate\r\n",
							"    ,sap_manualjournalsheader_duedate\r\n",
							"    ,sap_manualjournalsheader_doctotal\r\n",
							"    ,sap_manualjournalsheader_doctype\r\n",
							"    ,sap_manualjournalsheader_transid\r\n",
							"    ,sap_manualjournalsheader_memo\r\n",
							"    ,sap_manualjournalsline_linenum\r\n",
							"    ,sap_manualjournalsline_acctcode\r\n",
							"    ,sap_manualjournalsheader_objtype\r\n",
							"    ,sap_manualjournalsline_project\r\n",
							"    ,sap_manualjournalsline_vatgroup\r\n",
							"    ,sap_manualjournalsline_vatrate\r\n",
							"    ,sap_manualjournalsline_vatamount\r\n",
							"    ,sap_manualjournalsline_description\r\n",
							"    ,sap_manualjournalsline_jnlref1\r\n",
							"    ,sap_manualjournalsline_jnlref2\r\n",
							"    ,sap_manualjournalsline_subdepotcode\r\n",
							""
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stpurchasepricevariance = spark.sql(\"SELECT * FROM stpurchasepricevariance\")"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stpurchasepricevariance = stpurchasepricevariance.withColumnRenamed(\"sap_manualjournalsheader_postingdate\",\"sap_purchasepricevarianceheader_postingdate\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsheader_duedate\",\"sap_purchasepricevarianceheader_duedate\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsheader_doctotal\",\"sap_purchasepricevarianceheader_doctotal\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsheader_doctype\",\"sap_purchasepricevarianceheader_doctype\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsheader_transid\",\"sap_purchasepricevarianceheader_transid\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsheader_memo\",\"sap_purchasepricevarianceheader_memo\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_linenum\",\"sap_purchasepricevarianceline_linenum\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_acctcode\",\"sap_purchasepricevarianceline_acctcode\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsheader_objtype\",\"sap_purchasepricevarianceheader_objtype\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_project\",\"sap_purchasepricevarianceline_project\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_vatgroup\",\"sap_purchasepricevarianceline_vatgroup\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_vatrate\",\"sap_purchasepricevarianceline_vatrate\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_vatamount\",\"sap_purchasepricevarianceline_vatamount\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_description\",\"sap_purchasepricevarianceline_description\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_jnlref1\",\"sap_purchasepricevarianceline_jnlref1\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_jnlref2\",\"sap_purchasepricevarianceline_jnlref2\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_subdepotcode\",\"sap_purchasepricevarianceline_subdepotcode\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_linetotal\",\"sap_purchasepricevarianceline_linetotal\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_doccat\",\"sap_purchasepricevarianceline_doccat\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stpurchasepricevariance.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stpurchasepricevariance.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stcostofgoods\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    sap_manualjournalsheader_postingdate\r\n",
							"    ,sap_manualjournalsheader_duedate\r\n",
							"    ,sap_manualjournalsheader_doctotal\r\n",
							"    ,sap_manualjournalsheader_doctype\r\n",
							"    ,sap_manualjournalsheader_transid\r\n",
							"    ,sap_manualjournalsheader_memo\r\n",
							"    ,sap_manualjournalsline_linenum\r\n",
							"    ,sap_manualjournalsline_acctcode\r\n",
							"    ,sap_manualjournalsheader_objtype\r\n",
							"    ,sap_manualjournalsline_project\r\n",
							"    ,sap_manualjournalsline_vatgroup\r\n",
							"    ,sap_manualjournalsline_vatrate\r\n",
							"    ,sap_manualjournalsline_vatamount\r\n",
							"    ,sap_manualjournalsline_description\r\n",
							"    ,sap_manualjournalsline_jnlref1\r\n",
							"    ,sap_manualjournalsline_jnlref2\r\n",
							"    ,sap_manualjournalsline_subdepotcode\r\n",
							"    ,SUM(sap_manualjournalsline_credit-sap_manualjournalsline_debit)as sap_manualjournalsline_linetotal\r\n",
							"    ,'costofgoods' as  sap_manualjournalsline_doccat\r\n",
							"\r\n",
							"    -- ,si.sap_salesinvoiceline_itemcode as sap_manualjournalsline_itemcode\r\n",
							"    -- ,as sap_manualjournalsheader_cardcode\r\n",
							"    -- ,as sap_manualjournalsheader_cardref\r\n",
							"    -- ,i.sap_item_invntitem as sap_manualjournalsline_invntitem\r\n",
							"\r\n",
							"FROM stmanualjournals_tmp\r\n",
							"WHERE sap_manualjournalsline_acctcode =  '_SYS00000005261'\r\n",
							"AND UPPER(LEFT(sap_manualjournalsheader_memo, 8)) <> 'YEAR END'\r\n",
							"GROUP BY\r\n",
							"    sap_manualjournalsheader_postingdate\r\n",
							"    ,sap_manualjournalsheader_duedate\r\n",
							"    ,sap_manualjournalsheader_doctotal\r\n",
							"    ,sap_manualjournalsheader_doctype\r\n",
							"    ,sap_manualjournalsheader_transid\r\n",
							"    ,sap_manualjournalsheader_memo\r\n",
							"    ,sap_manualjournalsline_linenum\r\n",
							"    ,sap_manualjournalsline_acctcode\r\n",
							"    ,sap_manualjournalsheader_objtype\r\n",
							"    ,sap_manualjournalsline_project\r\n",
							"    ,sap_manualjournalsline_vatgroup\r\n",
							"    ,sap_manualjournalsline_vatrate\r\n",
							"    ,sap_manualjournalsline_vatamount\r\n",
							"    ,sap_manualjournalsline_description\r\n",
							"    ,sap_manualjournalsline_jnlref1\r\n",
							"    ,sap_manualjournalsline_jnlref2\r\n",
							"    ,sap_manualjournalsline_subdepotcode"
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stcostofgoods = spark.sql(\"SELECT * FROM stcostofgoods\")"
						],
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stcostofgoods = stcostofgoods.withColumnRenamed(\"sap_manualjournalsheader_postingdate\",\"sap_costofgoodsheader_postingdate\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsheader_duedate\",\"sap_costofgoodsheader_duedate\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsheader_doctotal\",\"sap_costofgoodsheader_doctotal\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsheader_doctype\",\"sap_costofgoodsheader_doctype\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsheader_transid\",\"sap_costofgoodsheader_transid\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsheader_memo\",\"sap_costofgoodsheader_memo\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_linenum\",\"sap_costofgoodsline_linenum\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_acctcode\",\"sap_costofgoodsline_acctcode\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsheader_objtype\",\"sap_costofgoodsheader_objtype\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_project\",\"sap_costofgoodsline_project\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_vatgroup\",\"sap_costofgoodsline_vatgroup\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_vatrate\",\"sap_costofgoodsline_vatrate\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_vatamount\",\"sap_costofgoodsline_vatamount\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_description\",\"sap_costofgoodsline_description\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_jnlref1\",\"sap_costofgoodsline_jnlref1\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_jnlref2\",\"sap_costofgoodsline_jnlref2\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_subdepotcode\",\"sap_costofgoodsline_subdepotcode\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_linetotal\",\"sap_costofgoodsline_linetotal\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_doccat\",\"sap_costofgoodsline_doccat\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stcostofgoods.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stcostofgoods.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 27
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stsalesinvoiceinventory\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    mjt.sap_manualjournalsheader_postingdate\r\n",
							"    ,mjt.sap_manualjournalsheader_duedate\r\n",
							"    ,mjt.sap_manualjournalsheader_doctotal\r\n",
							"    ,COALESCE(si.sap_salesinvoiceheader_doctype,'M') as sap_manualjournalsheader_doctype\r\n",
							"    ,mjt.sap_manualjournalsheader_transid\r\n",
							"    ,mjt.sap_manualjournalsheader_memo\r\n",
							"    ,mjt.sap_manualjournalsline_linenum\r\n",
							"    ,mjt.sap_manualjournalsline_acctcode\r\n",
							"    ,mjt.sap_manualjournalsheader_objtype\r\n",
							"    ,mjt.sap_manualjournalsline_project\r\n",
							"    ,mjt.sap_manualjournalsline_vatgroup\r\n",
							"    ,mjt.sap_manualjournalsline_vatrate\r\n",
							"    ,mjt.sap_manualjournalsline_vatamount\r\n",
							"    ,mjt.sap_manualjournalsline_description\r\n",
							"    ,mjt.sap_manualjournalsline_jnlref1\r\n",
							"    ,mjt.sap_manualjournalsline_jnlref2\r\n",
							"    ,mjt.sap_manualjournalsline_subdepotcode\r\n",
							"    ,SUM(mjt.sap_manualjournalsline_credit-mjt.sap_manualjournalsline_debit)as sap_manualjournalsline_linetotal\r\n",
							"    ,'salesinvoiceinventory' as  sap_manualjournalsline_doccat\r\n",
							"\r\n",
							"    ,si.sap_salesinvoiceline_itemcode as sap_manualjournalsline_itemcode\r\n",
							"    ,si.sap_salesinvoiceheader_cardcode as sap_manualjournalsheader_cardcode\r\n",
							"    ,si.sap_salesinvoiceheader_cardref as sap_manualjournalsheader_cardref\r\n",
							"    ,i.sap_item_invntitem as sap_manualjournalsline_invntitem\r\n",
							"\r\n",
							"FROM stmanualjournals_tmp mjt\r\n",
							"LEFT JOIN stsalesinvoice si on si.sap_salesinvoiceheader_transid = mjt.sap_manualjournalsheader_transid \r\n",
							"LEFT JOIN stitem i on i.sap_item_itemcode = si.sap_salesinvoiceline_itemcode\r\n",
							"WHERE mjt.sap_manualjournalsheader_objtype =  13\r\n",
							"AND i.sap_item_invntitem = 'Y'\r\n",
							"AND mjt.sap_account_groupmask in (5,6,7,8)\r\n",
							"GROUP BY\r\n",
							"    mjt.sap_manualjournalsheader_postingdate\r\n",
							"    ,mjt.sap_manualjournalsheader_duedate\r\n",
							"    ,mjt.sap_manualjournalsheader_doctotal\r\n",
							"    ,mjt.sap_manualjournalsheader_doctype\r\n",
							"    ,mjt.sap_manualjournalsheader_transid\r\n",
							"    ,mjt.sap_manualjournalsheader_memo\r\n",
							"    ,mjt.sap_manualjournalsline_linenum\r\n",
							"    ,mjt.sap_manualjournalsline_acctcode\r\n",
							"    ,mjt.sap_manualjournalsheader_objtype\r\n",
							"    ,mjt.sap_manualjournalsline_project\r\n",
							"    ,mjt.sap_manualjournalsline_vatgroup\r\n",
							"    ,mjt.sap_manualjournalsline_vatrate\r\n",
							"    ,mjt.sap_manualjournalsline_vatamount\r\n",
							"    ,mjt.sap_manualjournalsline_description\r\n",
							"    ,mjt.sap_manualjournalsline_jnlref1\r\n",
							"    ,mjt.sap_manualjournalsline_jnlref2\r\n",
							"    ,mjt.sap_manualjournalsline_subdepotcode\r\n",
							"    ,si.sap_salesinvoiceheader_doctype\r\n",
							"    ,si.sap_salesinvoiceline_itemcode\r\n",
							"    ,si.sap_salesinvoiceheader_cardcode\r\n",
							"    ,si.sap_salesinvoiceheader_cardref\r\n",
							"    ,i.sap_item_invntitem\r\n",
							""
						],
						"outputs": [],
						"execution_count": 28
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stsalesinvoiceinventory = spark.sql(\"SELECT * FROM stsalesinvoiceinventory\")"
						],
						"outputs": [],
						"execution_count": 29
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stsalesinvoiceinventory = stsalesinvoiceinventory.withColumnRenamed(\"sap_manualjournalsheader_postingdate\",\"sap_salesinvoiceinventoryheader_postingdate\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsheader_duedate\",\"sap_salesinvoiceinventoryheader_duedate\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsheader_doctotal\",\"sap_salesinvoiceinventoryheader_doctotal\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsheader_doctype\",\"sap_salesinvoiceinventoryheader_doctype\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsheader_transid\",\"sap_salesinvoiceinventoryheader_transid\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsheader_memo\",\"sap_salesinvoiceinventoryheader_memo\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_linenum\",\"sap_salesinvoiceinventoryline_linenum\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_acctcode\",\"sap_salesinvoiceinventoryline_acctcode\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsheader_objtype\",\"sap_salesinvoiceinventoryheader_objtype\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_project\",\"sap_salesinvoiceinventoryline_project\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_vatgroup\",\"sap_salesinvoiceinventoryline_vatgroup\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_vatrate\",\"sap_salesinvoiceinventoryline_vatrate\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_vatamount\",\"sap_salesinvoiceinventoryline_vatamount\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_description\",\"sap_salesinvoiceinventoryline_description\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_jnlref1\",\"sap_salesinvoiceinventoryline_jnlref1\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_jnlref2\",\"sap_salesinvoiceinventoryline_jnlref2\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_subdepotcode\",\"sap_salesinvoiceinventoryline_subdepotcode\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_linetotal\",\"sap_salesinvoiceinventoryline_linetotal\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_doccat\",\"sap_salesinvoiceinventoryline_doccat\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_itemcode\",\"sap_salesinvoiceinventoryline_itemcode\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsheader_cardcode\",\"sap_salesinvoiceinventoryheader_cardcode\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsheader_cardref\",\"sap_salesinvoiceinventoryheader_cardref\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_invntitem\",\"sap_salesinvoiceinventoryline_invntitem\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 30
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stsalesinvoiceinventory.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stsalesinvoiceinventory.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stdiscountjournal\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    sap_manualjournalsheader_postingdate\r\n",
							"    ,sap_manualjournalsheader_duedate\r\n",
							"    ,sap_manualjournalsheader_doctotal\r\n",
							"    ,sap_manualjournalsheader_doctype\r\n",
							"    ,sap_manualjournalsheader_transid\r\n",
							"    ,sap_manualjournalsheader_memo\r\n",
							"    ,sap_manualjournalsline_linenum\r\n",
							"    ,sap_manualjournalsline_acctcode\r\n",
							"    ,sap_manualjournalsheader_objtype\r\n",
							"    ,sap_manualjournalsline_project\r\n",
							"    ,sap_manualjournalsline_vatgroup\r\n",
							"    ,sap_manualjournalsline_vatrate\r\n",
							"    ,sap_manualjournalsline_vatamount\r\n",
							"    ,sap_manualjournalsline_description\r\n",
							"    ,sap_manualjournalsline_jnlref1\r\n",
							"    ,sap_manualjournalsline_jnlref2\r\n",
							"    ,sap_manualjournalsline_subdepotcode\r\n",
							"    ,SUM(sap_manualjournalsline_credit-sap_manualjournalsline_debit)as sap_manualjournalsline_linetotal\r\n",
							"    ,'discountjournal' as  sap_manualjournalsline_doccat\r\n",
							"\r\n",
							"    -- ,si.sap_salesinvoiceline_itemcode as sap_manualjournalsline_itemcode\r\n",
							"    -- ,as sap_manualjournalsheader_cardcode\r\n",
							"    -- ,as sap_manualjournalsheader_cardref\r\n",
							"    -- ,i.sap_item_invntitem as sap_manualjournalsline_invntitem\r\n",
							"\r\n",
							"FROM stmanualjournals_tmp\r\n",
							"WHERE sap_manualjournalsheader_objtype =  321\r\n",
							"GROUP BY\r\n",
							"    sap_manualjournalsheader_postingdate\r\n",
							"    ,sap_manualjournalsheader_duedate\r\n",
							"    ,sap_manualjournalsheader_doctotal\r\n",
							"    ,sap_manualjournalsheader_doctype\r\n",
							"    ,sap_manualjournalsheader_transid\r\n",
							"    ,sap_manualjournalsheader_memo\r\n",
							"    ,sap_manualjournalsline_linenum\r\n",
							"    ,sap_manualjournalsline_acctcode\r\n",
							"    ,sap_manualjournalsheader_objtype\r\n",
							"    ,sap_manualjournalsline_project\r\n",
							"    ,sap_manualjournalsline_vatgroup\r\n",
							"    ,sap_manualjournalsline_vatrate\r\n",
							"    ,sap_manualjournalsline_vatamount\r\n",
							"    ,sap_manualjournalsline_description\r\n",
							"    ,sap_manualjournalsline_jnlref1\r\n",
							"    ,sap_manualjournalsline_jnlref2\r\n",
							"    ,sap_manualjournalsline_subdepotcode"
						],
						"outputs": [],
						"execution_count": 32
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stdiscountjournal = spark.sql(\"SELECT * FROM stdiscountjournal\")"
						],
						"outputs": [],
						"execution_count": 33
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stdiscountjournal = stdiscountjournal.withColumnRenamed(\"sap_manualjournalsheader_postingdate\",\"sap_discountjournalheader_postingdate\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsheader_duedate\",\"sap_discountjournalheader_duedate\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsheader_doctotal\",\"sap_discountjournalheader_doctotal\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsheader_doctype\",\"sap_discountjournalheader_doctype\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsheader_transid\",\"sap_discountjournalheader_transid\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsheader_memo\",\"sap_discountjournalheader_memo\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_linenum\",\"sap_discountjournalline_linenum\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_acctcode\",\"sap_discountjournalline_acctcode\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsheader_objtype\",\"sap_discountjournalheader_objtype\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_project\",\"sap_discountjournalline_project\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_vatgroup\",\"sap_discountjournalline_vatgroup\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_vatrate\",\"sap_discountjournalline_vatrate\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_vatamount\",\"sap_discountjournalline_vatamount\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_description\",\"sap_discountjournalline_description\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_jnlref1\",\"sap_discountjournalline_jnlref1\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_jnlref2\",\"sap_discountjournalline_jnlref2\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_subdepotcode\",\"sap_discountjournalline_subdepotcode\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_linetotal\",\"sap_discountjournalline_linetotal\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_doccat\",\"sap_discountjournalline_doccat\")"
						],
						"outputs": [],
						"execution_count": 34
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stdiscountjournal.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stdiscountjournal.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 35
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"--EXPENSES\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stmanualjournal1\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    sap_manualjournalsheader_postingdate\r\n",
							"    ,sap_manualjournalsheader_duedate\r\n",
							"    ,sap_manualjournalsheader_doctotal\r\n",
							"    ,sap_manualjournalsheader_doctype\r\n",
							"    ,sap_manualjournalsheader_transid\r\n",
							"    ,sap_manualjournalsheader_memo\r\n",
							"    ,sap_manualjournalsline_linenum\r\n",
							"    ,sap_manualjournalsline_acctcode\r\n",
							"    ,sap_manualjournalsheader_objtype\r\n",
							"    ,sap_manualjournalsline_project\r\n",
							"    ,sap_manualjournalsline_vatgroup\r\n",
							"    ,sap_manualjournalsline_vatrate\r\n",
							"    ,sap_manualjournalsline_vatamount\r\n",
							"    ,sap_manualjournalsline_description\r\n",
							"    ,sap_manualjournalsline_jnlref1\r\n",
							"    ,sap_manualjournalsline_jnlref2\r\n",
							"    ,sap_manualjournalsline_subdepotcode\r\n",
							"    ,SUM(sap_manualjournalsline_credit-sap_manualjournalsline_debit)as sap_manualjournalsline_linetotal\r\n",
							"    ,'manualjournal' as  sap_manualjournalsline_doccat\r\n",
							"\r\n",
							"    -- ,si.sap_salesinvoiceline_itemcode as sap_manualjournalsline_itemcode\r\n",
							"    -- ,as sap_manualjournalsheader_cardcode\r\n",
							"    -- ,as sap_manualjournalsheader_cardref\r\n",
							"    -- ,i.sap_item_invntitem as sap_manualjournalsline_invntitem\r\n",
							"\r\n",
							"FROM stmanualjournals_tmp\r\n",
							"WHERE (sap_manualjournalsheader_objtype in (30,-4)\r\n",
							"AND CONCAT(sap_account_segment_0,sap_account_segment_1,sap_account_segment_2) NOT IN ('572110ADHDO','511120ADHDO','571170ADHDO')\r\n",
							"AND sap_account_groupmask in (5,6,7,8))\r\n",
							"AND sap_account_level1name NOT IN ('ASSETS','CAPITAL AND RESERVES','LIABILITIES')\r\n",
							"AND (CONCAT(sap_manualjournalsheader_objtype,sap_account_groupmask) <> 307)\r\n",
							"\r\n",
							"GROUP BY\r\n",
							"    sap_manualjournalsheader_postingdate\r\n",
							"    ,sap_manualjournalsheader_duedate\r\n",
							"    ,sap_manualjournalsheader_doctotal\r\n",
							"    ,sap_manualjournalsheader_doctype\r\n",
							"    ,sap_manualjournalsheader_transid\r\n",
							"    ,sap_manualjournalsheader_memo\r\n",
							"    ,sap_manualjournalsline_linenum\r\n",
							"    ,sap_manualjournalsline_acctcode\r\n",
							"    ,sap_manualjournalsheader_objtype\r\n",
							"    ,sap_manualjournalsline_project\r\n",
							"    ,sap_manualjournalsline_vatgroup\r\n",
							"    ,sap_manualjournalsline_vatrate\r\n",
							"    ,sap_manualjournalsline_vatamount\r\n",
							"    ,sap_manualjournalsline_description\r\n",
							"    ,sap_manualjournalsline_jnlref1\r\n",
							"    ,sap_manualjournalsline_jnlref2\r\n",
							"    ,sap_manualjournalsline_subdepotcode\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 36
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"--REVENUE\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stmanualjournal2\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    sap_manualjournalsheader_postingdate\r\n",
							"    ,sap_manualjournalsheader_duedate\r\n",
							"    ,sap_manualjournalsheader_doctotal\r\n",
							"    ,sap_manualjournalsheader_doctype\r\n",
							"    ,sap_manualjournalsheader_transid\r\n",
							"    ,sap_manualjournalsheader_memo\r\n",
							"    ,sap_manualjournalsline_linenum\r\n",
							"    ,sap_manualjournalsline_acctcode\r\n",
							"    ,sap_manualjournalsheader_objtype\r\n",
							"    ,sap_manualjournalsline_project\r\n",
							"    ,sap_manualjournalsline_vatgroup\r\n",
							"    ,sap_manualjournalsline_vatrate\r\n",
							"    ,sap_manualjournalsline_vatamount\r\n",
							"    ,sap_manualjournalsline_description\r\n",
							"    ,sap_manualjournalsline_jnlref1\r\n",
							"    ,sap_manualjournalsline_jnlref2\r\n",
							"    ,sap_manualjournalsline_subdepotcode\r\n",
							"    ,SUM(sap_manualjournalsline_credit-sap_manualjournalsline_debit)as sap_manualjournalsline_linetotal\r\n",
							"    ,'manualjournal' as  sap_manualjournalsline_doccat\r\n",
							"\r\n",
							"    -- ,si.sap_salesinvoiceline_itemcode as sap_manualjournalsline_itemcode\r\n",
							"    -- ,as sap_manualjournalsheader_cardcode\r\n",
							"    -- ,as sap_manualjournalsheader_cardref\r\n",
							"    -- ,i.sap_item_invntitem as sap_manualjournalsline_invntitem\r\n",
							"\r\n",
							"FROM stmanualjournals_tmp\r\n",
							"WHERE (sap_manualjournalsheader_objtype in (30)\r\n",
							"AND CONCAT(sap_account_segment_0,sap_account_segment_1,sap_account_segment_2) NOT IN ('572110ADHDO','511120ADHDO','571170ADHDO')\r\n",
							"AND sap_account_groupmask in (4))\r\n",
							"AND sap_account_level1name NOT IN ('ASSETS','CAPITAL AND RESERVES','LIABILITIES')\r\n",
							"GROUP BY\r\n",
							"    sap_manualjournalsheader_postingdate\r\n",
							"    ,sap_manualjournalsheader_duedate\r\n",
							"    ,sap_manualjournalsheader_doctotal\r\n",
							"    ,sap_manualjournalsheader_doctype\r\n",
							"    ,sap_manualjournalsheader_transid\r\n",
							"    ,sap_manualjournalsheader_memo\r\n",
							"    ,sap_manualjournalsline_linenum\r\n",
							"    ,sap_manualjournalsline_acctcode\r\n",
							"    ,sap_manualjournalsheader_objtype\r\n",
							"    ,sap_manualjournalsline_project\r\n",
							"    ,sap_manualjournalsline_vatgroup\r\n",
							"    ,sap_manualjournalsline_vatrate\r\n",
							"    ,sap_manualjournalsline_vatamount\r\n",
							"    ,sap_manualjournalsline_description\r\n",
							"    ,sap_manualjournalsline_jnlref1\r\n",
							"    ,sap_manualjournalsline_jnlref2\r\n",
							"    ,sap_manualjournalsline_subdepotcode"
						],
						"outputs": [],
						"execution_count": 37
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stmanualjournal1 = spark.sql(\"SELECT * FROM stmanualjournal1\")\r\n",
							"stmanualjournal2 = spark.sql(\"SELECT * FROM stmanualjournal2\")"
						],
						"outputs": [],
						"execution_count": 38
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stmanualjournal = stmanualjournal1.unionByName(stmanualjournal2, allowMissingColumns = True)"
						],
						"outputs": [],
						"execution_count": 39
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stmanualjournal.createOrReplaceTempView(\"stmanualjournal\")"
						],
						"outputs": [],
						"execution_count": 40
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stmanualjournal.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stmanualjournal.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 41
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stdirectposting1\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    sap_manualjournalsheader_postingdate\r\n",
							"    ,sap_manualjournalsheader_duedate\r\n",
							"    ,sap_manualjournalsheader_doctotal\r\n",
							"    ,sap_manualjournalsheader_doctype\r\n",
							"    ,sap_manualjournalsheader_transid\r\n",
							"    ,sap_manualjournalsline_linenum\r\n",
							"    ,sap_manualjournalsheader_memo\r\n",
							"    ,sap_manualjournalsline_acctcode\r\n",
							"    ,sap_manualjournalsheader_objtype\r\n",
							"    ,sap_manualjournalsline_project\r\n",
							"    ,sap_manualjournalsline_vatgroup\r\n",
							"    ,sap_manualjournalsline_vatrate\r\n",
							"    ,sap_manualjournalsline_vatamount\r\n",
							"    ,sap_manualjournalsline_description\r\n",
							"    ,sap_manualjournalsline_jnlref1\r\n",
							"    ,sap_manualjournalsline_jnlref2\r\n",
							"    ,sap_manualjournalsline_subdepotcode\r\n",
							"    ,SUM(sap_manualjournalsline_credit-sap_manualjournalsline_debit)as sap_manualjournalsline_linetotal\r\n",
							"    ,'directposting' as  sap_manualjournalsline_doccat\r\n",
							"\r\n",
							"    -- ,si.sap_salesinvoiceline_itemcode as sap_manualjournalsline_itemcode\r\n",
							"    -- ,as sap_manualjournalsheader_cardcode\r\n",
							"    -- ,as sap_manualjournalsheader_cardref\r\n",
							"    -- ,i.sap_item_invntitem as sap_manualjournalsline_invntitem\r\n",
							"\r\n",
							"FROM stmanualjournals_tmp\r\n",
							"WHERE sap_manualjournalsheader_objtype in (46)\r\n",
							"AND CONCAT(sap_account_segment_0,sap_account_segment_1,sap_account_segment_2) NOT IN ('572110ADHDO')\r\n",
							"AND sap_account_groupmask in (5,6,7,8)\r\n",
							"GROUP BY\r\n",
							"    sap_manualjournalsheader_postingdate\r\n",
							"    ,sap_manualjournalsheader_duedate\r\n",
							"    ,sap_manualjournalsheader_doctotal\r\n",
							"    ,sap_manualjournalsheader_doctype\r\n",
							"    ,sap_manualjournalsheader_transid\r\n",
							"    ,sap_manualjournalsheader_memo\r\n",
							"    ,sap_manualjournalsline_linenum\r\n",
							"    ,sap_manualjournalsline_acctcode\r\n",
							"    ,sap_manualjournalsheader_objtype\r\n",
							"    ,sap_manualjournalsline_project\r\n",
							"    ,sap_manualjournalsline_vatgroup\r\n",
							"    ,sap_manualjournalsline_vatrate\r\n",
							"    ,sap_manualjournalsline_vatamount\r\n",
							"    ,sap_manualjournalsline_description\r\n",
							"    ,sap_manualjournalsline_jnlref1\r\n",
							"    ,sap_manualjournalsline_jnlref2\r\n",
							"    ,sap_manualjournalsline_subdepotcode"
						],
						"outputs": [],
						"execution_count": 42
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stdirectposting2\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    sap_manualjournalsheader_postingdate\r\n",
							"    ,sap_manualjournalsheader_duedate\r\n",
							"    ,sap_manualjournalsheader_doctotal\r\n",
							"    ,sap_manualjournalsheader_doctype\r\n",
							"    ,sap_manualjournalsheader_transid\r\n",
							"    ,sap_manualjournalsheader_memo\r\n",
							"    ,sap_manualjournalsline_linenum\r\n",
							"    ,sap_manualjournalsline_acctcode\r\n",
							"    ,sap_manualjournalsheader_objtype\r\n",
							"    ,sap_manualjournalsline_project\r\n",
							"    ,sap_manualjournalsline_vatgroup\r\n",
							"    ,sap_manualjournalsline_vatrate\r\n",
							"    ,sap_manualjournalsline_vatamount\r\n",
							"    ,sap_manualjournalsline_description\r\n",
							"    ,sap_manualjournalsline_jnlref1\r\n",
							"    ,sap_manualjournalsline_jnlref2\r\n",
							"    ,sap_manualjournalsline_subdepotcode\r\n",
							"    ,SUM(sap_manualjournalsline_credit-sap_manualjournalsline_debit)as sap_manualjournalsline_linetotal\r\n",
							"    ,'directposting' as  sap_manualjournalsline_doccat\r\n",
							"\r\n",
							"    -- ,si.sap_salesinvoiceline_itemcode as sap_manualjournalsline_itemcode\r\n",
							"    -- ,as sap_manualjournalsheader_cardcode\r\n",
							"    -- ,as sap_manualjournalsheader_cardref\r\n",
							"    -- ,i.sap_item_invntitem as sap_manualjournalsline_invntitem\r\n",
							"\r\n",
							"FROM stmanualjournals_tmp\r\n",
							"WHERE sap_manualjournalsheader_objtype in (24)\r\n",
							"AND CONCAT(sap_account_segment_0,sap_account_segment_1,sap_account_segment_2) NOT IN ('572110ADHDO')\r\n",
							"AND sap_account_groupmask in (5,6,7,8)\r\n",
							"GROUP BY\r\n",
							"    sap_manualjournalsheader_postingdate\r\n",
							"    ,sap_manualjournalsheader_duedate\r\n",
							"    ,sap_manualjournalsheader_doctotal\r\n",
							"    ,sap_manualjournalsheader_doctype\r\n",
							"    ,sap_manualjournalsheader_transid\r\n",
							"    ,sap_manualjournalsline_linenum\r\n",
							"    ,sap_manualjournalsheader_memo\r\n",
							"    ,sap_manualjournalsline_acctcode\r\n",
							"    ,sap_manualjournalsheader_objtype\r\n",
							"    ,sap_manualjournalsline_project\r\n",
							"    ,sap_manualjournalsline_vatgroup\r\n",
							"    ,sap_manualjournalsline_vatrate\r\n",
							"    ,sap_manualjournalsline_vatamount\r\n",
							"    ,sap_manualjournalsline_description\r\n",
							"    ,sap_manualjournalsline_jnlref1\r\n",
							"    ,sap_manualjournalsline_jnlref2\r\n",
							"    ,sap_manualjournalsline_subdepotcode"
						],
						"outputs": [],
						"execution_count": 43
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stdirectposting3\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    sap_manualjournalsheader_postingdate\r\n",
							"    ,sap_manualjournalsheader_duedate\r\n",
							"    ,sap_manualjournalsheader_doctotal\r\n",
							"    ,sap_manualjournalsheader_doctype\r\n",
							"    ,sap_manualjournalsheader_transid\r\n",
							"    ,sap_manualjournalsheader_memo\r\n",
							"    ,sap_manualjournalsline_linenum\r\n",
							"    ,sap_manualjournalsline_acctcode\r\n",
							"    ,sap_manualjournalsheader_objtype\r\n",
							"    ,sap_manualjournalsline_project\r\n",
							"    ,sap_manualjournalsline_vatgroup\r\n",
							"    ,sap_manualjournalsline_vatrate\r\n",
							"    ,sap_manualjournalsline_vatamount\r\n",
							"    ,sap_manualjournalsline_description\r\n",
							"    ,sap_manualjournalsline_jnlref1\r\n",
							"    ,sap_manualjournalsline_jnlref2\r\n",
							"    ,sap_manualjournalsline_subdepotcode\r\n",
							"    ,SUM(sap_manualjournalsline_credit-sap_manualjournalsline_debit)as sap_manualjournalsline_linetotal\r\n",
							"    ,'directposting' as  sap_manualjournalsline_doccat\r\n",
							"\r\n",
							"    -- ,si.sap_salesinvoiceline_itemcode as sap_manualjournalsline_itemcode\r\n",
							"    -- ,as sap_manualjournalsheader_cardcode\r\n",
							"    -- ,as sap_manualjournalsheader_cardref\r\n",
							"    -- ,i.sap_item_invntitem as sap_manualjournalsline_invntitem\r\n",
							"\r\n",
							"FROM stmanualjournals_tmp\r\n",
							"WHERE sap_manualjournalsheader_objtype in (46)\r\n",
							"AND CONCAT(sap_account_segment_0,sap_account_segment_1,sap_account_segment_2) NOT IN ('572110ADHDO')\r\n",
							"AND sap_account_groupmask in (4)\r\n",
							"GROUP BY\r\n",
							"    sap_manualjournalsheader_postingdate\r\n",
							"    ,sap_manualjournalsheader_duedate\r\n",
							"    ,sap_manualjournalsheader_doctotal\r\n",
							"    ,sap_manualjournalsheader_doctype\r\n",
							"    ,sap_manualjournalsheader_transid\r\n",
							"    ,sap_manualjournalsheader_memo\r\n",
							"    ,sap_manualjournalsline_linenum\r\n",
							"    ,sap_manualjournalsline_acctcode\r\n",
							"    ,sap_manualjournalsheader_objtype\r\n",
							"    ,sap_manualjournalsline_project\r\n",
							"    ,sap_manualjournalsline_vatgroup\r\n",
							"    ,sap_manualjournalsline_vatrate\r\n",
							"    ,sap_manualjournalsline_vatamount\r\n",
							"    ,sap_manualjournalsline_description\r\n",
							"    ,sap_manualjournalsline_jnlref1\r\n",
							"    ,sap_manualjournalsline_jnlref2\r\n",
							"    ,sap_manualjournalsline_subdepotcode"
						],
						"outputs": [],
						"execution_count": 44
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stdirectposting4\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    sap_manualjournalsheader_postingdate\r\n",
							"    ,sap_manualjournalsheader_duedate\r\n",
							"    ,sap_manualjournalsheader_doctotal\r\n",
							"    ,sap_manualjournalsheader_doctype\r\n",
							"    ,sap_manualjournalsheader_transid\r\n",
							"    ,sap_manualjournalsheader_memo\r\n",
							"    ,sap_manualjournalsline_linenum\r\n",
							"    ,sap_manualjournalsline_acctcode\r\n",
							"    ,sap_manualjournalsheader_objtype\r\n",
							"    ,sap_manualjournalsline_project\r\n",
							"    ,sap_manualjournalsline_vatgroup\r\n",
							"    ,sap_manualjournalsline_vatrate\r\n",
							"    ,sap_manualjournalsline_vatamount\r\n",
							"    ,sap_manualjournalsline_description\r\n",
							"    ,sap_manualjournalsline_jnlref1\r\n",
							"    ,sap_manualjournalsline_jnlref2\r\n",
							"    ,sap_manualjournalsline_subdepotcode\r\n",
							"    ,SUM(sap_manualjournalsline_credit-sap_manualjournalsline_debit)as sap_manualjournalsline_linetotal\r\n",
							"    ,'directposting' as  sap_manualjournalsline_doccat\r\n",
							"\r\n",
							"    -- ,si.sap_salesinvoiceline_itemcode as sap_manualjournalsline_itemcode\r\n",
							"    -- ,as sap_manualjournalsheader_cardcode\r\n",
							"    -- ,as sap_manualjournalsheader_cardref\r\n",
							"    -- ,i.sap_item_invntitem as sap_manualjournalsline_invntitem\r\n",
							"\r\n",
							"FROM stmanualjournals_tmp\r\n",
							"WHERE sap_manualjournalsheader_objtype in (24)\r\n",
							"AND CONCAT(sap_account_segment_0,sap_account_segment_1,sap_account_segment_2) NOT IN ('572110ADHDO')\r\n",
							"AND sap_account_groupmask in (4)\r\n",
							"GROUP BY\r\n",
							"    sap_manualjournalsheader_postingdate\r\n",
							"    ,sap_manualjournalsheader_duedate\r\n",
							"    ,sap_manualjournalsheader_doctotal\r\n",
							"    ,sap_manualjournalsheader_doctype\r\n",
							"    ,sap_manualjournalsheader_transid\r\n",
							"    ,sap_manualjournalsheader_memo\r\n",
							"    ,sap_manualjournalsline_linenum\r\n",
							"    ,sap_manualjournalsline_acctcode\r\n",
							"    ,sap_manualjournalsheader_objtype\r\n",
							"    ,sap_manualjournalsline_project\r\n",
							"    ,sap_manualjournalsline_vatgroup\r\n",
							"    ,sap_manualjournalsline_vatrate\r\n",
							"    ,sap_manualjournalsline_vatamount\r\n",
							"    ,sap_manualjournalsline_description\r\n",
							"    ,sap_manualjournalsline_jnlref1\r\n",
							"    ,sap_manualjournalsline_jnlref2\r\n",
							"    ,sap_manualjournalsline_subdepotcode"
						],
						"outputs": [],
						"execution_count": 45
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stdirectposting1 = spark.sql(\"SELECT * FROM stdirectposting1\")\r\n",
							"stdirectposting2 = spark.sql(\"SELECT * FROM stdirectposting2\")\r\n",
							"stdirectposting3 = spark.sql(\"SELECT * FROM stdirectposting3\")\r\n",
							"stdirectposting4 = spark.sql(\"SELECT * FROM stdirectposting4\")"
						],
						"outputs": [],
						"execution_count": 46
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stdirectposting = stdirectposting1.unionByName(stdirectposting2, allowMissingColumns = True).unionByName(stdirectposting3, allowMissingColumns = True).unionByName(stdirectposting4, allowMissingColumns = True)"
						],
						"outputs": [],
						"execution_count": 47
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stdirectposting = stdirectposting.withColumnRenamed(\"sap_manualjournalsheader_postingdate\",\"sap_directpostingheader_postingdate\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsheader_duedate\",\"sap_directpostingheader_duedate\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsheader_doctotal\",\"sap_directpostingheader_doctotal\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsheader_doctype\",\"sap_directpostingheader_doctype\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsheader_transid\",\"sap_directpostingheader_transid\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsheader_memo\",\"sap_directpostingheader_memo\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_linenum\",\"sap_directpostingline_linenum\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_acctcode\",\"sap_directpostingline_acctcode\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsheader_objtype\",\"sap_directpostingheader_objtype\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_project\",\"sap_directpostingline_project\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_vatgroup\",\"sap_directpostingline_vatgroup\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_vatrate\",\"sap_directpostingline_vatrate\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_vatamount\",\"sap_directpostingline_vatamount\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_description\",\"sap_directpostingline_description\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_jnlref1\",\"sap_directpostingline_jnlref1\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_jnlref2\",\"sap_directpostingline_jnlref2\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_subdepotcode\",\"sap_directpostingline_subdepotcode\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_linetotal\",\"sap_directpostingline_linetotal\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_doccat\",\"sap_directpostingline_doccat\")"
						],
						"outputs": [],
						"execution_count": 48
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stdirectposting.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stdirectposting.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 49
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": true,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# -- %%sql\r\n",
							"# -- --ASSETS\r\n",
							"# -- CREATE OR REPLACE TEMP VIEW\r\n",
							"# -- stgeneraljournal1\r\n",
							"# -- AS\r\n",
							"# -- SELECT\r\n",
							"# --     sap_manualjournalsheader_postingdate\r\n",
							"# --     ,sap_manualjournalsheader_duedate\r\n",
							"# --     ,sap_manualjournalsheader_doctotal\r\n",
							"# --     ,sap_manualjournalsheader_doctype\r\n",
							"# --     ,sap_manualjournalsheader_transid\r\n",
							"# --     ,sap_manualjournalsline_linenum\r\n",
							"# --     ,sap_manualjournalsline_acctcode\r\n",
							"# --     ,sap_manualjournalsheader_objtype\r\n",
							"# --     ,sap_manualjournalsline_project\r\n",
							"# --     ,sap_manualjournalsline_vatgroup\r\n",
							"# --     ,sap_manualjournalsline_vatrate\r\n",
							"# --     ,sap_manualjournalsline_vatamount\r\n",
							"# --     ,sap_manualjournalsline_description\r\n",
							"# --     ,sap_manualjournalsline_jnlref1\r\n",
							"# --     ,sap_manualjournalsline_jnlref2\r\n",
							"# --     ,sap_manualjournalsline_subdepotcode\r\n",
							"# --     ,SUM(sap_manualjournalsline_debit-sap_manualjournalsline_credit) as sap_manualjournalsline_linetotal\r\n",
							"# --     ,'generaljournal' as  sap_manualjournalsline_doccat\r\n",
							"\r\n",
							"# --     -- ,si.sap_salesinvoiceline_itemcode as sap_manualjournalsline_itemcode\r\n",
							"# --     -- ,as sap_manualjournalsheader_cardcode\r\n",
							"# --     -- ,as sap_manualjournalsheader_cardref\r\n",
							"# --     -- ,i.sap_item_invntitem as sap_manualjournalsline_invntitem\r\n",
							"\r\n",
							"# -- FROM stmanualjournals_tmp\r\n",
							"# -- WHERE sap_account_groupmask in (1)\r\n",
							"# -- GROUP BY\r\n",
							"# --     sap_manualjournalsheader_postingdate\r\n",
							"# --     ,sap_manualjournalsheader_duedate\r\n",
							"# --     ,sap_manualjournalsheader_doctotal\r\n",
							"# --     ,sap_manualjournalsheader_doctype\r\n",
							"# --     ,sap_manualjournalsheader_transid\r\n",
							"# --     ,sap_manualjournalsline_linenum\r\n",
							"# --     ,sap_manualjournalsline_acctcode\r\n",
							"# --     ,sap_manualjournalsheader_objtype\r\n",
							"# --     ,sap_manualjournalsline_project\r\n",
							"# --     ,sap_manualjournalsline_vatgroup\r\n",
							"# --     ,sap_manualjournalsline_vatrate\r\n",
							"# --     ,sap_manualjournalsline_vatamount\r\n",
							"# --     ,sap_manualjournalsline_description\r\n",
							"# --     ,sap_manualjournalsline_jnlref1\r\n",
							"# --     ,sap_manualjournalsline_jnlref2\r\n",
							"# --     ,sap_manualjournalsline_subdepotcode"
						],
						"outputs": [],
						"execution_count": 50
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": true,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# -- %%sql\r\n",
							"# -- --LIABILITIES\r\n",
							"# -- CREATE OR REPLACE TEMP VIEW\r\n",
							"# -- stgeneraljournal2\r\n",
							"# -- AS\r\n",
							"# -- SELECT\r\n",
							"# --     sap_manualjournalsheader_postingdate\r\n",
							"# --     ,sap_manualjournalsheader_duedate\r\n",
							"# --     ,sap_manualjournalsheader_doctotal\r\n",
							"# --     ,sap_manualjournalsheader_doctype\r\n",
							"# --     ,sap_manualjournalsheader_transid\r\n",
							"# --     ,sap_manualjournalsline_linenum\r\n",
							"# --     ,sap_manualjournalsline_acctcode\r\n",
							"# --     ,sap_manualjournalsheader_objtype\r\n",
							"# --     ,sap_manualjournalsline_project\r\n",
							"# --     ,sap_manualjournalsline_vatgroup\r\n",
							"# --     ,sap_manualjournalsline_vatrate\r\n",
							"# --     ,sap_manualjournalsline_vatamount\r\n",
							"# --     ,sap_manualjournalsline_description\r\n",
							"# --     ,sap_manualjournalsline_jnlref1\r\n",
							"# --     ,sap_manualjournalsline_jnlref2\r\n",
							"# --     ,sap_manualjournalsline_subdepotcode\r\n",
							"# --     ,SUM(sap_manualjournalsline_credit-sap_manualjournalsline_debit) as sap_manualjournalsline_linetotal\r\n",
							"# --     ,'generaljournal' as  sap_manualjournalsline_doccat\r\n",
							"\r\n",
							"# --     -- ,si.sap_salesinvoiceline_itemcode as sap_manualjournalsline_itemcode\r\n",
							"# --     -- ,as sap_manualjournalsheader_cardcode\r\n",
							"# --     -- ,as sap_manualjournalsheader_cardref\r\n",
							"# --     -- ,i.sap_item_invntitem as sap_manualjournalsline_invntitem\r\n",
							"\r\n",
							"# -- FROM stmanualjournals_tmp\r\n",
							"# -- WHERE sap_account_groupmask in (2)\r\n",
							"# -- GROUP BY\r\n",
							"# --     sap_manualjournalsheader_postingdate\r\n",
							"# --     ,sap_manualjournalsheader_duedate\r\n",
							"# --     ,sap_manualjournalsheader_doctotal\r\n",
							"# --     ,sap_manualjournalsheader_doctype\r\n",
							"# --     ,sap_manualjournalsheader_transid\r\n",
							"# --     ,sap_manualjournalsline_linenum\r\n",
							"# --     ,sap_manualjournalsline_acctcode\r\n",
							"# --     ,sap_manualjournalsheader_objtype\r\n",
							"# --     ,sap_manualjournalsline_project\r\n",
							"# --     ,sap_manualjournalsline_vatgroup\r\n",
							"# --     ,sap_manualjournalsline_vatrate\r\n",
							"# --     ,sap_manualjournalsline_vatamount\r\n",
							"# --     ,sap_manualjournalsline_description\r\n",
							"# --     ,sap_manualjournalsline_jnlref1\r\n",
							"# --     ,sap_manualjournalsline_jnlref2\r\n",
							"# --     ,sap_manualjournalsline_subdepotcode"
						],
						"outputs": [],
						"execution_count": 51
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": true,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# -- %%sql\r\n",
							"# -- --CAPITAL AND RESERVES\r\n",
							"# -- CREATE OR REPLACE TEMP VIEW\r\n",
							"# -- stgeneraljournal3\r\n",
							"# -- AS\r\n",
							"# -- SELECT\r\n",
							"# --     sap_manualjournalsheader_postingdate\r\n",
							"# --     ,sap_manualjournalsheader_duedate\r\n",
							"# --     ,sap_manualjournalsheader_doctotal\r\n",
							"# --     ,sap_manualjournalsheader_doctype\r\n",
							"# --     ,sap_manualjournalsheader_transid\r\n",
							"# --     ,sap_manualjournalsline_linenum\r\n",
							"# --     ,sap_manualjournalsline_acctcode\r\n",
							"# --     ,sap_manualjournalsheader_objtype\r\n",
							"# --     ,sap_manualjournalsline_project\r\n",
							"# --     ,sap_manualjournalsline_vatgroup\r\n",
							"# --     ,sap_manualjournalsline_vatrate\r\n",
							"# --     ,sap_manualjournalsline_vatamount\r\n",
							"# --     ,sap_manualjournalsline_description\r\n",
							"# --     ,sap_manualjournalsline_jnlref1\r\n",
							"# --     ,sap_manualjournalsline_jnlref2\r\n",
							"# --     ,sap_manualjournalsline_subdepotcode\r\n",
							"# --     ,SUM(sap_manualjournalsline_credit-sap_manualjournalsline_debit) as sap_manualjournalsline_linetotal\r\n",
							"# --     ,'generaljournal' as  sap_manualjournalsline_doccat\r\n",
							"\r\n",
							"# --     -- ,si.sap_salesinvoiceline_itemcode as sap_manualjournalsline_itemcode\r\n",
							"# --     -- ,as sap_manualjournalsheader_cardcode\r\n",
							"# --     -- ,as sap_manualjournalsheader_cardref\r\n",
							"# --     -- ,i.sap_item_invntitem as sap_manualjournalsline_invntitem\r\n",
							"\r\n",
							"# -- FROM stmanualjournals_tmp\r\n",
							"# -- WHERE sap_account_groupmask in (3)\r\n",
							"# -- GROUP BY\r\n",
							"# --     sap_manualjournalsheader_postingdate\r\n",
							"# --     ,sap_manualjournalsheader_duedate\r\n",
							"# --     ,sap_manualjournalsheader_doctotal\r\n",
							"# --     ,sap_manualjournalsheader_doctype\r\n",
							"# --     ,sap_manualjournalsheader_transid\r\n",
							"# --     ,sap_manualjournalsline_linenum\r\n",
							"# --     ,sap_manualjournalsline_acctcode\r\n",
							"# --     ,sap_manualjournalsheader_objtype\r\n",
							"# --     ,sap_manualjournalsline_project\r\n",
							"# --     ,sap_manualjournalsline_vatgroup\r\n",
							"# --     ,sap_manualjournalsline_vatrate\r\n",
							"# --     ,sap_manualjournalsline_vatamount\r\n",
							"# --     ,sap_manualjournalsline_description\r\n",
							"# --     ,sap_manualjournalsline_jnlref1\r\n",
							"# --     ,sap_manualjournalsline_jnlref2\r\n",
							"# --     ,sap_manualjournalsline_subdepotcode"
						],
						"outputs": [],
						"execution_count": 52
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"--TURNOVER\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stgeneraljournal4\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    sap_manualjournalsheader_postingdate\r\n",
							"    ,sap_manualjournalsheader_duedate\r\n",
							"    ,sap_manualjournalsheader_doctotal\r\n",
							"    ,sap_manualjournalsheader_doctype\r\n",
							"    ,sap_manualjournalsheader_transid\r\n",
							"    ,sap_manualjournalsheader_memo\r\n",
							"    ,sap_manualjournalsline_linenum\r\n",
							"    ,sap_manualjournalsline_acctcode\r\n",
							"    ,sap_manualjournalsheader_objtype\r\n",
							"    ,sap_manualjournalsline_project\r\n",
							"    ,sap_manualjournalsline_vatgroup\r\n",
							"    ,sap_manualjournalsline_vatrate\r\n",
							"    ,sap_manualjournalsline_vatamount\r\n",
							"    ,sap_manualjournalsline_description\r\n",
							"    ,sap_manualjournalsline_jnlref1\r\n",
							"    ,sap_manualjournalsline_jnlref2\r\n",
							"    ,sap_manualjournalsline_subdepotcode\r\n",
							"    ,SUM(sap_manualjournalsline_credit-sap_manualjournalsline_debit) as sap_manualjournalsline_linetotal\r\n",
							"    ,'generaljournal' as  sap_manualjournalsline_doccat\r\n",
							"\r\n",
							"    -- ,si.sap_salesinvoiceline_itemcode as sap_manualjournalsline_itemcode\r\n",
							"    -- ,as sap_manualjournalsheader_cardcode\r\n",
							"    -- ,as sap_manualjournalsheader_cardref\r\n",
							"    -- ,i.sap_item_invntitem as sap_manualjournalsline_invntitem\r\n",
							"\r\n",
							"FROM stmanualjournals_tmp\r\n",
							"WHERE sap_account_groupmask in (4)\r\n",
							"GROUP BY\r\n",
							"    sap_manualjournalsheader_postingdate\r\n",
							"    ,sap_manualjournalsheader_duedate\r\n",
							"    ,sap_manualjournalsheader_doctotal\r\n",
							"    ,sap_manualjournalsheader_doctype\r\n",
							"    ,sap_manualjournalsheader_transid\r\n",
							"    ,sap_manualjournalsheader_memo\r\n",
							"    ,sap_manualjournalsline_linenum\r\n",
							"    ,sap_manualjournalsline_acctcode\r\n",
							"    ,sap_manualjournalsheader_objtype\r\n",
							"    ,sap_manualjournalsline_project\r\n",
							"    ,sap_manualjournalsline_vatgroup\r\n",
							"    ,sap_manualjournalsline_vatrate\r\n",
							"    ,sap_manualjournalsline_vatamount\r\n",
							"    ,sap_manualjournalsline_description\r\n",
							"    ,sap_manualjournalsline_jnlref1\r\n",
							"    ,sap_manualjournalsline_jnlref2\r\n",
							"    ,sap_manualjournalsline_subdepotcode"
						],
						"outputs": [],
						"execution_count": 53
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"--COST OF SALES\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stgeneraljournal5\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    sap_manualjournalsheader_postingdate\r\n",
							"    ,sap_manualjournalsheader_duedate\r\n",
							"    ,sap_manualjournalsheader_doctotal\r\n",
							"    ,sap_manualjournalsheader_doctype\r\n",
							"    ,sap_manualjournalsheader_transid\r\n",
							"    ,sap_manualjournalsheader_memo\r\n",
							"    ,sap_manualjournalsline_linenum\r\n",
							"    ,sap_manualjournalsline_acctcode\r\n",
							"    ,sap_manualjournalsheader_objtype\r\n",
							"    ,sap_manualjournalsline_project\r\n",
							"    ,sap_manualjournalsline_vatgroup\r\n",
							"    ,sap_manualjournalsline_vatrate\r\n",
							"    ,sap_manualjournalsline_vatamount\r\n",
							"    ,sap_manualjournalsline_description\r\n",
							"    ,sap_manualjournalsline_jnlref1\r\n",
							"    ,sap_manualjournalsline_jnlref2\r\n",
							"    ,sap_manualjournalsline_subdepotcode\r\n",
							"    ,SUM(sap_manualjournalsline_credit-sap_manualjournalsline_debit) as sap_manualjournalsline_linetotal\r\n",
							"    ,'generaljournal' as  sap_manualjournalsline_doccat\r\n",
							"\r\n",
							"    -- ,si.sap_salesinvoiceline_itemcode as sap_manualjournalsline_itemcode\r\n",
							"    -- ,as sap_manualjournalsheader_cardcode\r\n",
							"    -- ,as sap_manualjournalsheader_cardref\r\n",
							"    -- ,i.sap_item_invntitem as sap_manualjournalsline_invntitem\r\n",
							"\r\n",
							"FROM stmanualjournals_tmp\r\n",
							"WHERE sap_account_groupmask in (5)\r\n",
							"GROUP BY\r\n",
							"    sap_manualjournalsheader_postingdate\r\n",
							"    ,sap_manualjournalsheader_duedate\r\n",
							"    ,sap_manualjournalsheader_doctotal\r\n",
							"    ,sap_manualjournalsheader_doctype\r\n",
							"    ,sap_manualjournalsheader_transid\r\n",
							"    ,sap_manualjournalsheader_memo\r\n",
							"    ,sap_manualjournalsline_linenum\r\n",
							"    ,sap_manualjournalsline_acctcode\r\n",
							"    ,sap_manualjournalsheader_objtype\r\n",
							"    ,sap_manualjournalsline_project\r\n",
							"    ,sap_manualjournalsline_vatgroup\r\n",
							"    ,sap_manualjournalsline_vatrate\r\n",
							"    ,sap_manualjournalsline_vatamount\r\n",
							"    ,sap_manualjournalsline_description\r\n",
							"    ,sap_manualjournalsline_jnlref1\r\n",
							"    ,sap_manualjournalsline_jnlref2\r\n",
							"    ,sap_manualjournalsline_subdepotcode"
						],
						"outputs": [],
						"execution_count": 54
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"--OPERATING COSTS\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stgeneraljournal6\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    sap_manualjournalsheader_postingdate\r\n",
							"    ,sap_manualjournalsheader_duedate\r\n",
							"    ,sap_manualjournalsheader_doctotal\r\n",
							"    ,sap_manualjournalsheader_doctype\r\n",
							"    ,sap_manualjournalsheader_transid\r\n",
							"    ,sap_manualjournalsheader_memo\r\n",
							"    ,sap_manualjournalsline_linenum\r\n",
							"    ,sap_manualjournalsline_acctcode\r\n",
							"    ,sap_manualjournalsheader_objtype\r\n",
							"    ,sap_manualjournalsline_project\r\n",
							"    ,sap_manualjournalsline_vatgroup\r\n",
							"    ,sap_manualjournalsline_vatrate\r\n",
							"    ,sap_manualjournalsline_vatamount\r\n",
							"    ,sap_manualjournalsline_description\r\n",
							"    ,sap_manualjournalsline_jnlref1\r\n",
							"    ,sap_manualjournalsline_jnlref2\r\n",
							"    ,sap_manualjournalsline_subdepotcode\r\n",
							"    ,SUM(sap_manualjournalsline_credit-sap_manualjournalsline_debit) as sap_manualjournalsline_linetotal\r\n",
							"    ,'generaljournal' as  sap_manualjournalsline_doccat\r\n",
							"\r\n",
							"    -- ,si.sap_salesinvoiceline_itemcode as sap_manualjournalsline_itemcode\r\n",
							"    -- ,as sap_manualjournalsheader_cardcode\r\n",
							"    -- ,as sap_manualjournalsheader_cardref\r\n",
							"    -- ,i.sap_item_invntitem as sap_manualjournalsline_invntitem\r\n",
							"\r\n",
							"FROM stmanualjournals_tmp\r\n",
							"WHERE sap_account_groupmask in (6)\r\n",
							"AND sap_manualjournalsheader_transid NOT IN (311478,311479,311480,311481,311482,311484,311485) --HACK TO REMOVE GARBAGE IN SAP\r\n",
							"GROUP BY\r\n",
							"    sap_manualjournalsheader_postingdate\r\n",
							"    ,sap_manualjournalsheader_duedate\r\n",
							"    ,sap_manualjournalsheader_doctotal\r\n",
							"    ,sap_manualjournalsheader_doctype\r\n",
							"    ,sap_manualjournalsheader_transid\r\n",
							"    ,sap_manualjournalsheader_memo\r\n",
							"    ,sap_manualjournalsline_linenum\r\n",
							"    ,sap_manualjournalsline_acctcode\r\n",
							"    ,sap_manualjournalsheader_objtype\r\n",
							"    ,sap_manualjournalsline_project\r\n",
							"    ,sap_manualjournalsline_vatgroup\r\n",
							"    ,sap_manualjournalsline_vatrate\r\n",
							"    ,sap_manualjournalsline_vatamount\r\n",
							"    ,sap_manualjournalsline_description\r\n",
							"    ,sap_manualjournalsline_jnlref1\r\n",
							"    ,sap_manualjournalsline_jnlref2\r\n",
							"    ,sap_manualjournalsline_subdepotcode"
						],
						"outputs": [],
						"execution_count": 55
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"--TAX AND EXTRAORINARY ITEMS\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stgeneraljournal7\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    sap_manualjournalsheader_postingdate\r\n",
							"    ,sap_manualjournalsheader_duedate\r\n",
							"    ,sap_manualjournalsheader_doctotal\r\n",
							"    ,sap_manualjournalsheader_doctype\r\n",
							"    ,sap_manualjournalsheader_transid\r\n",
							"    ,sap_manualjournalsheader_memo\r\n",
							"    ,sap_manualjournalsline_linenum\r\n",
							"    ,sap_manualjournalsline_acctcode\r\n",
							"    ,sap_manualjournalsheader_objtype\r\n",
							"    ,sap_manualjournalsline_project\r\n",
							"    ,sap_manualjournalsline_vatgroup\r\n",
							"    ,sap_manualjournalsline_vatrate\r\n",
							"    ,sap_manualjournalsline_vatamount\r\n",
							"    ,sap_manualjournalsline_description\r\n",
							"    ,sap_manualjournalsline_jnlref1\r\n",
							"    ,sap_manualjournalsline_jnlref2\r\n",
							"    ,sap_manualjournalsline_subdepotcode\r\n",
							"    ,SUM(sap_manualjournalsline_credit-sap_manualjournalsline_debit) as sap_manualjournalsline_linetotal\r\n",
							"    ,'generaljournal' as  sap_manualjournalsline_doccat\r\n",
							"\r\n",
							"    -- ,si.sap_salesinvoiceline_itemcode as sap_manualjournalsline_itemcode\r\n",
							"    -- ,as sap_manualjournalsheader_cardcode\r\n",
							"    -- ,as sap_manualjournalsheader_cardref\r\n",
							"    -- ,i.sap_item_invntitem as sap_manualjournalsline_invntitem\r\n",
							"\r\n",
							"FROM stmanualjournals_tmp\r\n",
							"WHERE sap_account_groupmask in (8)\r\n",
							"\r\n",
							"GROUP BY\r\n",
							"    sap_manualjournalsheader_postingdate\r\n",
							"    ,sap_manualjournalsheader_duedate\r\n",
							"    ,sap_manualjournalsheader_doctotal\r\n",
							"    ,sap_manualjournalsheader_doctype\r\n",
							"    ,sap_manualjournalsheader_transid\r\n",
							"    ,sap_manualjournalsheader_memo\r\n",
							"    ,sap_manualjournalsline_linenum\r\n",
							"    ,sap_manualjournalsline_acctcode\r\n",
							"    ,sap_manualjournalsheader_objtype\r\n",
							"    ,sap_manualjournalsline_project\r\n",
							"    ,sap_manualjournalsline_vatgroup\r\n",
							"    ,sap_manualjournalsline_vatrate\r\n",
							"    ,sap_manualjournalsline_vatamount\r\n",
							"    ,sap_manualjournalsline_description\r\n",
							"    ,sap_manualjournalsline_jnlref1\r\n",
							"    ,sap_manualjournalsline_jnlref2\r\n",
							"    ,sap_manualjournalsline_subdepotcode"
						],
						"outputs": [],
						"execution_count": 56
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stgeneraljournal1 = spark.sql(\"SELECT * FROM stgeneraljournal1\")\r\n",
							"# stgeneraljournal2 = spark.sql(\"SELECT * FROM stgeneraljournal2\")\r\n",
							"# stgeneraljournal3 = spark.sql(\"SELECT * FROM stgeneraljournal3\")\r\n",
							"stgeneraljournal4 = spark.sql(\"SELECT * FROM stgeneraljournal4\")\r\n",
							"stgeneraljournal5 = spark.sql(\"SELECT * FROM stgeneraljournal5\")\r\n",
							"stgeneraljournal6 = spark.sql(\"SELECT * FROM stgeneraljournal6\")\r\n",
							"stgeneraljournal7 = spark.sql(\"SELECT * FROM stgeneraljournal7\")"
						],
						"outputs": [],
						"execution_count": 57
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stgeneraljournal = stgeneraljournal4.unionByName(stgeneraljournal5, allowMissingColumns = True).unionByName(stgeneraljournal6, allowMissingColumns = True).unionByName(stgeneraljournal7, allowMissingColumns = True)"
						],
						"outputs": [],
						"execution_count": 58
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stgeneraljournal = stgeneraljournal.withColumnRenamed(\"sap_manualjournalsheader_postingdate\",\"sap_generaljournalheader_postingdate\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsheader_duedate\",\"sap_generaljournalheader_duedate\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsheader_doctotal\",\"sap_generaljournalheader_doctotal\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsheader_doctype\",\"sap_generaljournalheader_doctype\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsheader_transid\",\"sap_generaljournalheader_transid\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsheader_memo\",\"sap_generaljournalheader_memo\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_linenum\",\"sap_generaljournalline_linenum\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_acctcode\",\"sap_generaljournalline_acctcode\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsheader_objtype\",\"sap_generaljournalheader_objtype\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_project\",\"sap_generaljournalline_project\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_vatgroup\",\"sap_generaljournalline_vatgroup\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_vatrate\",\"sap_generaljournalline_vatrate\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_vatamount\",\"sap_generaljournalline_vatamount\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_description\",\"sap_generaljournalline_description\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_jnlref1\",\"sap_generaljournalline_jnlref1\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_jnlref2\",\"sap_generaljournalline_jnlref2\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_subdepotcode\",\"sap_generaljournalline_subdepotcode\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_linetotal\",\"sap_generaljournalline_linetotal\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_doccat\",\"sap_generaljournalline_doccat\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 59
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stgeneraljournal.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stgeneraljournal.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 60
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"SUCCESS\")"
						],
						"outputs": [],
						"execution_count": 61
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/09_DIST_STDCTransfer')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Ops/Distribution"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "f4f90490-670d-4169-8960-148013ef1c42"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse_Prod/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace-prod/bigDataPools/PRDSparkPool",
						"name": "PRDSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/PRDSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# #Create DataFrame for the dbodc_transfer LMS Table\r\n",
							"# dbodc_transfer = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodc_transfer.parquet', format='parquet')\r\n",
							"# dbodc_transfer.createOrReplaceTempView(\"dbodc_transfer\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stdctransfer\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"\r\n",
							"    dct.id as lms_dctransfer_id\r\n",
							"    ,dct.destinationstoreid as lms_dctransfer_destinationstoreid\r\n",
							"    ,dct.transfercode as lms_dctransfer_transfercode\r\n",
							"    ,dct.audittype as lms_dctransfer_audittype\r\n",
							"    ,dct.qty as lms_dctransfer_qty\r\n",
							"    ,dct.dc_prepackid as lms_dctransfer_dc_prepackid\r\n",
							"    ,dct.acceptancedate as lms_dctransfer_acceptancedate\r\n",
							"    ,dct.auditflag as lms_dctransfer_auditflag\r\n",
							"    ,dct.audituserid as lms_dctransfer_audituserid\r\n",
							"    ,dct.auditlocid as lms_dctransfer_auditlocid\r\n",
							"    ,dct.counted as lms_dctransfer_counted\r\n",
							"    ,dct.damagecount as lms_dctransfer_damagecount\r\n",
							"    ,dct.uploaded as lms_dctransfer_uploaded\r\n",
							"    ,dct.dc_torderid as lms_dctransfer_dc_torderid\r\n",
							"    ,dct.duedate as lms_dctransfer_duedate\r\n",
							"    ,dct.destinationlocationcode as lms_dctransfer_destinationlocationcode\r\n",
							"    ,dct.destinationlocationtypecode as lms_dctransfer_destinationlocationtypecode\r\n",
							"    ,dct.processfamilycode as lms_dctransfer_processfamilycode\r\n",
							"    ,dct.grnversion as lms_dctransfer_grnversion\r\n",
							"\r\n",
							"FROM dbodc_transfer dct\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stdctransfer = spark.sql(\"SELECT * FROM stdctransfer\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stdctransfer.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stdctransfer.parquet', mode = \"overwrite\")\r\n",
							"print(\"SUCCESS\")"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/09_LH_STTripTrackingHistory')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Ops/Linehaul"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "6e3f8ef8-2ee2-470d-b161-597dd188927f"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse_Prod/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace-prod/bigDataPools/PRDSparkPool",
						"name": "PRDSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						}
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# #Create DataFrame for the publictriptrackinghistory TMS Table\r\n",
							"# publictriptrackinghistory = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publictriptrackinghistory.parquet', format='parquet')\r\n",
							"# publictriptrackinghistory.createOrReplaceTempView(\"publictriptrackinghistory\")"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"sttriptrackinghistory\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    t.id as tms_triptrackinghistory_id\r\n",
							"    ,t.tripid as tms_triptrackinghistory_tripid\r\n",
							"    ,t.controllerid as tms_triptrackinghistory_controllerid\r\n",
							"    ,to_timestamp(t.fromdate) as tms_triptrackinghistory_fromdate\r\n",
							"    ,to_timestamp(t.todate) as tms_triptrackinghistory_todate\r\n",
							"    ,t.actionedby as tms_triptrackinghistory_actionedby\r\n",
							"    ,t.controllername as tms_triptrackinghistory_controllername\r\n",
							"    ,t.actionedbyname as tms_triptrackinghistory_actionedbyname\r\n",
							"\r\n",
							"\r\n",
							"FROM publictriptrackinghistory t"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sttriptrackinghistory = spark.sql(\"SELECT * FROM sttriptrackinghistory\")"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# sttriptrackinghistory.write.parquet('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Structured Data/OPS/sttriptrackinghistory.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"SUCCESS\")"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/09_STTransactions')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Finance"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "0e795848-884a-48ed-b9d3-ee1513120c68"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse_Prod/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace-prod/bigDataPools/PRDSparkPool",
						"name": "PRDSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/PRDSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 72
					},
					{
						"cell_type": "code",
						"source": [
							"# #Create DataFrame for the stdocuments SAP Table\r\n",
							"# stdocuments = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/FINANCE/stdocuments.parquet', format='parquet')\r\n",
							"# stdocuments.createOrReplaceTempView(\"stdocuments\")\r\n",
							"\r\n",
							"# ######################################################################## UNION\r\n",
							"\r\n",
							"# #Create DataFrame for the stmanualjournal SAP Table\r\n",
							"# stmanualjournal = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/FINANCE/stmanualjournal.parquet', format='parquet')\r\n",
							"# stmanualjournal.createOrReplaceTempView(\"stmanualjournal\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stdirectposting SAP Table\r\n",
							"# stdirectposting = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/FINANCE/stdirectposting.parquet', format='parquet')\r\n",
							"# stdirectposting.createOrReplaceTempView(\"stdirectposting\")\r\n",
							"\r\n",
							"# ######################################################################## UNION\r\n",
							"\r\n",
							"# #Create DataFrame for the stgeneraljournal SAP Table\r\n",
							"# stgeneraljournal = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/FINANCE/stgeneraljournal.parquet', format='parquet')\r\n",
							"# stgeneraljournal.createOrReplaceTempView(\"stgeneraljournal\")\r\n",
							"\r\n",
							"# ######################################################################## UNION\r\n",
							"\r\n",
							"# #Create DataFrame for the ststocktransactions SAP Table\r\n",
							"# ststocktransactions = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/FINANCE/ststocktransactions.parquet', format='parquet')\r\n",
							"# ststocktransactions.createOrReplaceTempView(\"ststocktransactions\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stdiscountjournal SAP Table\r\n",
							"# stdiscountjournal = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/FINANCE/stdiscountjournal.parquet', format='parquet')\r\n",
							"# stdiscountjournal.createOrReplaceTempView(\"stdiscountjournal\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stsalesinvoiceinventory SAP Table\r\n",
							"# stsalesinvoiceinventory = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/FINANCE/stsalesinvoiceinventory.parquet', format='parquet')\r\n",
							"# stsalesinvoiceinventory.createOrReplaceTempView(\"stsalesinvoiceinventory\")\r\n",
							"\r\n",
							"# #Create DataFrame for the ststockrevaluations SAP Table\r\n",
							"# ststockrevaluations = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/FINANCE/ststockrevaluations.parquet', format='parquet')\r\n",
							"# ststockrevaluations.createOrReplaceTempView(\"ststockrevaluations\")\r\n",
							"\r\n",
							"# ######################################################################## UNION\r\n",
							"\r\n",
							"# #Create DataFrame for the stpurchasepricevariance SAP Table\r\n",
							"# stpurchasepricevariance = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/FINANCE/stpurchasepricevariance.parquet', format='parquet')\r\n",
							"# stpurchasepricevariance.createOrReplaceTempView(\"stpurchasepricevariance\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stcostofgoods SAP Table\r\n",
							"# stcostofgoods = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/FINANCE/stcostofgoods.parquet', format='parquet')\r\n",
							"# stcostofgoods.createOrReplaceTempView(\"stcostofgoods\")\r\n",
							"\r\n",
							"\r\n",
							"# ######################################################################## LEFT JOIN\r\n",
							"\r\n",
							"# #Create DataFrame for the stchartofaccounts SAP Table\r\n",
							"# stchartofaccounts = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/FINANCE/stchartofaccounts.parquet', format='parquet')\r\n",
							"# stchartofaccounts.createOrReplaceTempView(\"stchartofaccounts\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stitems SAP Table\r\n",
							"# stitems = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/FINANCE/stitems.parquet', format='parquet')\r\n",
							"# stitems.createOrReplaceTempView(\"stitems\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 73
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# %%pyspark\r\n",
							"# exceptions = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Finance and Operations Mapping Sources/SAP_Transactions_Exceptions.csv', format='csv'\r\n",
							"# , header=True\r\n",
							"# )\r\n",
							""
						],
						"outputs": [],
						"execution_count": 74
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# import pandas as pd\r\n",
							"# from pyspark.sql.types import DecimalType\r\n",
							"# from pyspark.sql.functions import col"
						],
						"outputs": [],
						"execution_count": 75
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"exceptions = exceptions.withColumn('sap_transactionsline_linetotal', col('sap_transactionsline_linetotal').cast(DecimalType(30,6)))\\\r\n",
							".withColumn('sap_transactionsheader_docnum', col('sap_transactionsheader_docnum').cast('int'))\\\r\n",
							".withColumn('sap_transactionsheader_transid', col('sap_transactionsheader_transid').cast('int'))\\\r\n",
							".withColumn('sap_transactionsline_u_ir_number', col('sap_transactionsline_u_ir_number').cast('int'))\r\n",
							""
						],
						"outputs": [],
						"execution_count": 76
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"exceptions.createOrReplaceTempView(\"exceptions\")"
						],
						"outputs": [],
						"execution_count": 77
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stdocuments = stdocuments.withColumnRenamed(\"sap_documentsline_polookupid\",\"sap_transactionsline_polookupid\")\\\r\n",
							".withColumnRenamed(\"sap_documentsheader_docentry\",\"sap_transactionsheader_docentry\")\\\r\n",
							".withColumnRenamed(\"sap_documentsheader_docnum\",\"sap_transactionsheader_docnum\")\\\r\n",
							".withColumnRenamed(\"sap_documentsheader_doctype\",\"sap_transactionsheader_doctype\")\\\r\n",
							".withColumnRenamed(\"sap_documentsheader_objtype\",\"sap_transactionsheader_objtype\")\\\r\n",
							".withColumnRenamed(\"sap_documentsheader_postingdate\",\"sap_transactionsheader_postingdate\")\\\r\n",
							".withColumnRenamed(\"sap_documentsheader_duedate\",\"sap_transactionsheader_duedate\")\\\r\n",
							".withColumnRenamed(\"sap_documentsheader_cardcode\",\"sap_transactionsheader_cardcode\")\\\r\n",
							".withColumnRenamed(\"sap_documentsheader_cardref\",\"sap_transactionsheader_cardref\")\\\r\n",
							".withColumnRenamed(\"sap_documentsheader_discprcnt\",\"sap_transactionsheader_discprcnt\")\\\r\n",
							".withColumnRenamed(\"sap_documentsheader_discsum\",\"sap_transactionsheader_discsum\")\\\r\n",
							".withColumnRenamed(\"sap_documentsheader_doctotal\",\"sap_transactionsheader_doctotal\")\\\r\n",
							".withColumnRenamed(\"sap_documentsheader_comments\",\"sap_transactionsheader_comments\")\\\r\n",
							".withColumnRenamed(\"sap_documentsheader_transid\",\"sap_transactionsheader_transid\")\\\r\n",
							".withColumnRenamed(\"sap_documentsheader_u_expense_depot\",\"sap_transactionsheader_u_expense_depot\")\\\r\n",
							".withColumnRenamed(\"sap_documentsline_linenum\",\"sap_transactionsline_linenum\")\\\r\n",
							".withColumnRenamed(\"sap_documentsline_linestatus\",\"sap_transactionsline_linestatus\")\\\r\n",
							".withColumnRenamed(\"sap_documentsline_itemcode\",\"sap_transactionsline_itemcode\")\\\r\n",
							".withColumnRenamed(\"sap_documentsline_dscription\",\"sap_transactionsline_dscription\")\\\r\n",
							".withColumnRenamed(\"sap_documentsline_quantity\",\"sap_transactionsline_quantity\")\\\r\n",
							".withColumnRenamed(\"sap_documentsline_openqty\",\"sap_transactionsline_openqty\")\\\r\n",
							".withColumnRenamed(\"sap_documentsline_price\",\"sap_transactionsline_price\")\\\r\n",
							".withColumnRenamed(\"sap_documentsline_rate\",\"sap_transactionsline_rate\")\\\r\n",
							".withColumnRenamed(\"sap_documentsline_discprcnt\",\"sap_transactionsline_discprcnt\")\\\r\n",
							".withColumnRenamed(\"sap_documentsline_linetotal\",\"sap_transactionsline_linetotal\")\\\r\n",
							".withColumnRenamed(\"sap_documentsline_opensum\",\"sap_transactionsline_opensum\")\\\r\n",
							".withColumnRenamed(\"sap_documentsline_pricebefdi\",\"sap_transactionsline_pricebefdi\")\\\r\n",
							".withColumnRenamed(\"sap_documentsline_docdate\",\"sap_transactionsline_docdate\")\\\r\n",
							".withColumnRenamed(\"sap_documentsline_project\",\"sap_transactionsline_project\")\\\r\n",
							".withColumnRenamed(\"sap_documentsline_vatprcnt\",\"sap_transactionsline_vatprcnt\")\\\r\n",
							".withColumnRenamed(\"sap_documentsline_vatgroup\",\"sap_transactionsline_vatgroup\")\\\r\n",
							".withColumnRenamed(\"sap_documentsline_vatamount\",\"sap_transactionsline_vatamount\")\\\r\n",
							".withColumnRenamed(\"sap_documentsline_u_reason\",\"sap_transactionsline_u_reason\")\\\r\n",
							".withColumnRenamed(\"sap_documentsline_u_expenseitem\",\"sap_transactionsline_u_expenseitem\")\\\r\n",
							".withColumnRenamed(\"sap_documentsline_u_fueldate\",\"sap_transactionsline_u_fueldate\")\\\r\n",
							".withColumnRenamed(\"sap_documentsline_acctcode\",\"sap_transactionsline_acctcode\")\\\r\n",
							".withColumnRenamed(\"sap_documentsline_doccat\",\"sap_transactionsline_doccat\")\\\r\n",
							".withColumnRenamed(\"sap_documentsline_subdepotcode\",\"sap_transactionsline_subdepotcode\")\\\r\n",
							".withColumnRenamed(\"sap_documentsline_u_rate\",\"sap_transactionsline_u_rate\")\\\r\n",
							".withColumnRenamed(\"sap_documentsline_u_ir_number\",\"sap_transactionsline_u_ir_number\")\\\r\n",
							".withColumnRenamed(\"sap_documentsline_discsum\",\"sap_transactionsline_discsum\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 78
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stmanualjournal = stmanualjournal.withColumnRenamed(\"sap_manualjournalsheader_postingdate\",\"sap_transactionsheader_postingdate\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsheader_duedate\",\"sap_transactionsheader_duedate\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsheader_doctotal\",\"sap_transactionsheader_doctotal\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsheader_doctype\",\"sap_transactionsheader_doctype\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsheader_transid\",\"sap_transactionsheader_transid\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsheader_memo\",\"sap_transactionsheader_memo\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_linenum\",\"sap_transactionsline_linenum\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_acctcode\",\"sap_transactionsline_acctcode\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsheader_objtype\",\"sap_transactionsheader_objtype\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_project\",\"sap_transactionsline_project\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_vatgroup\",\"sap_transactionsline_vatgroup\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_vatrate\",\"sap_transactionsline_vatrate\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_vatamount\",\"sap_transactionsline_vatamount\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_description\",\"sap_transactionsline_description\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_jnlref1\",\"sap_transactionsline_jnlref1\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_jnlref2\",\"sap_transactionsline_jnlref2\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_subdepotcode\",\"sap_transactionsline_subdepotcode\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_linetotal\",\"sap_transactionsline_linetotal\")\\\r\n",
							".withColumnRenamed(\"sap_manualjournalsline_doccat\",\"sap_transactionsline_doccat\")"
						],
						"outputs": [],
						"execution_count": 79
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stdirectposting = stdirectposting.withColumnRenamed(\"sap_directpostingheader_postingdate\",\"sap_transactionsheader_postingdate\")\\\r\n",
							".withColumnRenamed(\"sap_directpostingheader_duedate\",\"sap_transactionsheader_duedate\")\\\r\n",
							".withColumnRenamed(\"sap_directpostingheader_doctotal\",\"sap_transactionsheader_doctotal\")\\\r\n",
							".withColumnRenamed(\"sap_directpostingheader_doctype\",\"sap_transactionsheader_doctype\")\\\r\n",
							".withColumnRenamed(\"sap_directpostingheader_transid\",\"sap_transactionsheader_transid\")\\\r\n",
							".withColumnRenamed(\"sap_directpostingheader_memo\",\"sap_transactionsheader_memo\")\\\r\n",
							".withColumnRenamed(\"sap_directpostingline_linenum\",\"sap_transactionsline_linenum\")\\\r\n",
							".withColumnRenamed(\"sap_directpostingline_acctcode\",\"sap_transactionsline_acctcode\")\\\r\n",
							".withColumnRenamed(\"sap_directpostingheader_objtype\",\"sap_transactionsheader_objtype\")\\\r\n",
							".withColumnRenamed(\"sap_directpostingline_project\",\"sap_transactionsline_project\")\\\r\n",
							".withColumnRenamed(\"sap_directpostingline_vatgroup\",\"sap_transactionsline_vatgroup\")\\\r\n",
							".withColumnRenamed(\"sap_directpostingline_vatrate\",\"sap_transactionsline_vatrate\")\\\r\n",
							".withColumnRenamed(\"sap_directpostingline_vatamount\",\"sap_transactionsline_vatamount\")\\\r\n",
							".withColumnRenamed(\"sap_directpostingline_description\",\"sap_transactionsline_description\")\\\r\n",
							".withColumnRenamed(\"sap_directpostingline_jnlref1\",\"sap_transactionsline_jnlref1\")\\\r\n",
							".withColumnRenamed(\"sap_directpostingline_jnlref2\",\"sap_transactionsline_jnlref2\")\\\r\n",
							".withColumnRenamed(\"sap_directpostingline_subdepotcode\",\"sap_transactionsline_subdepotcode\")\\\r\n",
							".withColumnRenamed(\"sap_directpostingline_linetotal\",\"sap_transactionsline_linetotal\")\\\r\n",
							".withColumnRenamed(\"sap_directpostingline_doccat\",\"sap_transactionsline_doccat\")"
						],
						"outputs": [],
						"execution_count": 80
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stgeneraljournal = stgeneraljournal.withColumnRenamed(\"sap_generaljournalheader_postingdate\",\"sap_transactionsheader_postingdate\")\\\r\n",
							".withColumnRenamed(\"sap_generaljournalheader_duedate\",\"sap_transactionsheader_duedate\")\\\r\n",
							".withColumnRenamed(\"sap_generaljournalheader_doctotal\",\"sap_transactionsheader_doctotal\")\\\r\n",
							".withColumnRenamed(\"sap_generaljournalheader_doctype\",\"sap_transactionsheader_doctype\")\\\r\n",
							".withColumnRenamed(\"sap_generaljournalheader_transid\",\"sap_transactionsheader_transid\")\\\r\n",
							".withColumnRenamed(\"sap_generaljournalheader_memo\",\"sap_transactionsheader_memo\")\\\r\n",
							".withColumnRenamed(\"sap_generaljournalline_linenum\",\"sap_transactionsline_linenum\")\\\r\n",
							".withColumnRenamed(\"sap_generaljournalline_acctcode\",\"sap_transactionsline_acctcode\")\\\r\n",
							".withColumnRenamed(\"sap_generaljournalheader_objtype\",\"sap_transactionsheader_objtype\")\\\r\n",
							".withColumnRenamed(\"sap_generaljournalline_project\",\"sap_transactionsline_project\")\\\r\n",
							".withColumnRenamed(\"sap_generaljournalline_vatgroup\",\"sap_transactionsline_vatgroup\")\\\r\n",
							".withColumnRenamed(\"sap_generaljournalline_vatrate\",\"sap_transactionsline_vatrate\")\\\r\n",
							".withColumnRenamed(\"sap_generaljournalline_vatamount\",\"sap_transactionsline_vatamount\")\\\r\n",
							".withColumnRenamed(\"sap_generaljournalline_description\",\"sap_transactionsline_description\")\\\r\n",
							".withColumnRenamed(\"sap_generaljournalline_jnlref1\",\"sap_transactionsline_jnlref1\")\\\r\n",
							".withColumnRenamed(\"sap_generaljournalline_jnlref2\",\"sap_transactionsline_jnlref2\")\\\r\n",
							".withColumnRenamed(\"sap_generaljournalline_subdepotcode\",\"sap_transactionsline_subdepotcode\")\\\r\n",
							".withColumnRenamed(\"sap_generaljournalline_linetotal\",\"sap_transactionsline_linetotal\")\\\r\n",
							".withColumnRenamed(\"sap_generaljournalline_doccat\",\"sap_transactionsline_doccat\")"
						],
						"outputs": [],
						"execution_count": 81
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"ststocktransactions = ststocktransactions.withColumnRenamed(\"sap_stocktransactionsheader_docentry\",\"sap_transactionsheader_docentry\")\\\r\n",
							".withColumnRenamed(\"sap_stocktransactionsheader_docnum\",\"sap_transactionsheader_docnum\")\\\r\n",
							".withColumnRenamed(\"sap_stocktransactionsheader_doctype\",\"sap_transactionsheader_doctype\")\\\r\n",
							".withColumnRenamed(\"sap_stocktransactionsheader_objtype\",\"sap_transactionsheader_objtype\")\\\r\n",
							".withColumnRenamed(\"sap_stocktransactionsheader_postingdate\",\"sap_transactionsheader_postingdate\")\\\r\n",
							".withColumnRenamed(\"sap_stocktransactionsheader_duedate\",\"sap_transactionsheader_duedate\")\\\r\n",
							".withColumnRenamed(\"sap_stocktransactionsheader_cardcode\",\"sap_transactionsheader_cardcode\")\\\r\n",
							".withColumnRenamed(\"sap_stocktransactionsheader_cardref\",\"sap_transactionsheader_cardref\")\\\r\n",
							".withColumnRenamed(\"sap_stocktransactionsheader_discprcnt\",\"sap_transactionsheader_discprcnt\")\\\r\n",
							".withColumnRenamed(\"sap_stocktransactionsheader_discsum\",\"sap_transactionsheader_discsum\")\\\r\n",
							".withColumnRenamed(\"sap_stocktransactionsheader_doctotal\",\"sap_transactionsheader_doctotal\")\\\r\n",
							".withColumnRenamed(\"sap_stocktransactionsheader_comments\",\"sap_transactionsheader_comments\")\\\r\n",
							".withColumnRenamed(\"sap_stocktransactionsheader_transid\",\"sap_transactionsheader_transid\")\\\r\n",
							".withColumnRenamed(\"sap_stocktransactionsheader_u_expense_depot\",\"sap_transactionsheader_u_expense_depot\")\\\r\n",
							".withColumnRenamed(\"sap_stocktransactionsline_linenum\",\"sap_transactionsline_linenum\")\\\r\n",
							".withColumnRenamed(\"sap_stocktransactionsline_linestatus\",\"sap_transactionsline_linestatus\")\\\r\n",
							".withColumnRenamed(\"sap_stocktransactionsline_itemcode\",\"sap_transactionsline_itemcode\")\\\r\n",
							".withColumnRenamed(\"sap_stocktransactionsline_dscription\",\"sap_transactionsline_dscription\")\\\r\n",
							".withColumnRenamed(\"sap_stocktransactionsline_quantity\",\"sap_transactionsline_quantity\")\\\r\n",
							".withColumnRenamed(\"sap_stocktransactionsline_openqty\",\"sap_transactionsline_openqty\")\\\r\n",
							".withColumnRenamed(\"sap_stocktransactionsline_price\",\"sap_transactionsline_price\")\\\r\n",
							".withColumnRenamed(\"sap_stocktransactionsline_rate\",\"sap_transactionsline_rate\")\\\r\n",
							".withColumnRenamed(\"sap_stocktransactionsline_discprcnt\",\"sap_transactionsline_discprcnt\")\\\r\n",
							".withColumnRenamed(\"sap_stocktransactionsline_linetotal\",\"sap_transactionsline_linetotal\")\\\r\n",
							".withColumnRenamed(\"sap_stocktransactionsline_opensum\",\"sap_transactionsline_opensum\")\\\r\n",
							".withColumnRenamed(\"sap_stocktransactionsline_pricebefdi\",\"sap_transactionsline_pricebefdi\")\\\r\n",
							".withColumnRenamed(\"sap_stocktransactionsline_docdate\",\"sap_transactionsline_docdate\")\\\r\n",
							".withColumnRenamed(\"sap_stocktransactionsline_project\",\"sap_transactionsline_project\")\\\r\n",
							".withColumnRenamed(\"sap_stocktransactionsline_vatprcnt\",\"sap_transactionsline_vatprcnt\")\\\r\n",
							".withColumnRenamed(\"sap_stocktransactionsline_vatgroup\",\"sap_transactionsline_vatgroup\")\\\r\n",
							".withColumnRenamed(\"sap_stocktransactionsline_vatamount\",\"sap_transactionsline_vatamount\")\\\r\n",
							".withColumnRenamed(\"sap_stocktransactionsline_u_reason\",\"sap_transactionsline_u_reason\")\\\r\n",
							".withColumnRenamed(\"sap_stocktransactionsline_u_expenseitem\",\"sap_transactionsline_u_expenseitem\")\\\r\n",
							".withColumnRenamed(\"sap_stocktransactionsline_u_fueldate\",\"sap_transactionsline_u_fueldate\")\\\r\n",
							".withColumnRenamed(\"sap_stocktransactionsline_acctcode\",\"sap_transactionsline_acctcode\")\\\r\n",
							".withColumnRenamed(\"sap_stocktransactionsline_doccat\",\"sap_transactionsline_doccat\")\\\r\n",
							".withColumnRenamed(\"sap_stocktransactionsline_subdepotcode\",\"sap_transactionsline_subdepotcode\")\\\r\n",
							".withColumnRenamed(\"sap_stocktransactionsline_discsum\",\"sap_transactionsline_discsum\")"
						],
						"outputs": [],
						"execution_count": 82
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stdiscountjournal = stdiscountjournal.withColumnRenamed(\"sap_discountjournalheader_postingdate\",\"sap_transactionsheader_postingdate\")\\\r\n",
							".withColumnRenamed(\"sap_discountjournalheader_duedate\",\"sap_transactionsheader_duedate\")\\\r\n",
							".withColumnRenamed(\"sap_discountjournalheader_doctotal\",\"sap_transactionsheader_doctotal\")\\\r\n",
							".withColumnRenamed(\"sap_discountjournalheader_doctype\",\"sap_transactionsheader_doctype\")\\\r\n",
							".withColumnRenamed(\"sap_discountjournalheader_transid\",\"sap_transactionsheader_transid\")\\\r\n",
							".withColumnRenamed(\"sap_discountjournalheader_memo\",\"sap_transactionsheader_memo\")\\\r\n",
							".withColumnRenamed(\"sap_discountjournalline_linenum\",\"sap_transactionsline_linenum\")\\\r\n",
							".withColumnRenamed(\"sap_discountjournalline_acctcode\",\"sap_transactionsline_acctcode\")\\\r\n",
							".withColumnRenamed(\"sap_discountjournalheader_objtype\",\"sap_transactionsheader_objtype\")\\\r\n",
							".withColumnRenamed(\"sap_discountjournalline_project\",\"sap_transactionsline_project\")\\\r\n",
							".withColumnRenamed(\"sap_discountjournalline_vatgroup\",\"sap_transactionsline_vatgroup\")\\\r\n",
							".withColumnRenamed(\"sap_discountjournalline_vatrate\",\"sap_transactionsline_vatrate\")\\\r\n",
							".withColumnRenamed(\"sap_discountjournalline_vatamount\",\"sap_transactionsline_vatamount\")\\\r\n",
							".withColumnRenamed(\"sap_discountjournalline_description\",\"sap_transactionsline_description\")\\\r\n",
							".withColumnRenamed(\"sap_discountjournalline_jnlref1\",\"sap_transactionsline_jnlref1\")\\\r\n",
							".withColumnRenamed(\"sap_discountjournalline_jnlref2\",\"sap_transactionsline_jnlref2\")\\\r\n",
							".withColumnRenamed(\"sap_discountjournalline_subdepotcode\",\"sap_transactionsline_subdepotcode\")\\\r\n",
							".withColumnRenamed(\"sap_discountjournalline_linetotal\",\"sap_transactionsline_linetotal\")\\\r\n",
							".withColumnRenamed(\"sap_discountjournalline_doccat\",\"sap_transactionsline_doccat\")"
						],
						"outputs": [],
						"execution_count": 83
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stsalesinvoiceinventory = stsalesinvoiceinventory.withColumnRenamed(\"sap_salesinvoiceinventoryheader_postingdate\",\"sap_transactionsheader_postingdate\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceinventoryheader_duedate\",\"sap_transactionsheader_duedate\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceinventoryheader_doctotal\",\"sap_transactionsheader_doctotal\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceinventoryheader_doctype\",\"sap_transactionsheader_doctype\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceinventoryheader_transid\",\"sap_transactionsheader_transid\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceinventoryheader_memo\",\"sap_transactionsheader_memo\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceinventoryline_linenum\",\"sap_transactionsline_linenum\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceinventoryline_acctcode\",\"sap_transactionsline_acctcode\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceinventoryheader_objtype\",\"sap_transactionsheader_objtype\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceinventoryline_project\",\"sap_transactionsline_project\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceinventoryline_vatgroup\",\"sap_transactionsline_vatgroup\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceinventoryline_vatrate\",\"sap_transactionsline_vatrate\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceinventoryline_vatamount\",\"sap_transactionsline_vatamount\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceinventoryline_description\",\"sap_transactionsline_description\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceinventoryline_jnlref1\",\"sap_transactionsline_jnlref1\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceinventoryline_jnlref2\",\"sap_transactionsline_jnlref2\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceinventoryline_subdepotcode\",\"sap_transactionsline_subdepotcode\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceinventoryline_linetotal\",\"sap_transactionsline_linetotal\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceinventoryline_doccat\",\"sap_transactionsline_doccat\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceinventoryline_itemcode\",\"sap_transactionsline_itemcode\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceinventoryheader_cardcode\",\"sap_transactionsheader_cardcode\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceinventoryheader_cardref\",\"sap_transactionsheader_cardref\")\\\r\n",
							".withColumnRenamed(\"sap_salesinvoiceinventoryline_invntitem\",\"sap_transactionsline_invntitem\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 84
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"ststockrevaluations = ststockrevaluations.withColumnRenamed(\"sap_stockrevaluationsheader_docentry\",\"sap_transactionsheader_docentry\")\\\r\n",
							".withColumnRenamed(\"sap_stockrevaluationsheader_docnum\",\"sap_transactionsheader_docnum\")\\\r\n",
							".withColumnRenamed(\"sap_stockrevaluationsheader_objtype\",\"sap_transactionsheader_objtype\")\\\r\n",
							".withColumnRenamed(\"sap_stockrevaluationsheader_postingdate\",\"sap_transactionsheader_postingdate\")\\\r\n",
							".withColumnRenamed(\"sap_stockrevaluationsheader_comments\",\"sap_transactionsheader_comments\")\\\r\n",
							".withColumnRenamed(\"sap_stockrevaluationsheader_transid\",\"sap_transactionsheader_transid\")\\\r\n",
							".withColumnRenamed(\"sap_stockrevaluationsline_linenum\",\"sap_transactionsline_linenum\")\\\r\n",
							".withColumnRenamed(\"sap_stockrevaluationsline_itemcode\",\"sap_transactionsline_itemcode\")\\\r\n",
							".withColumnRenamed(\"sap_stockrevaluationsline_dscription\",\"sap_transactionsline_dscription\")\\\r\n",
							".withColumnRenamed(\"sap_stockrevaluationsline_linetotal\",\"sap_transactionsline_linetotal\")\\\r\n",
							".withColumnRenamed(\"sap_stockrevaluationsline_acctcode\",\"sap_transactionsline_acctcode\")\\\r\n",
							".withColumnRenamed(\"sap_stockrevaluationsline_doccat\",\"sap_transactionsline_doccat\")\\\r\n",
							".withColumnRenamed(\"sap_stockrevaluationsline_subdepotcode\",\"sap_transactionsline_subdepotcode\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 85
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stpurchasepricevariance = stpurchasepricevariance.withColumnRenamed(\"sap_purchasepricevarianceheader_postingdate\",\"sap_transactionsheader_postingdate\")\\\r\n",
							".withColumnRenamed(\"sap_purchasepricevarianceheader_duedate\",\"sap_transactionsheader_duedate\")\\\r\n",
							".withColumnRenamed(\"sap_purchasepricevarianceheader_doctotal\",\"sap_transactionsheader_doctotal\")\\\r\n",
							".withColumnRenamed(\"sap_purchasepricevarianceheader_doctype\",\"sap_transactionsheader_doctype\")\\\r\n",
							".withColumnRenamed(\"sap_purchasepricevarianceheader_transid\",\"sap_transactionsheader_transid\")\\\r\n",
							".withColumnRenamed(\"sap_purchasepricevarianceheader_memo\",\"sap_transactionsheader_memo\")\\\r\n",
							".withColumnRenamed(\"sap_purchasepricevarianceline_linenum\",\"sap_transactionsline_linenum\")\\\r\n",
							".withColumnRenamed(\"sap_purchasepricevarianceline_acctcode\",\"sap_transactionsline_acctcode\")\\\r\n",
							".withColumnRenamed(\"sap_purchasepricevarianceheader_objtype\",\"sap_transactionsheader_objtype\")\\\r\n",
							".withColumnRenamed(\"sap_purchasepricevarianceline_project\",\"sap_transactionsline_project\")\\\r\n",
							".withColumnRenamed(\"sap_purchasepricevarianceline_vatgroup\",\"sap_transactionsline_vatgroup\")\\\r\n",
							".withColumnRenamed(\"sap_purchasepricevarianceline_vatrate\",\"sap_transactionsline_vatrate\")\\\r\n",
							".withColumnRenamed(\"sap_purchasepricevarianceline_vatamount\",\"sap_transactionsline_vatamount\")\\\r\n",
							".withColumnRenamed(\"sap_purchasepricevarianceline_description\",\"sap_transactionsline_description\")\\\r\n",
							".withColumnRenamed(\"sap_purchasepricevarianceline_jnlref1\",\"sap_transactionsline_jnlref1\")\\\r\n",
							".withColumnRenamed(\"sap_purchasepricevarianceline_jnlref2\",\"sap_transactionsline_jnlref2\")\\\r\n",
							".withColumnRenamed(\"sap_purchasepricevarianceline_subdepotcode\",\"sap_transactionsline_subdepotcode\")\\\r\n",
							".withColumnRenamed(\"sap_purchasepricevarianceline_linetotal\",\"sap_transactionsline_linetotal\")\\\r\n",
							".withColumnRenamed(\"sap_purchasepricevarianceline_doccat\",\"sap_transactionsline_doccat\")"
						],
						"outputs": [],
						"execution_count": 86
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stcostofgoods = stcostofgoods.withColumnRenamed(\"sap_costofgoodsheader_postingdate\",\"sap_transactionsheader_postingdate\")\\\r\n",
							".withColumnRenamed(\"sap_costofgoodsheader_duedate\",\"sap_transactionsheader_duedate\")\\\r\n",
							".withColumnRenamed(\"sap_costofgoodsheader_doctotal\",\"sap_transactionsheader_doctotal\")\\\r\n",
							".withColumnRenamed(\"sap_costofgoodsheader_doctype\",\"sap_transactionsheader_doctype\")\\\r\n",
							".withColumnRenamed(\"sap_costofgoodsheader_transid\",\"sap_transactionsheader_transid\")\\\r\n",
							".withColumnRenamed(\"sap_costofgoodsheader_memo\",\"sap_transactionsheader_memo\")\\\r\n",
							".withColumnRenamed(\"sap_costofgoodsline_linenum\",\"sap_transactionsline_linenum\")\\\r\n",
							".withColumnRenamed(\"sap_costofgoodsline_acctcode\",\"sap_transactionsline_acctcode\")\\\r\n",
							".withColumnRenamed(\"sap_costofgoodsheader_objtype\",\"sap_transactionsheader_objtype\")\\\r\n",
							".withColumnRenamed(\"sap_costofgoodsline_project\",\"sap_transactionsline_project\")\\\r\n",
							".withColumnRenamed(\"sap_costofgoodsline_vatgroup\",\"sap_transactionsline_vatgroup\")\\\r\n",
							".withColumnRenamed(\"sap_costofgoodsline_vatrate\",\"sap_transactionsline_vatrate\")\\\r\n",
							".withColumnRenamed(\"sap_costofgoodsline_vatamount\",\"sap_transactionsline_vatamount\")\\\r\n",
							".withColumnRenamed(\"sap_costofgoodsline_description\",\"sap_transactionsline_description\")\\\r\n",
							".withColumnRenamed(\"sap_costofgoodsline_jnlref1\",\"sap_transactionsline_jnlref1\")\\\r\n",
							".withColumnRenamed(\"sap_costofgoodsline_jnlref2\",\"sap_transactionsline_jnlref2\")\\\r\n",
							".withColumnRenamed(\"sap_costofgoodsline_subdepotcode\",\"sap_transactionsline_subdepotcode\")\\\r\n",
							".withColumnRenamed(\"sap_costofgoodsline_linetotal\",\"sap_transactionsline_linetotal\")\\\r\n",
							".withColumnRenamed(\"sap_costofgoodsline_doccat\",\"sap_transactionsline_doccat\")"
						],
						"outputs": [],
						"execution_count": 87
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"transactions_tmp = stdocuments.unionByName(stmanualjournal, allowMissingColumns = True).unionByName(stdirectposting, allowMissingColumns = True).unionByName(stgeneraljournal, allowMissingColumns = True).unionByName(ststocktransactions, allowMissingColumns = True).unionByName(stdiscountjournal, allowMissingColumns = True).unionByName(stsalesinvoiceinventory, allowMissingColumns = True).unionByName(ststockrevaluations, allowMissingColumns = True).unionByName(stpurchasepricevariance, allowMissingColumns = True).unionByName(stcostofgoods, allowMissingColumns = True)\r\n",
							"transactions_tmp.createOrReplaceTempView(\"transactions_tmp\")"
						],
						"outputs": [],
						"execution_count": 88
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"sttransactions_tmp2\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    t.sap_transactionsline_polookupid\r\n",
							"    ,t.sap_transactionsheader_docentry \r\n",
							"    ,t.sap_transactionsheader_docnum \r\n",
							"    ,t.sap_transactionsheader_doctype \r\n",
							"    ,t.sap_transactionsheader_objtype \r\n",
							"    ,t.sap_transactionsheader_postingdate \r\n",
							"    ,t.sap_transactionsheader_duedate \r\n",
							"    ,t.sap_transactionsheader_cardcode \r\n",
							"    ,t.sap_transactionsheader_cardref \r\n",
							"    ,t.sap_transactionsheader_discprcnt \r\n",
							"    ,t.sap_transactionsheader_discsum \r\n",
							"    ,t.sap_transactionsheader_doctotal \r\n",
							"    ,COALESCE(t.sap_transactionsheader_comments,sap_transactionsheader_memo) as sap_transactionsheader_comments\r\n",
							"    ,t.sap_transactionsheader_transid \r\n",
							"    ,t.sap_transactionsheader_u_expense_depot \r\n",
							"    ,t.sap_transactionsline_linenum \r\n",
							"    ,t.sap_transactionsline_linestatus \r\n",
							"    ,t.sap_transactionsline_itemcode \r\n",
							"    ,t.sap_transactionsline_dscription \r\n",
							"    ,t.sap_transactionsline_quantity \r\n",
							"    ,t.sap_transactionsline_openqty \r\n",
							"    ,t.sap_transactionsline_price \r\n",
							"    ,t.sap_transactionsline_rate \r\n",
							"    ,t.sap_transactionsline_discprcnt \r\n",
							"    ,t.sap_transactionsline_linetotal\r\n",
							"    -- ,(sap_transactionsline_linetotal+sap_transactionsline_discsum) as sap_transactionsline_linetotallessdiscount\r\n",
							"    ,(sap_transactionsline_linetotal+(sap_transactionsline_linetotal*(sap_transactionsheader_discprcnt*-1/100))) as sap_transactionsline_linetotallessdiscount\r\n",
							"    ,t.sap_transactionsline_opensum \r\n",
							"    ,t.sap_transactionsline_pricebefdi \r\n",
							"    ,t.sap_transactionsline_docdate \r\n",
							"    ,t.sap_transactionsline_project \r\n",
							"    ,t.sap_transactionsline_vatprcnt \r\n",
							"    ,t.sap_transactionsline_vatgroup \r\n",
							"    ,t.sap_transactionsline_vatamount \r\n",
							"    ,t.sap_transactionsline_u_reason \r\n",
							"    ,t.sap_transactionsline_u_expenseitem \r\n",
							"    ,t.sap_transactionsline_u_fueldate \r\n",
							"    ,t.sap_transactionsline_acctcode \r\n",
							"    ,t.sap_transactionsline_doccat \r\n",
							"    ,COALESCE(t.sap_transactionsline_subdepotcode,c.sap_account_segment_2) sap_transactionsline_subdepotcode\r\n",
							"    ,t.sap_transactionsline_u_rate \r\n",
							"    ,t.sap_transactionsline_u_ir_number \r\n",
							"    ,t.sap_transactionsline_discsum \r\n",
							"    ,t.sap_transactionsline_vatrate \r\n",
							"    ,t.sap_transactionsline_description \r\n",
							"    ,t.sap_transactionsline_jnlref1 \r\n",
							"    ,t.sap_transactionsline_jnlref2 \r\n",
							"    ,i.sap_item_invntitem as sap_transactionsline_invntitem\r\n",
							"    ,CASE \r\n",
							"\t\tWHEN (t.sap_transactionsline_doccat = 'salesinvoice' and i.sap_item_invntitem = 'Y') THEN 1\r\n",
							"\t\tWHEN (t.sap_transactionsline_doccat = 'goodsreturn' and c.sap_account_groupmask IN (5,6)) THEN 1\r\n",
							"\t\tWHEN (t.sap_transactionsline_doccat NOT IN ('generaljournal','manualjournal','directposting') and c.sap_account_groupmask IN (1,2,3)) THEN 1\r\n",
							"\t\tELSE 0\r\n",
							"\tEND as exclude\r\n",
							"\r\n",
							"FROM transactions_tmp t\r\n",
							"LEFT JOIN stchartofaccounts c on c.sap_account_acctcode = t.sap_transactionsline_acctcode\r\n",
							"LEFT JOIN stitems i on i.sap_item_itemcode = t.sap_transactionsline_itemcode\r\n",
							"WHERE c.sap_account_accttype <> 'COST RECOVERY'\r\n",
							"AND t.sap_transactionsheader_postingdate >= '2016-04-01 00:00:00.000'\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 89
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sttransactions_tmp2 = spark.sql(\"SELECT * FROM sttransactions_tmp2\")"
						],
						"outputs": [],
						"execution_count": 90
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"sttransactionsexceptions\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"\r\n",
							"    sap_transactionsheader_cardcode\r\n",
							"    ,sap_transactionsline_acctcode\r\n",
							"    ,sap_transactionsline_itemcode\r\n",
							"    ,to_date(sap_transactionsheader_postingdate,\"dd/MM/yyyy\") as sap_transactionsheader_postingdate\r\n",
							"    ,sap_transactionsline_subdepotcode\r\n",
							"    ,sap_transactionsline_doccat\r\n",
							"    ,sap_transactionsheader_docnum\r\n",
							"    ,sap_transactionsline_project\r\n",
							"    ,sap_transactionsline_linetotal \r\n",
							"    ,sap_transactionsline_linetotal as sap_transactionsline_linetotallessdiscount\r\n",
							"    ,sap_transactionsline_linetotal as sap_transactionsline_linetotalfinal\r\n",
							"    ,sap_transactionsheader_cardref\r\n",
							"    ,sap_transactionsheader_comments\r\n",
							"    ,sap_transactionsheader_transid\r\n",
							"    ,sap_transactionsline_u_ir_number\r\n",
							"    --,sap_transactionsline_goodsreceiptnumber\r\n",
							"    ,sap_transactionsline_exception\r\n",
							"\r\n",
							"FROM exceptions\r\n",
							""
						],
						"outputs": [],
						"execution_count": 91
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sttransactionsexceptions = spark.sql(\"SELECT * FROM sttransactionsexceptions\")"
						],
						"outputs": [],
						"execution_count": 92
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"sttransactionsdetail_tmp\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"  t.sap_transactionsline_polookupid\r\n",
							"  ,t.sap_transactionsheader_docentry \r\n",
							"  ,t.sap_transactionsheader_docnum \r\n",
							"  ,t.sap_transactionsheader_doctype \r\n",
							"  ,t.sap_transactionsheader_objtype \r\n",
							"  ,to_date(t.sap_transactionsheader_postingdate,'yyyy-MM-dd') as sap_transactionsheader_postingdate\r\n",
							"  ,to_date(t.sap_transactionsheader_duedate,'yyyy-MM-dd') as sap_transactionsheader_duedate\r\n",
							"  ,t.sap_transactionsheader_cardcode \r\n",
							"  ,t.sap_transactionsheader_cardref \r\n",
							"  ,t.sap_transactionsheader_discprcnt \r\n",
							"  ,t.sap_transactionsheader_discsum \r\n",
							"  ,t.sap_transactionsheader_doctotal \r\n",
							"  ,t.sap_transactionsheader_comments \r\n",
							"  ,t.sap_transactionsheader_transid \r\n",
							"  ,t.sap_transactionsheader_u_expense_depot \r\n",
							"  ,t.sap_transactionsline_linenum \r\n",
							"  ,t.sap_transactionsline_linestatus \r\n",
							"  ,t.sap_transactionsline_itemcode \r\n",
							"  ,t.sap_transactionsline_dscription \r\n",
							"  ,t.sap_transactionsline_quantity \r\n",
							"  ,t.sap_transactionsline_openqty \r\n",
							"  ,t.sap_transactionsline_price \r\n",
							"  ,t.sap_transactionsline_rate \r\n",
							"  ,t.sap_transactionsline_discprcnt \r\n",
							"  ,t.sap_transactionsline_linetotal\r\n",
							"  ,t.sap_transactionsline_linetotallessdiscount\r\n",
							"  ,COALESCE(t.sap_transactionsline_linetotallessdiscount,t.sap_transactionsline_linetotal) as sap_transactionsline_linetotalfinal\r\n",
							"  ,CASE \r\n",
							"      WHEN \r\n",
							"      t.sap_transactionsline_doccat in ('generaljournal','purchaseorder') THEN 0\r\n",
							"      ELSE COALESCE(t.sap_transactionsline_linetotallessdiscount,t.sap_transactionsline_linetotal)\r\n",
							"    END as sap_transactionsline_linetotalfinalexclgjpo\r\n",
							"  ,t.sap_transactionsline_opensum \r\n",
							"  ,t.sap_transactionsline_pricebefdi \r\n",
							"  ,to_date(t.sap_transactionsline_docdate,'yyyy-MM-dd') as  sap_transactionsline_docdate\r\n",
							"  ,t.sap_transactionsline_project \r\n",
							"  ,t.sap_transactionsline_vatprcnt \r\n",
							"  ,t.sap_transactionsline_vatgroup \r\n",
							"  ,t.sap_transactionsline_vatamount \r\n",
							"  ,t.sap_transactionsline_u_reason \r\n",
							"  ,t.sap_transactionsline_u_expenseitem \r\n",
							"  ,to_date(t.sap_transactionsline_u_fueldate,'yyyy-MM-dd') as sap_transactionsline_u_fueldate\r\n",
							"  ,t.sap_transactionsline_acctcode \r\n",
							"  ,t.sap_transactionsline_doccat \r\n",
							"  ,t.sap_transactionsline_subdepotcode \r\n",
							"  ,t.sap_transactionsline_u_rate \r\n",
							"  ,t.sap_transactionsline_u_ir_number \r\n",
							"  ,t.sap_transactionsline_discsum \r\n",
							"  ,t.sap_transactionsline_vatrate \r\n",
							"  ,t.sap_transactionsline_description \r\n",
							"  ,t.sap_transactionsline_jnlref1 \r\n",
							"  ,t.sap_transactionsline_jnlref2 \r\n",
							"  ,t.sap_transactionsline_invntitem \r\n",
							"\r\n",
							"FROM sttransactions_tmp2 t\r\n",
							"WHERE t.exclude = 0\r\n",
							"\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 93
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sttransactionsdetail_tmp = spark.sql(\"SELECT * FROM sttransactionsdetail_tmp\")"
						],
						"outputs": [],
						"execution_count": 94
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sttransactionsdetail = sttransactionsdetail_tmp.unionByName(sttransactionsexceptions, allowMissingColumns = True)\r\n",
							"sttransactionsdetail.createOrReplaceTempView(\"sttransactionsdetail\")"
						],
						"outputs": [],
						"execution_count": 95
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# sttransactionsdetail.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/sttransactionsdetail.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 96
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"sttransactionssummary\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"   sap_transactionsline_acctcode\r\n",
							"  ,sap_transactionsline_doccat\r\n",
							"  ,to_date(sap_transactionsheader_postingdate,'yyyy-MM-dd') as sap_transactionsheader_postingdate\r\n",
							"  ,sap_transactionsline_subdepotcode\r\n",
							"  ,SUM(sap_transactionsline_linetotal) sap_transactionsline_linetotal\r\n",
							"  ,SUM(sap_transactionsline_linetotalfinal) sap_transactionsline_linetotalfinal\r\n",
							"  ,SUM(sap_transactionsline_linetotalfinalexclgjpo) sap_transactionsline_linetotalfinalexclgjpo\r\n",
							"FROM sttransactionsdetail t\r\n",
							"GROUP BY\r\n",
							"\tsap_transactionsline_acctcode,\r\n",
							"  sap_transactionsheader_postingdate,\r\n",
							"  sap_transactionsline_doccat,\r\n",
							"  sap_transactionsline_subdepotcode\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 97
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sttransactionssummary = spark.sql(\"SELECT * FROM sttransactionssummary\")"
						],
						"outputs": [],
						"execution_count": 98
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# sttransactionssummary.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/sttransactionssummary.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 99
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"SUCCESS\")"
						],
						"outputs": [],
						"execution_count": 100
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/101_RptParcelLevel_CenturionSorterReport')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Reports/LMS"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "aba8b12d-f3b5-4a1b-ad68-3287ca773388"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace/bigDataPools/ApacheSparkPool",
						"name": "ApacheSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/ApacheSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# #Create DataFrame for the rptparcellevel LMS Table\r\n",
							"# rptparcellevel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/RPT/rptparcellevel.parquet', format='parquet')\r\n",
							"# rptparcellevel.createOrReplaceTempView(\"rptparcellevel\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dboplclpnroutinghistorycenturion LMS Table\r\n",
							"# dboplclpnroutinghistorycenturion = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboplc_lpnrouting_history_centurion.parquet', format='parquet')\r\n",
							"# dboplclpnroutinghistorycenturion.createOrReplaceTempView(\"dboplclpnroutinghistorycenturion\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"centurionsorterreport\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"\r\n",
							" c.id\r\n",
							",c.autosystemid\r\n",
							",c.lpn\r\n",
							",c.status\r\n",
							",c.datereceived as date\r\n",
							",c.dateupdated\r\n",
							",c.plcdivertid\r\n",
							",cldivertid\r\n",
							",c.divertdescription\r\n",
							",p.lms_parcel_orderbillcustid as billingcustomerid\r\n",
							",p.lms_parcel_orderbillcustname as billingcustomer\r\n",
							",p.lms_parcel_orderdelivercustid as deliverycustomerid\r\n",
							",p.lms_parcel_orderdelivercustname as deliverycustomer\r\n",
							",p.lms_parcel_orderdelivercustcref as deliverycustcref\r\n",
							",p.lms_parcel_orderdelivercustsrouteid as deliverycustsrouteid\r\n",
							",p.lms_parcel_sroutedescriptiondelivercust as deliverycustsroutedescription\r\n",
							",p.lms_parcel_sroutecodedelivercust as deliverycustsroutecode\r\n",
							"\r\n",
							"\r\n",
							"FROM dboplclpnroutinghistorycenturion c\r\n",
							"LEFT JOIN rptparcellevel p on p.lms_parcel_barcode  = c.lpn\r\n",
							"WHERE c.datereceived >= to_timestamp(year(current_date())-1||'-'||(month(current_date()))||'-'||'01')\r\n",
							""
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"centurionsorterreport = spark.sql(\"SELECT * FROM centurionsorterreport\")"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# centurionsorterreport.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/RPT/centurionsorterreport.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 6
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/102_RptParcelLevel_VolumiserLogReport')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Reports/LMS"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "2bcd0234-410a-4d22-a6de-b86ce996ca00"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace/bigDataPools/ApacheSparkPool",
						"name": "ApacheSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/ApacheSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# #Create DataFrame for the rptparcellevel LMS Table\r\n",
							"# rptparcellevel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/RPT/rptparcellevel.parquet', format='parquet')\r\n",
							"# rptparcellevel.createOrReplaceTempView(\"rptparcellevel\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbovolumiserlog  LMS Table\r\n",
							"# dbovolumiserlog = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbovolumiserlog.parquet', format='parquet')\r\n",
							"# dbovolumiserlog.createOrReplaceTempView(\"dbovolumiserlog\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbovolumiserlocation  LMS Table\r\n",
							"# dbovolumiserlocation = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbovolumiserlocation.parquet', format='parquet')\r\n",
							"# dbovolumiserlocation.createOrReplaceTempView(\"dbovolumiserlocation\")"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"volumiserlogreport\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"\r\n",
							"    p.lms_parcel_id id\r\n",
							"    ,CAST(coalesce(p.lms_parcel_barcode,p.lms_parcel_id) as varchar(128)) as parcelbarcode\r\n",
							"    ,p.lms_parcel_consignid as consignid\r\n",
							"    ,p.lms_parcel_orderbillcustid as billingcustomerid\r\n",
							"    ,p.lms_parcel_orderbillcustname as billingcustomer\r\n",
							"    ,vl.dateupdated date\r\n",
							"    ,(p.lms_parcel_volumiserlength * 10) as lmsvolumiserlengthmm\r\n",
							"\t,(p.lms_parcel_volumiserwidth * 10) as lmsvolumiserwidthmm\r\n",
							"\t,(p.lms_parcel_volumiserheight * 10)  as lmsvolumiserheightmm\r\n",
							"    ,p.lms_parcel_weight as lmsweightkg\r\n",
							"    ,(((prevLength)*(prevWidth)*(prevHeight)) / 5000) as lmscalculatedvolumetricweightkg\r\n",
							"    ,case\r\n",
							"        when ((p.lms_parcel_pl*lms_parcel_pw*lms_parcel_ph)/ 5000) > p.lms_parcel_weight then 'LMSVolumetricWeight'\r\n",
							"        else 'LMSActualWeight'\r\n",
							"    end lmsgreaterweight\r\n",
							"    ,(vl.newlength) as sickvolumiserlengthmm\r\n",
							"\t,(vl.newwidth) as sickvolumiserwidthmm\r\n",
							"\t,(vl.newheight) as sickvolumiserheightmm\r\n",
							"    ,(vl.newweight / 1000) as sickvolumiserweightkg\r\n",
							"    ,((vl.newlength /10)*(vl.newwidth /10)*(vl.newheight /10))/5000 sickcalculatedvolumetricweightkg\r\n",
							"    ,case\r\n",
							"        when ((p.lms_parcel_volumiserlength*p.lms_parcel_volumiserwidth*p.lms_parcel_volumiserheight)/5000)/1000 > p.lms_parcel_volumiserweight/1000 then 'SickVolumetricWeight'\r\n",
							"        else 'SickActualWeight'\r\n",
							"    end sickgreaterweight\r\n",
							"    ,vloc.id as scanlocationid\r\n",
							"    ,vloc.location as scanlocation\r\n",
							"        \r\n",
							"FROM dbovolumiserlog vl\r\n",
							"LEFT JOIN rptparcellevel p on p.lms_parcel_id = vl.parcelid\r\n",
							"LEFT JOIN dbovolumiserlocation vloc on vloc.volumiserid = vl.volumiserid\r\n",
							"\r\n",
							"WHERE vl.dateupdated >= to_timestamp(year(current_date())-1||'-'||(month(current_date()))||'-'||'01')\r\n",
							"\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"volumiserlogreport = spark.sql(\"SELECT * FROM volumiserlogreport\")"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# volumiserlogreport.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/RPT/volumiserlogreport.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 10
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/103_RptParcelLevel_ParcelDamageReport')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Reports/LMS"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "d97dcd04-dc98-42e5-aec3-8932dff1712f"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace/bigDataPools/ApacheSparkPool",
						"name": "ApacheSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/ApacheSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# #Create DataFrame for the rptparcellevel LMS Table\r\n",
							"# rptparcellevel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/RPT/rptparcellevel.parquet', format='parquet')\r\n",
							"# rptparcellevel.createOrReplaceTempView(\"rptparcellevel\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbolocation LMS Table\r\n",
							"# dbolocation = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbolocation.parquet', format='parquet')\r\n",
							"# dbolocation.createOrReplaceTempView(\"dbolocation\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbocsdmessageparcel LMS Table\r\n",
							"# dbocsdmessageparcel = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbocsd_messageparcel.parquet', format='parquet')\r\n",
							"# dbocsdmessageparcel.createOrReplaceTempView(\"dbocsdmessageparcel\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbocsdmessage  LMS Table\r\n",
							"# dbocsdmessage = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbocsd_message.parquet', format='parquet')\r\n",
							"# dbocsdmessage.createOrReplaceTempView(\"dbocsdmessage\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbousers LMS Table\r\n",
							"# dbousers = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbousers.parquet', format='parquet')\r\n",
							"# dbousers.createOrReplaceTempView(\"dbousers\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stloadchild LMS Table\r\n",
							"# stloadchild = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stloadchild.parquet', format='parquet')\r\n",
							"# stloadchild.createOrReplaceTempView(\"stloadchild\")"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"parceldamagereport\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"\r\n",
							"    csdmp.parcelid id,\r\n",
							"\tCOALESCE((COALESCE(p.lms_parcel_barcode,cast(p.lms_parcel_id as varchar(30)))),cast(csdmp.parcelid as varchar(30))) parcelbarcode,\r\n",
							"\tCAST(SUBSTRING(csdm.msbody,30,70) as int) loadid,\r\n",
							"\tcsdmp.msdate date, \r\n",
							"\tcsdm.msbody msg,\r\n",
							"    l.lms_loadchild_ldate loaddate,\r\n",
							"\tfloc.id fromdepotid,\r\n",
							"    floc.description fromdepot,\r\n",
							"\ttloc.id todepotid,\r\n",
							"    tloc.description todepot,\r\n",
							"\tu.id loggedbyid,\r\n",
							"    u.fullname loggedby,\r\n",
							"    uloc.id userdepotid, \r\n",
							"\tuloc.description userdepot\r\n",
							"    \r\n",
							"FROM dbocsdmessage csdm\r\n",
							"LEFT JOIN dbocsdmessageparcel csdmp on csdmp.messageid = csdm.id\r\n",
							"LEFT JOIN rptparcellevel p on p.lms_parcel_id = csdmp.parcelid\r\n",
							"LEFT JOIN dbousers u on u.id = csdmp.userid\r\n",
							"LEFT JOIN dbolocation uloc on uloc.id = u.locid\r\n",
							"LEFT JOIN dbolocation tloc on tloc.id = u.locid\r\n",
							"LEFT JOIN stloadchild l on lms_loadchild_id = SUBSTRING(csdm.msbody,30,70)\r\n",
							"LEFT JOIN dbolocation floc on floc.id = l.lms_loadchild_fromlocid\r\n",
							"\r\n",
							"WHERE MSBody like 'Damaged Carton on Manifest%'\r\n",
							"AND csdmp.msdate >= to_timestamp(year(current_date())-1||'-'||(month(current_date()))||'-'||'01')"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"parceldamagereport = spark.sql(\"SELECT * FROM parceldamagereport\")"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# parceldamagereport.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/RPT/parceldamagereport.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 5
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/104_RptParcelLevel_QCParcelDamageReport')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Reports/LMS"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "0318e8e3-825b-475a-a4f8-d130e6215552"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse_Prod/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace-prod/bigDataPools/PRDSparkPool",
						"name": "PRDSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/PRDSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# #Create DataFrame for the rptparcellevel LMS Table\r\n",
							"# rptparcellevel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/RPT/rptparcellevel.parquet', format='parquet')\r\n",
							"# rptparcellevel.createOrReplaceTempView(\"rptparcellevel\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbodamagedparcelaudit LMS Table\r\n",
							"# dbodamagedparcelaudit = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodamaged_parcel_audit.parquet', format='parquet')\r\n",
							"# dbodamagedparcelaudit.createOrReplaceTempView(\"dbodamagedparcelaudit\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbodamagedparcelaudittapetype LMS Table\r\n",
							"# dbodamagedparcelaudittapetype = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodamaged_parcel_audit_tapetype.parquet', format='parquet')\r\n",
							"# dbodamagedparcelaudittapetype.createOrReplaceTempView(\"dbodamagedparcelaudittapetype\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbovolumiserlocation  LMS Table\r\n",
							"# dbovolumiserlocation = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbovolumiserlocation.parquet', format='parquet')\r\n",
							"# dbovolumiserlocation.createOrReplaceTempView(\"dbovolumiserlocation\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbodamagedparcelauditfluteprofiles  LMS Table\r\n",
							"# dbodamagedparcelauditfluteprofiles = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodamaged_parcel_audit_flute_profiles.parquet', format='parquet')\r\n",
							"# dbodamagedparcelauditfluteprofiles.createOrReplaceTempView(\"dbodamagedparcelauditfluteprofiles\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbousers LMS Table\r\n",
							"# dbousers = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbousers.parquet', format='parquet')\r\n",
							"# dbousers.createOrReplaceTempView(\"dbousers\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbolocation LMS Table\r\n",
							"# dbolocation = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbolocation.parquet', format='parquet')\r\n",
							"# dbolocation.createOrReplaceTempView(\"dbolocation\")\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# from pyspark.sql.functions import *\r\n",
							"# from pyspark.sql.window import Window"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"dbodamagedparcelaudit_tmp1\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"dpa.*,\r\n",
							"fp.description as flutedescription\r\n",
							"FROM  dbodamagedparcelaudit dpa\r\n",
							"LEFT JOIN dbodamagedparcelauditfluteprofiles fp on dpa.flutemeasure between fp.startrange and fp.endrange"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"dbodamagedparcelaudit_tmp1 = spark.sql(\"SELECT * FROM dbodamagedparcelaudit_tmp1\")"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"partitionda=Window.partitionBy(\"parcelid\").orderBy(col(\"parcelid\").asc(),col(\"id\").desc())\r\n",
							"dbodamagedparcelaudit_tmp1 = dbodamagedparcelaudit_tmp1.withColumn(\"rn\",row_number().over(partitionda))\r\n",
							"dbodamagedparcelaudit_tmp1.createOrReplaceTempView(\"dbodamagedparcelaudit_tmp2\")"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"qcparceldamagereport\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"\r\n",
							"     dpa.id\r\n",
							"    ,p.lms_parcel_id parcelid\r\n",
							"    ,COALESCE((COALESCE(p.lms_parcel_barcode,cast(p.lms_parcel_id as varchar(30)))),cast(dpa.parcelid as varchar(30))) parcelbarcode\r\n",
							"    ,p.lms_parcel_orderbillcustid as billingcustomerid\r\n",
							"    ,p.lms_parcel_orderbillcustname as billingcustomer\r\n",
							"    ,u.id auditbyid\r\n",
							"    ,u.fullname auditby\r\n",
							"    ,l.id locationid\r\n",
							"    ,l.description location\r\n",
							"    ,dpa.auditdatetime date\r\n",
							"    ,p.lms_parcel_weight parcelweightkg\r\n",
							"    ,p.lms_parcel_pw parcelwidthcm\r\n",
							"    ,p.lms_parcel_pl parcellengthcm\r\n",
							"    ,p.lms_parcel_ph parcelheightcm\r\n",
							"    ,p.lms_parcel_volumiserweight volumiserweightcm\r\n",
							"    ,p.lms_parcel_volumiserlength volumiserlengthcm\r\n",
							"    ,p.lms_parcel_volumiserheight volumiserheightcm\r\n",
							"    ,p.lms_parcel_volumiserwidth as volumiserwidth\r\n",
							"    ,dpa.quantity auditquantity\r\n",
							"    ,dpa.weight auditweight\r\n",
							"    ,dpa.flutemeasure\r\n",
							"    ,CASE \r\n",
							"        WHEN dpa.shrinkwrap = 1 THEN 'YES' \r\n",
							"        ELSE 'NO'\r\n",
							"    END as shrinkwrap\r\n",
							"    ,dpa.comment\r\n",
							"    ,CAST(dpa.parcelutilization as varchar(128)) || ' %' as utilization\r\n",
							"\t,dpa.missing as missing\r\n",
							"\t,dpa.damaged as damaged\r\n",
							"    ,dpa.flutedescription as flutedescription\r\n",
							"    ,dpatt.description as tapetype\r\n",
							"    \r\n",
							"FROM dbodamagedparcelaudit_tmp2 dpa \r\n",
							"LEFT JOIN dbodamagedparcelaudittapetype dpatt on dpatt.id = dpa.tapeusedid\r\n",
							"LEFT JOIN rptparcellevel p on p.lms_parcel_id = dpa.parcelid \r\n",
							"LEFT JOIN dbousers u on u.id = dpa.userid\r\n",
							"LEFT JOIN dbolocation l on l.id = dpa.locid\r\n",
							"WHERE COALESCE(dpa.damagetypeid,3) = 3 and dpa.id <> 0\r\n",
							"AND dpa.auditdatetime >= to_timestamp(year(current_date())-1||'-'||(month(current_date()))||'-'||'01')\r\n",
							""
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"qcparceldamagereport = spark.sql(\"SELECT * FROM qcparceldamagereport\")"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# qcparceldamagereport.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/RPT/qcparceldamagereport.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 9
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/105_RptTrackLevel_BranchVolumesReport')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Reports/LMS"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "70591a63-3b85-42f0-b077-ef053a44873c"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace/bigDataPools/DevSparkPool",
						"name": "DevSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/DevSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# #Create DataFrame for the stlmstrack LMS Table\r\n",
							"# stlmstrack = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stlmstrack.parquet', format='parquet')\r\n",
							"# stlmstrack.createOrReplaceTempView(\"stlmstrack\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stparcel LMS Table\r\n",
							"# stparcel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stparcel.parquet', format='parquet')\r\n",
							"# stparcel.createOrReplaceTempView(\"stparcel\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stdeliverypickupcustomer LMS Table\r\n",
							"# stdeliverypickupcustomer = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stdeliverypickupcustomer.parquet', format='parquet')\r\n",
							"# stdeliverypickupcustomer.createOrReplaceTempView(\"stdeliverypickupcustomer\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stbillcustomer LMS Table\r\n",
							"# stbillcustomer = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stbillcustomer.parquet', format='parquet')\r\n",
							"# stbillcustomer.createOrReplaceTempView(\"stbillcustomer\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stlocation LMS Table\r\n",
							"# stlocation = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stlocation.parquet', format='parquet')\r\n",
							"# stlocation.createOrReplaceTempView(\"stlocation\")\r\n",
							"\r\n",
							"# #create dataframe for the stsroute lms table\r\n",
							"# stsroute = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stsroute.parquet', format='parquet')\r\n",
							"# stsroute.createOrReplaceTempView(\"stsroute\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stzone LMS Table\r\n",
							"# stzone = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stzone.parquet', format='parquet')\r\n",
							"# stzone.createOrReplaceTempView(\"stzone\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbodcorder LMS Table\r\n",
							"# dbodcorder = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodc_order.parquet', format='parquet')\r\n",
							"# dbodcorder.createOrReplaceTempView(\"dbodcorder\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbodcprepack LMS Table\r\n",
							"# dbodcprepack = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodc_prepack.parquet', format='parquet')\r\n",
							"# dbodcprepack.createOrReplaceTempView(\"dbodcprepack\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbodctransfer LMS Table\r\n",
							"# dbodctransfer = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodc_transfer.parquet', format='parquet')\r\n",
							"# dbodctransfer.createOrReplaceTempView(\"dbodctransfer\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"<mark>**rpttracklevel_branchvolumes**</mark>"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"rpttracklevel_in\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"\r\n",
							"lt.lms_location_id as locationid,\r\n",
							"lt.lms_location_description as location,\r\n",
							"t.lms_track_closedt as date,\r\n",
							"bc.lms_customer_id as  billingcustomerid,\r\n",
							"bc.lms_customer_name as  billingcustomer,\r\n",
							"dz.lms_zone_description as deliverycustcustomerzonedesc,\r\n",
							"ds.lms_sroute_code as deliverysroutecode,\r\n",
							"dc.lms_customer_cref as deliverycustomercref,\r\n",
							"dc.lms_customer_id as  deliverycustomerid,\r\n",
							"dc.lms_customer_name as  deliverycustomer,\r\n",
							"ld.lms_location_id as deliverycustomerzonelocationid,\r\n",
							"ld.lms_location_description as deliverycustomerzonelocationdescription,\r\n",
							"1 as pid,\r\n",
							"t.lms_track_parcelweight as parcelweight,\r\n",
							"COALESCE\r\n",
							"(\r\n",
							"\tCASE\t\r\n",
							"\t\tWHEN COALESCE(lt.lms_location_description,'') <> COALESCE(ld.lms_location_description,'') THEN 1\r\n",
							"\t\tELSE 0\r\n",
							"\tEND,0\r\n",
							") as crossdock,\r\n",
							"0 as dcr,\r\n",
							"0 as pidd,\r\n",
							"0 as parcelweightd,\r\n",
							"0 as pidl,\r\n",
							"0 as parcelweightl\r\n",
							"\r\n",
							"FROM stlmstrack t\r\n",
							"LEFT JOIN stlocation lt on lt.lms_location_id = t.lms_track_tolocid\r\n",
							"LEFT JOIN stbillcustomer bc on bc.lms_customer_id =  t.lms_track_orderbillcustid\r\n",
							"LEFT JOIN stdeliverypickupcustomer dc on dc.lms_customer_id =  t.lms_track_orderdelivercustid\r\n",
							"--LEFT JOIN stdeliverypickupcustomer pc on pc.lms_customer_id =  t.lms_track_orderpickupcustid\r\n",
							"LEFT JOIN stsroute ds on ds.lms_sroute_id = dc.lms_customer_srouteid\r\n",
							"LEFT JOIN stzone dz on dz.lms_zone_id = ds.lms_sroute_zoneid\r\n",
							"LEFT JOIN stlocation ld on ld.lms_location_id = dz.lms_zone_locid\r\n",
							"\r\n",
							"WHERE \r\n",
							"t.lms_track_closedt >= (to_timestamp(year(current_date())-1||'-'||(month(current_date()))||'-'||'01'))\r\n",
							"AND lms_track_closedt IS NOT NULL\r\n",
							"AND t.lms_track_tracktypeid = 2     \r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"rpttracklevel_in = spark.sql(\"SELECT * FROM rpttracklevel_in\")"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"rpttracklevel_outdn\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"\r\n",
							"lf.lms_location_id as locationid,\r\n",
							"lf.lms_location_description as location,\r\n",
							"t.lms_track_opendt as date,\r\n",
							"bc.lms_customer_id as  billingcustomerid,\r\n",
							"bc.lms_customer_name as  billingcustomer,\r\n",
							"dz.lms_zone_description as deliverycustcustomerzonedesc,\r\n",
							"ds.lms_sroute_code as deliverysroutecode,\r\n",
							"dc.lms_customer_cref as deliverycustomercref,\r\n",
							"dc.lms_customer_id as  deliverycustomerid,\r\n",
							"dc.lms_customer_name as  deliverycustomer,\r\n",
							"ld.lms_location_id as deliverycustomerzonelocationid,\r\n",
							"ld.lms_location_description as deliverycustomerzonelocationdescription,\r\n",
							"0 as pid,\r\n",
							"0 as parcelweight,\r\n",
							"0 as crossdock,\r\n",
							"0 as dcr,\r\n",
							"1 as pidd,\r\n",
							"t.lms_track_parcelweight as parcelweightd,\r\n",
							"0 as pidl,\r\n",
							"0 as parcelweightl\r\n",
							"\r\n",
							"FROM stlmstrack t\r\n",
							"LEFT JOIN stlocation lf on lf.lms_location_id = t.lms_track_fromlocid\r\n",
							"LEFT JOIN stbillcustomer bc on bc.lms_customer_id =  t.lms_track_orderbillcustid\r\n",
							"LEFT JOIN stdeliverypickupcustomer dc on dc.lms_customer_id =  t.lms_track_orderdelivercustid\r\n",
							"--LEFT JOIN stdeliverypickupcustomer pc on pc.lms_customer_id =  t.lms_track_orderpickupcustid\r\n",
							"LEFT JOIN stsroute ds on ds.lms_sroute_id = dc.lms_customer_srouteid\r\n",
							"LEFT JOIN stzone dz on dz.lms_zone_id = ds.lms_sroute_zoneid\r\n",
							"LEFT JOIN stlocation ld on ld.lms_location_id = dz.lms_zone_locid\r\n",
							"\r\n",
							"WHERE \r\n",
							"t.lms_track_opendt >= (to_timestamp(year(current_date())-1||'-'||(month(current_date()))||'-'||'01'))\r\n",
							"AND t.lms_track_tracktypeid = 6     "
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"rpttracklevel_outdn = spark.sql(\"SELECT * FROM rpttracklevel_outdn\")"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"rpttracklevel_outlh\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"\r\n",
							"lf.lms_location_id as locationid,\r\n",
							"lf.lms_location_description as location,\r\n",
							"t.lms_track_opendt as date,\r\n",
							"bc.lms_customer_id as  billingcustomerid,\r\n",
							"bc.lms_customer_name as  billingcustomer,\r\n",
							"dz.lms_zone_description as deliverycustcustomerzonedesc,\r\n",
							"ds.lms_sroute_code as deliverysroutecode,\r\n",
							"dc.lms_customer_cref as deliverycustomercref,\r\n",
							"dc.lms_customer_id as  deliverycustomerid,\r\n",
							"dc.lms_customer_name as  deliverycustomer,\r\n",
							"ld.lms_location_id as deliverycustomerzonelocationid,\r\n",
							"ld.lms_location_description as deliverycustomerzonelocationdescription,\r\n",
							"0 as pid,\r\n",
							"0 as parcelweight,\r\n",
							"0 as crossdock,\r\n",
							"0 as dcr,\r\n",
							"0 as pidd,\r\n",
							"0 parcelweightd,\r\n",
							"0 as pidl,\r\n",
							"t.lms_track_parcelweight as parcelweightl\r\n",
							"\r\n",
							"FROM stlmstrack t\r\n",
							"LEFT JOIN stlocation lf on lf.lms_location_id = t.lms_track_fromlocid\r\n",
							"LEFT JOIN stbillcustomer bc on bc.lms_customer_id =  t.lms_track_orderbillcustid\r\n",
							"LEFT JOIN stdeliverypickupcustomer dc on dc.lms_customer_id =  t.lms_track_orderdelivercustid\r\n",
							"--LEFT JOIN stdeliverypickupcustomer pc on pc.lms_customer_id =  t.lms_track_orderpickupcustid\r\n",
							"LEFT JOIN stsroute ds on ds.lms_sroute_id = dc.lms_customer_srouteid\r\n",
							"LEFT JOIN stzone dz on dz.lms_zone_id = ds.lms_sroute_zoneid\r\n",
							"LEFT JOIN stlocation ld on ld.lms_location_id = dz.lms_zone_locid\r\n",
							"\r\n",
							"WHERE \r\n",
							"t.lms_track_opendt >= (to_timestamp(year(current_date())-1||'-'||(month(current_date()))||'-'||'01'))\r\n",
							"AND t.lms_track_tracktypeid <> 6     "
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"rpttracklevel_outlh = spark.sql(\"SELECT * FROM rpttracklevel_outlh\")"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"rpttracklevel_dcrec\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"\r\n",
							"lt.lms_location_id as locationid,\r\n",
							"lt.lms_location_description as location,\r\n",
							"dct.acceptancedate as date,\r\n",
							"bc.lms_customer_id as  billingcustomerid,\r\n",
							"bc.lms_customer_name as  billingcustomer,\r\n",
							"dz.lms_zone_description as deliverycustcustomerzonedesc,\r\n",
							"ds.lms_sroute_code as deliverysroutecode,\r\n",
							"dc.lms_customer_cref as deliverycustomercref,\r\n",
							"dc.lms_customer_id as  deliverycustomerid,\r\n",
							"dc.lms_customer_name as  deliverycustomer,\r\n",
							"ld.lms_location_id as deliverycustomerzonelocationid,\r\n",
							"ld.lms_location_description as deliverycustomerzonelocationdescription,\r\n",
							"1 as pid,\r\n",
							"p.lms_parcel_weight as parcelweight,\r\n",
							"0 as crossdock,\r\n",
							"1 as dcr,\r\n",
							"0 as pidd,\r\n",
							"0 parcelweightd,\r\n",
							"0 as pidl,\r\n",
							"0 as parcelweightl\r\n",
							"\r\n",
							"FROM stparcel p\r\n",
							"LEFT JOIN dbodctransfer dct on dct.transfercode = p.lms_parcel_barcode\r\n",
							"LEFT JOIN dbodcprepack dcp  on dcp.ID = dct.dc_prepackid\r\n",
							"LEFT JOIN dbodcorder dco on dco.id = dcp.dc_orderid\r\n",
							"LEFT JOIN stlocation lt on lt.lms_location_id = \r\n",
							"CASE dco.dccode \r\n",
							"    WHEN 'JHB' then 62\r\n",
							"    WHEN 'CT' then 7\r\n",
							"    WHEN 'CC_DUR' then 10\r\n",
							"    WHEN 'RHDC' then 10\r\n",
							"    WHEN 'BLM' then 8\r\n",
							"    WHEN 'PE' then 11\r\n",
							"END\r\n",
							"LEFT JOIN stbillcustomer bc on bc.lms_customer_id =  p.lms_parcel_orderbillcustid\r\n",
							"LEFT JOIN stdeliverypickupcustomer dc on dc.lms_customer_id =  p.lms_parcel_orderdelivercustid\r\n",
							"--LEFT JOIN stdeliverypickupcustomer pc on pc.lms_customer_id =  p.lms_parcel_orderpickupcustid\r\n",
							"LEFT JOIN stsroute ds on ds.lms_sroute_id = dc.lms_customer_srouteid\r\n",
							"LEFT JOIN stzone dz on dz.lms_zone_id = ds.lms_sroute_zoneid\r\n",
							"LEFT JOIN stlocation ld on ld.lms_location_id = dz.lms_zone_locid\r\n",
							"\r\n",
							"WHERE \r\n",
							"dct.acceptancedate >= (to_timestamp(year(current_date())-1||'-'||(month(current_date()))||'-'||'01'))\r\n",
							""
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"rpttracklevel_dcrec = spark.sql(\"SELECT * FROM rpttracklevel_dcrec\")"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"rpttracklevel_rec\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"\r\n",
							"lf.lms_location_id as locationid,\r\n",
							"lf.lms_location_description as location,\r\n",
							"p.lms_parcel_acceptancedate as date,\r\n",
							"bc.lms_customer_id as  billingcustomerid,\r\n",
							"bc.lms_customer_name as  billingcustomer,\r\n",
							"dz.lms_zone_description as deliverycustcustomerzonedesc,\r\n",
							"ds.lms_sroute_code as deliverysroutecode,\r\n",
							"dc.lms_customer_cref as deliverycustomercref,\r\n",
							"dc.lms_customer_id as  deliverycustomerid,\r\n",
							"dc.lms_customer_name as  deliverycustomer,\r\n",
							"ld.lms_location_id as deliverycustomerzonelocationid,\r\n",
							"ld.lms_location_description as deliverycustomerzonelocationdescription,\r\n",
							"1 as pid,\r\n",
							"p.lms_parcel_weight as parcelweight,\r\n",
							"0 as crossdock,\r\n",
							"1 as dcr,\r\n",
							"0 as pidd,\r\n",
							"0 parcelweightd,\r\n",
							"0 as pidl,\r\n",
							"0 as parcelweightl\r\n",
							"\r\n",
							"FROM stparcel p\r\n",
							"LEFT JOIN stbillcustomer bc on bc.lms_customer_id =  p.lms_parcel_orderbillcustid\r\n",
							"LEFT JOIN stdeliverypickupcustomer dc on dc.lms_customer_id =  p.lms_parcel_orderdelivercustid\r\n",
							"LEFT JOIN stdeliverypickupcustomer pc on pc.lms_customer_id =  p.lms_parcel_orderpickupcustid\r\n",
							"LEFT JOIN stsroute ds on ds.lms_sroute_id = dc.lms_customer_srouteid\r\n",
							"LEFT JOIN stzone dz on dz.lms_zone_id = ds.lms_sroute_zoneid\r\n",
							"LEFT JOIN stsroute ps on ps.lms_sroute_id = pc.lms_customer_srouteid\r\n",
							"LEFT JOIN stzone pz on pz.lms_zone_id = ps.lms_sroute_zoneid\r\n",
							"LEFT JOIN stlocation ld on ld.lms_location_id = dz.lms_zone_locid\r\n",
							"LEFT JOIN stlocation lf on lf.lms_location_id = pz.lms_zone_locid\r\n",
							"\r\n",
							"WHERE \r\n",
							"p.lms_parcel_acceptancedate >= (to_timestamp(year(current_date())-1||'-'||(month(current_date()))||'-'||'01'))\r\n",
							"AND  ((bc.lms_customer_id = 344917 AND lf.lms_location_id = 7)\tOR (bc.lms_customer_id = 383489 AND lf.lms_location_id = 10) OR (bc.lms_customer_id = 579600 AND lf.lms_location_id = 31))"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"rpttracklevel_rec = spark.sql(\"SELECT * FROM rpttracklevel_rec\")"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"rpttracklevel_hourlydailyvolumesunion = rpttracklevel_in.unionByName(rpttracklevel_outdn, allowMissingColumns = True).unionByName(rpttracklevel_outlh, allowMissingColumns = True).unionByName(rpttracklevel_dcrec, allowMissingColumns = True).unionByName(rpttracklevel_rec, allowMissingColumns = True)\r\n",
							"rpttracklevel_hourlydailyvolumesunion.createOrReplaceTempView(\"rpttracklevel_hourlydailyvolumesunion\")"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"branchvolumesreport\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"\r\n",
							"    locationid,\r\n",
							"    location,\r\n",
							"    date,\r\n",
							"    hour(date) as thour,\r\n",
							"    sum(pid) as aggrpid,\r\n",
							"    sum(parcelweight) as aggrparcelweightrecived,\r\n",
							"    sum(pidl) as aggrpidl,\r\n",
							"    sum(parcelweightl) as aggrparcelweightlinehaul,\r\n",
							"    sum(pidd) as aggrpidd,\r\n",
							"    sum(parcelweightd) as aggrparcelweightdelivery\r\n",
							"\r\n",
							"FROM rpttracklevel_hourlydailyvolumesunion\r\n",
							"\r\n",
							"GROUP BY \r\n",
							"locationid,\r\n",
							"Location,\r\n",
							"date,\r\n",
							"hour(date)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"branchvolumesreport = spark.sql(\"SELECT * FROM branchvolumesreport\")"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# branchvolumesreport.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/RPT/branchvolumesreport.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 16
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/106_RptLoadLevel_VehicleUtilisationReport')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Reports/LMS"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "5e04368d-ef7d-42a7-abcf-ac88915d7a90"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace/bigDataPools/ApacheSparkPool",
						"name": "ApacheSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/ApacheSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# #Create DataFrame for the rptloadparentlevel LMS Table\r\n",
							"# rptloadparentlevel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/RPT/rptloadparentlevel.parquet', format='parquet')\r\n",
							"# rptloadparentlevel.createOrReplaceTempView(\"rptloadparentlevel\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"vehicleutilisationreport\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"     lms_loadparent_parentloadid as parentloadid\r\n",
							"    ,lms_loadparent_childloadids as childloadids\r\n",
							"    ,lms_loadparent_ldate as date\r\n",
							"    ,lms_loadparent_estdepdate as estdeptdte\r\n",
							"    ,lms_loadparent_originalstartdate as originalstartdate\r\n",
							"    ,lms_loadparent_locationfromid as locationid\r\n",
							"    ,lms_loadparent_locationfromdescription as location\r\n",
							"    ,lms_loadparent_locationtoid as tolocationid\r\n",
							"    ,lms_loadparent_locationtodescription as tolocation\r\n",
							"    ,lms_loadparent_vehcileid as vehicleid\r\n",
							"    ,lms_loadparent_vehcileuid as vehicleuid\r\n",
							"    ,lms_loadparent_vehcilefleetcode as fleetcode\r\n",
							"    ,lms_loadparent_vehicleregistrationnumber as regno\r\n",
							"    ,lms_loadparent_vehicleinternalvolume as vehicleinternalvolume\r\n",
							"    ,lms_loadparent_lmsvehodo as lmsvehodo\r\n",
							"    ,lms_loadparent_lmsvehodo2 as lmsvehodo2\r\n",
							"    ,lms_loadparent_lmsodokms as lmsodokms\r\n",
							"    ,lms_loadparent_orvstartodo as orvstartodo\r\n",
							"    ,lms_loadparent_orvstopodo as orvstopodo\r\n",
							"    ,lms_loadparent_orvodokms as orvodokms\r\n",
							"    ,(lms_loadparent_orvdistance)/1000 as orvdistance\r\n",
							"    ,(lms_loadparent_orvduration)/3600 as orvduration\r\n",
							"    ,lms_loadparent_noparcels as noparcels\r\n",
							"    ,lms_loadparent_weight as weight\r\n",
							"    ,lms_loadparent_chargeweight as chargeweight\r\n",
							"    ,lms_loadparent_nochildloads as nochildloads\r\n",
							"    ,lms_loadparent_nodeliverycustomers as nodeliverycustomers\r\n",
							"    ,lms_loadparent_expressdelivercustomers as expressdelivercustomers\r\n",
							"    ,lms_loadparent_retailelivercustomers as retailelivercustomers\r\n",
							"    ,CAST(lms_loadparent_vehicleinternalvolume as varchar(16)) || ' * 200 @ 100% = ' || (lms_loadparent_vehicleinternalvolume*200) as requiredvolume\r\n",
							"\r\n",
							"    ,CASE \r\n",
							"        WHEN lms_loadparent_vehicleinternalvolume is null THEN 'undifined'\r\n",
							"        WHEN  (lms_loadparent_chargeweight > lms_loadparent_weight) THEN \r\n",
							"        CAST \r\n",
							"            (\r\n",
							"                (\r\n",
							"                    (lms_loadparent_chargeweight)\r\n",
							"                    /\r\n",
							"                    (lms_loadparent_vehicleinternalvolume*200)*100\r\n",
							"                    \r\n",
							"                )    AS varchar(16)\r\n",
							"                \r\n",
							"            )||' %'\r\n",
							"        ELSE CAST\r\n",
							"            (\r\n",
							"                (\r\n",
							"                    (lms_loadparent_weight)\r\n",
							"                    /\r\n",
							"                    (lms_loadparent_vehicleinternalvolume*200)*100\r\n",
							"                ) AS  varchar(16)\r\n",
							"            )||' %'\r\n",
							"                         \r\n",
							"\tEND as utilisationprct\r\n",
							"\r\n",
							"FROM rptloadparentlevel lp\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 27
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"vehicleutilisationreport = spark.sql(\"SELECT * FROM vehicleutilisationreport\")"
						],
						"outputs": [],
						"execution_count": 28
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# vehicleutilisationreport.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/RPT/vehicleutilisationreport.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 30
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/107_RptParcelLevel_DailyOpsStrikeRateReport')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Reports/LMS"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "737a33e9-3a3e-4ed6-b827-cedd5fcb941d"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace/bigDataPools/DevSparkPool",
						"name": "DevSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/DevSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# #Create DataFrame for the rptparcellevel LMS Table\r\n",
							"# rptparcellevel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/RPT/rptparcellevel.parquet', format='parquet')\r\n",
							"# rptparcellevel.createOrReplaceTempView(\"rptparcellevel\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbolocation LMS Table\r\n",
							"# dbolocation = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbolocation.parquet', format='parquet')\r\n",
							"# dbolocation.createOrReplaceTempView(\"dbolocation\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbozone LMS Table\r\n",
							"# dbozone = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbozone.parquet', format='parquet')\r\n",
							"# dbozone.createOrReplaceTempView(\"dbozone\")"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"dailyopsstrikeratereport\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"\t\r\n",
							"     p.lms_parcel_orderdelivercustcref as delcustomerref\r\n",
							"    ,p.lms_parcel_orderbillcustid as billingcustomerid\r\n",
							"    ,p.lms_parcel_orderbillcustname as billingcustomername\r\n",
							"    ,p.lms_parcel_orderdelivercustid as deliverycustomerid\r\n",
							"    ,p.lms_parcel_orderdelivercustname as deliverycustomername\r\n",
							"    ,p.lms_parcel_orderdelivercustsrouteid as deliverycustomersrouteid\r\n",
							"    ,p.lms_parcel_sroutecodedelivercust as deliverycustomersroutecode\r\n",
							"    ,l.description as depot\r\n",
							"    ,to_date(p.lms_parcel_waybillpoddate) as date\r\n",
							"    ,MAX(YEAR(p.lms_parcel_waybillpoddate)) as podyear\r\n",
							"    ,MAX(MONTH(p.lms_parcel_waybillpoddate)) as podmonth\r\n",
							"    ,MAX(DAY(p.lms_parcel_waybillpoddate)) as podday\r\n",
							"    ,COUNT(p.lms_parcel_id) as noofparcels\r\n",
							"\r\n",
							"FROM  rptparcellevellimitload p\r\n",
							"LEFT JOIN dbozone z on z.id = p.lms_parcel_orderdelivercustsrouteid\r\n",
							"LEFT JOIN dbolocation l on l.id = z.locid \r\n",
							"WHERE p.lms_parcel_waybillpoddate >= to_timestamp(year(current_date())-1||'-'||(month(current_date()))||'-'||'01')\r\n",
							"GROUP BY\r\n",
							"     lms_parcel_orderdelivercustcref\r\n",
							"    ,lms_parcel_orderbillcustid\r\n",
							"    ,lms_parcel_orderbillcustname\r\n",
							"    ,lms_parcel_orderdelivercustid\r\n",
							"    ,lms_parcel_orderdelivercustname\r\n",
							"    ,lms_parcel_orderdelivercustsrouteid\r\n",
							"    ,lms_parcel_sroutecodedelivercust\r\n",
							"    ,l.description\r\n",
							"    ,to_date(p.lms_parcel_waybillpoddate)"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"dailyopsstrikeratereport = spark.sql(\"SELECT * FROM dailyopsstrikeratereport\")"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# dailyopsstrikeratereport.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/RPT/dailyopsstrikeratereport.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 10
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/108_RptBillConsignmentLevel_RevenueVerticalReport')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Reports/LMS"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "9e33349b-81b8-4f49-9760-c749cf1e1276"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace/bigDataPools/ApacheSparkPool",
						"name": "ApacheSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/ApacheSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# #Create DataFrame for the rptbillconsignmentlevel LMS Table\r\n",
							"# rptbillconsignmentlevel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/RPT/rptbillconsignmentlevel.parquet', format='parquet')\r\n",
							"# rptbillconsignmentlevel.createOrReplaceTempView(\"rptbillconsignmentlevel\")\r\n",
							"\r\n",
							"# #Create DataFrame for the rptbillconsignmentexceptionlevel LMS Table\r\n",
							"# rptbillconsignmentexceptionlevel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/RPT/rptbillconsignmentexceptionlevel.parquet', format='parquet')\r\n",
							"# rptbillconsignmentexceptionlevel.createOrReplaceTempView(\"rptbillconsignmentexceptionlevel\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"revenueverticalreport_tmp1\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"     bc.billcust as billingcustomerid\r\n",
							"    ,bc.billingcustomername\r\n",
							"    ,bc.pickupcustid\r\n",
							"    ,bc.pickupcustomername\r\n",
							"    ,bc.deliverycustid\r\n",
							"    ,bc.deliverycustomername\r\n",
							"    ,bc.pickupsrouteid\r\n",
							"    ,bc.pickupsroutedescription\r\n",
							"    ,bc.deliverysrouteid\r\n",
							"    ,bc.deliverysroutedescription\r\n",
							"    ,bc.pickupzoneid\r\n",
							"    ,bc.pickupzonedescription\r\n",
							"    ,bc.deliveryzoneid\r\n",
							"    ,bc.deliveryzonedescription\r\n",
							"    ,bc.pickuplocationid\r\n",
							"    ,bc.pickuplocationdescription\r\n",
							"    ,bc.deliverylocationid\r\n",
							"    ,bc.deliverylocationdescription\r\n",
							"    ,CASE \r\n",
							"        WHEN (coalesce(bc.overborderflag,0) = 0 AND coalesce(bc.specdel,0) = 0)  THEN 'Local Normal Distribution'\r\n",
							"        WHEN (coalesce(bc.overborderflag,0) = 1 AND coalesce(bc.specdel,0) = 0)  THEN 'OverBorder Normal Distribution'\r\n",
							"        WHEN (coalesce(bc.overborderflag,0) = 0 AND coalesce(bc.specdel,0) = 1)  THEN 'Local Special/Direct Distribution'\r\n",
							"        WHEN (coalesce(bc.overborderflag,0) = 1 AND coalesce(bc.specdel,0) = 1)  THEN 'OverBorder Special/Direct Distribution'\r\n",
							"    END AS  distributiondesc\r\n",
							"    ,to_date(bc.cdate1) as date\r\n",
							"    ,SUM(coalesce(bc.noofparcels, 0)) as noofparcels\r\n",
							"    ,SUM(coalesce(bc.aweight, 0)) kgsconsignedaweight\r\n",
							"    ,SUM(coalesce(bc.vweight, 0)) kgsconsignedvweight\r\n",
							"    ,SUM(coalesce(bc.chargeweight, 0)) as kgsconsignedchargeweight\r\n",
							"    ,SUM(CASE WHEN coalesce(bc.specdel, 0) = 0 THEN coalesce(bc.chargeWeight, 0) ELSE 0 END) as normalchargeweight\r\n",
							"    ,SUM(CASE WHEN coalesce(bc.specdel, 0) = 1 THEN coalesce(bc.chargeWeight, 0) ELSE 0 END) as specialdeliverychargeweight\r\n",
							"\r\n",
							"    ,CASE\r\n",
							"        WHEN coalesce(bc.samedelcount,0) > 1 THEN SUM(coalesce(bc.CombineRouteCharge, 0))\r\n",
							"        ELSE SUM(coalesce(bc.RouteCharge, 0))\r\n",
							"    END + SUM(coalesce(bc.TotFuelSurcharge, 0)) + SUM(coalesce(bc.TotDocSurcharge, 0)) + SUM(coalesce(bc.TotOtherSurcharge, 0)) + SUM(coalesce(bc.TotCovidSurcharge, 0)) totalcharge\r\n",
							"\r\n",
							"    ,SUM(coalesce(bc.RouteCharge, 0)) as standardcharge\r\n",
							"\r\n",
							"    ,CASE\r\n",
							"        WHEN (coalesce(bc.samedelcount,0) > 1)THEN SUM(coalesce(bc.CombineRouteCharge, 0))\r\n",
							"        ELSE 0\r\n",
							"    END sdconsolidatedcharge\r\n",
							"    ,SUM(coalesce(bc.TotFuelSurcharge, 0)) as fuelsurcharge\r\n",
							"    ,SUM(coalesce(bc.TotDocSurcharge, 0)) as docsurcharge\r\n",
							"    ,SUM(coalesce(bc.TotOtherSurcharge, 0)) as othersurcharge\r\n",
							"    ,SUM(coalesce(bc.TotCovidSurcharge, 0)) as covidsurcharge\r\n",
							"    ,0.00 as exception\r\n",
							"FROM rptbillconsignmentlevel bc \r\n",
							"WHERE bc.chargeweight > 0\r\n",
							"AND coalesce(bc.RouteMissingInd,0) = 0\r\n",
							"GROUP BY\r\n",
							"     bc.billcust\r\n",
							"    ,bc.billingcustomername\r\n",
							"    ,bc.pickupcustid\r\n",
							"    ,bc.pickupcustomername\r\n",
							"    ,bc.deliverycustid\r\n",
							"    ,bc.deliverycustomername\r\n",
							"    ,bc.pickupsrouteid\r\n",
							"    ,bc.pickupsroutedescription\r\n",
							"    ,bc.deliverysrouteid\r\n",
							"    ,bc.deliverysroutedescription\r\n",
							"    ,bc.pickupzoneid\r\n",
							"    ,bc.pickupzonedescription\r\n",
							"    ,bc.deliveryzoneid\r\n",
							"    ,bc.deliveryzonedescription\r\n",
							"    ,bc.pickuplocationid\r\n",
							"    ,bc.pickuplocationdescription\r\n",
							"    ,bc.deliverylocationid\r\n",
							"    ,bc.deliverylocationdescription\r\n",
							"    ,bc.samedelcount\r\n",
							"    ,bc.specdel\r\n",
							"    ,bc.overborderflag\r\n",
							"    ,to_date(bc.cdate1)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"revenueverticalreport_tmp1 = spark.sql(\"SELECT * FROM revenueverticalreport_tmp1\")"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"revenueverticalreport_tmp2\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"     bc.billcust as billingcustomerid\r\n",
							"    ,bc.billingcustomername\r\n",
							"    ,bc.pickupcustid\r\n",
							"    ,bc.pickupcustomername\r\n",
							"    ,bc.deliverycustid\r\n",
							"    ,bc.deliverycustomername\r\n",
							"    ,bc.pickupsrouteid\r\n",
							"    ,bc.pickupsroutedescription\r\n",
							"    ,bc.deliverysrouteid\r\n",
							"    ,bc.deliverysroutedescription\r\n",
							"    ,bc.pickupzoneid\r\n",
							"    ,bc.pickupzonedescription\r\n",
							"    ,bc.deliveryzoneid\r\n",
							"    ,bc.deliveryzonedescription\r\n",
							"    ,bc.pickuplocationid\r\n",
							"    ,bc.pickuplocationdescription\r\n",
							"    ,bc.deliverylocationid\r\n",
							"    ,bc.deliverylocationdescription\r\n",
							"    ,CASE \r\n",
							"        WHEN (coalesce(bc.overborderflag,0) = 0 AND coalesce(bc.specdel,0) = 0)  THEN 'Local Normal Distribution'\r\n",
							"        WHEN (coalesce(bc.overborderflag,0) = 1 AND coalesce(bc.specdel,0) = 0)  THEN 'OverBorder Normal Distribution'\r\n",
							"        WHEN (coalesce(bc.overborderflag,0) = 0 AND coalesce(bc.specdel,0) = 1)  THEN 'Local Special/Direct Distribution'\r\n",
							"        WHEN (coalesce(bc.overborderflag,0) = 1 AND coalesce(bc.specdel,0) = 1)  THEN 'OverBorder Special/Direct Distribution'\r\n",
							"    END AS  distributiondesc\r\n",
							"    ,to_date(bc.cdate1) as date\r\n",
							"    ,SUM(coalesce(bc.noofparcels, 0)) as noofparcels\r\n",
							"    ,SUM(coalesce(bc.aweight, 0)) kgsconsignedaweight\r\n",
							"    ,SUM(coalesce(bc.vweight, 0)) kgsconsignedvweight\r\n",
							"    ,SUM(coalesce(bc.chargeweight, 0)) as kgsconsignedchargeweight\r\n",
							"    ,SUM(CASE WHEN coalesce(bc.specdel, 0) = 0 THEN coalesce(bc.chargeWeight, 0) ELSE 0 END) as normalchargeweight\r\n",
							"    ,SUM(CASE WHEN coalesce(bc.specdel, 0) = 1 THEN coalesce(bc.chargeWeight, 0) ELSE 0 END) as specialdeliverychargeweight\r\n",
							"\r\n",
							"    ,CASE\r\n",
							"        WHEN coalesce(bc.samedelcount,0) > 1 THEN SUM(coalesce(bc.CombineRouteCharge, 0))\r\n",
							"        ELSE SUM(coalesce(bc.RouteCharge, 0))\r\n",
							"    END + SUM(coalesce(bc.TotFuelSurcharge, 0)) + SUM(coalesce(bc.TotDocSurcharge, 0)) + SUM(coalesce(bc.TotOtherSurcharge, 0)) + SUM(coalesce(bc.TotCovidSurcharge, 0)) totalcharge\r\n",
							"\r\n",
							"    ,SUM(coalesce(bc.RouteCharge, 0)) as standardcharge\r\n",
							"\r\n",
							"    ,CASE\r\n",
							"        WHEN (coalesce(bc.samedelcount,0) > 1)THEN SUM(coalesce(bc.CombineRouteCharge, 0))\r\n",
							"        ELSE 0\r\n",
							"    END sdconsolidatedcharge\r\n",
							"\r\n",
							"\r\n",
							"    ,SUM(coalesce(bc.TotFuelSurcharge, 0)) as fuelsurcharge\r\n",
							"    ,SUM(coalesce(bc.TotDocSurcharge, 0)) as docsurcharge\r\n",
							"    ,SUM(coalesce(bc.TotOtherSurcharge, 0)) as othersurcharge\r\n",
							"    ,SUM(coalesce(bc.TotCovidSurcharge, 0)) as covidsurcharge\r\n",
							"    ,SUM(coalesce(bc.Routecharge, 0.00)) exception\r\n",
							"FROM rptbillconsignmentexceptionlevel bc \r\n",
							"WHERE bc.chargeweight != 0\r\n",
							"AND coalesce(bc.RouteMissingInd,0) = 0\r\n",
							"GROUP BY\r\n",
							"     bc.billcust\r\n",
							"    ,bc.billingcustomername\r\n",
							"    ,bc.pickupcustid\r\n",
							"    ,bc.pickupcustomername\r\n",
							"    ,bc.deliverycustid\r\n",
							"    ,bc.deliverycustomername\r\n",
							"    ,bc.pickupsrouteid\r\n",
							"    ,bc.pickupsroutedescription\r\n",
							"    ,bc.deliverysrouteid\r\n",
							"    ,bc.deliverysroutedescription\r\n",
							"    ,bc.pickupzoneid\r\n",
							"    ,bc.pickupzonedescription\r\n",
							"    ,bc.deliveryzoneid\r\n",
							"    ,bc.deliveryzonedescription\r\n",
							"    ,bc.pickuplocationid\r\n",
							"    ,bc.pickuplocationdescription\r\n",
							"    ,bc.deliverylocationid\r\n",
							"    ,bc.deliverylocationdescription\r\n",
							"    ,bc.samedelcount\r\n",
							"    ,bc.specdel\r\n",
							"    ,bc.overborderflag\r\n",
							"    ,to_date(bc.cdate1)"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"revenueverticalreport_tmp2 = spark.sql(\"SELECT * FROM revenueverticalreport_tmp2\")"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"revenueverticalreport_tmp3 = revenueverticalreport_tmp1.unionByName(revenueverticalreport_tmp2, allowMissingColumns = True)\r\n",
							"revenueverticalreport_tmp3.createOrReplaceTempView(\"revenueverticalreport_tmp3\")"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"revenueverticalreport\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"     billingcustomerid\r\n",
							"    ,billingcustomername\r\n",
							"    ,pickupcustid\r\n",
							"    ,pickupcustomername\r\n",
							"    ,deliverycustid\r\n",
							"    ,deliverycustomername\r\n",
							"    ,pickupsrouteid\r\n",
							"    ,pickupsroutedescription\r\n",
							"    ,deliverysrouteid\r\n",
							"    ,deliverysroutedescription\r\n",
							"    ,pickupzoneid\r\n",
							"    ,pickupzonedescription\r\n",
							"    ,deliveryzoneid\r\n",
							"    ,deliveryzonedescription\r\n",
							"    ,pickuplocationid\r\n",
							"    ,pickuplocationdescription\r\n",
							"    ,deliverylocationid\r\n",
							"    ,deliverylocationdescription\r\n",
							"    ,distributiondesc\r\n",
							"    ,date\r\n",
							"    ,SUM(noofparcels) as noofparcels\r\n",
							"    ,SUM(kgsconsignedaweight) as kgsconsignedaweight\r\n",
							"    ,SUM(kgsconsignedvweight) as kgsconsignedvweight\r\n",
							"    ,SUM(kgsconsignedchargeweight) as kgsconsignedchargeweight\r\n",
							"    ,SUM(normalchargeweight) as normalchargeweight\r\n",
							"    ,SUM(specialdeliverychargeweight) as specialdeliverychargeweight\r\n",
							"    ,SUM(totalcharge) as totalcharge\r\n",
							"    ,SUM(standardcharge) as standardcharge\r\n",
							"    ,SUM(sdconsolidatedcharge) as sdconsolidatedcharge\r\n",
							"    ,SUM(fuelsurcharge) as fuelsurcharge\r\n",
							"    ,SUM(docsurcharge) as docsurcharge\r\n",
							"    ,SUM(othersurcharge) as othersurcharge\r\n",
							"    ,SUM(covidsurcharge) as covidsurcharge\r\n",
							"    ,SUM(exception) as exception\r\n",
							"FROM revenueverticalreport_tmp3\r\n",
							"GROUP BY      \r\n",
							"     billingcustomerid\r\n",
							"    ,billingcustomername\r\n",
							"    ,pickupcustid\r\n",
							"    ,pickupcustomername\r\n",
							"    ,deliverycustid\r\n",
							"    ,deliverycustomername\r\n",
							"    ,pickupsrouteid\r\n",
							"    ,pickupsroutedescription\r\n",
							"    ,deliverysrouteid\r\n",
							"    ,deliverysroutedescription\r\n",
							"    ,pickupzoneid\r\n",
							"    ,pickupzonedescription\r\n",
							"    ,deliveryzoneid\r\n",
							"    ,deliveryzonedescription\r\n",
							"    ,pickuplocationid\r\n",
							"    ,pickuplocationdescription\r\n",
							"    ,deliverylocationid\r\n",
							"    ,deliverylocationdescription\r\n",
							"    ,distributiondesc\r\n",
							"    ,date"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"revenueverticalreport = spark.sql(\"SELECT * FROM revenueverticalreport\")"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# revenueverticalreport.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/RPT/revenueverticalreport.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 16
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/109_RptParcelLevel_ManifestWeightAnalysis')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Reports/LMS"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "9f370435-c1e4-489a-bb4e-eac6f27b1400"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace/bigDataPools/ApacheSparkPool",
						"name": "ApacheSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/ApacheSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# #Create DataFrame for the rptparcellevel LMS Table\r\n",
							"# rptparcellevel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/RPT/rptparcellevel.parquet', format='parquet')\r\n",
							"# rptparcellevel.createOrReplaceTempView(\"rptparcellevel\")\r\n",
							"\r\n",
							"# #Create DataFrame for the rpttracklevel LMS Table\r\n",
							"# rpttracklevel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/RPT/rpttracklevel.parquet', format='parquet')\r\n",
							"# rpttracklevel.createOrReplaceTempView(\"rpttracklevel\")\r\n",
							"\r\n",
							"# #create dataframe for the stsroute lms table\r\n",
							"# stsroute = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stsroute.parquet', format='parquet')\r\n",
							"# stsroute.createOrReplaceTempView(\"stsroute\")\r\n",
							"\r\n",
							"# #create dataframe for the stzone lms table\r\n",
							"# stzone = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stzone.parquet', format='parquet')\r\n",
							"# stzone.createOrReplaceTempView(\"stzone\")\r\n",
							"\r\n",
							"# #create dataframe for the stlocation lms table\r\n",
							"# stlocation = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stlocation.parquet', format='parquet')\r\n",
							"# stlocation.createOrReplaceTempView(\"stlocation\")"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"manifestweightanalysisreport\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"     t.lms_track_loadid as manifestid\r\n",
							"    ,p.lms_parcel_orderbillcustid as billingcustomerid\r\n",
							"    ,p.lms_parcel_orderbillcustname as billingcustomer\r\n",
							"    ,p.lms_parcel_orderdelivercustid as deliverycustomerid\r\n",
							"    ,p.lms_parcel_orderdelivercustname as deliverycustomer\r\n",
							"    ,CAST(p.lms_parcel_id as varchar(20))||'('||COALESCE(p.lms_parcel_barcode,'')||')' as parcelno\r\n",
							"    ,p.lms_parcel_consignmentcref as consignmentcref\r\n",
							"    ,p.lms_parcel_weight as weight\r\n",
							"    ,p.lms_parcel_chargeweight as chargeweight\r\n",
							"    ,p.lms_parcel_volweight as volweight\r\n",
							"    ,(p.lms_parcel_pl * p.lms_parcel_ph * p.lms_parcel_pw)/5000 as  calculatedvolweight\r\n",
							"    ,(p.lms_parcel_pl * p.lms_parcel_ph * p.lms_parcel_pw)/1000000 as calculatedcubes\r\n",
							"    ,p.lms_parcel_customerbarcode as customerbarcode\r\n",
							"    ,CASE \r\n",
							"        WHEN p.lms_parcel_ptype = 1  THEN 'STATIONARY'\r\n",
							"\t\tWHEN p.lms_parcel_ptype = 2  THEN 'IBT'\r\n",
							"\t\tWHEN p.lms_parcel_ptype  =3  THEN 'NORMAL DC'\r\n",
							"\t\tWHEN p.lms_parcel_ptype = 20 THEN 'RTV'\r\n",
							"\t\tWHEN p.lms_parcel_ptype = 21 THEN 'RECALL'\r\n",
							"\t\tWHEN p.lms_parcel_ptype = 22 THEN 'REDIST'\r\n",
							"\t\tELSE 'UNKNOWN'\r\n",
							"\tEND as parceltype \r\n",
							"    ,p.lms_parcel_ordercustref as ordercustomereference\r\n",
							"    ,p.lms_parcel_ordercorderno as ordercorderno\r\n",
							"    ,l.lms_location_description as zonelocationdescription\r\n",
							"\r\n",
							"\r\n",
							"FROM rpttracklevel t\r\n",
							"LEFT JOIN rptparcellevel p on p.lms_parcel_id = t.lms_track_parcelid\r\n",
							"LEFT JOIN stsroute sr on sr.lms_sroute_id = p.lms_parcel_orderdelivercustsrouteid\r\n",
							"LEFT JOIN stzone z on z.lms_zone_id = lms_sroute_zoneid\r\n",
							"LEFT JOIN stlocation l on l.lms_location_id = z.lms_zone_locid\r\n",
							""
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"manifestweightanalysisreport = spark.sql(\"SELECT * FROM manifestweightanalysisreport\")"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# manifestweightanalysisreport.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/RPT/manifestweightanalysisreport.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 11
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/10_DIST_STEndorsments')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Ops/Distribution"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "9dd9d3a7-28c0-47b2-a9c5-bf55b05f6041"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse_Prod/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace-prod/bigDataPools/PRDSparkPool",
						"name": "PRDSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/PRDSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# #create dataframe for the publicclaim orv table\r\n",
							"# publicclaim = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicclaim.parquet', format='parquet')\r\n",
							"# publicclaim.createOrReplaceTempView(\"publicclaim\")\r\n",
							"\r\n",
							"# #create dataframe for the publicdelivery orv table\r\n",
							"# publicdelivery = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicdelivery.parquet', format='parquet')\r\n",
							"# publicdelivery.createOrReplaceTempView(\"publicdelivery\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbowbendorsement LMS Table\r\n",
							"# dbowbendorsement = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbowbendorsement.parquet', format='parquet')\r\n",
							"# dbowbendorsement.createOrReplaceTempView(\"dbowbendorsement\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbodb_endorsement LMS Table\r\n",
							"# dbodb_endorsement = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodb_endorsement.parquet', format='parquet')\r\n",
							"# dbodb_endorsement.createOrReplaceTempView(\"dbodb_endorsement\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stendorsments_tmp\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    c.id\r\n",
							"    ,c.delid\r\n",
							"\t,c.claimtype\r\n",
							"\t,to_timestamp(c.claimdate) as claimdate\r\n",
							"\t,c.did\r\n",
							"\t,c.parcelid\r\n",
							"\t,c.barcode\r\n",
							"\t,c.attachid\r\n",
							"\t,c.note\r\n",
							"    ,d.waybillid\r\n",
							"    \r\n",
							"FROM publicclaim c\r\n",
							"LEFT JOIN publicdelivery d on d.id = c.delid"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stendorsments\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    e.ID as lms_db_endorsement_id\r\n",
							"\t,e.Description as lms_db_endorsement_description\r\n",
							"    ,er.ID as lms_wbendorsement_id\r\n",
							"    ,er.Barcode as lms_wbendorsement_barcode\r\n",
							"    ,er.WaybillID as lms_wbendorsement_waybillid\r\n",
							"    ,et.id as orv_claim_id\r\n",
							"    ,et.delid as orv_claim_delid\r\n",
							"\t,et.claimtype as orv_claim_claimtype\r\n",
							"\t,et.claimdate as orv_claim_claimdate\r\n",
							"\t,et.did as orv_claim_did\r\n",
							"\t,et.parcelid as orv_claim_parcelid\r\n",
							"\t,et.barcode as orv_claim_barcode\r\n",
							"\t,et.attachid as orv_claim_attachid\r\n",
							"\t,et.note as orv_claim_note\r\n",
							"    \r\n",
							"FROM dbodb_endorsement e\r\n",
							"LEFT JOIN dbowbendorsement er on er.EType = e.ID\r\n",
							"LEFT JOIN stendorsments_tmp et on et.waybillid||'|'||et.barcode = er.WaybillID||'|'||er.barcode"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stendorsments = spark.sql(\"SELECT * FROM stendorsments\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stendorsments.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stendorsments.parquet', mode = \"overwrite\")\r\n",
							"print(\"SUCCESS\")"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/10_STFinanceDates')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Finance"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "9b611844-96c7-4605-982f-e93ff4a21abe"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace/bigDataPools/DevSparkPool",
						"name": "DevSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/DevSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 49
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# #Create DataFrame for the sttransactionssummary SAP Table\r\n",
							"# sttransactionssummary = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/FINANCE/sttransactionssummary.parquet', format='parquet')\r\n",
							"# sttransactionssummary.createOrReplaceTempView(\"sttransactionssummary\")"
						],
						"outputs": [],
						"execution_count": 50
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"endDateDF = spark.sql(\"SELECT max(sap_transactionsheader_postingdate) as enddate FROM sttransactionssummary\")"
						],
						"outputs": [],
						"execution_count": 51
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"endDate = endDateDF.first()['enddate']"
						],
						"outputs": [],
						"execution_count": 52
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import *\r\n",
							"beginDate = '2016-04-01'\r\n",
							"\r\n",
							"(\r\n",
							"  # spark.sql(f\"select explode(sequence(to_date('{beginDate}'), current_date(), interval 1 day)) as calendarDate\").createOrReplaceTempView('dates')\r\n",
							"  spark.sql(f\"select explode(sequence(to_date('{beginDate}'), to_date('{endDate}'), interval 1 day)) as calendarDate\").createOrReplaceTempView('dates')\r\n",
							")"
						],
						"outputs": [],
						"execution_count": 53
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stfinancedates\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"\r\n",
							"  year(calendarDate) * 10000 + month(calendarDate) * 100 + day(calendarDate) AS date_dim_id,\r\n",
							"  CalendarDate AS sap_transactionsheader_postingdate,\r\n",
							"  to_unix_timestamp(calendarDate) AS epoch,\r\n",
							"  year(calendarDate) AS actual_year,\r\n",
							"  year(calendarDate)||'-'||date_format(calendarDate,'MM') AS actual_year_month,\r\n",
							"  date_format(calendarDate, 'MMMM') as actual_month_name,\r\n",
							"  date_format(calendarDate,'MM')as actual_month_number,\r\n",
							"  date_format(calendarDate, 'EEEE') as actual_day_name,\r\n",
							"  dayofweek(calendarDate) AS actual_day_of_week,\r\n",
							"  case\r\n",
							"    when weekday(calendarDate) < 5 then 1\r\n",
							"    else 0\r\n",
							"  end as is_week_day,\r\n",
							"  dayofmonth(calendarDate) as actual_day_of_month,\r\n",
							"  case\r\n",
							"    when calendarDate = last_day(calendarDate) then 1\r\n",
							"    else 0\r\n",
							"  end as is_last_day_of_month,\r\n",
							"  date_format(last_day(calendarDate),'dd') AS last_day_of_month,\r\n",
							"  dayofyear(calendarDate) as actual_day_of_year,\r\n",
							"  weekofyear(calendarDate) as actual_week_of_year,\r\n",
							"  quarter(calendarDate) as actual_quarter_of_year,\r\n",
							"  /* Use fiscal periods needed by organization fiscal calendar */\r\n",
							"  case\r\n",
							"    when month(calendarDate) >= 4 then year(calendarDate) + 1\r\n",
							"    else year(calendarDate)\r\n",
							"  end as fiscal_year,\r\n",
							"  case\r\n",
							"    when month(calendarDate) >= 4 then (year(calendarDate) + 1)||'-'||date_format(calendarDate,'MM')\r\n",
							"    else year(calendarDate)||'-'||date_format(calendarDate,'MM')\r\n",
							"  end as fiscal_year_month,\r\n",
							"  case\r\n",
							"    when month(calendarDate) = 4 then 1\r\n",
							"    when month(calendarDate) = 5 then 2\r\n",
							"    when month(calendarDate) = 6 then 3\r\n",
							"    when month(calendarDate) = 7 then 4\r\n",
							"    when month(calendarDate) = 8 then 5\r\n",
							"    when month(calendarDate) = 9 then 6\r\n",
							"    when month(calendarDate) = 10 then 7\r\n",
							"    when month(calendarDate) = 11 then 8\r\n",
							"    when month(calendarDate) = 12 then 9\r\n",
							"    when month(calendarDate) = 1 then 10\r\n",
							"    when month(calendarDate) = 2 then 11\r\n",
							"    when month(calendarDate) = 3 then 12\r\n",
							"    else 0\r\n",
							"  end as fiscal_month_sort,\r\n",
							"  month(calendarDate) as actual_month_sort,\r\n",
							"  weekofyear(date_add(calendarDate, -1)) as actual_week_of_year_sun,\r\n",
							"  dayofweek(date_add(calendarDate, -1)) as actual_day_of_week_sun\r\n",
							"\r\n",
							"from\r\n",
							"  dates\r\n",
							"order by\r\n",
							"\r\n",
							"  year(calendarDate) desc,  case\r\n",
							"    when month(calendarDate) = 4 then 1\r\n",
							"    when month(calendarDate) = 5 then 2\r\n",
							"    when month(calendarDate) = 6 then 3\r\n",
							"    when month(calendarDate) = 7 then 4\r\n",
							"    when month(calendarDate) = 8 then 5\r\n",
							"    when month(calendarDate) = 9 then 6\r\n",
							"    when month(calendarDate) = 10 then 7\r\n",
							"    when month(calendarDate) = 11 then 8\r\n",
							"    when month(calendarDate) = 12 then 9\r\n",
							"    when month(calendarDate) = 1 then 10\r\n",
							"    when month(calendarDate) = 2 then 11\r\n",
							"    when month(calendarDate) = 3 then 12\r\n",
							"    else 0\r\n",
							"  end,calendarDate  asc\r\n",
							""
						],
						"outputs": [],
						"execution_count": 54
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stfinancedates = spark.sql(\"SELECT * FROM stfinancedates\")"
						],
						"outputs": [],
						"execution_count": 55
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stfinancedates.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stfinancedates.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 56
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/10_STJournals')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Finance"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "bef936b6-36bc-45c0-b536-d560859229c7"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse_Prod/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace-prod/bigDataPools/PRDSparkPool",
						"name": "PRDSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/PRDSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"if 'prod' in Environment:\r\n",
							"    StorageAccount = 'citylogisticsstorageprod'\r\n",
							"    StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"    StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"else:\r\n",
							"    StorageAccount = 'citylogisticsstorage'\r\n",
							"    StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"    StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# #Create DataFrame for the dboOJDT SAP Table\r\n",
							"# dboOJDT = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboOJDT.parquet', format='parquet')\r\n",
							"# dboOJDT.createOrReplaceTempView(\"dboOJDT\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dboJDT1 SAP Table\r\n",
							"# dboJDT1 = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboJDT1.parquet', format='parquet')\r\n",
							"# dboJDT1.createOrReplaceTempView(\"dboJDT1\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stchartofaccounts SAP Table\r\n",
							"# stchartofaccounts = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Structured Data/FINANCE/stchartofaccounts.parquet', format='parquet')\r\n",
							"# stchartofaccounts.createOrReplaceTempView(\"stchartofaccounts\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stsalesinvoice SAP Table\r\n",
							"# stsalesinvoice = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Structured Data/FINANCE/stsalesinvoice.parquet', format='parquet')\r\n",
							"# stsalesinvoice.createOrReplaceTempView(\"stsalesinvoice\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stitem SAP Table\r\n",
							"# stitem = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Structured Data/FINANCE/stitem.parquet', format='parquet')\r\n",
							"# stitem.createOrReplaceTempView(\"stitem\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stdocuments SAP Table\r\n",
							"# stdocuments = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Structured Data/FINANCE/stdocuments.parquet', format='parquet')\r\n",
							"# stdocuments.createOrReplaceTempView(\"stdocuments\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stjournals\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    ojdt.refdate as sap_journalsheader_postingdate\r\n",
							"    ,ojdt.duedate as sap_journalsheader_duedate\r\n",
							"    ,ojdt.loctotal as sap_journalsheader_doctotal\r\n",
							"    ,'J' as sap_journalsheader_doctype\r\n",
							"    ,ojdt.transid as sap_journalsheader_transid\r\n",
							"    ,jdt1.line_id as sap_journalsline_linenum\r\n",
							"    ,jdt1.account as sap_journalsline_acctcode\r\n",
							"    ,ojdt.transtype as sap_journalsheader_objtype\r\n",
							"    ,jdt1.project as sap_journalsline_project\r\n",
							"    ,jdt1.vatgroup as sap_journalsline_vatgroup\r\n",
							"    ,jdt1.vatrate as sap_journalsline_vatrate\r\n",
							"    ,jdt1.vatamount as sap_journalsline_vatamount\r\n",
							"    ,jdt1.linememo as sap_journalsline_description\r\n",
							"    ,jdt1.ref1 as sap_journalsline_jnlref1\r\n",
							"    ,jdt1.ref2 as sap_journalsline_jnlref2\r\n",
							"    ,jdt1.u_depot as sap_journalsline_subdepotcode\r\n",
							"    ,CASE \r\n",
							"        --JOURNAL\r\n",
							"        WHEN (coa.sap_account_groupmask IN (1)) THEN sum((jdt1.debit-jdt1.credit))\r\n",
							"        WHEN (coa.sap_account_groupmask IN (2,3,4,5,6,8)) THEN sum((jdt1.credit-jdt1.debit))\r\n",
							"        ELSE NULL \r\n",
							"    END as  sap_journalsline_linetotal\r\n",
							"    ,CASE \r\n",
							"        WHEN (coa.sap_account_groupmask IN (1,2,3,4,5,6,8)) THEN 'journal'\r\n",
							"        ELSE NULL \r\n",
							"    END as  sap_journalsline_doccat\r\n",
							"    ,si.sap_salesinvoiceline_itemcode as sap_journalsline_itemcode\r\n",
							"    ,CASE \r\n",
							"        WHEN (si.sap_salesinvoiceheader_transid = ojdt.transid \r\n",
							"            AND (ojdt.transtype = 13 AND coa.sap_account_groupmask IN (5,6,7,8)) \r\n",
							"            AND si.sap_salesinvoiceheader_canceled = 'N') THEN sap_salesinvoiceheader_cardcode\r\n",
							"        WHEN (d.sap_documentsheader_transid = ojdt.transid AND coa.sap_account_groupmask IN (1,2,3,4,5,6,8)) THEN d.sap_documentsheader_cardcode\r\n",
							"        ELSE NULL\r\n",
							"    END sap_journalsheader_cardcode\r\n",
							"    ,CASE \r\n",
							"        WHEN (si.sap_salesinvoiceheader_transid = ojdt.transid \r\n",
							"            AND (ojdt.transtype = 13 AND coa.sap_account_groupmask IN (5,6,7,8)) \r\n",
							"            AND si.sap_salesinvoiceheader_canceled = 'N') THEN sap_salesinvoiceheader_numatcard\r\n",
							"        WHEN (d.sap_documentsheader_transid = ojdt.transid AND coa.sap_account_groupmask IN (1,2,3,4,5,6,8)) THEN d.sap_documentsheader_numatcard\r\n",
							"        ELSE NULL\r\n",
							"    END sap_journalsheader_cardref\r\n",
							"   \r\n",
							"FROM dboOJDT ojdt\r\n",
							"LEFT JOIN dboJDT1 jdt1 on jdt1.transid = ojdt.transid\r\n",
							"LEFT JOIN stchartofaccounts coa on coa.sap_account_acctcode = jdt1.account\r\n",
							"LEFT JOIN stsalesinvoice si on si.sap_salesinvoiceheader_transid = jdt1.transid \r\n",
							"    AND (ojdt.transtype = 13 AND coa.sap_account_groupmask IN (5,6,7,8)) \r\n",
							"    AND si.sap_salesinvoiceheader_canceled = 'N'\r\n",
							"LEFT JOIN stitem i on i.sap_item_itemcode = si.sap_salesinvoiceline_itemcode \r\n",
							"    AND (ojdt.transtype = 13 AND coa.sap_account_groupmask IN (5,6,7,8))\r\n",
							"LEFT JOIN stdocuments d on d.sap_documentsheader_transid = jdt1.transid \r\n",
							"    AND coa.sap_account_groupmask IN (1,2,3,4,5,6,8) AND d.sap_documentsline_doccat NOT IN ('journal','purchaseorder')\r\n",
							"GROUP BY\r\n",
							"    ojdt.memo\r\n",
							"    ,ojdt.transtype\r\n",
							"    ,ojdt.duedate\r\n",
							"    ,ojdt.loctotal\r\n",
							"    ,coa.sap_account_groupmask\r\n",
							"    ,coa.sap_account_acctname\r\n",
							"    ,coa.sap_account_segment_0\r\n",
							"    ,coa.sap_account_segment_1\r\n",
							"    ,coa.sap_account_segment_2\r\n",
							"    ,ojdt.refdate\r\n",
							"    ,ojdt.doctype\r\n",
							"    ,ojdt.transid\r\n",
							"    ,jdt1.line_id\r\n",
							"    ,jdt1.account\r\n",
							"    ,jdt1.transtype\r\n",
							"    ,jdt1.refdate\r\n",
							"    ,jdt1.project\r\n",
							"    ,jdt1.vatgroup\r\n",
							"    ,jdt1.vatrate\r\n",
							"    ,jdt1.vatamount\r\n",
							"    ,jdt1.linememo\r\n",
							"    ,jdt1.ref1\r\n",
							"    ,jdt1.ref2\r\n",
							"    ,jdt1.baseref\r\n",
							"    ,jdt1.u_depot\r\n",
							"    ,si.sap_salesinvoiceline_itemcode\r\n",
							"    ,si.sap_salesinvoiceheader_doctype\r\n",
							"    ,si.sap_salesinvoiceheader_transid\r\n",
							"    ,si.sap_salesinvoiceheader_canceled\r\n",
							"    ,si.sap_salesinvoiceheader_cardcode\r\n",
							"    ,si.sap_salesinvoiceheader_numatcard\r\n",
							"    ,d.sap_documentsheader_transid\r\n",
							"    ,d.sap_documentsheader_cardcode\r\n",
							"    ,d.sap_documentsheader_numatcard\r\n",
							"    ,i.sap_item_invntitem\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stjournals = spark.sql(\"SELECT * FROM stjournals\")\r\n",
							"# stjournals.write.parquet('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Structured Data/FINANCE/stjournals.parquet', mode = \"overwrite\")\r\n",
							"print(\"SUCCESS\")"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/110_RptParcelLevel_DCReceivingDaily')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Reports/LMS"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "ae4c4f6d-4a11-4dde-ab58-1a8d98cc177c"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace/bigDataPools/ApacheSparkPool",
						"name": "ApacheSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/ApacheSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# #Create DataFrame for the rptparcellevel LMS Table\r\n",
							"# rptparcellevel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/RPT/rptparcellevel.parquet', format='parquet')\r\n",
							"# rptparcellevel.createOrReplaceTempView(\"rptparcellevel\")\r\n",
							"\r\n",
							"# #Create DataFrame for the rpttracklevel LMS Table\r\n",
							"# rpttracklevel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/RPT/rpttracklevel.parquet', format='parquet')\r\n",
							"# rpttracklevel.createOrReplaceTempView(\"rpttracklevel\")\r\n",
							"\r\n",
							"# #create dataframe for the stlocation lms table\r\n",
							"# stlocation = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stlocation.parquet', format='parquet')\r\n",
							"# stlocation.createOrReplaceTempView(\"stlocation\")\r\n",
							"\r\n",
							"# #create dataframe for the dbodcorder lms table\r\n",
							"# dbodcorder = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodc_order.parquet', format='parquet')\r\n",
							"# dbodcorder.createOrReplaceTempView(\"dbodcorder\")\r\n",
							"\r\n",
							"# #create dataframe for the dbodctransfer lms table\r\n",
							"# dbodctransfer = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodc_transfer.parquet', format='parquet')\r\n",
							"# dbodctransfer.createOrReplaceTempView(\"dbodctransfer\")\r\n",
							"\r\n",
							"# #create dataframe for the dbodcprepack lms table\r\n",
							"# dbodcprepack = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodc_prepack.parquet', format='parquet')\r\n",
							"# dbodcprepack.createOrReplaceTempView(\"dbodcprepack\")\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"dcreceivingdailyreport\r\n",
							"AS\r\n",
							"SELECT\r\n",
							" p.lms_parcel_orderbillcustid as billingcustomerid\r\n",
							",p.lms_parcel_orderbillcustname as billingcustomername\r\n",
							",p.lms_parcel_orderdelivercustid as deliverycustomerid\r\n",
							",p.lms_parcel_orderdelivercustname as deliverycustomername\r\n",
							",p.lms_parcel_consignmentcdate as date\r\n",
							",dco.supplierdesc as supplierdescription\r\n",
							",dco.purchordernumber as purchaseordernumber\r\n",
							",dco.appointmentcode\r\n",
							",dco.apptstartdatetime as appointmentstartdatetime\r\n",
							",dco.apptenddatetime as appointmentenddatetime\r\n",
							",dco.arrivedat\r\n",
							",dco.lastuploadeddate\r\n",
							",dco.dccode\r\n",
							",dco.ispandaenabled\r\n",
							",dcpp.length as dcprepacklength\r\n",
							",dcpp.height as dcprepackheight\r\n",
							",dcpp.width as dcprepackwidth\r\n",
							",dct.transfercode as dctransfercode\r\n",
							",dct.acceptancedate as dctransferacceptancedate\r\n",
							",dct.auditflag as dctransferauditflag\r\n",
							",dct.counted as dctransfercounted\r\n",
							",dct.uploaded as dctransferuploaded\r\n",
							",p.lms_parcel_weight as parcelweight\r\n",
							",p.lms_parcel_volweight as parcelvolweight\r\n",
							",p.lms_parcel_chargeweight as parcelchargeweight\r\n",
							",p.lms_parcel_waybillid as waybillid\r\n",
							",p.lms_parcel_consignid as consignid\r\n",
							",lms_location_description as parcellocation\r\n",
							"\r\n",
							"FROM dbodcorder dco\r\n",
							"LEFT JOIN dbodcprepack dcpp on dcpp.dc_orderid = dco.id\r\n",
							"LEFT JOIN dbodctransfer dct on dct.dc_prepackid = dcpp.id\r\n",
							"LEFT JOIN rptparcellevel p on p.lms_parcel_barcode = dct.transfercode\r\n",
							"LEFT JOIN stlocation l on l.lms_location_id = p.lms_parcel_locid\r\n",
							""
						],
						"outputs": [],
						"execution_count": 30
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"dcreceivingdailyreport = spark.sql(\"SELECT * FROM dcreceivingdailyreport\")"
						],
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# dcreceivingdailyreport.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/RPT/dcreceivingdailyreport.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 32
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/111_RptWaybillLevel_HiltiWBSummary')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Reports/LMS"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "ff10602e-b0ef-4639-b523-a281d1aa9ee9"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace/bigDataPools/ApacheSparkPool",
						"name": "ApacheSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/ApacheSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# #Create DataFrame for the rptwaybilllevel LMS Table\r\n",
							"# rptwaybilllevel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/RPT/rptwaybilllevel.parquet', format='parquet')\r\n",
							"# rptwaybilllevel.createOrReplaceTempView(\"rptwaybilllevel\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stparcel LMS Table\r\n",
							"# stparcel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/RPT/stparcel.parquet', format='parquet')\r\n",
							"# stparcel.createOrReplaceTempView(\"stparcel\")\r\n",
							"\r\n",
							"# #Create DataFrame for the deliverypickupcustomer LMS Table\r\n",
							"# deliverypickupcustomer = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/RPT/deliverypickupcustomer.parquet', format='parquet')\r\n",
							"# deliverypickupcustomer.createOrReplaceTempView(\"deliverypickupcustomer\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbocustacc LMS Table\r\n",
							"# dbocustacc = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbocustacc.parquet', format='parquet')\r\n",
							"# dbocustacc.createOrReplaceTempView(\"dbocustacc\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbohiltigroupcode LMS Table\r\n",
							"# dbohiltigroupcode = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbohiltigroupcode.parquet', format='parquet')\r\n",
							"# dbohiltigroupcode.createOrReplaceTempView(\"dbohiltigroupcode\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbohiltigroup LMS Table\r\n",
							"# dbohiltigroup = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbohiltigroup.parquet', format='parquet')\r\n",
							"# dbohiltigroup.createOrReplaceTempView(\"dbohiltigroup\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbohiltirouteratetype LMS Table\r\n",
							"# dbohiltirouteratetype = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbohiltirouteratetype.parquet', format='parquet')\r\n",
							"# dbohiltirouteratetype.createOrReplaceTempView(\"dbohiltirouteratetype\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"hiltiwbsummaryreport_tmp\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    to_date(w.lms_waybill_date) date,\r\n",
							"    w.lms_waybill_id waybillid,\r\n",
							"\thgr.Description division,\r\n",
							"\tCASE \r\n",
							"        WHEN CAST(CONVERT(VARCHAR(10),w.lms_waybill_inservice+1, 111) AS DATETIME) +'00:00:00'  > w.lms_waybill_poddate THEN 1 \r\n",
							"\t    ELSE 0 \r\n",
							"\tEND tfhinserviceindicator,\r\n",
							"    CASE \r\n",
							"\t    WHEN w.lms_waybill_inservice >  w.lms_waybill_poddate THEN 1 \r\n",
							"\t    ELSE 0 \r\n",
							"\tEND inserviceindicator\r\n",
							"FROM rptwaybilllevel w\r\n",
							"INNER JOIN stparcel p ON p.lms_parcel_waybillid =  w.lms_waybill_id\r\n",
							"INNER JOIN deliverypickupcustomer dc ON dc.lms_customer_id = p.lms_parcel_orderdelivercustid    \r\n",
							"LEFT JOIN dbocustacc ca ON ca.id = w.lms_waybill_custaccid\r\n",
							"LEFT JOIN dbohiltigroupcode hg ON custAcc.CRef = hg.Code  \r\n",
							"LEFT JOIN dbohiltigroup hgr ON hgr.ID = COALEASCE(hg.hiltigroupid,1)  \r\n",
							"LEFT JOIN dbohiltirouteratetype rt ON rt.routeratetypeid = dc.lms_customer_brouteid AND COALEASCE(hg.hiltigroupid,1) = rt.hiltigroupid\r\n",
							"\t\r\n",
							"GROUP BY\r\n",
							"w.lms_waybill_id,\r\n",
							"w.lms_waybill_poddate,\r\n",
							"w.lms_waybill_inservice,\r\n",
							"hgr.description"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"hiltiwbsummaryreport\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"\tdivision,\r\n",
							"    date,\r\n",
							"\tCount(waybillid) waybills,\r\n",
							"\tSum(tfhinserviceindicator) inservicemidnight,\t\r\n",
							"\tSum(inserviceindicator) inservicetargetdeliverytime,\r\n",
							"\tCast((Cast(Sum(InserviceIndicator) as numeric (13,2))/Cast(Count(Waybill)as numeric(13,2)))*100 as numeric (13,2)) as inservicemidnightpct,\r\n",
							"\tCast((Cast(Sum(TFHInserviceIndicator) as numeric (13,2))/cast(Count(Waybill)as numeric (13,2)))*100 as numeric (13,2)) as inservicetargetdeliverytimepct\r\n",
							"    \r\n",
							"FROM hiltiwbsummaryreport_tmp \r\n",
							"GROUP BY\r\n",
							"division,\r\n",
							"date"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"hiltiwbregionsummaryreport = spark.sql(\"SELECT * FROM hiltiwbregionsummaryreport\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# hiltiwbregionsummaryreport.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/RPT/hiltiwbregionsummaryreport.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/112_RptParcelLevel_OTDBillingReport')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Reports/LMS"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "77b7f50e-2e99-4ea3-b952-ab8331273963"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace/bigDataPools/DevSparkPool",
						"name": "DevSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/DevSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# #Create DataFrame for the rptparcellevel LMS Table\r\n",
							"# rptparcellevel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/RPT/rptparcellevel.parquet', format='parquet')\r\n",
							"# rptparcellevel.createOrReplaceTempView(\"rptparcellevel\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stlocation LMS Table\r\n",
							"# stlocation = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stlocation.parquet', format='parquet')\r\n",
							"# stlocation.createOrReplaceTempView(\"stlocation\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stzone LMS Table\r\n",
							"# stzone = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stzone.parquet', format='parquet')\r\n",
							"# stzone.createOrReplaceTempView(\"stzone\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stsroute LMS Table\r\n",
							"# stsroute = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stsroute.parquet', format='parquet')\r\n",
							"# stsroute.createOrReplaceTempView(\"stsroute\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"otdbillingreport\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"p.lms_parcel_barcode barcode,\r\n",
							"p.lms_parcel_orderbillcustid billingcustomerid,\r\n",
							"p.lms_parcel_orderbillcustname billingcustomername,\r\n",
							"p.lms_parcel_orderdelivercustid deliverycustomerid,\r\n",
							"p.lms_parcel_orderdelivercustname deliverycustomername,\r\n",
							"p.lms_parcel_orderdelivercustcref cref,\r\n",
							"p.lms_parcel_weight weight,\r\n",
							"p.lms_parcel_pdate date,\r\n",
							"p.lms_parcel_consignmentcdate consigndate,\r\n",
							"p.lms_parcel_waybillpoddate poddate,\r\n",
							"dsr.lms_sroute_id deliverycustsrouteid,\r\n",
							"p.lms_parcel_sroutedescriptiondelivercust deliverycustsroutedescription,\r\n",
							"pl.lms_location_id currentlocationid,\r\n",
							"pl.lms_location_description currentlocation,\r\n",
							"dl.lms_location_id deliverybranchid,\r\n",
							"dl.lms_location_description deliverybranch,\r\n",
							"CASE \r\n",
							"    WHEN dsr.lms_sroute_localflag = 1 THEN 'Local' ELSE 'Outlying' \r\n",
							"END AS localoutlying,\r\n",
							"\r\n",
							"CASE \r\n",
							"    WHEN p.lms_parcel_waybillid IS NULL AND p.lms_parcel_locid = 1 THEN 'In-Transit Linehaul'\r\n",
							"    WHEN p.lms_parcel_waybillid IS NOT NULL AND p.lms_parcel_locid = 1 THEN 'On Delivery'\r\n",
							"    WHEN p.lms_parcel_locid = 2 THEN 'Delivered'\r\n",
							"    WHEN pl.lms_location_locationtypeid = 1 AND p.lms_parcel_waybillid IS NOT NULL THEN 'Parcel on Floor - Redelivery'\r\n",
							"    WHEN pl.lms_location_locationtypeid = 1 AND p.lms_parcel_waybillid IS NULL THEN 'Parcel on Floor'\r\n",
							"    ELSE 'Not Defined'\r\n",
							"END AS parcelstatus,\r\n",
							"p.lms_parcel_waybillid waybillid\r\n",
							"\r\n",
							"FROM\r\n",
							"rptparcellevellimitload p\r\n",
							"LEFT JOIN stlocation pl ON p.lms_parcel_locid = pl.lms_location_id\r\n",
							"LEFT JOIN stsroute dsr ON p.lms_parcel_orderdelivercustsrouteid = dsr.lms_sroute_id\r\n",
							"LEFT JOIN stzone dz ON dsr.lms_sroute_zoneid = dz.lms_zone_id\r\n",
							"LEFT JOIN stlocation dl ON dz.lms_zone_locid = dl.lms_location_id\r\n",
							"\r\n",
							"WHERE\r\n",
							"UPPER(SUBSTRING(p.lms_parcel_barcode,1,3)) = 'OFT' \r\n",
							"AND p.lms_parcel_orderbillcustid NOT IN (191787, 626416)"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"otdbillingreport = spark.sql(\"SELECT * FROM otdbillingreport\")"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# otdbillingreport.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/RPT/otdbillingreport.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 5
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/113_RptParcelLevel_EcomBillingReport')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Reports/LMS"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "ce656387-2785-4567-b319-cd789a96c3d2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace/bigDataPools/DevSparkPool",
						"name": "DevSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/DevSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# #Create DataFrame for the rptparcellevel LMS Table\r\n",
							"# rptparcellevel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/RPT/rptparcellevel.parquet', format='parquet')\r\n",
							"# rptparcellevel.createOrReplaceTempView(\"rptparcellevel\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stparcel LMS Table\r\n",
							"# stparcel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stparcel.parquet', format='parquet')\r\n",
							"# stparcel.createOrReplaceTempView(\"stparcel\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stdeliverypickupcustomer LMS Table\r\n",
							"# stdeliverypickupcustomer = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stdeliverypickupcustomer.parquet', format='parquet')\r\n",
							"# stdeliverypickupcustomer.createOrReplaceTempView(\"stdeliverypickupcustomer\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stwaybill LMS Table\r\n",
							"# stwaybill = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stwaybill.parquet', format='parquet')\r\n",
							"# stwaybill.createOrReplaceTempView(\"stwaybill\")\r\n",
							"\r\n",
							"# # #Create DataFrame for the stlocation LMS Table\r\n",
							"# stlocation = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stlocation.parquet', format='parquet')\r\n",
							"# stlocation.createOrReplaceTempView(\"stlocation\")\r\n",
							"\r\n",
							"# # #Create DataFrame for the stlocation LMS Table\r\n",
							"# stlocation = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stlocation.parquet', format='parquet')\r\n",
							"# stlocation.createOrReplaceTempView(\"stlocation\")\r\n",
							"\r\n",
							"# # #Create DataFrame for the dbobill_parceltype LMS Table\r\n",
							"# dbobill_parceltype = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS//dbobill_parceltype.parquet', format='parquet')\r\n",
							"# dbobill_parceltype.createOrReplaceTempView(\"dbobill_parceltype\")\r\n",
							"\r\n",
							"# # #Create DataFrame for the storder LMS Table\r\n",
							"# storder = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/storder.parquet', format='parquet')\r\n",
							"# storder.createOrReplaceTempView(\"storder\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"\r\n",
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"ecombillingreport\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"p.lms_parcel_orderbillcustid billingcustomerid,\r\n",
							"p.lms_parcel_orderbillcustname billingcustomername,\r\n",
							"p.lms_parcel_orderdelivercustid deliverycustomerid,\r\n",
							"p.lms_parcel_orderdelivercustname deliverycustomername,\r\n",
							"p.lms_parcel_orderpickupcustid pickupcustomerid,\r\n",
							"p.lms_parcel_orderpickupcustname pickucustomername,\r\n",
							"p.lms_parcel_barcode barcode,\r\n",
							"p.lms_parcel_weight weight,\r\n",
							"p.lms_parcel_volweight volweight,\r\n",
							"p.lms_parcel_ordercorderno orderno,\r\n",
							"p.lms_parcel_waybillid waybillid,\r\n",
							"p.lms_parcel_consignmentcdate consigndate,\r\n",
							"p.lms_parcel_pdate date,\r\n",
							"p.lms_parcel_waybilldate deliverycreatedateldate,\r\n",
							"p.lms_parcel_waybillpoddate deliverypoddate,\r\n",
							"p.lms_parcel_description parceldescription,\r\n",
							"p.lms_parcel_sroutedescriptiondelivercust deliverycustomersroutedescription,\r\n",
							"p.lms_parcel_ptype ptype,\r\n",
							"p.lms_parcel_disflag assembly,\r\n",
							"dc.lms_customer_adres1 deladres1,\r\n",
							"dc.lms_customer_adres2 deladres2,\r\n",
							"dc.lms_customer_pcode delpcode,\r\n",
							"w.lms_waybill_debriefdt podcaptured,\r\n",
							"o.lms_order_courierid courier,\r\n",
							"pl.lms_location_id parcellocationid,\r\n",
							"pl.lms_location_description parcellocationdescription,\r\n",
							"pt.Description parceltypedescription\r\n",
							"\r\n",
							"\r\n",
							"FROM\r\n",
							"rptparcellevellimitload p\r\n",
							"LEFT JOIN stdeliverypickupcustomer dc ON p.lms_parcel_orderdelivercustid = dc.lms_customer_id\r\n",
							"LEFT JOIN stwaybill w ON p.lms_parcel_waybillid = w.lms_waybill_id\r\n",
							"LEFT JOIN stlocation pl ON p.lms_parcel_locid = pl.lms_location_id\r\n",
							"LEFT JOIN dbobill_parceltype pt ON p.lms_parcel_ptype = pt.PType\r\n",
							"LEFT JOIN storder o ON p.lms_parcel_orderid = o.lms_order_id\r\n",
							"\r\n",
							"WHERE p.lms_parcel_orderbillcustid IN\r\n",
							"(293944,\r\n",
							"293941,\r\n",
							"293942,\r\n",
							"293945,\r\n",
							"293943)"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"ecombillingreport = spark.sql(\"SELECT * FROM ecombillingreport\")"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# ecombillingreport.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/RPT/ecombillingreport.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 8
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/114_RptParcelLevel_MrPIBTReport')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Reports/LMS"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "a4313c6c-38f9-4d74-8e24-53827f5cc027"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace/bigDataPools/DevSparkPool",
						"name": "DevSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/DevSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# #Create DataFrame for the rptparcellevel LMS Table\r\n",
							"# rptparcellevel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/RPT/rptparcellevel.parquet', format='parquet')\r\n",
							"# rptparcellevel.createOrReplaceTempView(\"rptparcellevel\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stconsignment LMS Table\r\n",
							"# stconsignment = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stconsignment.parquet', format='parquet')\r\n",
							"# stconsignment.createOrReplaceTempView(\"stconsignment\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stparcel LMS Table\r\n",
							"# stparcel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stparcel.parquet', format='parquet')\r\n",
							"# stparcel.createOrReplaceTempView(\"stparcel\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stlmstrack LMS Table\r\n",
							"# stlmstrack = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stlmstrack.parquet', format='parquet')\r\n",
							"# stlmstrack.createOrReplaceTempView(\"stlmstrack\")"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"mrpibtreport\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"p.lms_parcel_barcode barcode,\r\n",
							"c.lms_consignment_cref cref,\r\n",
							"p.lms_parcel_pdate date,\r\n",
							"p.lms_parcel_consignmentcdate consignmentdate,\r\n",
							"p.lms_parcel_weight weight,\r\n",
							"p.lms_parcel_orderpickupcustid pickupcustomerid,\r\n",
							"p.lms_parcel_orderpickupcustname pickupcustomername,\r\n",
							"p.lms_parcel_orderpickupcustcref pickupcustomercref,\r\n",
							"p.lms_parcel_sroutedescriptionpickupcust pickupcustomersroute,\r\n",
							"p.lms_parcel_orderdelivercustid delivercustomerid,\r\n",
							"p.lms_parcel_orderdelivercustname delivercustomername,\r\n",
							"p.lms_parcel_orderdelivercustcref delivercustomercref,\r\n",
							"p.lms_parcel_sroutedescriptiondelivercust delivercustomersroute,\r\n",
							"p.lms_parcel_orderbillcustid billcustomerid,\r\n",
							"p.lms_parcel_orderbillcustname billcustomername,\r\n",
							"pu.CODDate storeconfirmation,\r\n",
							"MAX(CASE WHEN t.lms_track_id IS NULL THEN 'NeverScanned' ELSE 'HasScans' END) loadedflag,\r\n",
							"MAX(t.lms_track_opendt) latestloaddate\r\n",
							"\r\n",
							"FROM\r\n",
							"rptparcellevellimitload p\r\n",
							"\r\n",
							"LEFT JOIN stconsignment c ON p.lms_parcel_consignid = c.lms_consignment_id\r\n",
							"LEFT JOIN sstparcelunion pu ON p.lms_parcel_id = pu.id\r\n",
							"LEFT JOIN stlmstrack t ON p.lms_parcel_id = t.lms_track_parcelid\r\n",
							"\r\n",
							"WHERE p.lms_parcel_ptype IN (10,2,20,21,22) \r\n",
							"AND p.lms_parcel_orderbillcustid IN (15,16,17,18,20)\r\n",
							"\r\n",
							"GROUP BY\r\n",
							"p.lms_parcel_barcode,\r\n",
							"c.lms_consignment_cref,\r\n",
							"p.lms_parcel_consignmentcdate,\r\n",
							"p.lms_parcel_weight,\r\n",
							"p.lms_parcel_orderpickupcustid,\r\n",
							"p.lms_parcel_orderpickupcustname,\r\n",
							"p.lms_parcel_orderpickupcustcref,\r\n",
							"p.lms_parcel_sroutedescriptionpickupcust,\r\n",
							"p.lms_parcel_orderdelivercustid,\r\n",
							"p.lms_parcel_orderdelivercustname,\r\n",
							"p.lms_parcel_orderdelivercustcref,\r\n",
							"p.lms_parcel_sroutedescriptiondelivercust,\r\n",
							"p.lms_parcel_orderbillcustid,\r\n",
							"p.lms_parcel_orderbillcustname,\r\n",
							"p.lms_parcel_pdate,\r\n",
							"pu.CODDate"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"mrpibtreport = spark.sql(\"SELECT * FROM mrpibtreport\")"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# mrpibtreport.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/RPT/mrpibtreport.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 22
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/11_DIST_STCollection')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Ops/Distribution"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "7dfcc490-ed10-4a33-8498-a098b377d2a3"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace/bigDataPools/DevSparkPool",
						"name": "DevSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/DevSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"# #Create DataFrame for the dbocollect LMS Table\r\n",
							"# dbocollect = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbocollect.parquet', format='parquet')\r\n",
							"# dbocollect.createOrReplaceTempView(\"dbocollect\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publicdraftcollection ORV Table\r\n",
							"# publicdelivery = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicdelivery.parquet', format='parquet')\r\n",
							"# publicdelivery.createOrReplaceTempView(\"publicdelivery\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publicdraftcollection ORV Table\r\n",
							"# publicdraftcollection = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicdraftcollection.parquet', format='parquet')\r\n",
							"# publicdraftcollection.createOrReplaceTempView(\"publicdraftcollection\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publicdispatch ORV Table\r\n",
							"# publicdispatch = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicdispatch.parquet', format='parquet')\r\n",
							"# publicdispatch.createOrReplaceTempView(\"publicdispatch\")\r\n",
							"\r\n",
							"# #create dataframe for the publicpod orv table\r\n",
							"# publicpod = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicpod.parquet', format='parquet')\r\n",
							"# publicpod.createOrReplaceTempView(\"publicpod\")\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# from pyspark.sql.functions import *\r\n",
							"# from pyspark.sql.window import Window\r\n",
							"# from pyspark.sql.types import IntegralType"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"partitionfirst=Window.partitionBy(\"did\").orderBy(col(\"did\").asc())"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"pod_tmp = publicpod.withColumn(\"row_num\",row_number().over(partitionfirst))"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"pod_tmp.createOrReplaceTempView(\"pod_tmp\")"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"partitionfirst=Window.partitionBy(\"collectionwaybillid\").orderBy(col(\"collectionwaybillid\").asc()).orderBy(col(\"id\").asc())"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"draftcollection_tmp = publicdraftcollection.withColumn(\"row_num\",row_number().over(partitionfirst))"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"draftcollection_tmp.createOrReplaceTempView(\"draftcollection_tmp\")"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stcollection\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"     c.id as lms_collect_id\r\n",
							"    ,c.date as lms_collect_date\r\n",
							"    ,c.cservice as lms_collect_cservice\r\n",
							"    ,c.billcust as lms_collect_billcust\r\n",
							"    ,c.pickupcustid as lms_collect_pickupcustid\r\n",
							"    ,c.delivercustid as lms_collect_delivercustid\r\n",
							"    ,c.locationid as lms_collect_locationid\r\n",
							"    ,c.noofparcels as lms_collect_noofparcels\r\n",
							"    ,c.weight as lms_collect_weight\r\n",
							"    ,c.timeready as lms_collect_timeready\r\n",
							"    -- ,c.vehiclecapacity as lms_collect_vehiclecapacity\r\n",
							"    ,c.collectrefno as lms_collect_collectrefno\r\n",
							"    ,c.timeclosed as lms_collect_timeclosed\r\n",
							"    -- ,c.securitycollect as lms_collect_securitycollect\r\n",
							"    -- ,c.confirmedby as lms_collect_confirmedby\r\n",
							"    ,c.courierid as lms_collect_courierid\r\n",
							"    ,c.confirmeddate as lms_collect_confirmeddate\r\n",
							"    -- ,c.transferreddate as lms_collect_transferreddate\r\n",
							"    ,c.senddate as lms_collect_senddate\r\n",
							"    ,c.orderid as lms_collect_orderid\r\n",
							"    -- ,c.orderdate as lms_collect_orderdate\r\n",
							"    -- ,c.userid as lms_collect_userid\r\n",
							"    ,c.reason as lms_collect_reason\r\n",
							"    ,c.cubes as lms_collect_cubes\r\n",
							"    ,c.custref as lms_collect_custref\r\n",
							"    ,c.custref3 as lms_collect_custref3\r\n",
							"    ,c.custaccid as lms_collect_custaccid\r\n",
							"    ,c.collectparentid as lms_collect_collectparentid\r\n",
							"    ,c.duedate as lms_collect_duedate\r\n",
							"    -- ,c.specdel as lms_collect_specdel\r\n",
							"    -- ,c.flunits as lms_collect_flunits\r\n",
							"    -- ,c.flkgs as lms_collect_flkgs\r\n",
							"    -- ,c.quote as lms_collect_quote\r\n",
							"    -- ,c.bill_vehicletype as lms_collect_bill_vehicletype\r\n",
							"    ,c.collectstatusid as lms_collect_collectstatusid\r\n",
							"    ,CASE \r\n",
							"        WHEN c.collectstatusid = 0 THEN 'Created' \r\n",
							"        WHEN c.collectstatusid = 1 THEN 'On Collection' \r\n",
							"        WHEN c.collectstatusid = 2 THEN 'Collected' \r\n",
							"        WHEN c.collectstatusid = 3 THEN 'Complete' \r\n",
							"        WHEN c.collectstatusid = 4 THEN 'Cancelled' \r\n",
							"        WHEN c.collectstatusid = 5 THEN 'Failed' \r\n",
							"        ELSE c.collectstatusid\r\n",
							"    END as lms_collect_collectstatus\r\n",
							"    ,c.bill_collectstatusid as lms_collect_bill_collectstatusid\r\n",
							"    ,c.cancelreason as lms_collect_cancelreason\r\n",
							"    ,c.cancelcomments as lms_collect_cancelcomments\r\n",
							"    ,c.loadid as lms_collect_loadid\r\n",
							"    ,c.custref2 as lms_collect_custref2\r\n",
							"    ,c.created as lms_collect_created\r\n",
							"    ,c.oncollection as lms_collect_oncollection\r\n",
							"    ,c.collected as lms_collect_collected\r\n",
							"    ,c.complete as lms_collect_complete\r\n",
							"    ,c.cancelled as lms_collect_cancelled\r\n",
							"    ,c.failed as lms_collect_failed\r\n",
							"    ,c.ctype as lms_collect_ctype\r\n",
							"    ,c.quoteid as lms_collect_quoteid\r\n",
							"    ,c.lastupdate as lms_collect_lastupdate\r\n",
							"    ,c.notes as lms_collect_notes\r\n",
							"    ,c.docurl as lms_collect_docurl\r\n",
							"    ,c.docdate as lms_collect_docdate\r\n",
							"    -- ,c.orvstatus as lms_collect_orvstatus\r\n",
							"    -- ,c.orvcode as lms_collect_orvcode\r\n",
							"    ,c.oldloadid as lms_collect_oldloadid\r\n",
							"\r\n",
							"    ,dc.id as orv_draftcollection_id\r\n",
							"    ,dc.tolocid as orv_draftcollection_tolocid\r\n",
							"    ,dc.fromlocid as orv_draftcollection_fromlocid\r\n",
							"    ,dc.loadid as orv_draftcollection_loadid\r\n",
							"    ,dc.status as orv_draftcollection_status\r\n",
							"    ,to_timestamp(dc.importedate) as orv_draftcollection_importedate\r\n",
							"    ,dc.fromstr as orv_draftcollection_fromstr\r\n",
							"    ,dc.tostr as orv_draftcollection_tostr\r\n",
							"    ,dc.did as orv_draftcollection_did\r\n",
							"    ,dc.mergedlmsid as orv_draftcollection_mergedlmsid\r\n",
							"    ,to_timestamp(dc.rejectdate) as orv_draftcollection_rejectdate\r\n",
							"    ,to_timestamp(dc.confirmdate) as orv_draftcollection_confirmdate\r\n",
							"    ,dc.nocollection as orv_draftcollection_nocollection\r\n",
							"    ,dc.customerref as orv_draftcollection_customerref\r\n",
							"    ,dc.collectionwaybillid as orv_draftcollection_collectionwaybillid\r\n",
							"    ,to_timestamp(dc.collectbydate) as orv_draftcollection_collectbydate\r\n",
							"    ,dc.rejectreason as orv_draftcollection_rejectreason\r\n",
							"    ,dc.cancelctr as orv_draftcollection_cancelctr\r\n",
							"    ,dc.courierid as orv_draftcollection_courierid\r\n",
							"    ,dc.collectstatusid as orv_draftcollection_collectstatusid\r\n",
							"    ,d.id as orv_delivery_id\r\n",
							"    ,d.did as orv_delivery_did\r\n",
							"    ,d.waybillid as orv_delivery_waybillid\r\n",
							"    ,d.address as orv_delivery_address\r\n",
							"    ,d.town as orv_delivery_town\r\n",
							"    ,d.province as orv_delivery_province\r\n",
							"    ,d.country as orv_delivery_country\r\n",
							"    ,d.lat as orv_delivery_lat\r\n",
							"    ,d.lng as orv_delivery_lng\r\n",
							"    ,d.placeid as orv_delivery_placeid\r\n",
							"    ,d.what3words as orv_delivery_what3words\r\n",
							"    ,d.zipcode as orv_delivery_zipcode\r\n",
							"    ,to_timestamp(d.eta) as orv_delivery_eta\r\n",
							"    ,to_timestamp(d.actualeta) as orv_delivery_actualeta\r\n",
							"    ,to_timestamp(d.deliverydate) as orv_delivery_deliverydate\r\n",
							"    ,to_timestamp(d.deliverby) as orv_delivery_deliverby\r\n",
							"    ,d.deliveryorder as orv_delivery_deliveryorder\r\n",
							"    ,to_timestamp(d.tsgeofenceenter) as orv_delivery_tsgeofenceenter\r\n",
							"    ,to_timestamp(d.tsscanningstart) as orv_delivery_tsscanningstart\r\n",
							"    ,to_timestamp(d.tsscanningstop) as orv_delivery_tsscanningstop\r\n",
							"    ,to_timestamp(d.tspodsignature) as orv_delivery_tspodsignature\r\n",
							"    ,d.aid as orv_delivery_aid\r\n",
							"    ,d.numprcls as orv_delivery_numprcls\r\n",
							"    ,d.geocodingstatus as orv_delivery_geocodingstatus\r\n",
							"    ,to_timestamp(d.skipdate) as orv_delivery_skipdate\r\n",
							"    ,d.skipreason as orv_delivery_skipreason\r\n",
							"    ,d.status as orv_delivery_status\r\n",
							"    ,d.partialmatch as orv_delivery_partialmatch\r\n",
							"    ,d.approximated as orv_delivery_approximated\r\n",
							"    ,d.lmsorder as orv_delivery_lmsorder\r\n",
							"    ,d.optimizedorder as orv_delivery_optimizedorder\r\n",
							"    ,d.seal as orv_delivery_seal\r\n",
							"    ,d.customerinvoice as orv_delivery_customerinvoice\r\n",
							"    ,d.customerdn as orv_delivery_customerdn\r\n",
							"    ,d.grv as orv_delivery_grv\r\n",
							"    ,d.chepslip as orv_delivery_chepslip\r\n",
							"    ,d.cleandelivery as orv_delivery_cleandelivery\r\n",
							"    ,d.trackingcode as orv_delivery_trackingcode\r\n",
							"    ,d.mode as orv_delivery_mode\r\n",
							"    ,d.customorder as orv_delivery_customorder\r\n",
							"    ,d.verified as orv_delivery_verified\r\n",
							"    ,d.altered as orv_delivery_altered\r\n",
							"    ,d.orderid as orv_delivery_orderid\r\n",
							"    ,d.mergeid as orv_delivery_mergeid\r\n",
							"    ,d.orderref as orv_delivery_orderref\r\n",
							"    ,d.drivernote as orv_delivery_drivernote\r\n",
							"    ,d.guid as orv_delivery_guid\r\n",
							"    ,d.timedefinite as orv_delivery_timedefinite\r\n",
							"    ,d.orvcode as orv_delivery_orvcode\r\n",
							"    ,d.vaid as orv_delivery_vaid\r\n",
							"    ,to_timestamp(d.delegatedate) as orv_delivery_delegatedate\r\n",
							"    ,d.delegatemode as orv_delivery_delegatemode\r\n",
							"    ,d.debriefmode as orv_delivery_debriefmode\r\n",
							"    ,d.debriefed as orv_delivery_debriefed\r\n",
							"    ,to_timestamp(d.insdate) as orv_delivery_insdate\r\n",
							"    ,d.reviseddebrief as orv_delivery_reviseddebrief\r\n",
							"    ,d.lmsdebriefed as orv_delivery_lmsdebriefed\r\n",
							"    ,to_timestamp(d.uncanceldate) as orv_delivery_uncanceldate\r\n",
							"    ,to_timestamp(d.podlaterdate) as orv_delivery_podlaterdate\r\n",
							"    ,to_timestamp(d.podmanualdate) as orv_delivery_podmanualdate\r\n",
							"    ,d.manualdebriefreason as orv_delivery_manualdebriefreason\r\n",
							"    ,d.submanualdebriefreason as orv_delivery_submanualdebriefreason\r\n",
							"    ,d.outofgeofencereason as orv_delivery_outofgeofencereason\r\n",
							"    ,to_timestamp(d.tsentergeofenceapp) as orv_delivery_tsentergeofenceapp\r\n",
							"    ,to_timestamp(d.tsexitgeofenceapp) as orv_delivery_tsexitgeofenceapp\r\n",
							"    ,d.comebacklater as orv_delivery_comebacklater\r\n",
							"    ,d.uncancelusername as orv_delivery_uncancelusername\r\n",
							"    ,d.cref as orv_delivery_cref\r\n",
							"    ,d.division as orv_delivery_division\r\n",
							"    ,d.descriptionofgoods as orv_delivery_descriptionofgoods\r\n",
							"    ,d.skiplng as orv_delivery_skiplng\r\n",
							"    ,d.skiplat as orv_delivery_skiplat\r\n",
							"    ,d.courierid as orv_delivery_courierid\r\n",
							"    ,d.posreason as orv_delivery_posreason\r\n",
							"    ,to_timestamp(d.moddate) as orv_delivery_moddate\r\n",
							"    ,d.mallid as orv_delivery_mallid\r\n",
							"    ,d.mallgid as orv_delivery_mallgid\r\n",
							"    ,d.lmsidskipreason as orv_delivery_lmsidskipreason\r\n",
							"    ,d.deleteforimport as orv_delivery_deleteforimport\r\n",
							"    ,d.podoutofgeofence as orv_delivery_podoutofgeofence\r\n",
							"    ,d.originalcustomerid as orv_delivery_originalcustomerid\r\n",
							"    ,p.id as orv_pod_id\r\n",
							"    ,p.name as orv_pod_name\r\n",
							"    ,to_timestamp(p.poddate) as orv_pod_poddate\r\n",
							"    ,p.lat as orv_pod_lat\r\n",
							"    ,p.lng as orv_pod_lng\r\n",
							"    ,p.customerrating as orv_pod_customerrating\r\n",
							"    ,p.customerfeedback as orv_pod_customerfeedback\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"FROM dbocollect c\r\n",
							"LEFT JOIN draftcollection_tmp dc on dc.collectionwaybillid = c.id and dc.row_num = 1\r\n",
							"LEFT JOIN publicdelivery d on d.waybillid =  dc.collectionwaybillid and d.mode = 'collect'\r\n",
							"LEFT JOIN pod_tmp p on p.did = d.id and p.row_num = 1\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stcollection = spark.sql(\"SELECT * FROM stcollection\")"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stcollection.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stcollection.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"SUCCESS\")"
						],
						"outputs": [],
						"execution_count": 13
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/11_STDocumentTypes')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Finance"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "f4bbbdfc-02ec-41e6-91b0-7b7be3896b25"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse_Prod/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace-prod/bigDataPools/PRDSparkPool",
						"name": "PRDSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/PRDSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"# #Create DataFrame for the sttransactionssummary SAP Table\r\n",
							"# sttransactionssummary = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/FINANCE/sttransactionssummary.parquet', format='parquet')\r\n",
							"# sttransactionssummary.createOrReplaceTempView(\"sttransactionssummary\")"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stdocumentcategories\r\n",
							"AS\r\n",
							"SELECT DISTINCT \r\n",
							"   t.sap_transactionsline_doccat \r\n",
							"  ,CASE \r\n",
							"      WHEN t.sap_transactionsline_doccat  = 'generaljournal' THEN 'General Journal'\r\n",
							"      WHEN t.sap_transactionsline_doccat  = 'manualjournal' THEN 'Manual Journal'\r\n",
							"      WHEN t.sap_transactionsline_doccat  = 'purchaseorder' THEN 'Purchase Order'\r\n",
							"      WHEN t.sap_transactionsline_doccat  = 'purchaseinvoice' THEN 'Purchase Invoice'\r\n",
							"      WHEN t.sap_transactionsline_doccat  = 'goodsissue' THEN 'Goods Issue'\r\n",
							"      WHEN t.sap_transactionsline_doccat  = 'salesinvoice' THEN 'Sales Invoice'\r\n",
							"      WHEN t.sap_transactionsline_doccat  = 'salescredit' THEN 'Sales Credit'\r\n",
							"      WHEN t.sap_transactionsline_doccat  = 'directposting' THEN 'Direct Posting'\r\n",
							"      WHEN t.sap_transactionsline_doccat  = 'purchasepricevariance' THEN 'Purchase Price Variance'\r\n",
							"      WHEN t.sap_transactionsline_doccat  = 'goodsreceiptnote' THEN 'Goods Receipt Note'\r\n",
							"      WHEN t.sap_transactionsline_doccat  = 'goodsreceipt' THEN 'Goods Receipt'\r\n",
							"      WHEN t.sap_transactionsline_doccat  = 'stockrevaluation' THEN 'Stock Revaluation'\r\n",
							"      WHEN t.sap_transactionsline_doccat  = 'purchasecredit' THEN 'Purchase Credit'\r\n",
							"      WHEN t.sap_transactionsline_doccat  = 'discountjournal' THEN 'Discount Journal'\r\n",
							"\tEND as sap_transactionsline_doccatname\r\n",
							"\r\n",
							"FROM sttransactionssummary t\r\n",
							"WHERE t.sap_transactionsline_doccat  IS NOT NULL\r\n",
							"ORDER BY t.sap_transactionsline_doccat \r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stdocumentcategories = spark.sql(\"SELECT * FROM stdocumentcategories\")"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stdocumentcategories.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stdocumentcategories.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"SUCCESS\")"
						],
						"outputs": [],
						"execution_count": 6
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/12_DIST_ST3MonthRoling')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Ops/Distribution"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "cb5c8f60-199a-4d7f-bad2-5b753b7abb9c"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse_Prod/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace-prod/bigDataPools/PRDSparkPool",
						"name": "PRDSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/PRDSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# #create dataframe for the stparcel lms table\r\n",
							"# stparcel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stparcel.parquet/', format='parquet')\r\n",
							"# stparcel.createOrReplaceTempView(\"stparcel\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stlmstrack LMS Table\r\n",
							"# stlmstrack = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stlmstrack.parquet', format='parquet')\r\n",
							"# stlmstrack.createOrReplaceTempView(\"stlmstrack\")\r\n",
							"\r\n",
							"# #create dataframe for the stconsignment lms table\r\n",
							"# stconsignment = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stconsignment.parquet/', format='parquet')\r\n",
							"# stconsignment.createOrReplaceTempView(\"stconsignment\")\r\n",
							"\r\n",
							"# #create dataframe for the stwaybill lms table\r\n",
							"# stwaybill = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stwaybill.parquet/', format='parquet')\r\n",
							"# stwaybill.createOrReplaceTempView(\"stwaybill\")\r\n",
							"\r\n",
							"# #create dataframe for the storder lms table\r\n",
							"# storder = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/storder.parquet/', format='parquet')\r\n",
							"# storder.createOrReplaceTempView(\"storder\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stloadparent LMS Table\r\n",
							"# stloadparent = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stloadparent.parquet', format='parquet')\r\n",
							"# stloadparent.createOrReplaceTempView(\"stloadparent\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stloadchild LMS Table\r\n",
							"# stloadchild = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stloadchild.parquet', format='parquet')\r\n",
							"# stloadchild.createOrReplaceTempView(\"stloadchild\")\r\n",
							"\r\n",
							"# # Create DataFrame for the stcollection LMS Table\r\n",
							"# stcollection = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stcollection.parquet', format='parquet')\r\n",
							"# stcollection.createOrReplaceTempView(\"stcollection\")\r\n",
							"\r\n",
							"# # Create DataFrame for the stdamagedparcelaudit LMS Table\r\n",
							"# stdamagedparcelaudit = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stdamagedparcelaudit.parquet', format='parquet')\r\n",
							"# stdamagedparcelaudit.createOrReplaceTempView(\"stdamagedparcelaudit\")"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# from pyspark.sql import Window\r\n",
							"# from pyspark.sql import functions as F\r\n",
							"# my_window = (Window.orderBy('daymonth').rowsBetween(Window.unboundedPreceding, 0))"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"PARCEL CURRENT AND PRIOR YEAR 3MONTHS"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stparcel3myoy\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    *\r\n",
							"FROM stparcel\r\n",
							"WHERE \r\n",
							"(\r\n",
							"    lms_parcel_pdate >= (to_timestamp(year(add_months(current_date(),-3))||'-'||month(add_months(current_date(),-3))||'-'||'01'))\r\n",
							"AND \r\n",
							"    lms_parcel_pdate <= (to_timestamp(year(current_date())||'-'||month(current_date())||'-'||day(current_date())||'T23:59:59'))\r\n",
							")\r\n",
							"OR \r\n",
							"(\r\n",
							"    lms_parcel_pdate >= (to_timestamp(year(add_months(current_date(),-3))-1||'-'||month(add_months(current_date(),-3))||'-'||'01'))\r\n",
							"AND \r\n",
							"    lms_parcel_pdate <= (to_timestamp(year(current_date())-1||'-'||month(current_date())||'-'||day(current_date())||'T23:59:59'))\r\n",
							")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 30
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stparcel3myoy = spark.sql(\"SELECT * FROM stparcel3myoy\")"
						],
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stparcel3myoy.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stparcel3myoy.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 32
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"PARCEL DAMAGED AUDIT CURRENT AND PRIOR YEAR 3 MONTHS"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stdamagedparcelaudit3myoy_tmp1\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    lms_damagedparcelaudit_parcelid\r\n",
							"FROM stdamagedparcelaudit\r\n",
							"WHERE \r\n",
							"(\r\n",
							"    lms_damagedparcelaudit_auditdatetime >= (to_timestamp(year(add_months(current_date(),-3))||'-'||month(add_months(current_date(),-3))||'-'||'01'))\r\n",
							"AND \r\n",
							"    lms_damagedparcelaudit_auditdatetime <= (to_timestamp(year(current_date())||'-'||month(current_date())||'-'||day(current_date())||'T23:59:59'))\r\n",
							")\r\n",
							"OR \r\n",
							"(\r\n",
							"    lms_damagedparcelaudit_auditdatetime >= (to_timestamp(year(add_months(current_date(),-3))-1||'-'||month(add_months(current_date(),-3))||'-'||'01'))\r\n",
							"AND \r\n",
							"    lms_damagedparcelaudit_auditdatetime <= (to_timestamp(year(current_date())-1||'-'||month(current_date())||'-'||day(current_date())||'T23:59:59'))\r\n",
							")"
						],
						"outputs": [],
						"execution_count": 34
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stdamagedparcelaudit3myoy_tmp2\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    lms_parcel_id as lms_damagedparcelaudit_parcelid\r\n",
							"FROM stparcel3myoy\r\n",
							"WHERE lms_parcel_id IS NOT NULL\r\n",
							""
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stdamagedparcelaudit3myoy_tmp3 = spark.sql(\"SELECT * FROM stdamagedparcelaudit3myoy_tmp1 UNION SELECT * FROM stdamagedparcelaudit3myoy_tmp2\")"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stdamagedparcelaudit3myoy_tmp3.createOrReplaceTempView(\"stdamagedparcelaudit3myoy_tmp3\")"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stdamagedparcelaudit3myoy\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    d.*\r\n",
							"FROM stdamagedparcelaudit3myoy_tmp3 t\r\n",
							"LEFT JOIN stdamagedparcelaudit d on d.lms_damagedparcelaudit_parcelid = t.lms_damagedparcelaudit_parcelid\r\n",
							"WHERE d.lms_damagedparcelaudit_parcelid IS NOT NULL"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stdamagedparcelaudit3myoy = spark.sql(\"SELECT * FROM stdamagedparcelaudit3myoy\")"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stdamagedparcelaudit3myoy.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stdamagedparcelaudit3myoy.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"CONSIGNEMT CURRENT AND PRIOR YEAR 3MONTHS"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stconsignment3myoy_tmp1\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    lms_consignment_id\r\n",
							"FROM stconsignment\r\n",
							"\r\n",
							"WHERE \r\n",
							"(\r\n",
							"    lms_consignment_cdate >= (to_timestamp(year(add_months(current_date(),-3))||'-'||month(add_months(current_date(),-3))||'-'||'01'))\r\n",
							"AND \r\n",
							"    lms_consignment_cdate <= (to_timestamp(year(current_date())||'-'||month(current_date())||'-'||day(current_date())||'T23:59:59'))\r\n",
							")\r\n",
							"OR \r\n",
							"(\r\n",
							"    lms_consignment_cdate >= (to_timestamp(year(add_months(current_date(),-3))-1||'-'||month(add_months(current_date(),-3))||'-'||'01'))\r\n",
							"AND \r\n",
							"    lms_consignment_cdate <= (to_timestamp(year(current_date())-1||'-'||month(current_date())||'-'||day(current_date())||'T23:59:59'))\r\n",
							")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stconsignment3myoy_tmp2\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    lms_parcel_consignid as lms_consignment_id\r\n",
							"FROM stparcel3myoy\r\n",
							"WHERE lms_parcel_consignid IS NOT NULL\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stconsignment3myoy_tmp3 = spark.sql(\"SELECT * FROM stconsignment3myoy_tmp1 UNION SELECT * FROM stconsignment3myoy_tmp2\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stconsignment3myoy_tmp3.createOrReplaceTempView(\"stconsignment3myoy_tmp3\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stconsignment3myoy\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    c.*\r\n",
							"FROM stconsignment3myoy_tmp3 t\r\n",
							"LEFT JOIN stconsignment c on c.lms_consignment_id = t.lms_consignment_id\r\n",
							"WHERE c.lms_consignment_id IS NOT NULL"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stconsignment3myoy = spark.sql(\"SELECT * FROM stconsignment3myoy\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stconsignment3myoy.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stconsignment3myoy.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"WAYBILL CURRENT AND PRIOR YEAR 3MONTHS"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stwaybill3myoy_tmp1\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    lms_waybill_id\r\n",
							"FROM stwaybill\r\n",
							"WHERE \r\n",
							"(\r\n",
							"    lms_waybill_date >= (to_timestamp(year(add_months(current_date(),-3))||'-'||month(add_months(current_date(),-3))||'-'||'01'))\r\n",
							"AND \r\n",
							"    lms_waybill_date <= (to_timestamp(year(current_date())||'-'||month(current_date())||'-'||day(current_date())||'T23:59:59'))\r\n",
							")\r\n",
							"OR \r\n",
							"(\r\n",
							"    lms_waybill_date >= (to_timestamp(year(add_months(current_date(),-3))-1||'-'||month(add_months(current_date(),-3))||'-'||'01'))\r\n",
							"AND \r\n",
							"    lms_waybill_date <= (to_timestamp(year(current_date())-1||'-'||month(current_date())||'-'||day(current_date())||'T23:59:59'))\r\n",
							")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stwaybill3myoy_tmp2\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    lms_parcel_waybillid as lms_waybill_id\r\n",
							"FROM stparcel3myoy\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stwaybill3myoy_tmp3 = spark.sql(\"SELECT * FROM stwaybill3myoy_tmp1 UNION SELECT * FROM stwaybill3myoy_tmp2\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stwaybill3myoy_tmp3.createOrReplaceTempView(\"stwaybill3myoy_tmp3\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stwaybill3myoy\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    w.*\r\n",
							"FROM stwaybill3myoy_tmp3 t\r\n",
							"LEFT JOIN stwaybill w on w.lms_waybill_id = t.lms_waybill_id\r\n",
							"WHERE w.lms_waybill_id IS NOT NULL"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stwaybill3myoy = spark.sql(\"SELECT * FROM stwaybill3myoy\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stwaybill3myoy.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stwaybill3myoy.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"ORDER CURRENT AND PRIOR YEAR 3MONTHS"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"storder3myoy_tmp1\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    lms_order_id\r\n",
							"FROM storder\r\n",
							"WHERE \r\n",
							"(\r\n",
							"    lms_order_hodate >= (to_timestamp(year(add_months(current_date(),-3))||'-'||month(add_months(current_date(),-3))||'-'||'01'))\r\n",
							"AND \r\n",
							"    lms_order_hodate <= (to_timestamp(year(current_date())||'-'||month(current_date())||'-'||day(current_date())||'T23:59:59'))\r\n",
							")\r\n",
							"OR \r\n",
							"(\r\n",
							"    lms_order_hodate >= (to_timestamp(year(add_months(current_date(),-3))-1||'-'||month(add_months(current_date(),-3))||'-'||'01'))\r\n",
							"AND \r\n",
							"    lms_order_hodate <= (to_timestamp(year(current_date())-1||'-'||month(current_date())||'-'||day(current_date())||'T23:59:59'))\r\n",
							")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"storder3myoy_tmp2\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    lms_parcel_waybillid as lms_waybill_id\r\n",
							"FROM stparcel3myoy"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"storder3myoy_tmp3 = spark.sql(\"SELECT * FROM storder3myoy_tmp1 UNION SELECT * FROM storder3myoy_tmp2\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"storder3myoy_tmp3.createOrReplaceTempView(\"storder3myoy_tmp3\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"storder3myoy\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    o.*\r\n",
							"FROM storder3myoy_tmp3 t\r\n",
							"LEFT JOIN storder o on o.lms_order_id = t.lms_order_id\r\n",
							"WHERE o.lms_order_id IS NOT NULL"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"storder3myoy = spark.sql(\"SELECT * FROM storder3myoy\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# storder3myoy.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/storder3myoy.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"TRACK CURRENT AND PRIOR YEAR 3MONTHS"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stlmstrack3myoy_tmp1\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    lms_track_parcelid,\r\n",
							"    lms_track_loadid\r\n",
							"FROM stlmstrack\r\n",
							"WHERE \r\n",
							"(\r\n",
							"    lms_track_opendt >= (to_timestamp(year(add_months(current_date(),-3))||'-'||month(add_months(current_date(),-3))||'-'||'01'))\r\n",
							"AND \r\n",
							"    lms_track_opendt <= (to_timestamp(year(current_date())||'-'||month(current_date())||'-'||day(current_date())||'T23:59:59'))\r\n",
							")\r\n",
							"OR \r\n",
							"(\r\n",
							"    lms_track_opendt >= (to_timestamp(year(add_months(current_date(),-3))-1||'-'||month(add_months(current_date(),-3))||'-'||'01'))\r\n",
							"AND \r\n",
							"    lms_track_opendt <= (to_timestamp(year(current_date())-1||'-'||month(current_date())||'-'||day(current_date())||'T23:59:59'))\r\n",
							")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stlmstrack3myoy_tmp = spark.sql(\"SELECT * FROM stlmstrack3myoy_tmp1\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stlmstrack3myoy_tmp.createOrReplaceTempView(\"stlmstrack3myoy_tmp\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stlmstrack3myoy_tmp2\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    lms_parcel_id as lms_track_parcelid\r\n",
							"FROM stparcel3myoy "
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stlmstrack3myoy_tmp3 = spark.sql(\"SELECT lms_track_parcelid FROM stlmstrack3myoy_tmp1 UNION SELECT * FROM stlmstrack3myoy_tmp2\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stlmstrack3myoy_tmp3.createOrReplaceTempView(\"stlmstrack3myoy_tmp3\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stlmstrack3myoy\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    t.*\r\n",
							"FROM stlmstrack3myoy_tmp3 t1\r\n",
							"LEFT JOIN stlmstrack t on t.lms_track_parcelid = t1.lms_track_parcelid\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stlmstrack3myoy = spark.sql(\"SELECT * FROM stlmstrack3myoy\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stlmstrack3myoy.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stlmstrack3myoy.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"LOAD CHILD CURRENT AND PRIOR YEAR 3MONTHS"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stloadchild3myoy_tmp1\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    lms_loadchild_id,\r\n",
							"    lms_loadchild_parentloadid\r\n",
							"FROM stloadchild\r\n",
							"WHERE \r\n",
							"(\r\n",
							"    lms_loadchild_ldate >= (to_timestamp(year(add_months(current_date(),-3))||'-'||month(add_months(current_date(),-3))||'-'||'01'))\r\n",
							"AND \r\n",
							"    lms_loadchild_ldate <= (to_timestamp(year(current_date())||'-'||month(current_date())||'-'||day(current_date())||'T23:59:59'))\r\n",
							")\r\n",
							"OR \r\n",
							"(\r\n",
							"    lms_loadchild_ldate >= (to_timestamp(year(add_months(current_date(),-3))-1||'-'||month(add_months(current_date(),-3))||'-'||'01'))\r\n",
							"AND \r\n",
							"    lms_loadchild_ldate <= (to_timestamp(year(current_date())-1||'-'||month(current_date())||'-'||day(current_date())||'T23:59:59'))\r\n",
							")\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stloadchild3myoy_tmp = spark.sql(\"SELECT * FROM stloadchild3myoy_tmp1\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stloadchild3myoy_tmp.createOrReplaceTempView(\"stloadchild3myoy_tmp\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stloadchild3myoy_tmp2\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    lms_track_loadid as lms_loadchild_id\r\n",
							"FROM stlmstrack3myoy_tmp"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stloadchild3myoy_tmp3 = spark.sql(\"SELECT lms_loadchild_id FROM stloadchild3myoy_tmp1 UNION SELECT * FROM stloadchild3myoy_tmp2\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stloadchild3myoy_tmp3.createOrReplaceTempView(\"stloadchild3myoy_tmp3\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stloadchild3myoy\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    lc.*\r\n",
							"FROM stloadchild3myoy_tmp3 t\r\n",
							"LEFT JOIN stloadchild lc on lc.lms_loadchild_id = t.lms_loadchild_id\r\n",
							"WHERE lc.lms_loadchild_id IS NOT NULL\r\n",
							"AND lc.lms_loadchild_ldate IS NOT NULL"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stloadchild3myoy = spark.sql(\"SELECT * FROM stloadchild3myoy\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stloadchild3myoy.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stloadchild3myoy.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"LOAD PARENT CURRENT AND PRIOR YEAR 3MONTHS"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stloadparent3myoy_tmp1\r\n",
							"AS\r\n",
							"SELECT DISTINCT\r\n",
							"    lms_loadchild_parentloadid as lms_loadparent_parentloadid\r\n",
							"FROM stloadchild3myoy_tmp \r\n",
							"WHERE lms_loadchild_parentloadid IS NOT NULL"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stloadparent3myoy_tmp2\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    lms_track_loadid as lms_loadparent_parentloadid\r\n",
							"FROM stlmstrack3myoy_tmp\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stloadparent3myoy_tmp3 = spark.sql(\"SELECT * FROM stloadparent3myoy_tmp1 UNION SELECT * FROM stloadparent3myoy_tmp2\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stloadparent3myoy_tmp3.createOrReplaceTempView(\"stloadparent3myoy_tmp3\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stloadparent3myoy\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    lp.*\r\n",
							"FROM stloadparent3myoy_tmp3 t\r\n",
							"LEFT JOIN stloadparent lp on lp.lms_loadparent_parentloadid = t.lms_loadparent_parentloadid\r\n",
							"WHERE lp.lms_loadparent_parentloadid IS NOT NULL"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stloadparent3myoy = spark.sql(\"SELECT * FROM stloadparent3myoy\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stloadparent3myoy.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stloadparent3myoy.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stcollection3myoy\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    *\r\n",
							"FROM stcollection\r\n",
							"WHERE \r\n",
							"(\r\n",
							"    lms_collect_date >= (to_timestamp(year(add_months(current_date(),-3))||'-'||month(add_months(current_date(),-3))||'-'||'01'))\r\n",
							"AND \r\n",
							"    lms_collect_date <= (to_timestamp(year(current_date())||'-'||month(current_date())||'-'||day(current_date())||'T23:59:59'))\r\n",
							")\r\n",
							"OR \r\n",
							"(\r\n",
							"    lms_collect_date >= (to_timestamp(year(add_months(current_date(),-3))-1||'-'||month(add_months(current_date(),-3))||'-'||'01'))\r\n",
							"AND \r\n",
							"    lms_collect_date <= (to_timestamp(year(current_date())-1||'-'||month(current_date())||'-'||day(current_date())||'T23:59:59'))\r\n",
							")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stcollection3myoy = spark.sql(\"SELECT * FROM stcollection3myoy\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stcollection3myoy.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stcollection3myoy.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**<mark>CUMULATIVE TOTALS BY DAY</mark>**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"staggrigateparcelbyordermeasurescy\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"     date_format(lms_parcel_orderhodate, \"MMdd\") as daymonth\r\n",
							"    ,count(DISTINCT lms_parcel_orderid) as lms_parcel_ho_orders_cy\r\n",
							"    ,count(DISTINCT lms_parcel_consignid) as lms_parcel_ho_consignments_cy\r\n",
							"    ,count(DISTINCT lms_parcel_id) as lms_parcel_ho_parcels_cy\r\n",
							"    ,count(DISTINCT lms_parcel_waybillid) as lms_parcel_ho_waybills_cy\r\n",
							"    ,sum(lms_parcel_weight) as lms_parcel_ho_weight_cy\r\n",
							"    ,sum(lms_parcel_chargeweight) as lms_parcel_ho_chargeweight_cy\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"FROM stparcel3myoy\r\n",
							"\r\n",
							"WHERE \r\n",
							"(\r\n",
							"    lms_parcel_orderhodate >= (to_timestamp(year(add_months(current_date(),-3))||'-'||month(add_months(current_date(),-3))||'-'||'01'))\r\n",
							"AND \r\n",
							"    lms_parcel_orderhodate <= (to_timestamp(year(current_date())||'-'||month(current_date())||'-'||day(current_date())||'T23:59:59'))\r\n",
							")\r\n",
							"\r\n",
							"GROUP BY date_format(lms_parcel_orderhodate, \"MMdd\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"staggrigateparcelbyordermeasurescy = spark.sql(\"SELECT * FROM staggrigateparcelbyordermeasurescy\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"staggrigateparcelbyordermeasurescycum = staggrigateparcelbyordermeasurescy.withColumn('lms_parcel_ho_orders_cy_cum', F.sum('lms_parcel_ho_orders_cy').over(my_window))\\\r\n",
							".withColumn('lms_parcel_ho_consignments_cy_cum', F.sum('lms_parcel_ho_consignments_cy').over(my_window))\\\r\n",
							".withColumn('lms_parcel_ho_parcels_cy_cum', F.sum('lms_parcel_ho_parcels_cy').over(my_window))\\\r\n",
							".withColumn('lms_parcel_ho_waybills_cy_cum', F.sum('lms_parcel_ho_waybills_cy').over(my_window))\\\r\n",
							".withColumn('lms_parcel_ho_weight_cy_cum', F.sum('lms_parcel_ho_weight_cy').over(my_window))\\\r\n",
							".withColumn('lms_parcel_ho_chargeweight_cy_cum', F.sum('lms_parcel_ho_chargeweight_cy').over(my_window))\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"##staggrigateparcelbyordermeasurescycum.createOrReplaceTempView(\"staggrigateparcelbyordermeasurescycum\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"staggrigateparcelbyordermeasurespy\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"     date_format(lms_parcel_orderhodate, \"MMdd\") as daymonth\r\n",
							"    ,count(DISTINCT lms_parcel_orderid) as lms_parcel_ho_orders_py\r\n",
							"    ,count(DISTINCT lms_parcel_consignid) as lms_parcel_ho_consignments_py\r\n",
							"    ,count(DISTINCT lms_parcel_id) as lms_parcel_ho_parcels_py\r\n",
							"    ,count(DISTINCT lms_parcel_waybillid) as lms_parcel_ho_waybills_py\r\n",
							"    ,sum(lms_parcel_weight) as lms_parcel_ho_weight_py\r\n",
							"    ,sum(lms_parcel_chargeweight) as lms_parcel_ho_chargeweight_py\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"FROM stparcel3myoy\r\n",
							"\r\n",
							"WHERE \r\n",
							"(\r\n",
							"    lms_parcel_orderhodate >= (to_timestamp(year(add_months(current_date(),-3))-1||'-'||month(add_months(current_date(),-3))||'-'||'01'))\r\n",
							"AND \r\n",
							"    lms_parcel_orderhodate <= (to_timestamp(year(current_date())-1||'-'||month(current_date())||'-'||day(current_date())||'T23:59:59'))\r\n",
							")\r\n",
							"\r\n",
							"GROUP BY date_format(lms_parcel_orderhodate, \"MMdd\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"staggrigateparcelbyordermeasurespy = spark.sql(\"SELECT * FROM staggrigateparcelbyordermeasurespy\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"staggrigateparcelbyordermeasurespycum = staggrigateparcelbyordermeasurespy.withColumn('lms_parcel_ho_orders_py_cum', F.sum('lms_parcel_ho_orders_py').over(my_window))\\\r\n",
							".withColumn('lms_parcel_ho_consignments_py_cum', F.sum('lms_parcel_ho_consignments_py').over(my_window))\\\r\n",
							".withColumn('lms_parcel_ho_parcels_py_cum', F.sum('lms_parcel_ho_parcels_py').over(my_window))\\\r\n",
							".withColumn('lms_parcel_ho_waybills_py_cum', F.sum('lms_parcel_ho_waybills_py').over(my_window))\\\r\n",
							".withColumn('lms_parcel_ho_weight_py_cum', F.sum('lms_parcel_ho_weight_py').over(my_window))\\\r\n",
							".withColumn('lms_parcel_ho_chargeweight_py_cum', F.sum('lms_parcel_ho_chargeweight_py').over(my_window))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"##staggrigateparcelbyordermeasurespycum.createOrReplaceTempView(\"staggrigateparcelbyordermeasurespycum\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"staggrigateparcelbyparcelmeasurescy\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"     date_format(lms_parcel_pdate, \"MMdd\") as daymonth\r\n",
							"    ,count(DISTINCT lms_parcel_orderid) as lms_parcel_p_orders_cy\r\n",
							"    ,count(DISTINCT lms_parcel_consignid) as lms_parcel_p_consignments_cy\r\n",
							"    ,count(DISTINCT lms_parcel_id) as lms_parcel_p_parcels_cy\r\n",
							"    ,count(DISTINCT lms_parcel_waybillid) as lms_parcel_p_waybills_cy\r\n",
							"    ,sum(lms_parcel_weight) as lms_parcel_p_weight_cy\r\n",
							"    ,sum(lms_parcel_chargeweight) as lms_parcel_p_chargeweight_cy\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"FROM stparcel3myoy\r\n",
							"\r\n",
							"WHERE \r\n",
							"(\r\n",
							"    lms_parcel_pdate >= (to_timestamp(year(add_months(current_date(),-3))||'-'||month(add_months(current_date(),-3))||'-'||'01'))\r\n",
							"AND \r\n",
							"    lms_parcel_pdate <= (to_timestamp(year(current_date())||'-'||month(current_date())||'-'||day(current_date())||'T23:59:59'))\r\n",
							")\r\n",
							"\r\n",
							"GROUP BY date_format(lms_parcel_pdate, \"MMdd\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"staggrigateparcelbyparcelmeasurescy = spark.sql(\"SELECT * FROM staggrigateparcelbyparcelmeasurescy\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"staggrigateparcelbyparcelmeasurescycum = staggrigateparcelbyparcelmeasurescy.withColumn('lms_parcel_p_orders_cy_cum', F.sum('lms_parcel_p_orders_cy').over(my_window))\\\r\n",
							".withColumn('lms_parcel_p_consignments_cy_cum', F.sum('lms_parcel_p_consignments_cy').over(my_window))\\\r\n",
							".withColumn('lms_parcel_p_parcels_cy_cum', F.sum('lms_parcel_p_parcels_cy').over(my_window))\\\r\n",
							".withColumn('lms_parcel_p_waybills_cy_cum', F.sum('lms_parcel_p_waybills_cy').over(my_window))\\\r\n",
							".withColumn('lms_parcel_p_weight_cy_cum', F.sum('lms_parcel_p_weight_cy').over(my_window))\\\r\n",
							".withColumn('lms_parcel_p_chargeweight_cy_cum', F.sum('lms_parcel_p_chargeweight_cy').over(my_window))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"##staggrigateparcelbyparcelmeasurescycum.createOrReplaceTempView(\"staggrigateparcelbyparcelmeasurescycum\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"staggrigateparcelbyparcelmeasurespy\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"     date_format(lms_parcel_pdate, \"MMdd\") as daymonth\r\n",
							"    ,count(DISTINCT lms_parcel_orderid) as lms_parcel_p_orders_py\r\n",
							"    ,count(DISTINCT lms_parcel_consignid) as lms_parcel_p_consignments_py\r\n",
							"    ,count(DISTINCT lms_parcel_id) as lms_parcel_p_parcels_py\r\n",
							"    ,count(DISTINCT lms_parcel_waybillid) as lms_parcel_p_waybills_py\r\n",
							"    ,sum(lms_parcel_weight) as lms_parcel_p_weight_py\r\n",
							"    ,sum(lms_parcel_chargeweight) as lms_parcel_p_chargeweight_py\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"FROM stparcel3myoy\r\n",
							"\r\n",
							"WHERE \r\n",
							"(\r\n",
							"    lms_parcel_pdate >= (to_timestamp(year(add_months(current_date(),-3))-1||'-'||month(add_months(current_date(),-3))||'-'||'01'))\r\n",
							"AND \r\n",
							"    lms_parcel_pdate <= (to_timestamp(year(current_date())-1||'-'||month(current_date())||'-'||day(current_date())||'T23:59:59'))\r\n",
							")\r\n",
							"\r\n",
							"GROUP BY date_format(lms_parcel_pdate, \"MMdd\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"staggrigateparcelbyparcelmeasurespy = spark.sql(\"SELECT * FROM staggrigateparcelbyparcelmeasurespy\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"staggrigateparcelbyparcelmeasurespycum = staggrigateparcelbyparcelmeasurespy.withColumn('lms_parcel_p_orders_py_cum', F.sum('lms_parcel_p_orders_py').over(my_window))\\\r\n",
							".withColumn('lms_parcel_p_consignments_py_cum', F.sum('lms_parcel_p_consignments_py').over(my_window))\\\r\n",
							".withColumn('lms_parcel_p_parcels_py_cum', F.sum('lms_parcel_p_parcels_py').over(my_window))\\\r\n",
							".withColumn('lms_parcel_p_waybills_py_cum', F.sum('lms_parcel_p_waybills_py').over(my_window))\\\r\n",
							".withColumn('lms_parcel_p_weight_py_cum', F.sum('lms_parcel_p_weight_py').over(my_window))\\\r\n",
							".withColumn('lms_parcel_p_chargeweight_py_cum', F.sum('lms_parcel_p_chargeweight_py').over(my_window))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"##staggrigateparcelbyparcelmeasurespycum.createOrReplaceTempView(\"staggrigateparcelbyparcelmeasurespycum\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"staggrigateparcelbyconsignmentmeasurescy\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"     date_format(lms_parcel_consignmentcdate, \"MMdd\") as daymonth\r\n",
							"    ,count(DISTINCT lms_parcel_orderid) as lms_parcel_c_orders_cy\r\n",
							"    ,count(DISTINCT lms_parcel_consignid) as lms_parcel_c_consignments_cy\r\n",
							"    ,count(DISTINCT lms_parcel_id) as lms_parcel_c_parcels_cy\r\n",
							"    ,count(DISTINCT lms_parcel_waybillid) as lms_parcel_c_waybills_cy\r\n",
							"    ,sum(lms_parcel_weight) as lms_parcel_c_weight_cy\r\n",
							"    ,sum(lms_parcel_chargeweight) as lms_parcel_c_chargeweight_cy\r\n",
							"\r\n",
							"\r\n",
							"FROM stparcel3myoy\r\n",
							"\r\n",
							"WHERE \r\n",
							"(\r\n",
							"    lms_parcel_consignmentcdate >= (to_timestamp(year(add_months(current_date(),-3))||'-'||month(add_months(current_date(),-3))||'-'||'01'))\r\n",
							"AND \r\n",
							"    lms_parcel_consignmentcdate <= (to_timestamp(year(current_date())||'-'||month(current_date())||'-'||day(current_date())||'T23:59:59'))\r\n",
							")\r\n",
							"\r\n",
							"GROUP BY date_format(lms_parcel_consignmentcdate, \"MMdd\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"staggrigateparcelbyconsignmentmeasurescy = spark.sql(\"SELECT * FROM staggrigateparcelbyconsignmentmeasurescy\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"staggrigateparcelbyconsignmentmeasurescycum = staggrigateparcelbyconsignmentmeasurescy.withColumn('lms_parcel_c_orders_cy_cum', F.sum('lms_parcel_c_orders_cy').over(my_window))\\\r\n",
							".withColumn('lms_parcel_c_consignments_cy_cum', F.sum('lms_parcel_c_consignments_cy').over(my_window))\\\r\n",
							".withColumn('lms_parcel_c_parcels_cy_cum', F.sum('lms_parcel_c_parcels_cy').over(my_window))\\\r\n",
							".withColumn('lms_parcel_c_waybills_cy_cum', F.sum('lms_parcel_c_waybills_cy').over(my_window))\\\r\n",
							".withColumn('lms_parcel_c_weight_cy_cum', F.sum('lms_parcel_c_weight_cy').over(my_window))\\\r\n",
							".withColumn('lms_parcel_c_chargeweight_cy_cum', F.sum('lms_parcel_c_chargeweight_cy').over(my_window))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"##staggrigateparcelbyconsignmentmeasurescycum.createOrReplaceTempView(\"staggrigateparcelbyconsignmentmeasurescycum\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"staggrigateparcelbyconsignmentmeasurespy\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"     date_format(lms_parcel_consignmentcdate, \"MMdd\") as daymonth\r\n",
							"    ,count(DISTINCT lms_parcel_orderid) as lms_parcel_c_orders_py\r\n",
							"    ,count(DISTINCT lms_parcel_consignid) as lms_parcel_c_consignments_py\r\n",
							"    ,count(DISTINCT lms_parcel_id) as lms_parcel_c_parcels_py\r\n",
							"    ,count(DISTINCT lms_parcel_waybillid) as lms_parcel_c_waybills_py\r\n",
							"    ,sum(lms_parcel_weight) as lms_parcel_c_weight_py\r\n",
							"    ,sum(lms_parcel_chargeweight) as lms_parcel_c_chargeweight_py\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"FROM stparcel3myoy\r\n",
							"\r\n",
							"WHERE \r\n",
							"(\r\n",
							"    lms_parcel_consignmentcdate >= (to_timestamp(year(add_months(current_date(),-3))-1||'-'||month(add_months(current_date(),-3))||'-'||'01'))\r\n",
							"AND \r\n",
							"    lms_parcel_consignmentcdate <= (to_timestamp(year(current_date())-1||'-'||month(current_date())||'-'||day(current_date())||'T23:59:59'))\r\n",
							")\r\n",
							"\r\n",
							"GROUP BY date_format(lms_parcel_consignmentcdate, \"MMdd\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"staggrigateparcelbyconsignmentmeasurespy = spark.sql(\"SELECT * FROM staggrigateparcelbyconsignmentmeasurespy\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"staggrigateparcelbyconsignmentmeasurespycum = staggrigateparcelbyconsignmentmeasurespy.withColumn('lms_parcel_c_orders_py_cum', F.sum('lms_parcel_c_orders_py').over(my_window))\\\r\n",
							".withColumn('lms_parcel_c_consignments_py_cum', F.sum('lms_parcel_c_consignments_py').over(my_window))\\\r\n",
							".withColumn('lms_parcel_c_parcels_py_cum', F.sum('lms_parcel_c_parcels_py').over(my_window))\\\r\n",
							".withColumn('lms_parcel_c_waybills_py_cum', F.sum('lms_parcel_c_waybills_py').over(my_window))\\\r\n",
							".withColumn('lms_parcel_c_weight_py_cum', F.sum('lms_parcel_c_weight_py').over(my_window))\\\r\n",
							".withColumn('lms_parcel_c_chargeweight_py_cum', F.sum('lms_parcel_c_chargeweight_py').over(my_window))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"##staggrigateparcelbyconsignmentmeasurespycum.createOrReplaceTempView(\"staggrigateparcelbyconsignmentmeasurespycum\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"staggrigateparcelbywaybillmeasurescy\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"     date_format(lms_parcel_waybilldate, \"MMdd\") as daymonth\r\n",
							"    ,count(DISTINCT lms_parcel_id) as lms_parcel_wb_parcels_cy\r\n",
							"    ,count(DISTINCT lms_parcel_waybillid) as lms_parcel_wb_waybills_cy\r\n",
							"    ,sum(lms_parcel_weight) as lms_parcel_wb_weight_cy\r\n",
							"    ,sum(lms_parcel_chargeweight) as lms_parcel_wb_chargeweight_cy\r\n",
							"\r\n",
							"\r\n",
							"FROM stparcel3myoy\r\n",
							"\r\n",
							"WHERE \r\n",
							"(\r\n",
							"    lms_parcel_waybilldate >= (to_timestamp(year(add_months(current_date(),-3))||'-'||month(add_months(current_date(),-3))||'-'||'01'))\r\n",
							"AND \r\n",
							"    lms_parcel_waybilldate <= (to_timestamp(year(current_date())||'-'||month(current_date())||'-'||day(current_date())||'T23:59:59'))\r\n",
							")\r\n",
							"\r\n",
							"GROUP BY date_format(lms_parcel_waybilldate, \"MMdd\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"staggrigateparcelbywaybillmeasurescy = spark.sql(\"SELECT * FROM staggrigateparcelbywaybillmeasurescy\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"staggrigateparcelbywaybillmeasurescycum = staggrigateparcelbywaybillmeasurescy.withColumn('lms_parcel_wb_parcels_cy_cum', F.sum('lms_parcel_wb_parcels_cy').over(my_window))\\\r\n",
							".withColumn('lms_parcel_wb_waybills_cy_cum', F.sum('lms_parcel_wb_waybills_cy').over(my_window))\\\r\n",
							".withColumn('lms_parcel_wb_weight_cy_cum', F.sum('lms_parcel_wb_weight_cy').over(my_window))\\\r\n",
							".withColumn('lms_parcel_wb_chargeweight_cy_cum', F.sum('lms_parcel_wb_chargeweight_cy').over(my_window))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"##staggrigateparcelbywaybillmeasurescycum.createOrReplaceTempView(\"staggrigateparcelbywaybillmeasurescycum\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"staggrigateparcelbywaybillmeasurespy\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"     date_format(lms_parcel_waybilldate, \"MMdd\") as daymonth\r\n",
							"    ,count(DISTINCT lms_parcel_id) as lms_parcel_wb_parcels_py\r\n",
							"    ,count(DISTINCT lms_parcel_waybillid) as lms_parcel_wb_waybills_py\r\n",
							"    ,sum(lms_parcel_weight) as lms_parcel_wb_weight_py\r\n",
							"    ,sum(lms_parcel_chargeweight) as lms_parcel_wb_chargeweight_py\r\n",
							"\r\n",
							"FROM stparcel3myoy\r\n",
							"\r\n",
							"WHERE \r\n",
							"(\r\n",
							"    lms_parcel_waybilldate >= (to_timestamp(year(add_months(current_date(),-3))-1||'-'||month(add_months(current_date(),-3))||'-'||'01'))\r\n",
							"AND \r\n",
							"    lms_parcel_waybilldate <= (to_timestamp(year(current_date())-1||'-'||month(current_date())||'-'||day(current_date())||'T23:59:59'))\r\n",
							")\r\n",
							"\r\n",
							"GROUP BY date_format(lms_parcel_waybilldate, \"MMdd\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"staggrigateparcelbywaybillmeasurespy = spark.sql(\"SELECT * FROM staggrigateparcelbywaybillmeasurespy\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"staggrigateparcelbywaybillmeasurespycum = staggrigateparcelbywaybillmeasurespy.withColumn('lms_parcel_wb_parcels_py_cum', F.sum('lms_parcel_wb_parcels_py').over(my_window))\\\r\n",
							".withColumn('lms_parcel_wb_waybills_py_cum', F.sum('lms_parcel_wb_waybills_py').over(my_window))\\\r\n",
							".withColumn('lms_parcel_wb_weight_py_cum', F.sum('lms_parcel_wb_weight_py').over(my_window))\\\r\n",
							".withColumn('lms_parcel_wb_chargeweight_py_cum', F.sum('lms_parcel_wb_chargeweight_py').over(my_window))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"##staggrigateparcelbywaybillmeasurespycum.createOrReplaceTempView(\"staggrigateparcelbywaybillmeasurespycum\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"staggrigatetrackbylinehaulloadmeasurescy\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"     date_format(lms_track_opendt, \"MMdd\") as daymonth\r\n",
							"    ,count(DISTINCT lms_track_loadid) as lms_track_t_totallinehaulloads_cy\r\n",
							"    ,count(DISTINCT lms_track_parcelid) as lms_track_t_totallinehaulparcels_cy\r\n",
							"    ,sum(lms_track_parcelweight) as lms_track_t_totallinehaulweight_cy\r\n",
							"    ,sum(lms_track_parcelchargeweight) as lms_track_t_totallinehaulchargeweight_cy\r\n",
							"\r\n",
							"FROM stlmstrack3myoy\r\n",
							"\r\n",
							"WHERE \r\n",
							"(\r\n",
							"    lms_track_opendt >= (to_timestamp(year(add_months(current_date(),-3))||'-'||month(add_months(current_date(),-3))||'-'||'01'))\r\n",
							"AND \r\n",
							"    lms_track_opendt <= (to_timestamp(year(current_date())||'-'||month(current_date())||'-'||day(current_date())||'T23:59:59'))\r\n",
							")\r\n",
							"AND lms_track_tracktypeid = 2\r\n",
							"\r\n",
							"GROUP BY date_format(lms_track_opendt, \"MMdd\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"staggrigatetrackbylinehaulloadmeasurescy = spark.sql(\"SELECT * FROM staggrigatetrackbylinehaulloadmeasurescy\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"staggrigatetrackbylinehaulloadmeasurescycum = staggrigatetrackbylinehaulloadmeasurescy.withColumn('lms_track_t_totallinehaulloads_cy_cum', F.sum('lms_track_t_totallinehaulloads_cy').over(my_window))\\\r\n",
							".withColumn('lms_track_t_totallinehaulparcels_cy_cum', F.sum('lms_track_t_totallinehaulparcels_cy').over(my_window))\\\r\n",
							".withColumn('lms_track_t_totallinehaulweight_cy_cum', F.sum('lms_track_t_totallinehaulweight_cy').over(my_window))\\\r\n",
							".withColumn('lms_track_t_totallinehaulchargeweight_cy_cum', F.sum('lms_track_t_totallinehaulchargeweight_cy').over(my_window))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"##staggrigatetrackbylinehaulloadmeasurescycum.createOrReplaceTempView(\"staggrigatetrackbylinehaulloadmeasurescycum\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"staggrigatetrackbylinehaulloadmeasurespy\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"     date_format(lms_track_opendt, \"MMdd\") as daymonth\r\n",
							"    ,count(DISTINCT lms_track_loadid) as lms_track_t_totallinehaulloads_py\r\n",
							"    ,count(DISTINCT lms_track_parcelid) as lms_track_t_totallinehaulparcels_py\r\n",
							"    ,sum(lms_track_parcelweight) as lms_track_t_totallinehaulweight_py\r\n",
							"    ,sum(lms_track_parcelchargeweight) as lms_track_t_totallinehaulchargeweight_py\r\n",
							"\r\n",
							"\r\n",
							"FROM stlmstrack3myoy\r\n",
							"\r\n",
							"WHERE \r\n",
							"(\r\n",
							"    lms_track_opendt >= (to_timestamp(year(add_months(current_date(),-3))-1||'-'||month(add_months(current_date(),-3))||'-'||'01'))\r\n",
							"AND \r\n",
							"    lms_track_opendt <= (to_timestamp(year(current_date())-1||'-'||month(current_date())||'-'||day(current_date())||'T23:59:59'))\r\n",
							")\r\n",
							"AND lms_track_tracktypeid = 2\r\n",
							"\r\n",
							"GROUP BY date_format(lms_track_opendt, \"MMdd\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"staggrigatetrackbylinehaulloadmeasurespy = spark.sql(\"SELECT * FROM staggrigatetrackbylinehaulloadmeasurespy\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"staggrigatetrackbylinehaulloadmeasurespycum = staggrigatetrackbylinehaulloadmeasurespy.withColumn('lms_track_t_totallinehaulloads_py_cum', F.sum('lms_track_t_totallinehaulloads_py').over(my_window))\\\r\n",
							".withColumn('lms_track_t_totallinehaulparcels_py_cum', F.sum('lms_track_t_totallinehaulparcels_py').over(my_window))\\\r\n",
							".withColumn('lms_track_t_totallinehaulweight_py_cum', F.sum('lms_track_t_totallinehaulweight_py').over(my_window))\\\r\n",
							".withColumn('lms_track_t_totallinehaulchargeweight_py_cum', F.sum('lms_track_t_totallinehaulchargeweight_py').over(my_window))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"##staggrigatetrackbylinehaulloadmeasurespycum.createOrReplaceTempView(\"staggrigatetrackbylinehaulloadmeasurespycum\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"staggrigatetrackbydistributionloadmeasurescy\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"     date_format(lms_track_opendt, \"MMdd\") as daymonth\r\n",
							"    ,count(DISTINCT lms_track_loadid) as lms_track_t_totaldistributionloads_cy\r\n",
							"    ,count(DISTINCT lms_track_parcelid) as lms_track_t_totaldistributionparcels_cy\r\n",
							"    ,sum(lms_track_parcelweight) as lms_track_t_totaldistributionweight_cy\r\n",
							"    ,sum(lms_track_parcelchargeweight) as lms_track_t_totaldistributionchargeweight_cy\r\n",
							"\r\n",
							"\r\n",
							"FROM stlmstrack3myoy\r\n",
							"\r\n",
							"WHERE \r\n",
							"(\r\n",
							"    lms_track_opendt >= (to_timestamp(year(add_months(current_date(),-3))||'-'||month(add_months(current_date(),-3))||'-'||'01'))\r\n",
							"AND \r\n",
							"    lms_track_opendt <= (to_timestamp(year(current_date())||'-'||month(current_date())||'-'||day(current_date())||'T23:59:59'))\r\n",
							")\r\n",
							"AND lms_track_tracktypeid = 6\r\n",
							"\r\n",
							"GROUP BY date_format(lms_track_opendt, \"MMdd\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"staggrigatetrackbydistributionloadmeasurescy = spark.sql(\"SELECT * FROM staggrigatetrackbydistributionloadmeasurescy\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"staggrigatetrackbydistributionloadmeasurescycum = staggrigatetrackbydistributionloadmeasurescy.withColumn('lms_track_t_totaldistributionloads_cy_cum', F.sum('lms_track_t_totaldistributionloads_cy').over(my_window))\\\r\n",
							".withColumn('lms_track_t_totaldistributionparcels_cy_cum', F.sum('lms_track_t_totaldistributionparcels_cy').over(my_window))\\\r\n",
							".withColumn('lms_track_t_totaldistributionweight_cy_cum', F.sum('lms_track_t_totaldistributionweight_cy').over(my_window))\\\r\n",
							".withColumn('lms_track_t_totaldistributionchargeweight_cy_cum', F.sum('lms_track_t_totaldistributionchargeweight_cy').over(my_window))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"##staggrigatetrackbydistributionloadmeasurescycum.createOrReplaceTempView(\"staggrigatetrackbydistributionloadmeasurescycum\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"staggrigatetrackbydistributionloadmeasurespy\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"     date_format(lms_track_opendt, \"MMdd\") as daymonth\r\n",
							"    ,count(DISTINCT lms_track_loadid) as lms_track_t_totaldistributionloads_py\r\n",
							"    ,count(DISTINCT lms_track_parcelid) as lms_track_t_totaldistributionparcels_py\r\n",
							"    ,sum(lms_track_parcelweight) as lms_track_t_totaldistributionweight_py\r\n",
							"    ,sum(lms_track_parcelchargeweight) as lms_track_t_totaldistributionchargeweight_py\r\n",
							"\r\n",
							"\r\n",
							"FROM stlmstrack3myoy\r\n",
							"\r\n",
							"WHERE \r\n",
							"(\r\n",
							"    lms_track_opendt >= (to_timestamp(year(add_months(current_date(),-3))-1||'-'||month(add_months(current_date(),-3))||'-'||'01'))\r\n",
							"AND \r\n",
							"    lms_track_opendt <= (to_timestamp(year(current_date())-1||'-'||month(current_date())||'-'||day(current_date())||'T23:59:59'))\r\n",
							")\r\n",
							"AND lms_track_tracktypeid = 6\r\n",
							"\r\n",
							"GROUP BY date_format(lms_track_opendt, \"MMdd\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"staggrigatetrackbydistributionloadmeasurespy = spark.sql(\"SELECT * FROM staggrigatetrackbydistributionloadmeasurespy\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"staggrigatetrackbydistributionloadmeasurespycum = staggrigatetrackbydistributionloadmeasurespy.withColumn('lms_track_t_totaldistributionloads_py_cum', F.sum('lms_track_t_totaldistributionloads_py').over(my_window))\\\r\n",
							".withColumn('lms_track_t_totaldistributionparcels_py_cum', F.sum('lms_track_t_totaldistributionparcels_py').over(my_window))\\\r\n",
							".withColumn('lms_track_t_totaldistributionweight_py_cum', F.sum('lms_track_t_totaldistributionweight_py').over(my_window))\\\r\n",
							".withColumn('lms_track_t_totaldistributionchargeweight_py_cum', F.sum('lms_track_t_totaldistributionchargeweight_py').over(my_window))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"##staggrigatetrackbydistributionloadmeasurescycum.createOrReplaceTempView(\"staggrigatetrackbydistributionloadmeasurescycum\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# staggrigateparcelbyordermeasurescycum\r\n",
							"# staggrigateparcelbyordermeasurespycum\r\n",
							"# staggrigateparcelbyparcelmeasurescycum\r\n",
							"# staggrigateparcelbyparcelmeasurespycum\r\n",
							"# staggrigateparcelbyconsignmentmeasurescycum\r\n",
							"# staggrigateparcelbyconsignmentmeasurespycum\r\n",
							"# staggrigateparcelbywaybillmeasurescycum\r\n",
							"# staggrigateparcelbywaybillmeasurespycum\r\n",
							"# staggrigatetrackbylinehaulloadmeasurescycum\r\n",
							"# staggrigatetrackbylinehaulloadmeasurespycum\r\n",
							"# staggrigatetrackbydistributionloadmeasurescycum\r\n",
							"# staggrigatetrackbydistributionloadmeasurespycum\r\n",
							"\r\n",
							"staggrigatemeasures = staggrigateparcelbyordermeasurescycum.join(staggrigateparcelbyordermeasurespycum,staggrigateparcelbyordermeasurespycum.daymonth == staggrigateparcelbyordermeasurescycum.daymonth,\"left\" )\\\r\n",
							".join(staggrigateparcelbyparcelmeasurescycum,staggrigateparcelbyparcelmeasurescycum.daymonth == staggrigateparcelbyordermeasurescycum.daymonth,\"left\" )\\\r\n",
							".join(staggrigateparcelbyparcelmeasurespycum,staggrigateparcelbyparcelmeasurespycum.daymonth == staggrigateparcelbyordermeasurescycum.daymonth,\"left\" )\\\r\n",
							".join(staggrigateparcelbyconsignmentmeasurescycum,staggrigateparcelbyconsignmentmeasurescycum.daymonth == staggrigateparcelbyordermeasurescycum.daymonth,\"left\" )\\\r\n",
							".join(staggrigateparcelbyconsignmentmeasurespycum,staggrigateparcelbyconsignmentmeasurespycum.daymonth == staggrigateparcelbyordermeasurescycum.daymonth,\"left\" )\\\r\n",
							".join(staggrigateparcelbywaybillmeasurescycum,staggrigateparcelbywaybillmeasurescycum.daymonth == staggrigateparcelbyordermeasurescycum.daymonth,\"left\" )\\\r\n",
							".join(staggrigateparcelbywaybillmeasurespycum,staggrigateparcelbywaybillmeasurespycum.daymonth == staggrigateparcelbyordermeasurescycum.daymonth,\"left\" )\\\r\n",
							".join(staggrigatetrackbylinehaulloadmeasurescycum,staggrigatetrackbylinehaulloadmeasurescycum.daymonth == staggrigateparcelbyordermeasurescycum.daymonth,\"left\" )\\\r\n",
							".join(staggrigatetrackbylinehaulloadmeasurespycum,staggrigatetrackbylinehaulloadmeasurespycum.daymonth == staggrigateparcelbyordermeasurescycum.daymonth,\"left\" )\\\r\n",
							".join(staggrigatetrackbydistributionloadmeasurescycum,staggrigatetrackbydistributionloadmeasurescycum.daymonth == staggrigateparcelbyordermeasurescycum.daymonth,\"left\" )\\\r\n",
							".join(staggrigatetrackbydistributionloadmeasurespycum,staggrigatetrackbydistributionloadmeasurespycum.daymonth == staggrigateparcelbyordermeasurescycum.daymonth,\"left\" )\\\r\n",
							".drop(staggrigateparcelbyordermeasurespycum.daymonth)\\\r\n",
							".drop(staggrigateparcelbyparcelmeasurescycum.daymonth)\\\r\n",
							".drop(staggrigateparcelbyparcelmeasurespycum.daymonth)\\\r\n",
							".drop(staggrigateparcelbyconsignmentmeasurescycum.daymonth)\\\r\n",
							".drop(staggrigateparcelbyconsignmentmeasurespycum.daymonth)\\\r\n",
							".drop(staggrigateparcelbywaybillmeasurescycum.daymonth)\\\r\n",
							".drop(staggrigateparcelbywaybillmeasurespycum.daymonth)\\\r\n",
							".drop(staggrigatetrackbylinehaulloadmeasurescycum.daymonth)\\\r\n",
							".drop(staggrigatetrackbylinehaulloadmeasurespycum.daymonth)\\\r\n",
							".drop(staggrigatetrackbydistributionloadmeasurescycum.daymonth)\\\r\n",
							".drop(staggrigatetrackbydistributionloadmeasurespycum.daymonth)\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"staggrigatemeasures.createOrReplaceTempView(\"staggrigatemeasures\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# staggrigatemeasures.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/staggrigatemeasures.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"SUCCESS\")"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/12_STActivity')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Finance"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "20122c77-4c9f-4b52-87db-8318b11bce97"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse_Prod/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace-prod/bigDataPools/PRDSparkPool",
						"name": "PRDSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/PRDSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"# #Create DataFrame for the stconsignment LMS Table\r\n",
							"# stconsignment = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stconsignment.parquet', format='parquet')\r\n",
							"# stconsignment.createOrReplaceTempView(\"stconsignment\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stlocation LMS Table\r\n",
							"# stlocation = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stlocation.parquet', format='parquet')\r\n",
							"# stlocation.createOrReplaceTempView(\"stlocation\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stzone LMS Table\r\n",
							"# stzone = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stzone.parquet', format='parquet')\r\n",
							"# stzone.createOrReplaceTempView(\"stzone\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stsroute LMS Table\r\n",
							"# stsroute = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stsroute.parquet', format='parquet')\r\n",
							"# stsroute.createOrReplaceTempView(\"stsroute\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stbillcustomer LMS Table\r\n",
							"# stbillcustomer = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stbillcustomer.parquet', format='parquet')\r\n",
							"# stbillcustomer.createOrReplaceTempView(\"stbillcustomer\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stdeliverypickupcustomer LMS Table\r\n",
							"# stdeliverypickupcustomer = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stdeliverypickupcustomer.parquet', format='parquet')\r\n",
							"# stdeliverypickupcustomer.createOrReplaceTempView(\"stdeliverypickupcustomer\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stlmstrack LMS Table\r\n",
							"# stlmstrack = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stlmstrack.parquet', format='parquet')\r\n",
							"# stlmstrack.createOrReplaceTempView(\"stlmstrack\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stloadchild LMS Table\r\n",
							"# stloadchild = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stloadchild.parquet', format='parquet')\r\n",
							"# stloadchild.createOrReplaceTempView(\"stloadchild\")\r\n",
							"\r\n",
							"# #Create DataFrame for the storder LMS Table\r\n",
							"# storder = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/storder.parquet', format='parquet')\r\n",
							"# storder.createOrReplaceTempView(\"storder\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stparcel LMS Table\r\n",
							"# stparcel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stparcel.parquet', format='parquet')\r\n",
							"# stparcel.createOrReplaceTempView(\"stparcel\")\r\n",
							"\r\n",
							"# #Create DataFrame for the sttrip LMS Table\r\n",
							"# sttrip = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/sttrip.parquet', format='parquet')\r\n",
							"# sttrip.createOrReplaceTempView(\"sttrip\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stbooking LMS Table\r\n",
							"# stbooking = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stbooking.parquet', format='parquet')\r\n",
							"# stbooking.createOrReplaceTempView(\"stbooking\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbovehiclebasic MD Table\r\n",
							"# dbovehiclebasic = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/CLMasterData/dbovehiclebasic.parquet', format='parquet')\r\n",
							"# dbovehiclebasic.createOrReplaceTempView(\"dbovehiclebasic\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbopeoplebasic MD Table\r\n",
							"# dbopeoplebasic = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/CLMasterData/dbopeoplebasic.parquet', format='parquet')\r\n",
							"# dbopeoplebasic.createOrReplaceTempView(\"dbopeoplebasic\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbovehicle LMS Table\r\n",
							"# dbovehicle = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbovehicle.parquet', format='parquet')\r\n",
							"# dbovehicle.createOrReplaceTempView(\"dbovehicle\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbodriver LMS Table\r\n",
							"# dbodriver = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodriver.parquet', format='parquet')\r\n",
							"# dbodriver.createOrReplaceTempView(\"dbodriver\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbolhroutes LMS Table\r\n",
							"# dbolhroutes = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbolh_routes.parquet', format='parquet')\r\n",
							"# dbolhroutes.createOrReplaceTempView(\"dbolhroutes\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publicroute LMS Table\r\n",
							"# publicroute = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publicroute.parquet', format='parquet')\r\n",
							"# publicroute.createOrReplaceTempView(\"publicroute\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publicthirdparty TMS Table\r\n",
							"# publicthirdparty = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publicthirdparty.parquet', format='parquet')\r\n",
							"# publicthirdparty.createOrReplaceTempView(\"publicthirdparty\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publiccustomer TMS Table\r\n",
							"# publiccustomer = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publiccustomer.parquet', format='parquet')\r\n",
							"# publiccustomer.createOrReplaceTempView(\"publiccustomer\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# import os\r\n",
							"# import pandas as pd\r\n",
							"# import numpy as np\r\n",
							"# from pyspark.sql.types import StructType, StructField, DoubleType, StringType, DecimalType\r\n",
							"# from pyspark.sql.functions import col, to_timestamp"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# # Variables\r\n",
							"# var_File_Path = 'abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Avis_Files/AvisUsage.xlsx'\r\n",
							"# var_File_Page = \"AvisUsage\"\r\n",
							"\r\n",
							"# # Processing\r\n",
							"# excel_file = pd.ExcelFile(var_File_Path)\r\n",
							"# excel_file_page = pd.read_excel(excel_file, var_File_Page,  engine='openpyxl')"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"excel_file_page = excel_file_page.astype({'Reg. No':'string','Fleet No':'string','FuelCompany':'string','Time':'string','Voucher No':'string','FuelStation':'string','Pickup Type':'string','Account':'string','Cost Centre':'string','Division':'string','Make':'string','Range':'string','Model':'string','Consum. Norm':'string'})"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"excel_file_page = excel_file_page.fillna(\"NULL\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stkmsandlts_tmp = spark.createDataFrame(excel_file_page)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stkmsandlts_tmp = stkmsandlts_tmp.withColumnRenamed(\"Reg. No\",\"excel_kmsandlts_registrationnumber\")\\\r\n",
							".withColumnRenamed(\"Fleet No\",\"excel_kmsandlts_fleetcode\")\\\r\n",
							".withColumnRenamed(\"Date\",\"excel_kmsandlts_date\")\\\r\n",
							".withColumnRenamed(\"Time\",\"excel_kmsandlts_time\")\\\r\n",
							".withColumnRenamed(\"FuelCompany\",\"excel_kmsandlts_fuelcompany\")\\\r\n",
							".withColumnRenamed(\"Voucher No\",\"excel_kmsandlts_voucherno\")\\\r\n",
							".withColumnRenamed(\"FuelStation\",\"excel_kmsandlts_fuelstation\")\\\r\n",
							".withColumnRenamed(\"PreviousOdo\",\"excel_kmsandlts_previousodo\")\\\r\n",
							".withColumnRenamed(\"Odo\",\"excel_kmsandlts_odo\")\\\r\n",
							".withColumnRenamed(\"Trip Km\",\"excel_kmsandlts_tripkm\")\\\r\n",
							".withColumnRenamed(\"Consum/100\",\"excel_kmsandlts_consumptionper100kms\")\\\r\n",
							".withColumnRenamed(\"Consum/Unit\",\"excel_kmsandlts_consumptionperunit\")\\\r\n",
							".withColumnRenamed(\"L Loss\",\"excel_kmsandlts_ltsloss\")\\\r\n",
							".withColumnRenamed(\"R Loss\",\"excel_kmsandlts_randsloss\")\\\r\n",
							".withColumnRenamed(\"TripLitres\",\"excel_kmsandlts_triplts\")\\\r\n",
							".withColumnRenamed(\"Price Per Unit\",\"excel_kmsandlts_priceperunit\")\\\r\n",
							".withColumnRenamed(\"Fuel Amount\",\"excel_kmsandlts_fuelamount\")\\\r\n",
							".withColumnRenamed(\"Pickup Type\",\"excel_kmsandlts_pickuptype\")\\\r\n",
							".withColumnRenamed(\"Account\",\"excel_kmsandlts_account\")\\\r\n",
							".withColumnRenamed(\"Cost Centre\",\"excel_kmsandlts_costcentre\")\\\r\n",
							".withColumnRenamed(\"Division\",\"excel_kmsandlts_division\")\\\r\n",
							".withColumnRenamed(\"Make\",\"excel_kmsandlts_make\")\\\r\n",
							".withColumnRenamed(\"Range\",\"excel_kmsandlts_range\")\\\r\n",
							".withColumnRenamed(\"Model\",\"excel_kmsandlts_model\")\\\r\n",
							".withColumnRenamed(\"Consum. Norm\",\"excel_kmsandlts_consumptionexpected\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stkmsandlts_tmp = stkmsandlts_tmp.withColumn('excel_kmsandlts_previousodo', col('excel_kmsandlts_previousodo').cast('int'))\\\r\n",
							".withColumn('excel_kmsandlts_odo', col('excel_kmsandlts_odo').cast('int'))\\\r\n",
							".withColumn('excel_kmsandlts_tripkm', col('excel_kmsandlts_tripkm').cast('int'))\\\r\n",
							".withColumn('excel_kmsandlts_consumptionper100kms', col('excel_kmsandlts_consumptionper100kms').cast(DecimalType(18,2)))\\\r\n",
							".withColumn('excel_kmsandlts_consumptionperunit', col('excel_kmsandlts_consumptionperunit').cast(DecimalType(18,2)))\\\r\n",
							".withColumn('excel_kmsandlts_ltsloss', col('excel_kmsandlts_ltsloss').cast(DecimalType(18,2)))\\\r\n",
							".withColumn('excel_kmsandlts_randsloss', col('excel_kmsandlts_randsloss').cast(DecimalType(18,2)))\\\r\n",
							".withColumn('excel_kmsandlts_priceperunit', col('excel_kmsandlts_priceperunit').cast(DecimalType(18,2)))\\\r\n",
							".withColumn('excel_kmsandlts_fuelamount', col('excel_kmsandlts_fuelamount').cast(DecimalType(18,2)))\\\r\n",
							".withColumn('excel_kmsandlts_consumptionexpected', col('excel_kmsandlts_consumptionexpected').cast(DecimalType(18,2)))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stkmsandlts_tmp.createOrReplaceTempView(\"stkmsandlts_tmp\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stkmsandlts\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"excel_kmsandlts_registrationnumber\r\n",
							",excel_kmsandlts_fleetcode\r\n",
							",to_date(excel_kmsandlts_date,'yyyy-MM-dd') as excel_kmsandlts_date\r\n",
							",excel_kmsandlts_time\r\n",
							",excel_kmsandlts_fuelcompany\r\n",
							",excel_kmsandlts_voucherno\r\n",
							",excel_kmsandlts_fuelstation\r\n",
							",excel_kmsandlts_previousodo\r\n",
							",excel_kmsandlts_odo\r\n",
							",excel_kmsandlts_tripkm\r\n",
							",excel_kmsandlts_consumptionper100kms\r\n",
							",excel_kmsandlts_consumptionperunit\r\n",
							",excel_kmsandlts_ltsloss\r\n",
							",excel_kmsandlts_randsloss\r\n",
							",excel_kmsandlts_triplts\r\n",
							",excel_kmsandlts_priceperunit\r\n",
							",excel_kmsandlts_fuelamount\r\n",
							",excel_kmsandlts_pickuptype\r\n",
							",excel_kmsandlts_account\r\n",
							",excel_kmsandlts_costcentre\r\n",
							",excel_kmsandlts_division\r\n",
							",excel_kmsandlts_make\r\n",
							",excel_kmsandlts_range\r\n",
							",excel_kmsandlts_model\r\n",
							",excel_kmsandlts_consumptionexpected\r\n",
							"FROM stkmsandlts_tmp"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stkmsandlts = spark.sql(\"SELECT * FROM stkmsandlts\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stkmsandlts.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stkmsandlts.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"sttmsactivitydetail\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"t.tms_trip_id as tms_activity_tripid\r\n",
							",t.tms_trip_driverid  as tms_activity_tripdriverid\r\n",
							",t.tms_trip_drivername  as tms_activity_tripdrivername\r\n",
							",t.tms_trip_vehicleid  as tms_activity_tripvehicleid\r\n",
							",t.tms_trip_vehiclefleetcode as tms_activity_tripvehiclefleetcode\r\n",
							",t.tms_trip_trailerid  as tms_activity_triptrailer1id\r\n",
							",t.tms_trip_trailer1fleetcode as tms_activity_triptrailer1fleetcode\r\n",
							",t.tms_trip_sectrailerid  as tms_activity_triptrailer2id\r\n",
							",t.tms_trip_trailer2fleetcode as tms_activity_triptrailer2fleetcode\r\n",
							",t.tms_trip_trailersize  as tms_activity_triptrailersize\r\n",
							",t.tms_trip_status as tms_activity_tripstatus\r\n",
							",t.tms_trip_routeid as tms_activity_tripparenetrouteid\r\n",
							",pr.name as tms_activity_tripparentroute\r\n",
							",t.tms_trip_opsrouteid as tms_activity_tripopsrouteid\r\n",
							",CASE\r\n",
							"    WHEN t.tms_trip_source = 'v1' THEN UPPER(sr.description)\r\n",
							"    WHEN t.tms_trip_source = 'v2' THEN UPPER(sr1.name)\r\n",
							" END tms_activity_tripopsroute\r\n",
							",to_date(t.tms_trip_dispatchdate) as tms_activity_tripdispatchdate\r\n",
							",upper(t.tms_trip_transporter) as tms_activity_triptransporter\r\n",
							",t.tms_trip_source as tms_activity_tripsource\r\n",
							"\r\n",
							"\r\n",
							",b.tms_booking_id as tms_activity_bookingid\r\n",
							",b.tms_booking_type as tms_activity_bookingtype\r\n",
							",b.tms_booking_trailersize as tms_activity_bookingtrailersize\r\n",
							",b.tms_booking_cubicvolume as tms_activity_bookingcubicvolume\r\n",
							",b.tms_booking_directbooking as tms_activity_directbooking\r\n",
							",b.tms_booking_local as tms_activity_localbooking\r\n",
							",b.tms_booking_source as tms_activity_bookingsource\r\n",
							",b.tms_booking_customerid as tms_activity_customerid\r\n",
							",ppc.name as tms_activity_primarycustomer\r\n",
							",b.tms_booking_childcustomerid as tms_activity_childcustomerid\r\n",
							",spc.name as tms_activity_secondarycustomer\r\n",
							",CASE\r\n",
							"    WHEN b.tms_booking_type = 'nonbooking' AND b.tms_booking_allocateddate IS NULL THEN to_date(t.tms_trip_dispatchdate)\r\n",
							"    ELSE to_date(b.tms_booking_allocateddate)\r\n",
							"END AS tms_activity_bookingallocateddate\r\n",
							"\r\n",
							"FROM sttrip t\r\n",
							"LEFT JOIN dbovehicle v1 on (v1.id)*-1 = t.tms_trip_vehicleid and t.tms_trip_source = 'v1'\r\n",
							"LEFT JOIN dbovehicle t1 on (t1.id)*-1 = t.tms_trip_trailerid and t.tms_trip_source = 'v1'\r\n",
							"LEFT JOIN dbovehicle t2 on (t2.id)*-1 = t.tms_trip_sectrailerid and t.tms_trip_source = 'v1'\r\n",
							"LEFT JOIN dbovehiclebasic vbv  on vbv.uid = t.tms_trip_vehicleid and t.tms_trip_source = 'v2'\r\n",
							"LEFT JOIN dbovehiclebasic vbt1  on vbt1.uid = t.tms_trip_trailerid and t.tms_trip_source = 'v2'\r\n",
							"LEFT JOIN dbovehiclebasic vbt2  on vbt2.uid = t.tms_trip_sectrailerid and t.tms_trip_source = 'v2'\r\n",
							"LEFT JOIN publicroute pr on pr.id = t.tms_trip_routeid \r\n",
							"LEFT JOIN dbolhroutes sr on (sr.id)*-1 = t.tms_trip_opsrouteid and t.tms_trip_source = 'v1'\r\n",
							"LEFT JOIN publicroute sr1 on sr1.id = t.tms_trip_opsrouteid and t.tms_trip_source = 'v2'\r\n",
							"LEFT JOIN stbooking b on b.tms_booking_tripid = t.tms_trip_id\r\n",
							"LEFT JOIN dbodriver d on (d.id)*-1 = t.tms_trip_driverid and t.tms_trip_source = 'v1'\r\n",
							"LEFT JOIN dbopeoplebasic pb on pb.uid = t.tms_trip_driverid and t.tms_trip_source = 'v2'\r\n",
							"LEFT JOIN publiccustomer ppc on ppc.id = b.tms_booking_customerid and  t.tms_trip_source = 'v2'\r\n",
							"LEFT JOIN publiccustomer spc on spc.id = b.tms_booking_childcustomerid and  t.tms_trip_source = 'v2'\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sttmsactivitydetail = spark.sql(\"SELECT * FROM sttmsactivitydetail\")"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# sttmsactivitydetail.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/sttmsactivitydetail.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"sttmsactivitysummary\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"tms_activity_tripid\r\n",
							",tms_activity_bookingid\r\n",
							",tms_activity_tripdrivername\r\n",
							",tms_activity_tripstatus\r\n",
							",tms_activity_tripopsroute\r\n",
							",tms_activity_tripdispatchdate\r\n",
							",tms_activity_tripsource\r\n",
							",tms_activity_directbooking\r\n",
							",tms_activity_localbooking\r\n",
							",tms_activity_triptrailersize\r\n",
							",tms_activity_triptransporter\r\n",
							",tms_activity_bookingtype\r\n",
							",tms_activity_bookingallocateddate\r\n",
							",tms_activity_primarycustomer\r\n",
							",tms_activity_secondarycustomer\r\n",
							",sum(tms_activity_bookingcubicvolume) as tms_activity_aggrbookingcubicvolume\r\n",
							",count(tms_activity_bookingid) as tms_activity_aggrbookingid\r\n",
							"\r\n",
							"\r\n",
							"FROM sttmsactivitydetail\r\n",
							"GROUP BY \r\n",
							"tms_activity_tripid\r\n",
							",tms_activity_bookingid\r\n",
							",tms_activity_tripdrivername\r\n",
							",tms_activity_tripstatus\r\n",
							",tms_activity_tripopsroute\r\n",
							",tms_activity_tripdispatchdate\r\n",
							",tms_activity_tripsource\r\n",
							",tms_activity_triptrailersize\r\n",
							",tms_activity_triptransporter\r\n",
							",tms_activity_directbooking\r\n",
							",tms_activity_localbooking\r\n",
							",tms_activity_bookingtype\r\n",
							",tms_activity_bookingallocateddate\r\n",
							",tms_activity_primarycustomer\r\n",
							",tms_activity_secondarycustomer\r\n",
							""
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sttmsactivitysummary = spark.sql(\"SELECT * FROM sttmsactivitysummary\")"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# sttmsactivitysummary.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/sttmsactivitysummary.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": true,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# OLD CODE \r\n",
							"# -- %%sql\r\n",
							"# -- CREATE OR REPLACE TEMP VIEW\r\n",
							"# -- stactivityconsignedto\r\n",
							"# -- AS\r\n",
							"# -- SELECT\r\n",
							"# --     c.lms_consignment_id as lms_activity_transid\r\n",
							"# --     ,'consignmentid' as lms_activity_transidtype\r\n",
							"# --     ,to_date(c.lms_consignment_cdate,'yyyy-MM-dd') as lms_activity_date\r\n",
							"# --     ,c.lms_consignment_billcust as lms_activity_billcustid\r\n",
							"# --     ,bc.lms_customer_name as lms_activity_billcustname\r\n",
							"# --     --,'' as lms_activity_fromlocation\r\n",
							"# --     ,l.lms_location_description as lms_activity_location\r\n",
							"# --     ,'consignment' as lms_activity_movementtype\r\n",
							"# --     ,'' as lms_activity_linehualbookingid\r\n",
							"# --     ,(c.lms_parcelbyconsignment_aggrweight) as lms_activity_aggrweight\r\n",
							"# --     ,(c.lms_parcelbyconsignment_aggrchargeweight) as lms_activity_aggrchargeweight\r\n",
							"# --     ,(c.lms_parcelbyconsignment_aggrvolweight) as lms_activity_aggrvolweight\r\n",
							"# --     ,(c.lms_parcelbyconsignment_aggrvolumiserweight) as lms_activity_aggrvolumiserweight\r\n",
							"# --     ,(c.lms_parcelbyconsignment_aggrmaxweight) as lms_activity_aggrmaxweight\r\n",
							"# --     ,(c.lms_parcelbyconsignment_aggrnoparcels) as lms_activity_aggrnoparcels\r\n",
							"# --     ,(c.lms_parcelbyconsignment_aggrweightexclspecdel) as lms_activity_aggrweightexclspecdel\r\n",
							"# --     ,(c.lms_parcelbyconsignment_aggrchargeweightexclspecdel) as lms_activity_aggrchargeweightexclspecdel\r\n",
							"# --     ,(c.lms_parcelbyconsignment_aggrvolweightexclspecdel) as lms_activity_aggrvolweightexclspecdel\r\n",
							"# --     ,(c.lms_parcelbyconsignment_aggrvolumiserweightexclspecdel) as lms_activity_aggrvolumiserweightexclspecdel\r\n",
							"# --     ,(c.lms_parcelbyconsignment_aggrmaxweightexclspecdel) as lms_activity_aggrmaxweightexclspecdel\r\n",
							"# --     ,(c.lms_parcelbyconsignment_aggrnoparcelsexclspecdel) as lms_activity_aggrnoparcelsexclspecdel\r\n",
							"\r\n",
							"\r\n",
							"# -- FROM stconsignment c\r\n",
							"# -- LEFT JOIN stbillcustomer bc on bc.lms_customer_id = c.lms_consignment_billcust\r\n",
							"# -- LEFT JOIN stdeliverypickupcustomer dpc on dpc.lms_customer_id = c.lms_consignment_delivercustid\r\n",
							"# -- LEFT JOIN stsroute sr on sr.lms_sroute_id = dpc.lms_customer_srouteid\r\n",
							"# -- LEFT JOIN stzone z on z.lms_zone_id = sr.lms_sroute_zoneid\r\n",
							"# -- LEFT JOIN stlocation l on l.lms_location_id = z.lms_zone_locid\r\n",
							"\r\n",
							"\r\n",
							"# -- --LEFT JOIN storder o on o.lms_order_id = p.lms_parcel_orderid\r\n",
							"\r\n",
							"# -- WHERE to_date(c.lms_consignment_cdate,'yyyy-MM-dd') >= '2016-04-01'\r\n",
							""
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stparcelorder_tmp\r\n",
							"AS\r\n",
							"SELECT DISTINCT\r\n",
							"     p.lms_parcel_consignid\r\n",
							"    ,p.lms_parcel_orderid\r\n",
							"    ,o.lms_order_billcustid\r\n",
							"    ,o.lms_order_delivercustid\r\n",
							"    ,sum(p.lms_parcel_weight) as lms_activity_aggrweight\r\n",
							"    ,sum(p.lms_parcel_chargeweight) as lms_activity_aggrchargeweight\r\n",
							"    ,sum(p.lms_parcel_volweight) as lms_activity_aggrvolweight\r\n",
							"    ,sum(p.lms_parcel_volumiserweight) as lms_activity_aggrvolumiserweight\r\n",
							"    ,sum(greatest(p.lms_parcel_weight,p.lms_parcel_chargeweight,p.lms_parcel_volweight,p.lms_parcel_volumiserweight)) as lms_activity_aggrmaxweight\r\n",
							"    ,count(p.lms_parcel_id) as lms_activity_aggrnoparcels\r\n",
							"    ,sum(CASE\r\n",
							"            WHEN o.lms_order_specdel = 0 THEN lms_parcel_weight\r\n",
							"            ELSE 0 \r\n",
							"        END) as lms_activity_aggrweightexclspecdel\r\n",
							"    ,sum(CASE\r\n",
							"            WHEN o.lms_order_specdel = 0 THEN lms_parcel_chargeweight\r\n",
							"            ELSE 0 \r\n",
							"        END) as lms_activity_aggrchargeweightexclspecdel\r\n",
							"    ,sum(CASE\r\n",
							"            WHEN o.lms_order_specdel = 0 THEN lms_parcel_volweight\r\n",
							"            ELSE 0 \r\n",
							"        END) as lms_activity_aggrvolweightexclspecdel\r\n",
							"    ,sum(CASE\r\n",
							"            WHEN o.lms_order_specdel = 0 THEN lms_parcel_volumiserweight\r\n",
							"            ELSE 0 \r\n",
							"        END) as lms_activity_aggrvolumiserweightexclspecdel\r\n",
							"\r\n",
							"    ,sum(CASE\r\n",
							"            WHEN o.lms_order_specdel = 0 THEN greatest(p.lms_parcel_weight,p.lms_parcel_chargeweight,p.lms_parcel_volweight,p.lms_parcel_volumiserweight)\r\n",
							"            ELSE 0 \r\n",
							"        END) as lms_activity_aggrmaxweightexclspecdel\r\n",
							"\r\n",
							"    ,sum(CASE\r\n",
							"            WHEN o.lms_order_specdel = 0 THEN 1\r\n",
							"            ELSE 0 \r\n",
							"        END) as lms_activity_aggrnoparcelsexclspecdel\r\n",
							"FROM stparcel p \r\n",
							"LEFT JOIN storder o on o.lms_order_id = p.lms_parcel_orderid\r\n",
							"GROUP BY  \r\n",
							"     p.lms_parcel_consignid\r\n",
							"    ,p.lms_parcel_orderid\r\n",
							"    ,o.lms_order_billcustid\r\n",
							"    ,o.lms_order_delivercustid\r\n",
							""
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stparcelorder_tmp = spark.sql(\"SELECT * FROM stparcelorder_tmp\")"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stactivityconsignedto\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    po.lms_parcel_consignid||'|'|| po.lms_parcel_orderid as lms_activity_transid\r\n",
							"    ,'consignmentid|orderid' as lms_activity_transidtype\r\n",
							"    ,to_date(c.lms_consignment_cdate,'yyyy-MM-dd') as lms_activity_date\r\n",
							"    ,po.lms_order_billcustid as lms_activity_billcustid\r\n",
							"    ,bc.lms_customer_name as lms_activity_billcustname\r\n",
							"    ,po.lms_order_delivercustid as lms_activity_delivercustid\r\n",
							"    ,dpc.lms_customer_name as lms_activity_delivercustname\r\n",
							"    ,l.lms_location_description as lms_activity_location\r\n",
							"    ,'consignment' as lms_activity_movementtype\r\n",
							"    ,'' as lms_activity_linehualbookingid\r\n",
							"    ,CAST(po.lms_activity_aggrweight AS decimal(18,2)) as lms_activity_aggrweight\r\n",
							"    ,CAST(po.lms_activity_aggrchargeweight AS decimal(18,2)) as lms_activity_aggrchargeweight\r\n",
							"    ,CAST(po.lms_activity_aggrvolweight AS decimal(18,2)) as lms_activity_aggrvolweight\r\n",
							"    ,CAST(po.lms_activity_aggrvolumiserweight AS decimal(18,2)) as lms_activity_aggrvolumiserweight\r\n",
							"    ,CAST(po.lms_activity_aggrmaxweight AS decimal(18,2)) as lms_activity_aggrmaxweight\r\n",
							"    ,po.lms_activity_aggrnoparcels\r\n",
							"    ,CAST(po.lms_activity_aggrweightexclspecdel AS decimal(18,2)) as lms_activity_aggrweightexclspecdel\r\n",
							"    ,CAST(po.lms_activity_aggrchargeweightexclspecdel AS decimal(18,2)) as lms_activity_aggrchargeweightexclspecdel\r\n",
							"    ,CAST(po.lms_activity_aggrvolweightexclspecdel AS decimal(18,2)) as lms_activity_aggrvolweightexclspecdel\r\n",
							"    ,CAST(po.lms_activity_aggrvolumiserweightexclspecdel AS decimal(18,2)) as lms_activity_aggrvolumiserweightexclspecdel\r\n",
							"    ,CAST(po.lms_activity_aggrmaxweightexclspecdel AS decimal(18,2)) as lms_activity_aggrmaxweightexclspecdel\r\n",
							"    ,po.lms_activity_aggrnoparcelsexclspecdel\r\n",
							"\r\n",
							"\r\n",
							"FROM stparcelorder_tmp po\r\n",
							"LEFT JOIN stconsignment c on c.lms_consignment_id = po.lms_parcel_consignid\r\n",
							"LEFT JOIN stbillcustomer bc on bc.lms_customer_id = po.lms_order_billcustid\r\n",
							"LEFT JOIN stdeliverypickupcustomer dpc on dpc.lms_customer_id = po.lms_order_delivercustid\r\n",
							"LEFT JOIN stsroute sr on sr.lms_sroute_id = dpc.lms_customer_srouteid\r\n",
							"LEFT JOIN stzone z on z.lms_zone_id = sr.lms_sroute_zoneid\r\n",
							"LEFT JOIN stlocation l on l.lms_location_id = z.lms_zone_locid\r\n",
							"\r\n",
							"WHERE to_date(c.lms_consignment_cdate,'yyyy-MM-dd') >= '2016-04-01'\r\n",
							"AND po.lms_parcel_consignid||'|'|| po.lms_parcel_orderid  is not null\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stactivityconsignedto = spark.sql(\"SELECT * FROM stactivityconsignedto\")"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## stactivityconsignedto.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stactivityconsignedto.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"--LOADED ACTIVITY\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stactivityhandled_tmp1\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"     lc.lms_loadchild_id as lms_activity_transid\r\n",
							"    ,'loadid' as lms_activity_transidtype\r\n",
							"    ,to_date(lms_track_opendt,'yyyy-MM-dd') as lms_activity_date\r\n",
							"    ,p.lms_parcel_orderbillcustid as lms_activity_billcustid\r\n",
							"    ,bc.lms_customer_name as lms_activity_billcustname\r\n",
							"    ,p.lms_parcel_orderdelivercustid as lms_activity_delivercustid\r\n",
							"    ,dpc.lms_customer_name as lms_activity_delivercustname\r\n",
							"    ,CASE \r\n",
							"        WHEN lms_track_tracktypeid = 2 THEN 'linehaul' \r\n",
							"        WHEN lms_track_tracktypeid = 6 THEN 'distribution'\r\n",
							"        ELSE '' \r\n",
							"    END as lms_activity_movementtype\r\n",
							"    ,lc.lms_loadchild_masterloadid as lms_activity_linehualbookingid\r\n",
							"    ,fl.lms_location_description as lms_activity_location\r\n",
							"    ,sum(p.lms_parcel_weight) as lms_activity_aggrweight\r\n",
							"    ,sum(p.lms_parcel_chargeweight) as lms_activity_aggrchargeweight\r\n",
							"    ,sum(p.lms_parcel_volweight) as lms_activity_aggrvolweight\r\n",
							"    ,sum(p.lms_parcel_volumiserweight) as lms_activity_aggrvolumiserweight\r\n",
							"    ,sum(greatest(p.lms_parcel_weight,p.lms_parcel_chargeweight,p.lms_parcel_volweight,p.lms_parcel_volumiserweight)) as lms_activity_aggrmaxweight\r\n",
							"    ,count(p.lms_parcel_id) as lms_activity_aggrnoparcels\r\n",
							"    ,sum(CASE\r\n",
							"            WHEN o.lms_order_specdel = 0 THEN lms_parcel_weight\r\n",
							"            ELSE 0 \r\n",
							"        END) as lms_activity_aggrweightexclspecdel\r\n",
							"    ,sum(CASE\r\n",
							"            WHEN o.lms_order_specdel = 0 THEN lms_parcel_chargeweight\r\n",
							"            ELSE 0 \r\n",
							"        END) as lms_activity_aggrchargeweightexclspecdel\r\n",
							"    ,sum(CASE\r\n",
							"            WHEN o.lms_order_specdel = 0 THEN lms_parcel_volweight\r\n",
							"            ELSE 0 \r\n",
							"        END) as lms_activity_aggrvolweightexclspecdel\r\n",
							"    ,sum(CASE\r\n",
							"            WHEN o.lms_order_specdel = 0 THEN lms_parcel_volumiserweight\r\n",
							"            ELSE 0 \r\n",
							"        END) as lms_activity_aggrvolumiserweightexclspecdel\r\n",
							"\r\n",
							"    ,sum(CASE\r\n",
							"            WHEN o.lms_order_specdel = 0 THEN greatest(p.lms_parcel_weight,p.lms_parcel_chargeweight,p.lms_parcel_volweight,p.lms_parcel_volumiserweight)\r\n",
							"            ELSE 0 \r\n",
							"        END) as lms_activity_aggrmaxweightexclspecdel\r\n",
							"\r\n",
							"    ,sum(CASE\r\n",
							"            WHEN o.lms_order_specdel = 0 THEN 1\r\n",
							"            ELSE 0 \r\n",
							"        END) as lms_activity_aggrnoparcelsexclspecdel\r\n",
							"\r\n",
							"\r\n",
							"FROM stlmstrack t\r\n",
							"LEFT JOIN stparcel p on p.lms_parcel_id = t.lms_track_parcelid\r\n",
							"LEFT JOIN stloadchild lc on lc.lms_loadchild_id = t.lms_track_loadid\r\n",
							"LEFT JOIN storder o on o.lms_order_id = p.lms_parcel_orderid\r\n",
							"LEFT JOIN stbillcustomer bc on bc.lms_customer_id = p.lms_parcel_orderbillcustid\r\n",
							"LEFT JOIN stdeliverypickupcustomer dpc on dpc.lms_customer_id = p.lms_parcel_orderdelivercustid\r\n",
							"LEFT JOIN stlocation fl on fl.lms_location_id = t.lms_track_fromlocid\r\n",
							"LEFT JOIN stlocation tl on tl.lms_location_id = t.lms_track_tolocid\r\n",
							"\r\n",
							"WHERE t.lms_track_tracktypeid in (2,6)\r\n",
							"AND to_date(t.lms_track_opendt,'yyyy-MM-dd') >= '2016-04-01'\r\n",
							"\r\n",
							"GROUP BY\r\n",
							"     lc.lms_loadchild_id\r\n",
							"    ,to_date(t.lms_track_opendt,'yyyy-MM-dd')\r\n",
							"    ,p.lms_parcel_orderbillcustid\r\n",
							"    ,bc.lms_customer_name\r\n",
							"    ,p.lms_parcel_orderdelivercustid\r\n",
							"    ,dpc.lms_customer_name\r\n",
							"    ,lc.lms_loadchild_masterloadid\r\n",
							"    ,fl.lms_location_description\r\n",
							"    ,tl.lms_location_description\r\n",
							"    ,t.lms_track_tracktypeid\r\n",
							"    ,o.lms_order_specdel\r\n",
							""
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stactivityhandled_tmp1 = spark.sql(\"SELECT * FROM stactivityhandled_tmp1\")"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"--OFFLOADED LINEHAUL ACTIVITY\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stactivityhandled_tmp2\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"     lc.lms_loadchild_id as lms_activity_transid\r\n",
							"    ,'loadid' as lms_activity_transidtype\r\n",
							"    ,to_date(lms_track_closedt,'yyyy-MM-dd') as lms_activity_date\r\n",
							"    ,p.lms_parcel_orderbillcustid as lms_activity_billcustid\r\n",
							"    ,bc.lms_customer_name as lms_activity_billcustname\r\n",
							"    ,p.lms_parcel_orderdelivercustid as lms_activity_delivercustid\r\n",
							"    ,dpc.lms_customer_name as lms_activity_delivercustname\r\n",
							"    ,CASE \r\n",
							"        WHEN lms_track_tracktypeid = 2 THEN 'linehaul offload' \r\n",
							"        WHEN lms_track_tracktypeid = 6 THEN 'distribution offload'\r\n",
							"        ELSE '' \r\n",
							"    END as lms_activity_movementtype\r\n",
							"    ,lc.lms_loadchild_masterloadid as lms_activity_linehualbookingid\r\n",
							"    ,tl.lms_location_description as lms_activity_location\r\n",
							"    ,sum(p.lms_parcel_weight) as lms_activity_aggrweight\r\n",
							"    ,sum(p.lms_parcel_chargeweight) as lms_activity_aggrchargeweight\r\n",
							"    ,sum(p.lms_parcel_volweight) as lms_activity_aggrvolweight\r\n",
							"    ,sum(p.lms_parcel_volumiserweight) as lms_activity_aggrvolumiserweight\r\n",
							"    ,sum(greatest(p.lms_parcel_weight,p.lms_parcel_chargeweight,p.lms_parcel_volweight,p.lms_parcel_volumiserweight)) as lms_activity_aggrmaxweight\r\n",
							"    ,count(p.lms_parcel_id) as lms_activity_aggrnoparcels\r\n",
							"    ,sum(CASE\r\n",
							"            WHEN o.lms_order_specdel = 0 THEN lms_parcel_weight\r\n",
							"            ELSE 0 \r\n",
							"        END) as lms_activity_aggrweightexclspecdel\r\n",
							"    ,sum(CASE\r\n",
							"            WHEN o.lms_order_specdel = 0 THEN lms_parcel_chargeweight\r\n",
							"            ELSE 0 \r\n",
							"        END) as lms_activity_aggrchargeweightexclspecdel\r\n",
							"    ,sum(CASE\r\n",
							"            WHEN o.lms_order_specdel = 0 THEN lms_parcel_volweight\r\n",
							"            ELSE 0 \r\n",
							"        END) as lms_activity_aggrvolweightexclspecdel\r\n",
							"    ,sum(CASE\r\n",
							"            WHEN o.lms_order_specdel = 0 THEN lms_parcel_volumiserweight\r\n",
							"            ELSE 0 \r\n",
							"        END) as lms_activity_aggrvolumiserweightexclspecdel\r\n",
							"\r\n",
							"    ,sum(CASE\r\n",
							"            WHEN o.lms_order_specdel = 0 THEN greatest(p.lms_parcel_weight,p.lms_parcel_chargeweight,p.lms_parcel_volweight,p.lms_parcel_volumiserweight)\r\n",
							"            ELSE 0 \r\n",
							"        END) as lms_activity_aggrmaxweightexclspecdel\r\n",
							"\r\n",
							"    ,sum(CASE\r\n",
							"            WHEN o.lms_order_specdel = 0 THEN 1\r\n",
							"            ELSE 0 \r\n",
							"        END) as lms_activity_aggrnoparcelsexclspecdel\r\n",
							"\r\n",
							"\r\n",
							"FROM stlmstrack t\r\n",
							"LEFT JOIN stparcel p on p.lms_parcel_id = t.lms_track_parcelid\r\n",
							"LEFT JOIN stloadchild lc on lc.lms_loadchild_id = t.lms_track_loadid\r\n",
							"LEFT JOIN storder o on o.lms_order_id = p.lms_parcel_orderid\r\n",
							"LEFT JOIN stbillcustomer bc on bc.lms_customer_id = p.lms_parcel_orderbillcustid\r\n",
							"LEFT JOIN stdeliverypickupcustomer dpc on dpc.lms_customer_id = p.lms_parcel_orderdelivercustid\r\n",
							"LEFT JOIN stlocation fl on fl.lms_location_id = t.lms_track_fromlocid\r\n",
							"LEFT JOIN stlocation tl on tl.lms_location_id = t.lms_track_tolocid\r\n",
							"\r\n",
							"WHERE t.lms_track_tracktypeid in (2)\r\n",
							"AND to_date(t.lms_track_closedt,'yyyy-MM-dd') >= '2016-04-01'\r\n",
							"\r\n",
							"GROUP BY\r\n",
							"     lc.lms_loadchild_id\r\n",
							"    ,to_date(t.lms_track_closedt,'yyyy-MM-dd')\r\n",
							"    ,p.lms_parcel_orderbillcustid\r\n",
							"    ,bc.lms_customer_name\r\n",
							"    ,p.lms_parcel_orderdelivercustid\r\n",
							"    ,dpc.lms_customer_name\r\n",
							"    ,lc.lms_loadchild_masterloadid\r\n",
							"    ,fl.lms_location_description\r\n",
							"    ,tl.lms_location_description\r\n",
							"    ,t.lms_track_tracktypeid\r\n",
							"    ,o.lms_order_specdel\r\n",
							""
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stactivityhandled_tmp2 = spark.sql(\"SELECT * FROM stactivityhandled_tmp2\")"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stactivityhandled_tmp3 = stactivityhandled_tmp1.unionByName(stactivityhandled_tmp2, allowMissingColumns = True)\r\n",
							"stactivityhandled_tmp3.createOrReplaceTempView(\"stactivityhandled_tmp3\")"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stactivityhandled\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"     t1.lms_activity_transid\r\n",
							"    ,t1.lms_activity_transidtype\r\n",
							"    ,t1.lms_activity_date\r\n",
							"    ,t1.lms_activity_billcustid\r\n",
							"    ,t1.lms_activity_billcustname\r\n",
							"    ,t1.lms_activity_delivercustid\r\n",
							"    ,t1.lms_activity_delivercustname\r\n",
							"    ,t1.lms_activity_movementtype\r\n",
							"    ,t1.lms_activity_linehualbookingid\r\n",
							"    ,t1.lms_activity_location\r\n",
							"    ,CAST(t1.lms_activity_aggrweight AS decimal(18,2)) as lms_activity_aggrweight\r\n",
							"    ,CAST(t1.lms_activity_aggrchargeweight AS decimal(18,2)) as lms_activity_aggrchargeweight\r\n",
							"    ,CAST(t1.lms_activity_aggrvolweight AS decimal(18,2)) as lms_activity_aggrvolweight\r\n",
							"    ,CAST(t1.lms_activity_aggrvolumiserweight AS decimal(18,2)) as lms_activity_aggrvolumiserweight\r\n",
							"    ,CAST(t1.lms_activity_aggrmaxweight AS decimal(18,2)) as lms_activity_aggrmaxweight\r\n",
							"    ,t1.lms_activity_aggrnoparcels\r\n",
							"    ,CAST(t1.lms_activity_aggrweightexclspecdel AS decimal(18,2)) as lms_activity_aggrweightexclspecdel\r\n",
							"    ,CAST(t1.lms_activity_aggrchargeweightexclspecdel AS decimal(18,2)) as lms_activity_aggrchargeweightexclspecdel\r\n",
							"    ,CAST(t1.lms_activity_aggrvolweightexclspecdel AS decimal(18,2)) as lms_activity_aggrvolweightexclspecdel\r\n",
							"    ,CAST(t1.lms_activity_aggrvolumiserweightexclspecdel AS decimal(18,2)) as lms_activity_aggrvolumiserweightexclspecdel\r\n",
							"    ,CAST(t1.lms_activity_aggrmaxweightexclspecdel AS decimal(18,2)) as lms_activity_aggrmaxweightexclspecdel\r\n",
							"    ,t1.lms_activity_aggrnoparcelsexclspecdel\r\n",
							"\r\n",
							"\r\n",
							"FROM stactivityhandled_tmp3 t1\r\n",
							"\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stactivityhandled = spark.sql(\"SELECT * FROM stactivityhandled\")"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## stactivityhandled.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stactivityhandled.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stactivitydetail = stactivityhandled.unionByName(stactivityconsignedto, allowMissingColumns = True)\r\n",
							"stactivitydetail.createOrReplaceTempView(\"stactivitydetail\")"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stactivitydetail.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stactivitydetail.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stactivitysummary\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"\r\n",
							" lms_activity_date\r\n",
							",lms_activity_movementtype\r\n",
							",lms_activity_location\r\n",
							",lms_activity_billcustname\r\n",
							",CAST(SUM(lms_activity_aggrweight) AS decimal(18,2)) as lms_activity_aggrweight\r\n",
							",CAST(SUM(lms_activity_aggrchargeweight) AS decimal(18,2)) as lms_activity_aggrchargeweight\r\n",
							",CAST(SUM(lms_activity_aggrvolweight) AS decimal(18,2)) as lms_activity_aggrvolweight\r\n",
							",CAST(SUM(lms_activity_aggrvolumiserweight) AS decimal(18,2)) as lms_activity_aggrvolumiserweight\r\n",
							",CAST(SUM(lms_activity_aggrmaxweight) AS decimal(18,2)) as lms_activity_aggrmaxweight\r\n",
							",SUM(lms_activity_aggrnoparcels) lms_activity_aggrnoparcels\r\n",
							",CAST(SUM(lms_activity_aggrweightexclspecdel) AS decimal(18,2)) as lms_activity_aggrweightexclspecdel\r\n",
							",CAST(SUM(lms_activity_aggrchargeweightexclspecdel) AS decimal(18,2)) as lms_activity_aggrchargeweightexclspecdel\r\n",
							",CAST(SUM(lms_activity_aggrvolweightexclspecdel) AS decimal(18,2)) as lms_activity_aggrvolweightexclspecdel\r\n",
							",CAST(SUM(lms_activity_aggrvolumiserweightexclspecdel) AS decimal(18,2)) as lms_activity_aggrvolumiserweightexclspecdel\r\n",
							",CAST(SUM(lms_activity_aggrmaxweightexclspecdel) AS decimal(18,2)) as lms_activity_aggrmaxweightexclspecdel\r\n",
							",SUM(lms_activity_aggrnoparcelsexclspecdel) lms_activity_aggrnoparcelsexclspecdel\r\n",
							"\r\n",
							"FROM stactivitydetail\r\n",
							"GROUP BY\r\n",
							"lms_activity_date\r\n",
							",lms_activity_movementtype\r\n",
							",lms_activity_location\r\n",
							",lms_activity_billcustname\r\n",
							""
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stactivitysummary = spark.sql(\"SELECT * FROM stactivitysummary\")"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stactivitysummary.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stactivitysummary.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"SUCCESS\")"
						],
						"outputs": [],
						"execution_count": 20
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/13_DIST_STOpsFinTransactions')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Ops/Distribution"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "879f8ba7-078e-4356-911b-ecb1344e2257"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace/bigDataPools/DevSparkPool",
						"name": "DevSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/DevSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# #Create DataFrame for the sttransactionsdetail SAP Table\r\n",
							"# sttransactionsdetail = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/FINANCE/sttransactionsdetail.parquet', format='parquet')\r\n",
							"# sttransactionsdetail.createOrReplaceTempView(\"sttransactionsdetail\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stchartofaccounts SAP Table\r\n",
							"# stchartofaccounts = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/FINANCE/stchartofaccounts.parquet', format='parquet')\r\n",
							"# stchartofaccounts.createOrReplaceTempView(\"stchartofaccounts\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stmdsapdepot SAP Table\r\n",
							"# stmdsapdepot = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/FINANCE/stmdsapdepot.parquet', format='parquet')\r\n",
							"# stmdsapdepot.createOrReplaceTempView(\"stmdsapdepot\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stbusinnesspartner SAP Table\r\n",
							"# stbusinnesspartner = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/FINANCE/stbusinnesspartner.parquet', format='parquet')\r\n",
							"# stbusinnesspartner.createOrReplaceTempView(\"stbusinnesspartner\")\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stopsfintransactions\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"  t.sap_transactionsline_acctcode \r\n",
							"  ,t.sap_transactionsheader_cardcode \r\n",
							"  ,t.sap_transactionsheader_cardref \r\n",
							"  ,t.sap_transactionsline_subdepotcode \r\n",
							"  ,t.sap_transactionsline_doccat \r\n",
							"  ,to_date(t.sap_transactionsheader_postingdate,'yyyy-MM-dd') as sap_transactionsheader_postingdate\r\n",
							"  ,to_date(t.sap_transactionsline_docdate,'yyyy-MM-dd') as  sap_transactionsline_docdate\r\n",
							"  ,CASE \r\n",
							"    WHEN \r\n",
							"      t.sap_transactionsline_doccat in ('generaljournal','purchaseorder') THEN 0\r\n",
							"    ELSE \r\n",
							"      COALESCE(t.sap_transactionsline_linetotallessdiscount,t.sap_transactionsline_linetotal)\r\n",
							"  END as sap_transactionsline_linetotalfinalexclgjpo\r\n",
							"  ,b.sap_businesspartners_cardname\r\n",
							"  ,d.md_depot_depotname\r\n",
							"  ,d.md_depot_division\r\n",
							"  ,d.md_depot_parentdepotcode\r\n",
							"  ,c.sap_account_level1name\r\n",
							"  ,c.sap_account_level4name\r\n",
							"\r\n",
							"FROM sttransactionsdetail t\r\n",
							"LEFT JOIN stchartofaccounts c on c.sap_account_acctcode = t.sap_transactionsline_acctcode \r\n",
							"LEFT JOIN stmdsapdepot d on d.sap_transactionsline_subdepotcode = t.sap_transactionsline_subdepotcode \r\n",
							"LEFT JOIN stbusinnesspartner b on b.sap_businesspartners_cardcode = t.sap_transactionsheader_cardcode \r\n",
							"WHERE (c.sap_account_level1name in ('TURNOVER') AND c.sap_account_level4name in ('REVENUE'))\r\n",
							"OR c.sap_account_level1name in ('COST OF SALES','OPERATING COSTS')\r\n",
							"AND t.sap_transactionsline_doccat not in ('generaljournal','purchaseorder','goodsreceiptnote') "
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stopsfintransactions = spark.sql(\"SELECT * FROM stopsfintransactions\")"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stopsfintransactions.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stopsfintransactions.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"SUCCESS\")"
						],
						"outputs": [],
						"execution_count": 6
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/13_STDepotMap')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Finance"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "29a5b9f8-2d28-40ba-8196-48fa8011e0c4"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse_Prod/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace-prod/bigDataPools/PRDSparkPool",
						"name": "PRDSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/PRDSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# import pandas as pd\r\n",
							"# from pyspark.sql.types import DecimalType\r\n",
							"# from pyspark.sql.functions import col"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# %%pyspark\r\n",
							"# stdepotmap_tmp = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Finance and Operations Mapping Sources/SAP_DepotLocation_Map.csv', format='csv'\r\n",
							"# , header=True\r\n",
							"# )"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stdepotmap_tmp = stdepotmap_tmp.withColumn('LOCATIONID', col('LOCATIONID').cast('int'))"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stdepotmap_tmp = stdepotmap_tmp.withColumnRenamed(\"LMS Depot Description\",\"LMS_Depot_Description\")"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stdepotmap_tmp.createOrReplaceTempView(\"stdepotmap_tmp\")"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stdepotmap\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    LOCATIONID as excel_depotmap_locationid\r\n",
							"    ,LMS_Depot_Description as excel_depotmap_locationdescription\r\n",
							"    ,DEPOTCODE as excel_depotmap_subdepot\r\n",
							"    ,PARENTDEPOTCODE as excel_depotmap_parentdepot\r\n",
							"FROM stdepotmap_tmp"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stdepotmap = spark.sql(\"SELECT * FROM stdepotmap\")"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stdepotmap.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stdepotmap.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"SUCCESS\")"
						],
						"outputs": [],
						"execution_count": 10
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/14_DIST_STReadMRPFiles')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Ops/Distribution"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "56c2546b-4d50-45fa-8c68-5753b6da454d"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace/bigDataPools/DevSparkPool",
						"name": "DevSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/DevSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# #create dataframe for the stparcel lms table\r\n",
							"# stparcel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stparcel.parquet/', format='parquet')\r\n",
							"# stparcel.createOrReplaceTempView(\"stparcel\")"
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# import os\r\n",
							"# import pyspark.sql\r\n",
							"# from pyspark.sql.types import StructType, StructField, DoubleType, StringType, DecimalType, IntegerType, DateType\r\n",
							"# from pyspark.sql.functions import *"
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# # Variables\r\n",
							"\r\n",
							"# var_File_Path = \"abfss://synapse@\"+ StorageAccountRead2 +\".dfs.core.windows.net/Unstructured Data/MRP\"\r\n",
							"# # Processing\r\n",
							"\r\n",
							"# mrp_csv_files = spark.read.option(\"header\", True).csv(var_File_Path)\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"mrp_csv_files = mrp_csv_files.withColumn('BillingExtractId', col(\"BillingExtractId\").cast('int'))\\\r\n",
							".withColumn('RetailYear', col('RetailYear').cast('int'))\\\r\n",
							".withColumn('RetailWeek', col('RetailWeek').cast('int'))\\\r\n",
							".withColumn('Quantity', col('Quantity').cast('int'))\\\r\n",
							".withColumn('VolumetricWeight', col('VolumetricWeight').cast(DecimalType(36,6)))\\\r\n",
							".withColumn('WeigtMeasureTypeId', col('WeigtMeasureTypeId').cast('int'))\\\r\n",
							".withColumn('FromStoreId', col('FromStoreId').cast('int'))\\\r\n",
							".withColumn('DestinationStoreId', col('DestinationStoreId').cast('int'))\\\r\n",
							".withColumn('ZoneId', col('ZoneId').cast('int'))\\\r\n",
							".withColumn('VolumetricRateExDC', col('VolumetricRateExDC').cast(DecimalType(36,6)))\\\r\n",
							".withColumn('VolumetricRateExDepot', col('VolumetricRateExDepot').cast(DecimalType(36,6)))\\\r\n",
							".withColumn('VolumetricRateExDC (Carriage Cost)', col('VolumetricRateExDC (Carriage Cost)').cast(DecimalType(36,6)))\\\r\n",
							".withColumn('VolumetricRateExDepot (Carriage Cost)', col('VolumetricRateExDepot (Carriage Cost)').cast(DecimalType(36,6)))\\\r\n",
							".withColumn('LH Carriage Cost', col('LH Carriage Cost').cast(DecimalType(36,6)))"
						],
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"mrp_csv_files = mrp_csv_files.withColumnRenamed(\"VolumetricRateExDC (Carriage Cost)\",\"VolumetricRateExDCCarriageCost\")\\\r\n",
							".withColumnRenamed(\"VolumetricRateExDepot (Carriage Cost)\",\"VolumetricRateExDepotCarriageCost\")\\\r\n",
							".withColumnRenamed(\"LH Carriage Cost\",\"LHCarriageCost\")"
						],
						"outputs": [],
						"execution_count": 27
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"mrp_csv_files = mrp_csv_files.fillna(\"NULL\")"
						],
						"outputs": [],
						"execution_count": 28
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"mrp_csv_files.createOrReplaceTempView(\"mrp_csv_files\")"
						],
						"outputs": [],
						"execution_count": 29
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stmrpcsvfiles\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"BillingExtractId as billingextractid,\r\n",
							"RetailYear as retailyear,\r\n",
							"RetailWeek as retailweek,\r\n",
							"ConsignmentNumber as consignmentnumber,\r\n",
							"TransferNumber as transfernumber,\r\n",
							"to_timestamp(REPLACE(TransferOutDate,' AM',''), 'd/M/yyyy HH:mm:ss') as transferoutdate,\r\n",
							"Quantity as quantity,\r\n",
							"VolumetricWeight  as volumetricweight ,\r\n",
							"WeigtMeasureTypeId as weigtmeasuretypeid,\r\n",
							"IsLineHaul as islinehaul,\r\n",
							"IsDistribution as isdistribution,\r\n",
							"FromStoreId as fromstoreid,\r\n",
							"DestinationStoreId as destinationstoreid,\r\n",
							"RouteCode as routecode,\r\n",
							"RouteDesc as routedesc,\r\n",
							"SubRouteCode as subroutecode,\r\n",
							"SubRouteDesc as subroutedesc,\r\n",
							"ZoneId as zoneid,\r\n",
							"ZRSNumber as zrsnumber,\r\n",
							"VolumetricRateExDC as volumetricrateexdc,\r\n",
							"VolumetricRateExDepot as volumetricrateexdepot,\r\n",
							"VolumetricRateExDCCarriageCost as volumetricrateexdccarriagecost,\r\n",
							"VolumetricRateExDepotCarriageCost as volumetricrateexdepotcarriagecost,\r\n",
							"LHCarriageCost as lhcarriagecost\r\n",
							"\r\n",
							"FROM mrp_csv_files"
						],
						"outputs": [],
						"execution_count": 30
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stmrpcsvfiles = spark.sql(\"SELECT * FROM stmrpcsvfiles\")"
						],
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# stmrpcsvfiles.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stmrpcsvfiles.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 32
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stparcelmrprecon\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    * \r\n",
							"FROM stparcel\r\n",
							"WHERE lms_parcel_pdate >= '2023-04-01 00:00:00' and lms_parcel_pdate <= '2024-03-31 23:59:59' \r\n",
							"AND lms_parcel_orderbillcustid IN (15,16,17,18,20,293941,293942,293943,293944,293945)"
						],
						"outputs": [],
						"execution_count": 33
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stparcelmrprecon = spark.sql(\"SELECT * FROM stparcelmrprecon\")"
						],
						"outputs": [],
						"execution_count": 34
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stparcelmrprecon.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stparcelmrprecon.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 35
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"sttransfers_tmp1\r\n",
							"AS\r\n",
							"SELECT DISTINCT\r\n",
							"    lms_parcel_barcode as combinetransfers\r\n",
							"    \r\n",
							"FROM stparcelmrprecon\r\n",
							"\r\n",
							"UNION\r\n",
							"\r\n",
							"SELECT DISTINCT\r\n",
							"    transfernumber as combinetransfers\r\n",
							"    \r\n",
							"FROM stmrpcsvfiles\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 36
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sttransfers_tmp1 = spark.sql(\"SELECT * FROM sttransfers_tmp1\")"
						],
						"outputs": [],
						"execution_count": 37
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"sttransfers\r\n",
							"AS\r\n",
							"SELECT DISTINCT\r\n",
							"     t.combinetransfers\r\n",
							"    ,LEFT(t.combinetransfers,5) as prefix\r\n",
							"    ,to_date(p.lms_parcel_pdate) as lms_parcel_pdate\r\n",
							"    ,to_date(m.transferoutdate) as transferoutdate\r\n",
							"    ,to_date(COALESCE(p.lms_parcel_pdate,m.transferoutdate)) as date\r\n",
							"    ,CASE \r\n",
							"        WHEN p.lms_parcel_pdate is null THEN 1\r\n",
							"        ELSE 0\r\n",
							"    END as existsinmrponly\r\n",
							"    ,CASE \r\n",
							"        WHEN m.transferoutdate is null THEN 1\r\n",
							"        ELSE 0\r\n",
							"    END as existsinlmsonly\r\n",
							"    ,CASE \r\n",
							"        WHEN p.lms_parcel_pdate is not null AND (m.transferoutdate) is not null THEN 1\r\n",
							"        ELSE 0\r\n",
							"    END as counter\r\n",
							"    \r\n",
							"\r\n",
							"    \r\n",
							"FROM sttransfers_tmp1 t\r\n",
							"LEFT JOIN stparcelmrprecon p on p.lms_parcel_barcode = t.combinetransfers\r\n",
							"LEFT JOIN stmrpcsvfiles m on m.transfernumber = t.combinetransfers\r\n",
							"WHERE t.combinetransfers is not NULL\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 45
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sttransfers = spark.sql(\"SELECT * FROM sttransfers\")"
						],
						"outputs": [],
						"execution_count": 46
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# sttransfers.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/sttransfers.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 47
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"SUCCESS\")"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/14_STPRPClocking')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Finance"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "bc0479d5-755d-4301-868f-57d1b01d199e"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace/bigDataPools/DevSparkPool",
						"name": "DevSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/DevSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# #Create DataFrame for the prpclockinghoursdetail CLMasterData Table\r\n",
							"# prpclockinghoursdetail = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/CLMasterData/prpclockinghoursdetail.parquet', format='parquet')\r\n",
							"# prpclockinghoursdetail.createOrReplaceTempView(\"prpclockinghoursdetail\")"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stprpclockinghoursdetail\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    id as prp_clockinghoursdetail_id\r\n",
							"    ,division as prp_clockinghoursdetail_division\r\n",
							"    ,costcentredescription as prp_clockinghoursdetail_costcentredescription\r\n",
							"    ,costcentrecode as prp_clockinghoursdetail_costcentrecode\r\n",
							"    ,teamname as prp_clockinghoursdetail_teamname\r\n",
							"    ,payroll as prp_clockinghoursdetail_payroll\r\n",
							"    ,employee as prp_clockinghoursdetail_employee\r\n",
							"    ,assignment as prp_clockinghoursdetail_assignment\r\n",
							"    ,idpassportno as prp_clockinghoursdetail_idpassportno\r\n",
							"    ,employeenumber as prp_clockinghoursdetail_employeenumber\r\n",
							"    ,vendorempnumber as prp_clockinghoursdetail_vendorempnumber\r\n",
							"    ,vendor as prp_clockinghoursdetail_vendor\r\n",
							"    ,jobcategory as prp_clockinghoursdetail_jobcategory\r\n",
							"    ,rate as prp_clockinghoursdetail_rate\r\n",
							"    ,to_date(date, 'dd MMM yyyy')  as prp_clockinghoursdetail_date\r\n",
							"    ,weekofyear(to_date(date, 'dd MMM yyyy'))-12  as prp_clockinghoursdetail_week\r\n",
							"    ,shift as prp_clockinghoursdetail_shift\r\n",
							"    ,shifttime as prp_clockinghoursdetail_shifttime\r\n",
							"    ,calctime as prp_clockinghoursdetail_calctime\r\n",
							"    ,clockin as prp_clockinghoursdetail_clockin\r\n",
							"    ,clockout as prp_clockinghoursdetail_clockout\r\n",
							"    ,approvalstatus as prp_clockinghoursdetail_approvalstatus\r\n",
							"    ,totinvval as prp_clockinghoursdetail_totinvval\r\n",
							"    ,finalflag as prp_clockinghoursdetail_finalflag\r\n",
							"FROM prpclockinghoursdetail\r\n",
							""
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stprpclockinghoursdetail = spark.sql(\"SELECT * FROM stprpclockinghoursdetail\")"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stprpclockinghoursdetail.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stprpclockinghoursdetail.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stprpclockinghoursdaily\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"     division as prp_clockinghoursdaily_division\r\n",
							"    ,costcentredescription as prp_clockinghoursdaily_costcentredescription\r\n",
							"    ,costcentrecode as prp_clockinghoursdaily_costcentrecode\r\n",
							"    ,teamname as prp_clockinghoursdaily_teamname\r\n",
							"    ,payroll as prp_clockinghoursdaily_payroll\r\n",
							"    ,vendor as prp_clockinghoursdaily_vendor\r\n",
							"    ,jobcategory as prp_clockinghoursdaily_jobcategory\r\n",
							"    ,to_date(date, 'dd MMM yyyy')  as prp_clockinghoursdaily_date\r\n",
							"    ,weekofyear(to_date(date, 'dd MMM yyyy'))-12  as prp_clockinghoursdaily_week\r\n",
							"    ,approvalstatus as prp_clockinghoursdaily_approvalstatus\r\n",
							"    ,sum(totinvval) as prp_clockinghoursdaily_totinvval\r\n",
							"    ,finalflag as prp_clockinghoursdaily_finalflag\r\n",
							"FROM prpclockinghoursdetail\r\n",
							"GROUP BY\r\n",
							"     division\r\n",
							"    ,costcentredescription\r\n",
							"    ,costcentrecode\r\n",
							"    ,teamname\r\n",
							"    ,payroll\r\n",
							"    ,vendor\r\n",
							"    ,jobcategory\r\n",
							"    ,to_date(date, 'dd MMM yyyy')\r\n",
							"    ,weekofyear(to_date(date, 'dd MMM yyyy'))-12\r\n",
							"    ,approvalstatus\r\n",
							"    ,finalflag\r\n",
							""
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stprpclockinghoursdaily = spark.sql(\"SELECT * FROM stprpclockinghoursdaily\")"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stprpclockinghoursdaily.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stprpclockinghoursdaily.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"SUCCESS\")"
						],
						"outputs": [],
						"execution_count": 12
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/15_DIST_STRetroDispatch')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Ops/Distribution"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "80ddb218-2bb4-4347-8c2f-d088d3bb5067"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace/bigDataPools/DevSparkPool",
						"name": "DevSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/DevSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# #Create DataFrame for the publicretroanalysis LMS Table\r\n",
							"# publicretroanalysis = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicreportdispatchretroanalysis.parquet', format='parquet')\r\n",
							"# publicretroanalysis.createOrReplaceTempView(\"publicretroanalysis\")"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stretroanalysis\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"p.id as orv_delivery_id\r\n",
							",p.waybillid as orv_delivery_waybillid\r\n",
							",p.deliveryid as orv_delivery_deliveryid\r\n",
							",p.manifest as orv_delivery_manifest\r\n",
							",p.dispatchid as orv_delivery_dispatchid\r\n",
							",p.depot as orv_delivery_depot\r\n",
							",p.fleetcode as orv_delivery_fleetcode\r\n",
							",p.driver as orv_delivery_driver\r\n",
							",p.progress as orv_delivery_progress\r\n",
							",to_timestamp(p.originaldeparturedate) as orv_delivery_originaldeparturedate\r\n",
							",to_timestamp(p.dispatchstartdate) as orv_delivery_dispatchstartdate\r\n",
							",to_timestamp(p.exitdepodate) as orv_delivery_exitdepodate\r\n",
							",to_timestamp(p.enterdepodate) as orv_delivery_enterdepodate\r\n",
							",p.totalpausetime as orv_delivery_totalpausetime\r\n",
							",to_timestamp(p.etatodepot) as orv_delivery_etatodepot\r\n",
							",p.delcustomer as orv_delivery_delcustomer\r\n",
							",p.deloriginalorder as orv_delivery_deloriginalorder\r\n",
							",p.deloptimisedorder as orv_delivery_deloptimisedorder\r\n",
							",p.delactualorder as orv_delivery_delactualorder\r\n",
							",p.distancetravelled as orv_delivery_distancetravelled\r\n",
							",to_timestamp(p.delactualeta) as orv_delivery_delactualeta\r\n",
							",to_timestamp(p.retroexpeta) as orv_delivery_retroexpeta\r\n",
							",to_timestamp(p.firstscandate) as orv_delivery_firstscandate\r\n",
							",p.deltaretroexpetatofisrtscan as orv_delivery_deltaretroexpetatofisrtscan\r\n",
							",p.delnoparcels as orv_delivery_delnoparcels\r\n",
							",to_timestamp(p.targetdeldate) as orv_delivery_targetdeldate\r\n",
							",p.actualdeltime as orv_delivery_actualdeltime\r\n",
							",to_timestamp(p.deliverydate) as orv_delivery_deliverydate\r\n",
							",to_timestamp(p.targetexittime) as orv_delivery_targetexittime\r\n",
							",to_timestamp(p.exittime) as orv_delivery_exittime\r\n",
							",p.delstatus as orv_delivery_delstatus\r\n",
							",p.deltalastscantopod as orv_delivery_deltalastscantopod\r\n",
							",to_timestamp(p.poddate) as orv_delivery_poddate\r\n",
							",p.deltadispatchstartdate as orv_delivery_deltadispatchstartdate\r\n",
							",p.deltadispatchexitandstart as orv_delivery_deltadispatchexitandstart\r\n",
							",p.deltaetatodepoandenterdepo as orv_delivery_deltaetatodepoandenterdepo\r\n",
							",p.stoppedindepo as orv_delivery_stoppedindepo\r\n",
							",p.targetdeltime as orv_delivery_targetdeltime\r\n",
							",p.actualtime as orv_delivery_actualtime\r\n",
							",p.deltaactualtimeandshouldtake as orv_delivery_deltaactualtimeandshouldtake\r\n",
							",p.shouldtake as orv_delivery_shouldtake\r\n",
							",to_timestamp(p.insdate) as orv_delivery_insdate\r\n",
							",p.originalorderkm as orv_delivery_originalorderkm\r\n",
							",p.optimisedorderkm as orv_delivery_optimisedorderkm\r\n",
							",p.totaldistancetravelledkm as orv_delivery_totaldistancetravelledkm\r\n",
							",to_timestamp(p.lastdeliverytime) as orv_delivery_lastdeliverytime\r\n",
							",to_timestamp(p.lastexitgeofencedeliverytime) as orv_delivery_lastexitgeofencedeliverytime\r\n",
							",p.totalactualdeliverytime as orv_delivery_totalactualdeliverytime\r\n",
							",p.totalestimateddeliverytime as orv_delivery_totalestimateddeliverytime\r\n",
							",p.deltadeliverytime as orv_delivery_deltadeliverytime\r\n",
							",p.distancefromlastdelivery as orv_delivery_distancefromlastdelivery\r\n",
							",p.lastdeliverytimetodepo as orv_delivery_lastdeliverytimetodepo\r\n",
							",to_timestamp(p.stopdispatch) as orv_delivery_stopdispatch\r\n",
							",p.totaldistanceactualkm as orv_delivery_totaldistanceactualkm\r\n",
							",p.totalestimateddrivetime as orv_delivery_totalestimateddrivetime\r\n",
							",p.lastdeliverycustomer as orv_delivery_lastdeliverycustomer\r\n",
							",p.actualdistancekm as orv_delivery_actualdistancekm\r\n",
							",p.estimateddrivetime as orv_delivery_estimateddrivetime\r\n",
							",p.billingcustomer as orv_delivery_billingcustomer\r\n",
							"\r\n",
							"FROM publicretroanalysis p\r\n",
							""
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stretroanalysis = spark.sql(\"SELECT * FROM stretroanalysis\")"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stretroanalysis.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stretroanalysis.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"SUCCESS\")"
						],
						"outputs": [],
						"execution_count": 13
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/16_DIST_STParcelDamagedAudit')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Ops/Distribution"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "bc89127e-8658-4f06-9265-ea669b27637a"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace/bigDataPools/DevSparkPool",
						"name": "DevSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/DevSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# #Create DataFrame for the dbodamagedparcelaudit LMS Table\r\n",
							"# dbodamagedparcelaudit = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodamagedparcelaudit.parquet', format='parquet')\r\n",
							"# dbodamagedparcelaudit.createOrReplaceTempView(\"dbodamagedparcelaudit\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stdamagedparcelauditfluteprofiles LMS Table\r\n",
							"# stdamagedparcelauditfluteprofiles = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stdamagedparcelauditfluteprofiles.parquet', format='parquet')\r\n",
							"# stdamagedparcelauditfluteprofiles.createOrReplaceTempView(\"stdamagedparcelauditfluteprofiles\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stdamagedparcelaudittapetype LMS Table\r\n",
							"# stdamagedparcelaudittapetype = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stdamagedparcelaudittapetype.parquet', format='parquet')\r\n",
							"# stdamagedparcelaudittapetype.createOrReplaceTempView(\"stdamagedparcelaudittapetype\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stdamagedparcelaudittype LMS Table\r\n",
							"# stdamagedparcelaudittype = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stdamagedparcelaudittype.parquet', format='parquet')\r\n",
							"# stdamagedparcelaudittype.createOrReplaceTempView(\"stdamagedparcelaudittype\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stlocation LMS Table\r\n",
							"# stlocation = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stlocation.parquet', format='parquet')\r\n",
							"# stlocation.createOrReplaceTempView(\"stlocation\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stparcel LMS Table\r\n",
							"# stparcel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stparcel.parquet', format='parquet')\r\n",
							"# stparcel.createOrReplaceTempView(\"stparcel\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stdamagedparcelaudit\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"d.id as lms_damagedparcelaudit_id\r\n",
							",d.parcelid as lms_damagedparcelaudit_parcelid\r\n",
							",d.userid as lms_damagedparcelaudit_userid\r\n",
							",d.auditdatetime as lms_damagedparcelaudit_auditdatetime\r\n",
							",d.locid as lms_damagedparcelaudit_locid\r\n",
							",d.loadid as lms_damagedparcelaudit_loadid\r\n",
							",d.quantity as lms_damagedparcelaudit_quantity\r\n",
							",d.flutemeasure as lms_damagedparcelaudit_flutemeasure\r\n",
							",d.weight as lms_damagedparcelaudit_weight\r\n",
							",d.length as lms_damagedparcelaudit_length\r\n",
							",d.width as lms_damagedparcelaudit_width\r\n",
							",d.height as lms_damagedparcelaudit_height\r\n",
							",d.shrinkwrap as lms_damagedparcelaudit_shrinkwrap\r\n",
							",d.tapeusedid as lms_damagedparcelaudit_tapeusedid\r\n",
							",d.missing as lms_damagedparcelaudit_missing\r\n",
							",d.damaged as lms_damagedparcelaudit_damaged\r\n",
							",d.parcelutilization as lms_damagedparcelaudit_parcelutilization\r\n",
							",d.damagetypeid as lms_damagedparcelaudit_damagetypeid\r\n",
							",d.typeid as lms_damagedparcelaudit_typeid\r\n",
							",p.lms_parcel_orderdelivercustid\r\n",
							",p.lms_parcel_orderbillcustid\r\n",
							",l.lms_location_description\r\n",
							",t.lms_damagedparcelaudittype_description\r\n",
							",tt.lms_damagedparcelaudittapetype_description\r\n",
							"\r\n",
							"FROM \r\n",
							"dbodamagedparcelaudit d\r\n",
							"LEFT JOIN stparcel p on d.parcelid = p.lms_parcel_id\r\n",
							"LEFT JOIN stlocation l on d.locid = l.lms_location_id\r\n",
							"LEFT JOIN stdamagedparcelaudittype t on d.damagetypeid = t.lms_damagedparcelaudittype_id\r\n",
							"LEFT JOIN stdamagedparcelaudittapetype tt on d.tapeusedid = tt.lms_damagedparcelaudittapetype_id\r\n",
							""
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stdamagedparcelaudit = spark.sql(\"SELECT * FROM stdamagedparcelaudit\")"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stdamagedparcelaudit.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stdamagedparcelaudit.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"SUCCESS\")"
						],
						"outputs": [],
						"execution_count": 16
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/301_RptTripLevel_linehaulleadtimereport')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Reports/TMS"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "8c44be04-c9a9-42a9-9660-128f4d368dcc"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace/bigDataPools/ApacheSparkPool",
						"name": "ApacheSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/ApacheSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 29
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# #Create DataFrame for the rpttriplevel LMS Table\r\n",
							"# rpttriplevel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/RPT/rpttriplevel.parquet', format='parquet')\r\n",
							"# rpttriplevel.createOrReplaceTempView(\"rpttriplevel\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 30
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"linehaulleadtimereport\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"\r\n",
							"     tripid\r\n",
							"    ,lastonscan \r\n",
							"    ,lastoffscan\r\n",
							"    ,dispatchstartdate\r\n",
							"    ,exitdepotgeofence\r\n",
							"    ,loadclosedate\r\n",
							"    ,startdate date\r\n",
							"    ,thirdpartyid\r\n",
							"    ,thirdpartyname\r\n",
							"    ,CAST(bookingids AS STRING) bookingids\r\n",
							"    ,CAST(loadids  AS STRING) loadids\r\n",
							"    ,noofparcels\r\n",
							"    ,weight\r\n",
							"    ,chargeweight\r\n",
							"    ,volweight\r\n",
							"    ,volumiserweight\r\n",
							"    ,(to_unix_timestamp(dispatchstartdate) - to_unix_timestamp(lastonscan)) lastonscantostartdispatch\r\n",
							"    ,(to_unix_timestamp(exitdepotgeofence) - to_unix_timestamp(lastonscan)) lastonscantoexitgeofence\r\n",
							"    ,(to_unix_timestamp(dispatchstartdate) - to_unix_timestamp(loadclosedate)) closeloadtostartdispatch\r\n",
							"    ,(to_unix_timestamp(exitdepotgeofence) - to_unix_timestamp(loadclosedate)) closeloadtoexitgeofence\r\n",
							"    ,effectiveloaddurationminutes\r\n",
							"    ,effectiveoffloaddurationminutes\r\n",
							"\r\n",
							"\r\n",
							"FROM rpttriplevel \r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"linehaulleadtimereport = spark.sql(\"SELECT * FROM linehaulleadtimereport\")"
						],
						"outputs": [],
						"execution_count": 32
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# linehaulleadtimereport.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/RPT/linehaulleadtimereport.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 33
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Dedupe on Track - 2023-05-18')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "AD-HOC"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "TESTSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "723602ee-c3f4-410e-ada0-8be7da867521"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse_Prod/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace-prod/bigDataPools/TESTSparkPool",
						"name": "TESTSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/TESTSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"df = spark.read.load('abfss://synapse@citylogisticsstorageprod.dfs.core.windows.net/Unstructured Data/LMS/dbotrack.parquet', format='parquet')\r\n",
							"df.createOrReplaceTempView(\"df\")"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df.write.parquet('abfss://synapse@citylogisticsstorageprod.dfs.core.windows.net/Unstructured Data_BU/LMS/dbotrack_BU_2023_07_03.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df = spark.read.load('abfss://synapse@citylogisticsstorageprod.dfs.core.windows.net/Unstructured Data_BU/LMS/dbotrack_BU_2023_07_03.parquet', format='parquet')\r\n",
							"df.createOrReplaceTempView(\"df\")"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df.dtypes"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"MAXVALUES\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"ID , MAX(CloseDt) CloseDt\r\n",
							"FROM df\r\n",
							"GROUP BY ID"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df.count()"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"SELECT COUNT(*) FROM MAXVALUES"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"--CREATE OR REPLACE TEMP VIEW\r\n",
							"--DEDUPE\r\n",
							"--AS\r\n",
							"SELECT \r\n",
							"df.ID , COUNT(df.ID)\r\n",
							"--COUNT(*)\r\n",
							"FROM df\r\n",
							"INNER JOIN MAXVALUES MV ON MV.ID = df.ID AND (MV.CloseDt = df.CloseDt OR MV.CloseDt is NULL)\r\n",
							"GROUP BY df.ID\r\n",
							"HAVING COUNT(ID) > 1"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"--CREATE OR REPLACE TEMP VIEW\r\n",
							"--DEDUPE\r\n",
							"--AS\r\n",
							"SELECT \r\n",
							"--df.ID , COUNT(df.ID)\r\n",
							"COUNT(*)\r\n",
							"FROM df\r\n",
							"INNER JOIN MAXVALUES MV ON MV.ID = df.ID AND (MV.CloseDt = df.CloseDt OR MV.CloseDt is NULL)\r\n",
							"--GROUP BY df.ID\r\n",
							"--HAVING COUNT(ID) > 1"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"DEDUPE\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"df.*\r\n",
							"FROM df\r\n",
							"INNER JOIN MAXVALUES MV ON MV.ID = df.ID AND (MV.CloseDt = df.CloseDt OR MV.CloseDt is NULL)"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"DEDUPE = spark.sql(\"SELECT * FROM DEDUPE\")"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"DEDUPE.write.parquet('abfss://synapse@citylogisticsstorageprod.dfs.core.windows.net/Unstructured Data/LMS/dbotrack.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Dedupe on bill_consignmentr')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "EXTRACT"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "TESTSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "abac35c9-fe4f-438e-aafa-f0e152b655fd"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse_Prod/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace-prod/bigDataPools/TESTSparkPool",
						"name": "TESTSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/TESTSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"df = spark.read.load('abfss://synapse@citylogisticsstorageprod.dfs.core.windows.net/Unstructured Data/LMS/dbobill_consignmentr.parquet', format='parquet')\r\n",
							"df.createOrReplaceTempView(\"df\")"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df.write.parquet('abfss://synapse@citylogisticsstorageprod.dfs.core.windows.net/Unstructured Data_BU/LMS/DeDupeWF/bill_consignmentrT1.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df = spark.read.load('abfss://synapse@citylogisticsstorageprod.dfs.core.windows.net/Unstructured Data_BU/LMS/DeDupeWF/bill_consignmentrT1.parquet', format='parquet')\r\n",
							"#df = spark.read.load('abfss://synapse@citylogisticsstorageprod.dfs.core.windows.net/Unstructured Data_BU/LMS/bill_consignmentr_BU_2023_06_29.parquet', format='parquet')\r\n",
							"df.createOrReplaceTempView(\"df\")"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df.dtypes"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"MAXVALUES\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"consignid , MAX(Id) Id\r\n",
							"FROM df\r\n",
							"GROUP BY consignid"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df.count()"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"SELECT COUNT(*) FROM MAXVALUES"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"--CREATE OR REPLACE TEMP VIEW\r\n",
							"--DEDUPE\r\n",
							"--AS\r\n",
							"SELECT \r\n",
							"df.consignid , COUNT(df.consignid)\r\n",
							"--COUNT(*)\r\n",
							"FROM df\r\n",
							"INNER JOIN MAXVALUES MV ON MV.Id = df.Id --AND (MV.CloseDt = df.CloseDt OR MV.CloseDt is NULL)\r\n",
							"GROUP BY df.consignid\r\n",
							"HAVING COUNT(consignid) > 1"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"--CREATE OR REPLACE TEMP VIEW\r\n",
							"--DEDUPE\r\n",
							"--AS\r\n",
							"SELECT \r\n",
							"--df.consignid , COUNT(df.consignid)\r\n",
							"COUNT(*)\r\n",
							"FROM df\r\n",
							"INNER JOIN MAXVALUES MV ON MV.Id = df.Id --AND (MV.CloseDt = df.CloseDt OR MV.CloseDt is NULL)\r\n",
							"--GROUP BY df.ID\r\n",
							"--HAVING COUNT(ID) > 1"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"DEDUPE\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"df.*\r\n",
							"FROM df\r\n",
							"INNER JOIN MAXVALUES MV ON MV.Id = df.Id --AND (MV.CloseDt = df.CloseDt OR MV.CloseDt is NULL)"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"DEDUPE = spark.sql(\"SELECT * FROM DEDUPE\")"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"DEDUPE.write.parquet('abfss://synapse@citylogisticsstorageprod.dfs.core.windows.net/Unstructured Data/LMS/dbobill_consignmentr.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from notebookutils import mssparkutils"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"mssparkutils.fs.rm('abfss://synapse@citylogisticsstorageprod.dfs.core.windows.net/Unstructured Data_BU/LMS/DeDupeWF/bill_consignmentrT1.parquet', True)"
						],
						"outputs": [],
						"execution_count": 14
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/IncrementalProdScript')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "835fc8fe-87a3-4d9e-921d-c6b9b0c75bd8"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse_Prod/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace-prod/bigDataPools/PRDSparkPool",
						"name": "PRDSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/PRDSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"tags": [
								"parameters"
							]
						},
						"source": [
							"cw_FolderName = \"\"\r\n",
							"ContainerName = \"\"\r\n",
							"DataLakeDF = \"\"\r\n",
							"Items = \"\"\r\n",
							"FullLoad = \"\"\r\n",
							"TempContainerName = \"\"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#cw_FolderName = \"Unstructured Data/SAP\"\r\n",
							"#ContainerName = \"synapse\"\r\n",
							"#DataLakeDF = \"citylogisticsstorageprod.dfs.core.windows.net/\"\r\n",
							"#Items = ''\r\n",
							"#FullLoad = \"true\"\r\n",
							"#TempContainerName = \"temp\""
						],
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import json\r\n",
							"from notebookutils import mssparkutils\r\n",
							"from pyspark.sql.functions import broadcast, col, count\r\n",
							"Items = json.loads(Items)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 27
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(Items)"
						],
						"outputs": [],
						"execution_count": 28
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Iterate through the cw_Items parameter\r\n",
							"failflag = 0\r\n",
							"failflaglist = []\r\n",
							"for i in Items :\r\n",
							"    try:\r\n",
							"        cw_FileName = i[\"Destination\"][\"FileName\"] + '.parquet'\r\n",
							"        IncField = i[\"Increment\"][\"IncField\"]\r\n",
							"        IdField = i[\"Increment\"][\"IdField\"]\r\n",
							"        print(cw_FileName, IncField, IdField)\r\n",
							"        copypath = 'abfss://' + TempContainerName + '@' + DataLakeDF + cw_FolderName + '/' + cw_FileName\r\n",
							"        t1path = ''\r\n",
							"        ## Perform parameter logic\r\n",
							"        if FullLoad == \"true\" : \r\n",
							"            print(\"FullLoad\")\r\n",
							"            FinalDF = spark.read.load(copypath, format='parquet') \r\n",
							"            Finalpath = 'abfss://' + ContainerName + '@' + DataLakeDF + cw_FolderName + '/' + cw_FileName\r\n",
							"        elif IncField == IdField :\r\n",
							"            print(\"Incremental ID LOAD\")\r\n",
							"            ##Load the Left File\r\n",
							"            path = 'abfss://' + ContainerName + '@' + DataLakeDF + '/' + cw_FolderName + '/' + cw_FileName\r\n",
							"            ProdFile = spark.read.load(path, format='parquet')\r\n",
							"            ##Load the Right File\r\n",
							"            IncrementFile = spark.read.load(copypath, format='parquet')\r\n",
							"            IncrementFileZeroCount = IncrementFile.filter(col(IncField).isNull()).count()\r\n",
							"            if IncrementFileZeroCount > 0 :\r\n",
							"                    raise Exception (\"IncField contains Null values\")\r\n",
							"            unioned = ProdFile.unionByName(IncrementFile, allowMissingColumns=False)\r\n",
							"            t1path = 'abfss://' + TempContainerName + '@' + DataLakeDF + cw_FolderName + '/T1_' + cw_FileName\r\n",
							"            unioned.write.parquet(t1path, mode='overwrite')\r\n",
							"            ##Load the Final File\r\n",
							"            FinalDF = spark.read.load(t1path, format='parquet')\r\n",
							"            Finalpath = 'abfss://' + ContainerName + '@' + DataLakeDF + '/' + cw_FolderName + '/' + cw_FileName\r\n",
							"            if unioned.count() < (0.75 * ProdFile.count()):\r\n",
							"                raise Exception (\"Error when Unioning table \" + cw_FileName + \" the Left side had \" + str(ProdFile.count()) + \" rows and the Unioned table has \" + str(unioned.count()))\r\n",
							"            else: \r\n",
							"                print(\"Union completed Succesfully\")\r\n",
							"            ##test = unioned.select(JoinFields)\r\n",
							"            UnionDistinctCount = unioned.select(IdField).distinct().count()\r\n",
							"            UnionCount = unioned.count()\r\n",
							"            if UnionDistinctCount != UnionCount :\r\n",
							"                    raise Exception (\"Duplicates when Unioning table \" + cw_FileName + \" the Unioned table has \" + str(UnionCount) + \" rows and the Distinct Unioned table has \" + str(UnionDistinctCount))\r\n",
							"            else: \r\n",
							"                print(\"Union completed Succesfully\")\r\n",
							"        elif IncField != IdField :\r\n",
							"            print(\"Incremental DATE LOAD\")\r\n",
							"            ##Load the Left File\r\n",
							"            path = 'abfss://' + ContainerName + '@' + DataLakeDF + '/' + cw_FolderName + '/' + cw_FileName\r\n",
							"            ProdFile = spark.read.load(path, format='parquet')\r\n",
							"            ##Load the Right File\r\n",
							"            IncrementFile = spark.read.load(copypath, format='parquet')\r\n",
							"            IncrementFileZeroCountIncField = IncrementFile.filter(col(IncField).isNull()).count()\r\n",
							"            IncrementFileZeroCountIdField = IncrementFile.filter(col(IdField).isNull()).count()\r\n",
							"            if (IncrementFileZeroCountIncField > 0) or (IncrementFileZeroCountIdField > 0) :\r\n",
							"                    print(\"IncField or IdField contains Null values\")\r\n",
							"                    raise Exception (\"IncField or IdField contains Null values\")\r\n",
							"            df_with_rows_deleted = ProdFile.join(broadcast(IncrementFile), on = IdField, how = 'left_anti')\r\n",
							"            unioned = df_with_rows_deleted.unionByName(IncrementFile, allowMissingColumns=False)\r\n",
							"            t1path = 'abfss://' + TempContainerName + '@' + DataLakeDF + cw_FolderName + '/T1_' + cw_FileName\r\n",
							"            unioned.write.parquet(t1path, mode='overwrite')\r\n",
							"            ##Load the Final File\r\n",
							"            FinalDF = spark.read.load(t1path, format='parquet')\r\n",
							"            Finalpath = 'abfss://' + ContainerName + '@' + DataLakeDF + '/' + cw_FolderName + '/' + cw_FileName\r\n",
							"            if unioned.count() < (0.75 * ProdFile.count()):\r\n",
							"                raise Exception (\"Error when Unioning table \" + cw_FileName + \" the Left side had \" + str(ProdFile.count()) + \" rows and the Unioned table has \" + str(unioned.count()))\r\n",
							"            else: \r\n",
							"                print(\"Union completed Succesfully\")\r\n",
							"            UnionDistinctCount = unioned.select(IdField).distinct().count()\r\n",
							"            UnionCount = unioned.count()\r\n",
							"            if UnionDistinctCount != UnionCount :\r\n",
							"                    raise Exception (\"Duplicates when Unioning table \" + cw_FileName + \" the Unioned table has \" + str(UnionCount) + \" rows and the Distinct Unioned table has \" + str(UnionDistinctCount))\r\n",
							"            else: \r\n",
							"                print(\"Union completed Succesfully\")\r\n",
							"        else:\r\n",
							"            raise ValueError(\"Parameters not correctly specified\")\r\n",
							"        print(Finalpath)\r\n",
							"        FinalDF.write.parquet(Finalpath, mode='overwrite')\r\n",
							"        print(copypath, t1path)\r\n",
							"        ## Remove the temporary tables and directories in the the temp container\r\n",
							"        mssparkutils.fs.rm(copypath, True) \r\n",
							"        if t1path != '' :\r\n",
							"            mssparkutils.fs.rm(t1path, True) \r\n",
							"    except:\r\n",
							"        failflag = 1\r\n",
							"        failflaglist += [i]\r\n",
							"## Fail pipeline if any iteration failed\r\n",
							"if failflag == 1:\r\n",
							"    print(failflaglist)\r\n",
							"    raise ValueError(\"One of the table iterations has failed\") "
						],
						"outputs": [],
						"execution_count": 29
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/PartitionSparkJob')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "TESTSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "48c9899c-10f6-439a-bd9a-abfc919e50eb"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse_Prod/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace-prod/bigDataPools/TESTSparkPool",
						"name": "TESTSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/TESTSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"cw_FolderName = \"\"\r\n",
							"cw_FileName = \"\"\r\n",
							"cw_IncFolderName = \"\"\r\n",
							"cw_IdField = \"\"\r\n",
							"ContainerName = \"\"\r\n",
							"DataLakeDF = \"\""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"##Load the Left File\r\n",
							"path = 'abfss://' + ContainerName + '@' + DataLakeDF + cw_FolderName + '/T1/' + cw_FileName\r\n",
							"LeftFile = spark.read.load(path, format='parquet')"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"Finalpath = 'abfss://' + ContainerName + '@' + DataLakeDF + '/' + cw_FolderName + '/' + cw_FileName"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"LeftFile.write.parquet(Finalpath, mode='overwrite')"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/RowCount')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "DEV/DataAutomationChecks"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "TESTSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "981a96ff-909d-40c1-81a1-e1360ee30446"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse_Prod/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace-prod/bigDataPools/TESTSparkPool",
						"name": "TESTSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/TESTSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql.functions import col, lit, current_timestamp\r\n",
							"from pyspark.sql import functions as F\r\n",
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType\r\n",
							"from datetime import datetime"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Define schema for the logging DataFrame\r\n",
							"schema = StructType([\r\n",
							"    StructField(\"TableName\", StringType(), True),\r\n",
							"    StructField(\"DateTime\", StringType(), True),\r\n",
							"    StructField(\"RowCount\", IntegerType(), True),\r\n",
							"    StructField(\"Diff\", StringType(), True)\r\n",
							"])\r\n",
							"\r\n",
							"log_df = spark.createDataFrame([], schema)"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create some test tables with mock data\r\n",
							"table1 = spark.createDataFrame([(1, 'foo'), (2, 'bar'), (3, 'baz'),(3, 'baz'),(3, 'baz')], ['id', 'value'])\r\n",
							"table2 = spark.createDataFrame([(4, 'apple'), (5, 'banana')], ['id', 'fruit'])\r\n",
							"table3 = spark.createDataFrame([(6, 'car'), (7, 'bus'), (8, 'bike'), (9, 'plane')], ['id', 'vehicle'])\r\n",
							"\r\n",
							"# Register the DataFrames as tables to mimic actual tables in a data lake\r\n",
							"table1.createOrReplaceTempView('table1')\r\n",
							"table2.createOrReplaceTempView('table2')\r\n",
							"table3.createOrReplaceTempView('table3')"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# List of table names you want to monitor\r\n",
							"tables_to_monitor = [\"table1\", \"table2\", \"table3\"]\r\n",
							"\r\n",
							"for table_name in tables_to_monitor:\r\n",
							"    try:\r\n",
							"        # Read the table from the data lake into a DataFrame\r\n",
							"        df = spark.table(table_name)\r\n",
							"\r\n",
							"        # Count the number of rows\r\n",
							"        current_count = df.count()\r\n",
							"\r\n",
							"        # Get the DateTime\r\n",
							"        current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\r\n",
							"\r\n",
							"        # Fetch the most recent row count for this table from log_df\r\n",
							"        latest_row = log_df.filter(F.col(\"TableName\") == table_name).orderBy(F.col(\"DateTime\").desc()).limit(1).collect()\r\n",
							"\r\n",
							"        if len(latest_row) == 0:\r\n",
							"            # This is the first entry for this table\r\n",
							"            diff = \"NA\"\r\n",
							"        else:\r\n",
							"            previous_count = latest_row[0][\"RowCount\"]\r\n",
							"            diff = current_count - previous_count\r\n",
							"            \r\n",
							"            # Alert if the row count stays the same\r\n",
							"            if diff == 0:\r\n",
							"                print(f\"Alert: Row count for {table_name} has not changed. Still {current_count}.\")\r\n",
							"\r\n",
							"        # Append the new data to log_df\r\n",
							"        new_row = spark.createDataFrame([(table_name, current_datetime, current_count, str(diff))], schema)\r\n",
							"        log_df = log_df.union(new_row)\r\n",
							"\r\n",
							"    except Exception as e:\r\n",
							"        print(f\"Error processing table {table_name}: {str(e)}\", file=sys.stderr)\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"display(log_df)"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Last Refresh Check"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.window import Window"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def update_log_and_check_error(log_df, table_df):\r\n",
							"    # Step 1: Read the last modified date from the log dataframe\r\n",
							"    latest_log_entry = log_df \\\r\n",
							"        .withColumn(\"LastModifiedDate\", F.to_timestamp(\"LastModifiedDate\")) \\\r\n",
							"        .groupBy(\"TableName\") \\\r\n",
							"        .agg(F.max(\"LastModifiedDate\").alias(\"LatestModifiedDate\"))\r\n",
							"\r\n",
							"    # Step 2: Read \"the_table\" dataframe\r\n",
							"    table_df = table_df.withColumn(\"LastModifiedDate\", F.to_timestamp(\"LastModifiedDate\"))\r\n",
							"\r\n",
							"    # Step 3: Compare last modified dates and log an error if they are the same\r\n",
							"    check_df = table_df \\\r\n",
							"        .join(latest_log_entry, \"TableName\") \\\r\n",
							"        .filter(\"LastModifiedDate = LatestModifiedDate\")\r\n",
							"\r\n",
							"    if check_df.count() > 0:\r\n",
							"        # Log an error or perform other actions as needed\r\n",
							"        print(\"Error: Last modified date in log is the same as the current table.\")\r\n",
							"        # You might want to log this error or raise an exception depending on your requirement\r\n",
							"\r\n",
							"    # Update the log dataframe with the latest modified date\r\n",
							"    new_log_entry = table_df \\\r\n",
							"        .groupBy(\"TableName\") \\\r\n",
							"        .agg(F.max(\"LastModifiedDate\").alias(\"LatestModifiedDate\"))\r\n",
							"\r\n",
							"    log_df = log_df.union(new_log_entry)\r\n",
							"\r\n",
							"    return log_df"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"log_data = [(\"table1\", \"2023-09-29 10:00:00\"),(\"table2\", \"2023-09-29 11:30:00\")]\r\n",
							"\r\n",
							"# Sample data for the test dataframe\r\n",
							"test_data = [(\"table1\", \"2023-09-29 12:00:00\"),\r\n",
							"                (\"table2\", \"2023-09-29 11:30:00\")]\r\n",
							"\r\n",
							"# Create log dataframe\r\n",
							"log_schema = [\"TableName\", \"LastModifiedDate\"]\r\n",
							"log_df = spark.createDataFrame(log_data, schema=log_schema)\r\n",
							"\r\n",
							"# Create test dataframe\r\n",
							"test_schema = [\"TableName\", \"LastModifiedDate\"]\r\n",
							"test_df = spark.createDataFrame(test_data, schema=test_schema)"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Update log and check for errors\r\n",
							"updated_log_df = update_log_and_check_error(log_df, test_df)\r\n",
							"\r\n",
							"# Show the updated log dataframe\r\n",
							"updated_log_df.show()"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Duplicates Sources Check"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def get_folder_list(spark, base_path):\r\n",
							"    folder_list = []\r\n",
							"\r\n",
							"    try:\r\n",
							"        URI = spark._jvm.java.net.URI\r\n",
							"        Path = spark._jvm.org.apache.hadoop.fs.Path\r\n",
							"        FileSystem = spark._jvm.org.apache.hadoop.fs.FileSystem\r\n",
							"\r\n",
							"        fs = FileSystem.get(URI(base_path), spark._jsc.hadoopConfiguration())\r\n",
							"        status = fs.listStatus(Path(base_path))\r\n",
							"\r\n",
							"        for file_status in status:\r\n",
							"            file_path = file_status.getPath().toString()\r\n",
							"\r\n",
							"            # Check if it's a directory\r\n",
							"            if fs.isDirectory(file_status.getPath()):\r\n",
							"                folder_list.append(file_path)\r\n",
							"\r\n",
							"        return folder_list\r\n",
							"\r\n",
							"    except Py4JJavaError as e:\r\n",
							"        print(\"Error occurred while listing folders: \", e)\r\n",
							"        return None"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def has_duplicates(input_list):\r\n",
							"    seen = set()\r\n",
							"    for item in input_list:\r\n",
							"        if item in seen:\r\n",
							"            return True  # Found a duplicate\r\n",
							"        seen.add(item)\r\n",
							"    return False"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## ALERTS"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.identity import DefaultAzureCredential\r\n",
							"from azure.monitor.query import LogsQueryClient\r\n",
							"\r\n",
							"credential = DefaultAzureCredential()\r\n",
							"query_client = LogsQueryClient(credential)"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SSTAggrParcelByX')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/01 SEMI-STRUCTURED/02_LMS Aggr"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "f7247c30-fe62-4060-bf7c-20e1476da296"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse_Prod/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace-prod/bigDataPools/PRDSparkPool",
						"name": "PRDSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/PRDSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# # Create SSTParcelByOrder \r\n",
							"# # Create SSTParcelByWaybill\r\n",
							"# # Create SSTParcelByConsignment\r\n",
							"# # Create SSTParcelByLoad\r\n",
							"\r\n",
							"# #Create DataFrame for the sstparcelunion LMS Table\r\n",
							"# sstparcelunion = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Semi Structured Data/OPS/sstparcelunion.parquet', format='parquet')\r\n",
							"# sstparcelunion.createOrReplaceTempView(\"sstparcelunion\")\r\n",
							"\r\n",
							"# #Create DataFrame for the ssttrackunion LMS Table\r\n",
							"# ssttrackunion = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Semi Structured Data/OPS/ssttrackunion.parquet', format='parquet')\r\n",
							"# ssttrackunion.createOrReplaceTempView(\"ssttrackunion\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dboorder LMS Table\r\n",
							"# dboorder = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboorder.parquet', format='parquet')\r\n",
							"# dboorder.createOrReplaceTempView(\"dboorder\")"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import greatest"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"sstparcelbyorder_tmp1\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"     P.orderid as lms_parcelbyorder_orderid\r\n",
							"    ,Sum(P.weight) as lms_parcelbyorder_aggrweight\r\n",
							"    ,Sum(P.chargeweight) as lms_parcelbyorder_aggrchargeweight\r\n",
							"    ,Sum(P.volweight) as lms_parcelbyorder_aggrvolweight\r\n",
							"    ,Sum(P.volumiserweight) as lms_parcelbyorder_aggrvolumiserweight\r\n",
							"    ,Sum(greatest(P.volumiserweight,P.weight,P.chargeweight,P.volumiserweight)) as lms_parcelbyorder_aggrmaxweight \r\n",
							"    ,Count(P.id) as lms_parcelbyorder_aggrnoparcels\r\n",
							"FROM sstparcelunion P\r\n",
							"GROUP BY P.orderid"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"sstparcelbyorder_tmp2\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"     P.orderid as lms_parcelbyorder_orderid\r\n",
							"    ,Sum(P.weight) as lms_parcelbyorder_aggrweightexclspecdel\r\n",
							"    ,Sum(P.chargeweight) as lms_parcelbyorder_aggrchargeweightexclspecdel\r\n",
							"    ,Sum(P.volweight) as lms_parcelbyorder_aggrvolweightexclspecdel\r\n",
							"    ,Sum(P.volumiserweight) as lms_parcelbyorder_aggrvolumiserweightexclspecdel\r\n",
							"    ,Sum(greatest(P.volumiserweight,P.weight,P.chargeweight,P.volumiserweight)) as lms_parcelbyorder_aggrmaxweightexclspecdel\r\n",
							"    ,Count(P.id) as lms_parcelbyorder_aggrnoparcelsexclspecdel\r\n",
							"FROM sstparcelunion P\r\n",
							"LEFT JOIN dboorder o on o.id = p.orderid\r\n",
							"WHERE o.specdel = 0\r\n",
							"GROUP BY P.orderid"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sstparcelbyorder_tmp1 = spark.sql(\"SELECT * FROM sstparcelbyorder_tmp1\")\r\n",
							"sstparcelbyorder_tmp2 = spark.sql(\"SELECT * FROM sstparcelbyorder_tmp2\")"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"sstparcelbyorder\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"     t1.lms_parcelbyorder_orderid\r\n",
							"    ,CAST(t1.lms_parcelbyorder_aggrweight AS decimal(28,2)) as lms_parcelbyorder_aggrweight\r\n",
							"    ,CAST(t1.lms_parcelbyorder_aggrchargeweight AS decimal(28,2)) as lms_parcelbyorder_aggrchargeweight\r\n",
							"    ,CAST(t1.lms_parcelbyorder_aggrvolweight AS decimal(28,2)) as lms_parcelbyorder_aggrvolweight\r\n",
							"    ,CAST(t1.lms_parcelbyorder_aggrvolumiserweight AS decimal(28,2)) as lms_parcelbyorder_aggrvolumiserweight\r\n",
							"    ,CAST(t1.lms_parcelbyorder_aggrmaxweight  AS decimal(28,2)) as lms_parcelbyorder_aggrmaxweight\r\n",
							"    ,t1.lms_parcelbyorder_aggrnoparcels\r\n",
							"    ,CAST(t2.lms_parcelbyorder_aggrweightexclspecdel AS decimal(28,2)) as lms_parcelbyorder_aggrweightexclspecdel\r\n",
							"    ,CAST(t2.lms_parcelbyorder_aggrchargeweightexclspecdel AS decimal(28,2)) as lms_parcelbyorder_aggrchargeweightexclspecdel\r\n",
							"    ,CAST(t2.lms_parcelbyorder_aggrvolweightexclspecdel AS decimal(28,2)) as lms_parcelbyorder_aggrvolweightexclspecdel\r\n",
							"    ,CAST(t2.lms_parcelbyorder_aggrvolumiserweightexclspecdel AS decimal(28,2)) as lms_parcelbyorder_aggrvolumiserweightexclspecdel\r\n",
							"    ,CAST(t2.lms_parcelbyorder_aggrmaxweightexclspecdel AS decimal(28,2)) as lms_parcelbyorder_aggrmaxweightexclspecdel\r\n",
							"    ,t2.lms_parcelbyorder_aggrnoparcelsexclspecdel\r\n",
							"FROM sstparcelbyorder_tmp1 t1\r\n",
							"LEFT JOIN sstparcelbyorder_tmp2 t2 on t2.lms_parcelbyorder_orderid = t1.lms_parcelbyorder_orderid\r\n",
							""
						],
						"outputs": [],
						"execution_count": 165
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sstparcelbyorder = spark.sql(\"SELECT * FROM sstparcelbyorder\")"
						],
						"outputs": [],
						"execution_count": 166
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# sstparcelbyorder.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Semi Structured Data/OPS/sstparcelbyorder.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 167
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"sstparcelbywaybill_tmp1\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"     P.waybillid as lms_parcelbywaybill_waybillid\r\n",
							"    ,Sum(P.weight) as lms_parcelbywaybill_aggrweight\r\n",
							"    ,Sum(P.chargeweight) as lms_parcelbywaybill_aggrchargeweight\r\n",
							"    ,Sum(P.volweight) as lms_parcelbywaybill_aggrvolweight\r\n",
							"    ,Sum(P.volumiserweight) as lms_parcelbywaybill_aggrvolumiserweight\r\n",
							"    ,Sum(greatest(P.volumiserweight,P.weight,P.chargeweight,P.volumiserweight)) as lms_parcelbywaybill_aggrmaxweight \r\n",
							"    ,Count(P.id) as lms_parcelbywaybill_aggrnoparcels\r\n",
							"FROM sstparcelunion P\r\n",
							"GROUP BY P.waybillid"
						],
						"outputs": [],
						"execution_count": 168
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"sstparcelbywaybill_tmp2\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"     P.waybillid as lms_parcelbywaybill_waybillid\r\n",
							"    ,Sum(P.weight) as lms_parcelbywaybill_aggrweightexclspecdel\r\n",
							"    ,Sum(P.chargeweight) as lms_parcelbywaybill_aggrchargeweightexclspecdel\r\n",
							"    ,Sum(P.volweight) as lms_parcelbywaybill_aggrvolweightexclspecdel\r\n",
							"    ,Sum(P.volumiserweight) as lms_parcelbywaybill_aggrvolumiserweightexclspecdel\r\n",
							"    ,Sum(greatest(P.volumiserweight,P.weight,P.chargeweight,P.volumiserweight)) as lms_parcelbywaybill_aggrmaxweightexclspecdel\r\n",
							"    ,Count(P.id) as lms_parcelbywaybill_aggrnoparcelsexclspecdel\r\n",
							"FROM sstparcelunion P\r\n",
							"LEFT JOIN dboorder o on o.id = p.orderid\r\n",
							"WHERE o.specdel = 0\r\n",
							"GROUP BY P.waybillid"
						],
						"outputs": [],
						"execution_count": 169
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sstparcelbywaybill_tmp1 = spark.sql(\"SELECT * FROM sstparcelbywaybill_tmp1\")\r\n",
							"sstparcelbywaybill_tmp2 = spark.sql(\"SELECT * FROM sstparcelbywaybill_tmp2\")"
						],
						"outputs": [],
						"execution_count": 170
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"sstparcelbywaybill\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"     t1.lms_parcelbywaybill_waybillid\r\n",
							"    ,CAST(t1.lms_parcelbywaybill_aggrweight AS decimal(28,2)) as lms_parcelbywaybill_aggrweight\r\n",
							"    ,CAST(t1.lms_parcelbywaybill_aggrchargeweight AS decimal(28,2)) as lms_parcelbywaybill_aggrchargeweight\r\n",
							"    ,CAST(t1.lms_parcelbywaybill_aggrvolweight AS decimal(28,2)) as lms_parcelbywaybill_aggrvolweight\r\n",
							"    ,CAST(t1.lms_parcelbywaybill_aggrvolumiserweight AS decimal(28,2)) as lms_parcelbywaybill_aggrvolumiserweight\r\n",
							"    ,CAST(t1.lms_parcelbywaybill_aggrmaxweight AS decimal(28,2)) as lms_parcelbywaybill_aggrmaxweight\r\n",
							"    ,t1.lms_parcelbywaybill_aggrnoparcels\r\n",
							"    ,CAST(t2.lms_parcelbywaybill_aggrweightexclspecdel AS decimal(28,2)) as lms_parcelbywaybill_aggrweightexclspecdel\r\n",
							"    ,CAST(t2.lms_parcelbywaybill_aggrchargeweightexclspecdel AS decimal(28,2)) as lms_parcelbywaybill_aggrchargeweightexclspecdel\r\n",
							"    ,CAST(t2.lms_parcelbywaybill_aggrvolweightexclspecdel AS decimal(28,2)) as lms_parcelbywaybill_aggrvolweightexclspecdel\r\n",
							"    ,CAST(t2.lms_parcelbywaybill_aggrvolumiserweightexclspecdel AS decimal(28,2)) as lms_parcelbywaybill_aggrvolumiserweightexclspecdel\r\n",
							"    ,CAST(t2.lms_parcelbywaybill_aggrmaxweightexclspecdel AS decimal(28,2)) as lms_parcelbywaybill_aggrmaxweightexclspecdel\r\n",
							"    ,t2.lms_parcelbywaybill_aggrnoparcelsexclspecdel\r\n",
							"FROM sstparcelbywaybill_tmp1 t1\r\n",
							"LEFT JOIN sstparcelbywaybill_tmp2 t2 on t2.lms_parcelbywaybill_waybillid = t1.lms_parcelbywaybill_waybillid"
						],
						"outputs": [],
						"execution_count": 171
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sstparcelbywaybill = spark.sql(\"SELECT * FROM sstparcelbywaybill\")"
						],
						"outputs": [],
						"execution_count": 172
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# sstparcelbywaybill.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Semi Structured Data/OPS/sstparcelbywaybill.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 173
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"sstparcelbyconsignment_tmp1\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"     P.consignid as lms_parcelbyconsignment_consignid\r\n",
							"    ,Sum(P.weight) as lms_parcelbyconsignment_aggrweight\r\n",
							"    ,Sum(P.chargeweight) as lms_parcelbyconsignment_aggrchargeweight\r\n",
							"    ,Sum(P.volweight) as lms_parcelbyconsignment_aggrvolweight\r\n",
							"    ,Sum(P.volumiserweight) as lms_parcelbyconsignment_aggrvolumiserweight\r\n",
							"    ,Sum(greatest(P.volumiserweight,P.weight,P.chargeweight,P.volumiserweight)) as lms_parcelbyconsignment_aggrmaxweight\r\n",
							"    ,Count(P.id) as lms_parcelbyconsignment_aggrnoparcels\r\n",
							"FROM sstparcelunion P\r\n",
							"GROUP BY P.consignid"
						],
						"outputs": [],
						"execution_count": 174
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"sstparcelbyconsignment_tmp2\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"     P.consignid as lms_parcelbyconsignment_consignid\r\n",
							"    ,Sum(P.weight) as lms_parcelbyconsignment_aggrweightexclspecdel\r\n",
							"    ,Sum(P.chargeweight) as lms_parcelbyconsignment_aggrchargeweightexclspecdel\r\n",
							"    ,Sum(P.volweight) as lms_parcelbyconsignment_aggrvolweightexclspecdel\r\n",
							"    ,Sum(P.volumiserweight) as lms_parcelbyconsignment_aggrvolumiserweightexclspecdel\r\n",
							"    ,Sum(greatest(P.volumiserweight,P.weight,P.chargeweight,P.volumiserweight)) as lms_parcelbyconsignment_aggrmaxweightexclspecdel\r\n",
							"    ,Count(P.id) as lms_parcelbyconsignment_aggrnoparcelsexclspecdel\r\n",
							"FROM sstparcelunion P\r\n",
							"LEFT JOIN dboorder o on o.id = p.orderid\r\n",
							"WHERE o.specdel = 0\r\n",
							"GROUP BY P.consignid"
						],
						"outputs": [],
						"execution_count": 175
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sstparcelbyconsignment_tmp1 = spark.sql(\"SELECT * FROM sstparcelbyconsignment_tmp1\")\r\n",
							"sstparcelbyconsignment_tmp2 = spark.sql(\"SELECT * FROM sstparcelbyconsignment_tmp2\")"
						],
						"outputs": [],
						"execution_count": 176
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"sstparcelbyconsignment\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"     t1.lms_parcelbyconsignment_consignid\r\n",
							"    ,CAST(t1.lms_parcelbyconsignment_aggrweight AS decimal(28,2)) as lms_parcelbyconsignment_aggrweight\r\n",
							"    ,CAST(t1.lms_parcelbyconsignment_aggrchargeweight AS decimal(28,2)) as lms_parcelbyconsignment_aggrchargeweight\r\n",
							"    ,CAST(t1.lms_parcelbyconsignment_aggrvolweight AS decimal(28,2)) as lms_parcelbyconsignment_aggrvolweight\r\n",
							"    ,CAST(t1.lms_parcelbyconsignment_aggrvolumiserweight AS decimal(28,2)) as lms_parcelbyconsignment_aggrvolumiserweight\r\n",
							"    ,CAST(t1.lms_parcelbyconsignment_aggrmaxweight AS decimal(28,2)) as lms_parcelbyconsignment_aggrmaxweight\r\n",
							"    ,t1.lms_parcelbyconsignment_aggrnoparcels\r\n",
							"    ,CAST(t2.lms_parcelbyconsignment_aggrweightexclspecdel AS decimal(28,2)) as lms_parcelbyconsignment_aggrweightexclspecdel\r\n",
							"    ,CAST(t2.lms_parcelbyconsignment_aggrchargeweightexclspecdel AS decimal(28,2)) as lms_parcelbyconsignment_aggrchargeweightexclspecdel\r\n",
							"    ,CAST(t2.lms_parcelbyconsignment_aggrvolweightexclspecdel AS decimal(28,2)) as lms_parcelbyconsignment_aggrvolweightexclspecdel\r\n",
							"    ,CAST(t2.lms_parcelbyconsignment_aggrvolumiserweightexclspecdel AS decimal(28,2)) as lms_parcelbyconsignment_aggrvolumiserweightexclspecdel\r\n",
							"    ,CAST(t2.lms_parcelbyconsignment_aggrmaxweightexclspecdel AS decimal(28,2)) as lms_parcelbyconsignment_aggrmaxweightexclspecdel\r\n",
							"    ,t2.lms_parcelbyconsignment_aggrnoparcelsexclspecdel \r\n",
							"FROM sstparcelbyconsignment_tmp1 t1\r\n",
							"LEFT JOIN sstparcelbyconsignment_tmp2 t2 on t2.lms_parcelbyconsignment_consignid = t1.lms_parcelbyconsignment_consignid"
						],
						"outputs": [],
						"execution_count": 177
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sstparcelbyconsignment = spark.sql(\"SELECT * FROM sstparcelbyconsignment\")"
						],
						"outputs": [],
						"execution_count": 178
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# sstparcelbyconsignment.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Semi Structured Data/OPS/sstparcelbyconsignment.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 179
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"sstparcelbyload_tmp1 \r\n",
							"AS\r\n",
							"SELECT \r\n",
							"     T.loadid as lms_parcelbyload_loadid\r\n",
							"    ,Sum(T.weight) as lms_parcelbyload_aggrweight\r\n",
							"    ,Sum(T.chargeweight) as lms_parcelbyload_aggrchargeweight\r\n",
							"    ,Sum(T.volweight) as lms_parcelbyload_aggrvolweight\r\n",
							"    ,Sum(T.volumiserweight) as lms_parcelbyload_aggrvolumiserweight\r\n",
							"    ,Sum(greatest(T.volumiserweight,T.weight,T.chargeweight,T.volumiserweight)) as lms_parcelbyload_aggrmaxweight\r\n",
							"    ,Count(T.parcelid) as lms_parcelbyload_aggrnoparcels\r\n",
							"FROM ssttrackunion T\r\n",
							"GROUP BY T.loadid"
						],
						"outputs": [],
						"execution_count": 180
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"sstparcelbyload_tmp2\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"     T.loadid as lms_parcelbyload_loadid\r\n",
							"    ,Sum(T.weight) as lms_parcelbyload_aggrweightexclspecdel\r\n",
							"    ,Sum(T.chargeweight) as lms_parcelbyload_aggrchargeweightexclspecdel\r\n",
							"    ,Sum(T.volweight) as lms_parcelbyload_aggrvolweightexclspecdel\r\n",
							"    ,Sum(T.volumiserweight) as lms_parcelbyload_aggrvolumiserweightexclspecdel\r\n",
							"    ,Sum(greatest(T.volumiserweight,T.weight,T.chargeweight,T.volumiserweight)) as lms_parcelbyload_aggrmaxweightexclspecdel\r\n",
							"    ,Count(T.id) as lms_parcelbyload_aggrnoparcelsexclspecdel\r\n",
							"FROM ssttrackunion T\r\n",
							"WHERE T.specdel = 0\r\n",
							"GROUP BY T.loadid"
						],
						"outputs": [],
						"execution_count": 181
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sstparcelbyload_tmp1 = spark.sql(\"SELECT * FROM sstparcelbyload_tmp1\")\r\n",
							"sstparcelbyload_tmp2 = spark.sql(\"SELECT * FROM sstparcelbyload_tmp2\")"
						],
						"outputs": [],
						"execution_count": 182
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"sstparcelbyload\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"     t1.lms_parcelbyload_loadid\r\n",
							"    ,CAST(t1.lms_parcelbyload_aggrweight AS decimal(28,2)) AS lms_parcelbyload_aggrweight\r\n",
							"    ,CAST(t1.lms_parcelbyload_aggrchargeweight  AS decimal(28,2)) AS lms_parcelbyload_aggrchargeweight\r\n",
							"    ,CAST(t1.lms_parcelbyload_aggrvolweight  AS decimal(28,2)) AS lms_parcelbyload_aggrvolweight\r\n",
							"    ,CAST(t1.lms_parcelbyload_aggrvolumiserweight  AS decimal(28,2)) AS lms_parcelbyload_aggrvolumiserweight\r\n",
							"    ,CAST(t1.lms_parcelbyload_aggrmaxweight  AS decimal(28,2)) AS lms_parcelbyload_aggrmaxweight\r\n",
							"    ,t1.lms_parcelbyload_aggrnoparcels\r\n",
							"    ,CAST(t2.lms_parcelbyload_aggrweightexclspecdel  AS decimal(28,2)) AS lms_parcelbyload_aggrweightexclspecdel\r\n",
							"    ,CAST(t2.lms_parcelbyload_aggrchargeweightexclspecdel  AS decimal(28,2)) AS lms_parcelbyload_aggrchargeweightexclspecdel\r\n",
							"    ,CAST(t2.lms_parcelbyload_aggrvolweightexclspecdel  AS decimal(28,2)) AS lms_parcelbyload_aggrvolweightexclspecdel\r\n",
							"    ,CAST(t2.lms_parcelbyload_aggrvolumiserweightexclspecdel  AS decimal(28,2)) AS lms_parcelbyload_aggvolumiserweightexclspecdel\r\n",
							"    ,CAST(t2.lms_parcelbyload_aggrmaxweightexclspecdel  AS decimal(28,2)) AS lms_parcelbyload_aggrmaxweightexclspecdel\r\n",
							"    ,t2.lms_parcelbyload_aggrnoparcelsexclspecdel\r\n",
							"FROM sstparcelbyload_tmp1 t1\r\n",
							"LEFT JOIN sstparcelbyload_tmp2 t2 on t2.lms_parcelbyload_loadid = t1.lms_parcelbyload_loadid"
						],
						"outputs": [],
						"execution_count": 183
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sstparcelbyload = spark.sql(\"SELECT * FROM sstparcelbyload\")"
						],
						"outputs": [],
						"execution_count": 184
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# sstparcelbyload.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Semi Structured Data/OPS/sstparcelbyload.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 185
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SSTConsignmentUnion')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/01 SEMI-STRUCTURED/01_LMS Union"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "bd5ee716-1189-4f6c-ba5a-6fd91b1228ab"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse_Prod/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace-prod/bigDataPools/PRDSparkPool",
						"name": "PRDSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/PRDSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# #Create DataFrame for the dboconsignment_archive LMS Table\r\n",
							"# dboconsignment_archive = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboconsignment_archive.parquet', format='parquet')\r\n",
							"# dboconsignment_archive.createOrReplaceTempView(\"dboconsignment_archive\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dboconsignment LMS Table\r\n",
							"# dboconsignment = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboconsignment.parquet', format='parquet')\r\n",
							"# dboconsignment.createOrReplaceTempView(\"dboconsignment\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"dboconsignmentunion = dboconsignment_archive.unionByName(dboconsignment, allowMissingColumns = True)\r\n",
							"dboconsignmentunion.createOrReplaceTempView(\"dboconsignmentunion\")"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"sstconsignmentunion\r\n",
							"AS\r\n",
							"SELECT * \r\n",
							"FROM dboconsignmentunion"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sstconsignmentunion = spark.sql(\"SELECT * FROM sstconsignmentunion\")"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# sstconsignmentunion.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Semi Structured Data/OPS/sstconsignmentunion.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 6
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SSTParcelUnion')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/01 SEMI-STRUCTURED/01_LMS Union"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "5a34a261-92ee-4503-86cc-74a6f4d50cc3"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace/bigDataPools/ApacheSparkPool",
						"name": "ApacheSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/ApacheSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"# #Create DataFrame for the dboparcel_archive LMS Table\r\n",
							"# dboparcel_archive = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboparcel_archive.parquet', format='parquet')\r\n",
							"# dboparcel_archive.createOrReplaceTempView(\"dboparcel_archive\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dboparcel LMS Table\r\n",
							"# dboparcel = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboparcel.parquet', format='parquet')\r\n",
							"# dboparcel.createOrReplaceTempView(\"dboparcel\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dboorder LMS Table\r\n",
							"# dboorder = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboorder.parquet', format='parquet')\r\n",
							"# dboorder.createOrReplaceTempView(\"dboorder\")"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"dboparcelunion = dboparcel_archive.unionByName(dboparcel, allowMissingColumns = True)\r\n",
							"dboparcelunion.createOrReplaceTempView(\"dboparcelunion\")"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"sstparcelunion\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    p.*\r\n",
							"    ,o.PickUpCustID\r\n",
							"    ,o.DeliverCustID\r\n",
							"    ,o.BillCustID\r\n",
							"    ,o.specdel\r\n",
							"FROM dboparcelunion p\r\n",
							"LEFT JOIN dboorder o on o.id = p.orderid"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sstparcelunion = spark.sql(\"SELECT * FROM sstparcelunion\")"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# sstparcelunion.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Semi Structured Data/OPS/sstparcelunion.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 6
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SSTTrackUnion')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/01 SEMI-STRUCTURED/01_LMS Union"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "a48f6d07-0b96-4b15-b0a9-57b43d920540"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace/bigDataPools/ApacheSparkPool",
						"name": "ApacheSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/ApacheSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"# #Create DataFrame for the dbotrack_archive LMS Table\r\n",
							"# dbotrack_archive = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbotrack_archive.parquet', format='parquet')\r\n",
							"# dbotrack_archive.createOrReplaceTempView(\"dbotrack_archive\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbotrack LMS Table\r\n",
							"# dbotrack = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbotrack.parquet', format='parquet')\r\n",
							"# dbotrack.createOrReplaceTempView(\"dbotrack\")\r\n",
							"\r\n",
							"# #Create DataFrame for the sstparcelunion LMS Table\r\n",
							"# sstparcelunion = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Semi Structured Data/OPS/sstparcelunion.parquet', format='parquet')\r\n",
							"# sstparcelunion.createOrReplaceTempView(\"sstparcelunion\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"dbotrackunion = dbotrack_archive.unionByName(dbotrack, allowMissingColumns = True)\r\n",
							"dbotrackunion.createOrReplaceTempView(\"dbotrackunion\")"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"ssttrackunion\r\n",
							"AS\r\n",
							"SELECT t.* \r\n",
							"    ,p.weight\r\n",
							"    ,p.chargeweight\r\n",
							"    ,p.volweight\r\n",
							"    ,p.volumiserweight\r\n",
							"    ,p.PickUpCustID\r\n",
							"    ,p.DeliverCustID\r\n",
							"    ,p.BillCustID\r\n",
							"    ,p.specdel\r\n",
							"    \r\n",
							"FROM dbotrackunion t\r\n",
							"LEFT JOIN sstparcelunion p on p.id = t.parcelid\r\n",
							""
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"ssttrackunion = spark.sql(\"SELECT * FROM ssttrackunion\")"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# ssttrackunion.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Semi Structured Data/OPS/ssttrackunion.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 6
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SSTWaybilsPerParcel')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/01 SEMI-STRUCTURED/01_LMS Union"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "fb6629e7-8a6f-482f-9fe5-6cee7259a6cc"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse_Prod/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace-prod/bigDataPools/PRDSparkPool",
						"name": "PRDSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/PRDSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '\r\n",
							""
						],
						"outputs": [],
						"execution_count": 60
					},
					{
						"cell_type": "code",
						"source": [
							"# #Create DataFrame for the sstparcelunion LMS Table\r\n",
							"# sstparcelunion = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Semi Structured Data/OPS/sstparcelunion.parquet', format='parquet')\r\n",
							"# sstparcelunion.createOrReplaceTempView(\"sstparcelunion\")\r\n",
							"\r\n",
							"# #Create DataFrame for the ssttrackunion LMS Table\r\n",
							"# ssttrackunion = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Semi Structured Data/OPS/ssttrackunion.parquet', format='parquet')\r\n",
							"# ssttrackunion.createOrReplaceTempView(\"ssttrackunion\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbowaybillsperparcel LMS Table\r\n",
							"# dbowaybillsperparcel = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbowaybillsperparcel.parquet', format='parquet')\r\n",
							"# dbowaybillsperparcel.createOrReplaceTempView(\"dbowaybillsperparcel\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbowaybill LMS Table\r\n",
							"# dbowaybill = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbowaybill.parquet', format='parquet')\r\n",
							"# dbowaybill.createOrReplaceTempView(\"dbowaybill\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 61
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"sstwaybillsperparcelunion_tmp1\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    null as id,\r\n",
							"    p.id as parcelid,\r\n",
							"    p.waybillid\r\n",
							"FROM SSTParcelUnion p\r\n",
							"UNION ALL\r\n",
							"SELECT \r\n",
							"    w.id,\r\n",
							"    w.parcelid,\r\n",
							"    w.waybillid\r\n",
							"FROM dbowaybillsperparcel w"
						],
						"outputs": [],
						"execution_count": 62
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"sstwaybillsperparcelunion_tmp2\r\n",
							"AS\r\n",
							"SELECT \r\n",
							"    w.loadid ||'_'||t.parcelid as trackkey,\r\n",
							"    t.*,\r\n",
							"    w.loadid \r\n",
							"FROM sstwaybillsperparcelunion_tmp1 t\r\n",
							"LEFT JOIN dbowaybill w on w.id = t.waybillid "
						],
						"outputs": [],
						"execution_count": 63
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sstwaybillsperparcelunion_tmp2 = spark.sql(\"SELECT * FROM sstwaybillsperparcelunion_tmp2\")"
						],
						"outputs": [],
						"execution_count": 64
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import *\r\n",
							"from pyspark.sql.window import Window"
						],
						"outputs": [],
						"execution_count": 65
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"partitionload=Window.partitionBy(\"trackkey\").orderBy(col(\"parcelid\"),col(\"id\").desc())\r\n",
							"sstwaybillsperparcelunion_tmp2 = sstwaybillsperparcelunion_tmp2.withColumn(\"rn\",row_number().over(partitionload))\r\n",
							"sstwaybillsperparcelunion_tmp2.createOrReplaceTempView(\"sstwaybillsperparcelunion_tmp2\")"
						],
						"outputs": [],
						"execution_count": 66
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"sstwaybillsperparcel\r\n",
							"AS\r\n",
							"SELECT DISTINCT\r\n",
							"    c.id as lms_waybillsperparcel_id,\r\n",
							"    a.trackkey as lms_waybillsperparcel_trackkey,\r\n",
							"    a.parcelid as lms_waybillsperparcel_parcelid,\r\n",
							"    a.waybillid as lms_waybillsperparcel_waybillid,\r\n",
							"    a.loadid as lms_waybillsperparcel_loadid,    \r\n",
							"    c.rdate as lms_waybillsperparcel_rdate,\r\n",
							"    c.returnedflag as lms_waybillsperparcel_returnedflag,\r\n",
							"    c.door_userid as lms_waybillsperparcel_dooruserid,\r\n",
							"    c.door_deliverydatetime as lms_waybillsperparcel_doordeliverydate,\r\n",
							"    c.stationid as lms_waybillsperparcel_stationid,\r\n",
							"    c.door_db_reasondetailid as lms_waybillsperparcel_doorreasondetailid,\r\n",
							"    c.door_comment as lms_waybillsperparcel_doorcomment,\r\n",
							"    c.debrief_userid as lms_waybillsperparcel_debriefuserid,\r\n",
							"    c.debrief_deliverydatetime as lms_waybillsperparcel_debriefdeliverydate,\r\n",
							"    c.debrief_comment as lms_waybillsperparcel_debriefcomment,\r\n",
							"    c.debrief_db_reasondetailid as lms_waybillsperparcel_debriefreasondetailid,\r\n",
							"    c.reasonupdates as lms_waybillsperparcel_reasonupdates,\r\n",
							"    c.callprogid as lms_waybillsperparcel_callprogid\r\n",
							"\r\n",
							"    -- c.orv_deliverydatetime as lms_waybillsperparcel_orvdeliverydate,\r\n",
							"    -- c.orv_db_reasondetailid as lms_waybillsperparcel_orvreasondetailid,\r\n",
							"\r\n",
							"\r\n",
							"FROM sstwaybillsperparcelunion_tmp2 a\r\n",
							"-- LEFT JOIN dbowaybillsperparcel c on c.waybillid = a.waybillid and c.parcelid = a.parcelid\r\n",
							"LEFT JOIN dbowaybillsperparcel c on c.id = a.id and a.id is not null\r\n",
							"WHERE  a.rn=1\r\n",
							""
						],
						"outputs": [],
						"execution_count": 67
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sstwaybillsperparcel = spark.sql(\"SELECT * FROM sstwaybillsperparcel\")"
						],
						"outputs": [],
						"execution_count": 68
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# sstwaybillsperparcel.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Semi Structured Data/OPS/sstwaybillsperparcel.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 69
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SST_RunOrder')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/01 SEMI-STRUCTURED"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "TESTSparkPoolL",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "8",
						"spark.autotune.trackingId": "8dbce8cf-cae1-4af1-839b-ab80bd096433"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse_Prod/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace-prod/bigDataPools/TESTSparkPoolL",
						"name": "TESTSparkPoolL",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/TESTSparkPoolL",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 16,
						"memory": 112
					},
					"sessionKeepAliveTimeout": 1
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"if 'prod' in Environment:\r\n",
							"    StorageAccount = 'citylogisticsstorageprod'\r\n",
							"    StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"    StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"    StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"else:\r\n",
							"    StorageAccount = 'citylogisticsstorage'\r\n",
							"    StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"    StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"    StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**<mark>RUN UNION NOTEBOOKS</mark>**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the dboconsignment_archive LMS Table\r\n",
							"dboconsignment_archive = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboconsignment_archive.parquet', format='parquet')\r\n",
							"dboconsignment_archive.createOrReplaceTempView(\"dboconsignment_archive\")\r\n",
							"\r\n",
							"#Create DataFrame for the dboconsignment LMS Table\r\n",
							"dboconsignment = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboconsignment.parquet', format='parquet')\r\n",
							"dboconsignment.createOrReplaceTempView(\"dboconsignment\")"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/01 SEMI-STRUCTURED/01_LMS Union/SSTConsignmentUnion"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sstconsignmentunion.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Semi Structured Data/OPS/sstconsignmentunion.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the dboparcel_archive LMS Table\r\n",
							"dboparcel_archive = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboparcel_archive.parquet', format='parquet')\r\n",
							"dboparcel_archive.createOrReplaceTempView(\"dboparcel_archive\")\r\n",
							"\r\n",
							"#Create DataFrame for the dboparcel LMS Table\r\n",
							"dboparcel = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboparcel.parquet', format='parquet')\r\n",
							"dboparcel.createOrReplaceTempView(\"dboparcel\")\r\n",
							"\r\n",
							"#Create DataFrame for the dboorder LMS Table\r\n",
							"dboorder = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboorder.parquet', format='parquet')\r\n",
							"dboorder.createOrReplaceTempView(\"dboorder\")"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/01 SEMI-STRUCTURED/01_LMS Union/SSTParcelUnion"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sstparcelunion.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Semi Structured Data/OPS/sstparcelunion.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the dbotrack_archive LMS Table\r\n",
							"dbotrack_archive = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbotrack_archive.parquet', format='parquet')\r\n",
							"dbotrack_archive.createOrReplaceTempView(\"dbotrack_archive\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbotrack LMS Table\r\n",
							"dbotrack = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbotrack.parquet', format='parquet')\r\n",
							"dbotrack.createOrReplaceTempView(\"dbotrack\")\r\n",
							"\r\n",
							"#Create DataFrame for the sstparcelunion LMS Table\r\n",
							"sstparcelunion = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Semi Structured Data/OPS/sstparcelunion.parquet', format='parquet')\r\n",
							"sstparcelunion.createOrReplaceTempView(\"sstparcelunion\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/01 SEMI-STRUCTURED/01_LMS Union/SSTTrackUnion"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"ssttrackunion.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Semi Structured Data/OPS/ssttrackunion.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the sstparcelunion LMS Table\r\n",
							"sstparcelunion = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Semi Structured Data/OPS/sstparcelunion.parquet', format='parquet')\r\n",
							"sstparcelunion.createOrReplaceTempView(\"sstparcelunion\")\r\n",
							"\r\n",
							"#Create DataFrame for the ssttrackunion LMS Table\r\n",
							"ssttrackunion = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Semi Structured Data/OPS/ssttrackunion.parquet', format='parquet')\r\n",
							"ssttrackunion.createOrReplaceTempView(\"ssttrackunion\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbowaybillsperparcel LMS Table\r\n",
							"dbowaybillsperparcel = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbowaybillsperparcel.parquet', format='parquet')\r\n",
							"dbowaybillsperparcel.createOrReplaceTempView(\"dbowaybillsperparcel\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbowaybill LMS Table\r\n",
							"dbowaybill = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbowaybill.parquet', format='parquet')\r\n",
							"dbowaybill.createOrReplaceTempView(\"dbowaybill\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/01 SEMI-STRUCTURED/01_LMS Union/SSTWaybilsPerParcel"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sstwaybillsperparcel.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Semi Structured Data/OPS/sstwaybillsperparcel.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**<mark>RUN AGGR NOTEBOOKS</mark>**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create SSTParcelByOrder \r\n",
							"# Create SSTParcelByWaybill\r\n",
							"# Create SSTParcelByConsignment\r\n",
							"# Create SSTParcelByLoad\r\n",
							"\r\n",
							"# IN DEV StorageAccountRead = StorageAccountWrite for sstparcelunion and ssttrackunion as these are writing to dev\r\n",
							"\r\n",
							"#Create DataFrame for the sstparcelunion LMS Table\r\n",
							"sstparcelunion = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Semi Structured Data/OPS/sstparcelunion.parquet', format='parquet')\r\n",
							"sstparcelunion.createOrReplaceTempView(\"sstparcelunion\")\r\n",
							"\r\n",
							"#Create DataFrame for the ssttrackunion LMS Table\r\n",
							"ssttrackunion = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Semi Structured Data/OPS/ssttrackunion.parquet', format='parquet')\r\n",
							"ssttrackunion.createOrReplaceTempView(\"ssttrackunion\")\r\n",
							"\r\n",
							"#Create DataFrame for the dboorder LMS Table\r\n",
							"dboorder = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboorder.parquet', format='parquet')\r\n",
							"dboorder.createOrReplaceTempView(\"dboorder\")"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/01 SEMI-STRUCTURED/02_LMS Aggr/SSTAggrParcelByX"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sstparcelbyorder.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Semi Structured Data/OPS/sstparcelbyorder.parquet', mode = \"overwrite\")\r\n",
							"sstparcelbywaybill.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Semi Structured Data/OPS/sstparcelbywaybill.parquet', mode = \"overwrite\")\r\n",
							"sstparcelbyconsignment.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Semi Structured Data/OPS/sstparcelbyconsignment.parquet', mode = \"overwrite\")\r\n",
							"sstparcelbyload.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Semi Structured Data/OPS/sstparcelbyload.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**<mark>RUN LMS+ORV NOTEBOOKS</mark>**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# #Create DataFrame for the dbocollect LMS Table\r\n",
							"# dbocollect = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbocollect.parquet', format='parquet')\r\n",
							"# dbocollect.createOrReplaceTempView(\"dbocollect\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbocustomer LMS Table\r\n",
							"# dbocustomer = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbocustomer.parquet', format='parquet')\r\n",
							"# dbocustomer.createOrReplaceTempView(\"dbocustomer\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbousers LMS Table\r\n",
							"# dbousers = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbousers.parquet', format='parquet')\r\n",
							"# dbousers.createOrReplaceTempView(\"dbousers\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbolocation LMS Table\r\n",
							"# dbolocation= spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbolocation.parquet', format='parquet')\r\n",
							"# dbolocation.createOrReplaceTempView(\"dbolocation\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbodb_collectstatus LMS Table\r\n",
							"# dbodb_collectstatus = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodb_collectstatus.parquet', format='parquet')\r\n",
							"# dbodb_collectstatus.createOrReplaceTempView(\"dbodb_collectstatus\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publicdraftcollection ORV Table\r\n",
							"# publicdraftcollection = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicdraftcollection.parquet', format='parquet')\r\n",
							"# publicdraftcollection.createOrReplaceTempView(\"publicdraftcollection\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publicdelivery ORV Table\r\n",
							"# publicdelivery = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicdelivery.parquet', format='parquet')\r\n",
							"# publicdelivery.createOrReplaceTempView(\"publicdelivery\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publicmall ORV Table\r\n",
							"# publicmall = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicmall.parquet', format='parquet')\r\n",
							"# publicmall.createOrReplaceTempView(\"publicmall\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publiccustomer ORV Table\r\n",
							"# publiccustomer = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publiccustomer.parquet', format='parquet')\r\n",
							"# publiccustomer.createOrReplaceTempView(\"publiccustomer\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# %run TRANSFORM/01 SEMI-STRUCTURED/03_LMS + ORV/SSTCollect"
						],
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# sstcollect.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Semi Structured Data/OPS/sstcollect.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# #Create DataFrame for the ssttrackunion LMS Table\r\n",
							"# ssttrackunion = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Semi Structured Data/OPS/ssttrackunion.parquet', format='parquet')\r\n",
							"# ssttrackunion.createOrReplaceTempView(\"ssttrackunion\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbolocation LMS Table\r\n",
							"# dbolocation= spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbolocation.parquet', format='parquet')\r\n",
							"# dbolocation.createOrReplaceTempView(\"dbolocation\")"
						],
						"outputs": [],
						"execution_count": 27
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# %run TRANSFORM/01 SEMI-STRUCTURED/03_LMS + ORV/SSTTrack"
						],
						"outputs": [],
						"execution_count": 28
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# ssttrack.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Semi Structured Data/OPS/ssttrack.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 29
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# #Create DataFrame for the sstconsignmentunion LMS Table\r\n",
							"# sstconsignmentunion = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Semi Structured Data/OPS/sstconsignmentunion.parquet', format='parquet')\r\n",
							"# sstconsignmentunion.createOrReplaceTempView(\"sstconsignmentunion\")\r\n",
							"\r\n",
							"# #Create DataFrame for the sstparcelbyconsignment LMS Table\r\n",
							"# sstparcelbyconsignment = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Semi Structured Data/OPS/sstparcelbyconsignment.parquet', format='parquet')\r\n",
							"# sstparcelbyconsignment.createOrReplaceTempView(\"sstparcelbyconsignment\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbobill_consignmentr LMS Table\r\n",
							"# dbobill_consignmentr = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbobill_consignmentr.parquet', format='parquet')\r\n",
							"# dbobill_consignmentr.createOrReplaceTempView(\"dbobill_consignmentr\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbousers LMS Table\r\n",
							"# dbousers = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbousers.parquet', format='parquet')\r\n",
							"# dbousers.createOrReplaceTempView(\"dbousers\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbocustomer LMS Table\r\n",
							"# dbocustomer = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbocustomer.parquet', format='parquet')\r\n",
							"# dbocustomer.createOrReplaceTempView(\"dbocustomer\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbobill_delivertype LMS Table\r\n",
							"# dbobill_delivertype = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbobill_delivertype.parquet', format='parquet')\r\n",
							"# dbobill_delivertype.createOrReplaceTempView(\"dbobill_delivertype\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbobill_zone LMS Table\r\n",
							"# dbobill_zone = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbobill_zone.parquet', format='parquet')\r\n",
							"# dbobill_zone.createOrReplaceTempView(\"dbobill_zone\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbobill_servicetype LMS Table\r\n",
							"# dbobill_servicetype = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbobill_servicetype.parquet', format='parquet')\r\n",
							"# dbobill_servicetype.createOrReplaceTempView(\"dbobill_servicetype\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbobill_zoneroute LMS Table\r\n",
							"# dbobill_zoneroute = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbobill_zoneroute.parquet', format='parquet')\r\n",
							"# dbobill_zoneroute.createOrReplaceTempView(\"dbobill_zoneroute\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbobill_billtypesr LMS Table\r\n",
							"# dbobill_billtypesr = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbobill_billtypesr.parquet', format='parquet')\r\n",
							"# dbobill_billtypesr.createOrReplaceTempView(\"dbobill_billtypesr\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbobill_routerate LMS Table\r\n",
							"# dbobill_routerate = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbobill_routerate.parquet', format='parquet')\r\n",
							"# dbobill_routerate.createOrReplaceTempView(\"dbobill_routerate\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbobroutemaster LMS Table\r\n",
							"# dbobroutemaster = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbobroutemaster.parquet', format='parquet')\r\n",
							"# dbobroutemaster.createOrReplaceTempView(\"dbobroutemaster\")"
						],
						"outputs": [],
						"execution_count": 30
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# %run TRANSFORM/01 SEMI-STRUCTURED/03_LMS + ORV/SSTConsignment"
						],
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# sstconsignment.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Semi Structured Data/OPS/sstconsignment.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 32
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# #Create DataFrame for the dboloads LMS Table\r\n",
							"# dboloads = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboloads.parquet', format='parquet')\r\n",
							"# dboloads.createOrReplaceTempView(\"dboloads\")\r\n",
							"\r\n",
							"# #Create DataFrame for the sstparcelbyload LMS Table\r\n",
							"# sstparcelbyload = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Semi Structured Data/OPS/sstparcelbyload.parquet', format='parquet')\r\n",
							"# sstparcelbyload.createOrReplaceTempView(\"sstparcelbyload\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbosroute LMS Table\r\n",
							"# dbosroute = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbosroute.parquet', format='parquet')\r\n",
							"# dbosroute.createOrReplaceTempView(\"dbosroute\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbodriver LMS Table\r\n",
							"# dbodriver = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodriver.parquet', format='parquet')\r\n",
							"# dbodriver.createOrReplaceTempView(\"dbodriver\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbovehicle LMS Table\r\n",
							"# dbovehicle = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbovehicle.parquet', format='parquet')\r\n",
							"# dbovehicle.createOrReplaceTempView(\"dbovehicle\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbovehiclebasic LMS Table\r\n",
							"# dbovehiclebasic = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/CLMasterData/dbovehiclebasic.parquet', format='parquet')\r\n",
							"# dbovehiclebasic.createOrReplaceTempView(\"dbovehiclebasic\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 33
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# %run TRANSFORM/01 SEMI-STRUCTURED/03_LMS + ORV/SSTLoadChild"
						],
						"outputs": [],
						"execution_count": 34
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# sstloadchild.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Semi Structured Data/OPS/sstloadchild.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 35
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# #Create DataFrame for the sstloadchild LMS Table\r\n",
							"# sstloadchild = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Semi Structured Data/OPS/sstloadchild.parquet', format='parquet')\r\n",
							"# sstloadchild.createOrReplaceTempView(\"sstloadchild\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publicdispatch ORV Table\r\n",
							"# publicdispatch = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicdispatch.parquet', format='parquet')\r\n",
							"# publicdispatch.createOrReplaceTempView(\"publicdispatch\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publicvehicle ORV Table\r\n",
							"# publicvehicle = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicvehicle.parquet', format='parquet')\r\n",
							"# publicvehicle.createOrReplaceTempView(\"publicvehicle\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publicorvuser ORV Table\r\n",
							"# publicorvuser = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicorvuser.parquet', format='parquet')\r\n",
							"# publicorvuser.createOrReplaceTempView(\"publicorvuser\")"
						],
						"outputs": [],
						"execution_count": 36
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# %run TRANSFORM/01 SEMI-STRUCTURED/03_LMS + ORV/SSTLoadParent"
						],
						"outputs": [],
						"execution_count": 37
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# sstloadparent.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Semi Structured Data/OPS/sstloadparent.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 38
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# #Create DataFrame for the dboorder LMS Table\r\n",
							"# dboorder = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboorder.parquet', format='parquet')\r\n",
							"# dboorder.createOrReplaceTempView(\"dboorder\")\r\n",
							"\r\n",
							"# #Create DataFrame for the sstparcelbyorder LMS Table\r\n",
							"# sstparcelbyorder = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Semi Structured Data/OPS/sstparcelbyorder.parquet', format='parquet')\r\n",
							"# sstparcelbyorder.createOrReplaceTempView(\"sstparcelbyorder\")"
						],
						"outputs": [],
						"execution_count": 39
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# %run TRANSFORM/01 SEMI-STRUCTURED/03_LMS + ORV/SSTOrder"
						],
						"outputs": [],
						"execution_count": 40
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# sstorder.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Semi Structured Data/OPS/sstorder.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 41
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# #Create DataFrame for the dboparcel LMS Table\r\n",
							"# dboparcel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Semi Structured Data/OPS/sstparcelunion.parquet', format='parquet')\r\n",
							"# dboparcel.createOrReplaceTempView(\"dboparcel\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbowaybill LMS Table\r\n",
							"# dbowaybill = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbowaybill.parquet', format='parquet')\r\n",
							"# dbowaybill.createOrReplaceTempView(\"dbowaybill\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dboorder LMS Table\r\n",
							"# dboorder = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboorder.parquet', format='parquet')\r\n",
							"# dboorder.createOrReplaceTempView(\"dboorder\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dboconsignment LMS Table\r\n",
							"# dboconsignment = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Semi Structured Data/OPS/sstconsignmentunion.parquet', format='parquet')\r\n",
							"# dboconsignment.createOrReplaceTempView(\"dboconsignment\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dboparceldetail LMS Table\r\n",
							"# dboparceldetail = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboparceldetail.parquet', format='parquet')\r\n",
							"# dboparceldetail.createOrReplaceTempView(\"dboparceldetail\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbocustomer LMS Table\r\n",
							"# dbocustomer = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbocustomer.parquet', format='parquet')\r\n",
							"# dbocustomer.createOrReplaceTempView(\"dbocustomer\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbousers LMS Table\r\n",
							"# dbousers = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbousers.parquet', format='parquet')\r\n",
							"# dbousers.createOrReplaceTempView(\"dbousers\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbobill_parceltype LMS Table\r\n",
							"# dbobill_parceltype = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbobill_parceltype.parquet', format='parquet')\r\n",
							"# dbobill_parceltype.createOrReplaceTempView(\"dbobill_parceltype\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbolocation LMS Table\r\n",
							"# dbolocation= spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbolocation.parquet', format='parquet')\r\n",
							"# dbolocation.createOrReplaceTempView(\"dbolocation\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbodstatus LMS Table\r\n",
							"# dbodstatus = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodstatus.parquet', format='parquet')\r\n",
							"# dbodstatus.createOrReplaceTempView(\"dbodstatus\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbovolumiser LMS Table\r\n",
							"# dbovolumiser = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbovolumiser.parquet', format='parquet')\r\n",
							"# dbovolumiser.createOrReplaceTempView(\"dbovolumiser\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbobill_parceltype LMS Table\r\n",
							"# dbobill_parceltype = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbobill_parceltype.parquet', format='parquet')\r\n",
							"# dbobill_parceltype.createOrReplaceTempView(\"dbobill_parceltype\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publicparcel ORV Table\r\n",
							"# publicparcel = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicparcel.parquet', format='parquet')\r\n",
							"# publicparcel.createOrReplaceTempView(\"publicparcel\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publicdelivery ORV Table\r\n",
							"# publicdelivery = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicdelivery.parquet', format='parquet')\r\n",
							"# publicdelivery.createOrReplaceTempView(\"publicdelivery\")"
						],
						"outputs": [],
						"execution_count": 42
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# %run TRANSFORM/01 SEMI-STRUCTURED/03_LMS + ORV/SSTParcel"
						],
						"outputs": [],
						"execution_count": 43
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# sstparcel.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Semi Structured Data/OPS/sstparcel.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 44
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# #Create DataFrame for the dbowaybill LMS Table\r\n",
							"# dbowaybill = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbowaybill.parquet', format='parquet')\r\n",
							"# dbowaybill.createOrReplaceTempView(\"dbowaybill\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbocustomer LMS Table\r\n",
							"# dbocustomer = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbocustomer.parquet', format='parquet')\r\n",
							"# dbocustomer.createOrReplaceTempView(\"dbocustomer\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbousers LMS Table\r\n",
							"# dbousers = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbousers.parquet', format='parquet')\r\n",
							"# dbousers.createOrReplaceTempView(\"dbousers\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbodb_delayreason LMS Table\r\n",
							"# dbodb_delayreason = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodb_delayreason.parquet', format='parquet')\r\n",
							"# dbodb_delayreason.createOrReplaceTempView(\"dbodb_delayreason\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbodb_reasondetail LMS Table\r\n",
							"# dbodb_reasondetail = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodb_reasondetail.parquet', format='parquet')\r\n",
							"# dbodb_reasondetail.createOrReplaceTempView(\"dbodb_reasondetail\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbodb_reasongroup LMS Table\r\n",
							"# dbodb_reasongroup = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodb_reasongroup.parquet', format='parquet')\r\n",
							"# dbodb_reasongroup.createOrReplaceTempView(\"dbodb_reasongroup\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbodb_deliverystatus LMS Table\r\n",
							"# dbodb_deliverystatus = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodb_deliverystatus.parquet', format='parquet')\r\n",
							"# dbodb_deliverystatus.createOrReplaceTempView(\"dbodb_deliverystatus\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbodb_mobilefeedbackreason LMS Table\r\n",
							"# dbodb_mobilefeedbackreason = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodb_mobilefeedbackreason.parquet', format='parquet')\r\n",
							"# dbodb_mobilefeedbackreason.createOrReplaceTempView(\"dbodb_mobilefeedbackreason\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publicdelivery ORV Table\r\n",
							"# publicdelivery = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicdelivery.parquet', format='parquet')\r\n",
							"# publicdelivery.createOrReplaceTempView(\"publicdelivery\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publicmall ORV Table\r\n",
							"# publicmall = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicmall.parquet', format='parquet')\r\n",
							"# publicmall.createOrReplaceTempView(\"publicmall\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publiccustomer ORV Table\r\n",
							"# publiccustomer = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publiccustomer.parquet', format='parquet')\r\n",
							"# publiccustomer.createOrReplaceTempView(\"publiccustomer\")\r\n",
							"\r\n",
							"# #Create DataFrame for the sstparcelbywaybill LMS Table\r\n",
							"# sstparcelbywaybill = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Semi Structured Data/OPS/sstparcelbywaybill.parquet', format='parquet')\r\n",
							"# sstparcelbywaybill.createOrReplaceTempView(\"sstparcelbywaybill\")"
						],
						"outputs": [],
						"execution_count": 45
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# %run TRANSFORM/01 SEMI-STRUCTURED/03_LMS + ORV/SSTWaybill"
						],
						"outputs": [],
						"execution_count": 46
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# sstwaybill.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Semi Structured Data/OPS/sstwaybill.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 47
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# #Create DataFrame for the ssttrackunion LMS Table\r\n",
							"# ssttrackunion = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Semi Structured Data/OPS/ssttrackunion.parquet', format='parquet')\r\n",
							"# ssttrackunion.createOrReplaceTempView(\"ssttrackunion\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbowaybillsperparcel LMS Table\r\n",
							"# dbowaybillsperparcel = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbowaybillsperparcel.parquet', format='parquet')\r\n",
							"# dbowaybillsperparcel.createOrReplaceTempView(\"dbowaybillsperparcel\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 48
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# %run TRANSFORM/01 SEMI-STRUCTURED/03_LMS + ORV/SSTWaybilsPerParcel"
						],
						"outputs": [],
						"execution_count": 49
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# sstwaybillsperparcel.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Semi Structured Data/OPS/sstwaybillsperparcel.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 50
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# #Create DataFrame for the publicnonbooking TMS Table\r\n",
							"# publicnonbooking = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publicnonbooking.parquet', format='parquet')\r\n",
							"# publicnonbooking.createOrReplaceTempView(\"publicnonbooking\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publicbooking TMS Table\r\n",
							"# publicbooking = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publicbooking.parquet', format='parquet')\r\n",
							"# publicbooking.createOrReplaceTempView(\"publicbooking\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publiccustomer TMS Table\r\n",
							"# publiccustomer = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publiccustomer.parquet', format='parquet')\r\n",
							"# publiccustomer.createOrReplaceTempView(\"publiccustomer\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publicroute TMS Table\r\n",
							"# publicroute = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publicroute.parquet', format='parquet')\r\n",
							"# publicroute.createOrReplaceTempView(\"publicroute\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publicthirdparty TMS Table\r\n",
							"# publicthirdparty = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publicthirdparty.parquet', format='parquet')\r\n",
							"# publicthirdparty.createOrReplaceTempView(\"publicthirdparty\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publicfinancedata TMS Table\r\n",
							"# publicfinancedata = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publicfinancedata.parquet', format='parquet')\r\n",
							"# publicfinancedata.createOrReplaceTempView(\"publicfinancedata\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 51
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# %run TRANSFORM/01 SEMI-STRUCTURED/04_TMS + ORV/SSTBooking"
						],
						"outputs": [],
						"execution_count": 52
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# sstbooking.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Semi Structured Data/OPS/sstbooking.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 53
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# #Create DataFrame for the publicinstruction TMS Table\r\n",
							"# publicinstruction = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publicinstruction.parquet', format='parquet')\r\n",
							"# publicinstruction.createOrReplaceTempView(\"publicinstruction\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publicaddress TMS Table\r\n",
							"# publicaddress = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publicaddress.parquet', format='parquet')\r\n",
							"# publicaddress.createOrReplaceTempView(\"publicaddress\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publiclhdelivery ORV Table\r\n",
							"# publiclhdelivery = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publiclhdelivery.parquet', format='parquet')\r\n",
							"# publiclhdelivery.createOrReplaceTempView(\"publiclhdelivery\")"
						],
						"outputs": [],
						"execution_count": 54
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# %run TRANSFORM/01 SEMI-STRUCTURED/04_TMS + ORV/SSTInstruction"
						],
						"outputs": [],
						"execution_count": 55
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# sstinstruction.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Semi Structured Data/OPS/sstinstruction.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 56
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# #Create DataFrame for the publictrip TMS Table\r\n",
							"# publictrip = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publictrip.parquet', format='parquet')\r\n",
							"# publictrip.createOrReplaceTempView(\"publictrip\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publicdriverpayitem TMS Table\r\n",
							"# publicdriverpayitem = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publicdriverpayitem.parquet', format='parquet')\r\n",
							"# publicdriverpayitem.createOrReplaceTempView(\"publicdriverpayitem\")"
						],
						"outputs": [],
						"execution_count": 57
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# %run TRANSFORM/01 SEMI-STRUCTURED/04_TMS + ORV/SSTTrip"
						],
						"outputs": [],
						"execution_count": 58
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# ssttrip.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Semi Structured Data/OPS/ssttrip.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 59
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/STMDDepot')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/MasterData"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "01b17762-3052-4d0d-bf43-5dd242a8ea9e"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse_Prod/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace-prod/bigDataPools/PRDSparkPool",
						"name": "PRDSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/PRDSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"source": [
							"# #Create DataFrame for the dbodepot MD Table\r\n",
							"# dbodepot = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/CLMasterData/dbodepot.parquet', format='parquet')\r\n",
							"# dbodepot.createOrReplaceTempView(\"dbodepot\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stmddepot\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    d.id as md_depot_id\r\n",
							"    ,d.itemlabel as md_depot_itemlabel\r\n",
							"    ,d.depotname as md_depot_depotname\r\n",
							"    ,d.parentdepotcode as md_depot_parentdepotcode\r\n",
							"    ,d.division as md_depot_division\r\n",
							"    ,d.address as md_depot_address\r\n",
							"    ,d.latitude as md_depot_latitude\r\n",
							"    ,d.longitude as md_depot_longitude\r\n",
							"\r\n",
							"FROM dbodepot d \r\n",
							""
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stmdsapdepot\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"     md_depot_id \r\n",
							"    ,md_depot_parentdepotcode\r\n",
							"    ,md_depot_itemlabel as sap_transactionsline_subdepotcode\r\n",
							"    ,md_depot_depotname\r\n",
							"    ,md_depot_division\r\n",
							"\r\n",
							"FROM stmddepot"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stmddepot = spark.sql(\"SELECT * FROM stmddepot\")\r\n",
							"stmdsapdepot = spark.sql(\"SELECT * FROM stmdsapdepot\")"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stmddepot.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/MD/stmddepot.parquet', mode = \"overwrite\")\r\n",
							"# stmdsapdepot.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stmdsapdepot.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"SUCCESS\")"
						],
						"outputs": [],
						"execution_count": 8
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/STMDPeople')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/MasterData"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "df7569e3-8577-463d-9356-9c410fc5cdf8"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse_Prod/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace-prod/bigDataPools/PRDSparkPool",
						"name": "PRDSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/PRDSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"source": [
							"# #Create DataFrame for the dbopeoplebasic MD Table\r\n",
							"# dbopeoplebasic = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/CLMasterData/dbopeoplebasic.parquet', format='parquet')\r\n",
							"# dbopeoplebasic.createOrReplaceTempView(\"dbopeoplebasic\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbopeopledriver MD Table\r\n",
							"# dbopeopledriver = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/CLMasterData/dbopeopledriver.parquet', format='parquet')\r\n",
							"# dbopeopledriver.createOrReplaceTempView(\"dbopeopledriver\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbopeoplepersonal MD Table\r\n",
							"# dbopeoplepersonal = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/CLMasterData/dbopeoplepersonal.parquet', format='parquet')\r\n",
							"# dbopeoplepersonal.createOrReplaceTempView(\"dbopeoplepersonal\")"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stmdpeople\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    pb.id as md_people_id\r\n",
							"    ,pb.empid as md_people_empid\r\n",
							"    ,pb.employeecode as md_people_employeecode\r\n",
							"    ,pb.uid as md_people_uid\r\n",
							"    ,pb.firstname as md_people_firstname\r\n",
							"    ,pb.lastname as md_people_lastname\r\n",
							"    ,pb.knownasname as md_people_knownasname\r\n",
							"    ,pb.birthdate as md_people_birthdate\r\n",
							"    ,pb.gender as md_people_gender\r\n",
							"    ,pb.cellno as md_people_cellno\r\n",
							"    ,pb.emailaddress as md_people_emailaddress\r\n",
							"    ,pb.nationality as md_people_nationality\r\n",
							"    ,pb.idnumber as md_people_idnumber\r\n",
							"    ,pb.passportno as md_people_passportno\r\n",
							"    ,pb.passportcountrycode as md_people_passportcountrycode\r\n",
							"    ,pb.idpassportno as md_people_idpassportno\r\n",
							"    ,to_timestamp(pb.dateengaged) as md_people_dateengaged\r\n",
							"    ,to_timestamp(pb.datejoinedgroup) as md_people_datejoinedgroup\r\n",
							"    ,pb.employeestatus as md_people_employeestatus\r\n",
							"    ,to_timestamp(pb.terminationdate) as md_people_terminationdate\r\n",
							"    ,pb.terminationreasonid as md_people_terminationreasonid\r\n",
							"    ,pb.depotcode as md_people_depotcode\r\n",
							"    ,pb.jobcategory as md_people_jobcategory\r\n",
							"    ,pb.jobtitle as md_people_jobtitle\r\n",
							"    ,pb.source as md_people_source\r\n",
							"    ,pb.insby as md_people_insby\r\n",
							"    ,pb.islinehaul as md_people_islinehaul\r\n",
							"    ,pb.companycode as md_people_companycode\r\n",
							"    ,pb.companydisplayname as md_people_companydisplayname\r\n",
							"    ,pb.companyrulecode as md_people_companyrulecode\r\n",
							"    ,pb.reportstoemployee as md_people_reportstoemployee\r\n",
							"    ,pb.reporttoempid as md_people_reporttoempid\r\n",
							"    ,pb.internalexternal as md_people_internalexternal\r\n",
							"    -- ,to_timestamp(pb.moddate) as md_people_moddate\r\n",
							"    -- ,to_timestamp(pb.insdate) as md_people_insdate\r\n",
							"    ,pd.id as md_people_driverid\r\n",
							"    ,pd.licenseno as md_people_licenseno\r\n",
							"    ,pd.driverrestriction as md_people_driverrestriction\r\n",
							"    ,pd.issuecountry as md_people_issuecountry\r\n",
							"    ,to_timestamp(pd.issuedate) as md_people_issuedate\r\n",
							"    ,to_timestamp(pd.licensevalidfrom) as md_people_licensevalidfrom\r\n",
							"    ,to_timestamp(pd.licensevalidto) as md_people_licensevalidto\r\n",
							"    ,pd.vehiclecode as md_people_vehiclecode\r\n",
							"    ,pd.vehiclerestriction as md_people_vehiclerestriction\r\n",
							"    ,pd.driverphoto as md_people_driverphoto\r\n",
							"    ,pd.licenseexpired as md_people_licenseexpired\r\n",
							"    ,pd.prdpcode as md_people_prdpcode\r\n",
							"    ,to_timestamp(pd.prdpexpiry) as md_people_prdpexpiry\r\n",
							"    ,pp.id as md_people_personalid\r\n",
							"    ,pp.physicalblock as md_people_physicalblock\r\n",
							"    ,pp.physicalcitytown as md_people_physicalcitytown\r\n",
							"    ,pp.physicalcomplex as md_people_physicalcomplex\r\n",
							"    ,pp.physicalcountrycode as md_people_physicalcountrycode\r\n",
							"    ,pp.physicaldistrictid as md_people_physicaldistrictid\r\n",
							"    ,pp.physicallevelfloor as md_people_physicallevelfloor\r\n",
							"    ,pp.physicalpostalcode as md_people_physicalpostalcode\r\n",
							"    ,pp.physicalprovince as md_people_physicalprovince\r\n",
							"    ,pp.physicalstreetfarmname as md_people_physicalstreetfarmname\r\n",
							"    ,pp.physicalstreetnumber as md_people_physicalstreetnumber\r\n",
							"    ,pp.physicalsuburbdistrict as md_people_physicalsuburbdistrict\r\n",
							"    ,pp.physicalunitpostalnumber as md_people_physicalunitpostalnumber\r\n",
							"    ,pp.postaladdressid as md_people_postaladdressid\r\n",
							"    ,pp.postaladdressservicetype as md_people_postaladdressservicetype\r\n",
							"    ,pp.postalblock as md_people_postalblock\r\n",
							"    ,pp.postalcitytown as md_people_postalcitytown\r\n",
							"    ,pp.postalcomplex as md_people_postalcomplex\r\n",
							"    ,pp.postalconcat as md_people_postalconcat\r\n",
							"    ,pp.postaldistrictid as md_people_postaldistrictid\r\n",
							"    ,pp.postallevelfloor as md_people_postallevelfloor\r\n",
							"    ,pp.postalpostalcode as md_people_postalpostalcode\r\n",
							"    ,pp.postalprovince as md_people_postalprovince\r\n",
							"    ,pp.postalstreetfarmname as md_people_postalstreetfarmname\r\n",
							"    ,pp.postalstreetnumber as md_people_postalstreetnumber\r\n",
							"    ,pp.postalsuburbdistrict as md_people_postalsuburbdistrict\r\n",
							"    ,pp.postalunitpostalnumber as md_people_postalunitpostalnumber\r\n",
							"    ,pp.racialgroup as md_people_racialgroup\r\n",
							"\r\n",
							"\r\n",
							"FROM dbopeoplebasic pb\r\n",
							"LEFT JOIN dbopeoplepersonal pp on pp.pid = pb.id\r\n",
							"LEFT JOIN dbopeopledriver pd on pd.pid = pb.id \r\n",
							""
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stmdpeople = spark.sql(\"SELECT * FROM stmdpeople\")"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stmdpeople.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/MD/stmdpeople.parquet', mode = \"overwrite\")\r\n",
							"\r\n",
							"print(\"SUCCESS\")"
						],
						"outputs": [],
						"execution_count": 10
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/STMDVehicle')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/MasterData"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "2ac57f88-10c7-417e-876e-87c1245a3c37"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse_Prod/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace-prod/bigDataPools/PRDSparkPool",
						"name": "PRDSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/PRDSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"# #Create DataFrame for the dbovehiclebasic MD Table\r\n",
							"# dbovehiclebasic = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/CLMasterData/dbovehiclebasic.parquet', format='parquet')\r\n",
							"# dbovehiclebasic.createOrReplaceTempView(\"dbovehiclebasic\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbovehicleadmin MD Table\r\n",
							"# dbovehicleadmin = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/CLMasterData/dbovehicleadmin.parquet', format='parquet')\r\n",
							"# dbovehicleadmin.createOrReplaceTempView(\"dbovehicleadmin\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbovehicleoperations MD Table\r\n",
							"# dbovehicleoperations = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/CLMasterData/dbovehicleoperations.parquet', format='parquet')\r\n",
							"# dbovehicleoperations.createOrReplaceTempView(\"dbovehicleoperations\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbovehiclefinance MD Table\r\n",
							"# dbovehiclefinance = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/CLMasterData/dbovehiclefinance.parquet', format='parquet')\r\n",
							"# dbovehiclefinance.createOrReplaceTempView(\"dbovehiclefinance\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbovehicletracking MD Table\r\n",
							"# dbovehicletracking = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/CLMasterData/dbovehicletracking.parquet', format='parquet')\r\n",
							"# dbovehicletracking.createOrReplaceTempView(\"dbovehicletracking\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbovehicle MD Table\r\n",
							"# dbovehicle = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbovehicle.parquet', format='parquet')\r\n",
							"# dbovehicle.createOrReplaceTempView(\"dbovehicle\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publicvehicle MD Table\r\n",
							"# publicvehicle = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicvehicle.parquet', format='parquet')\r\n",
							"# publicvehicle.createOrReplaceTempView(\"publicvehicle\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stmdvehicle\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    vb.id as md_vehicle_id\r\n",
							"    ,vb.uid as md_vehicle_uid\r\n",
							"    ,vb.vehicletype as md_vehicle_vehicletype\r\n",
							"    ,vb.internalexternal as md_vehicle_internalexternal\r\n",
							"    ,vb.activeflag as md_vehicle_activeflag\r\n",
							"    ,vb.vehiclestatus as md_vehicle_vehiclestatus\r\n",
							"    ,vb.chassisnumber as md_vehicle_chassisnumber\r\n",
							"    ,vb.depot as md_vehicle_depot\r\n",
							"    ,vb.enginenumber as md_vehicle_enginenumber\r\n",
							"    ,vb.fleetcode as md_vehicle_fleetcode\r\n",
							"    ,vb.fueltype as md_vehicle_fueltype\r\n",
							"    ,vb.registrationnumber as md_vehicle_registrationnumber\r\n",
							"    ,vb.vehicleapplication as md_vehicle_vehicleapplication\r\n",
							"    ,vb.vehiclebodymake as md_vehicle_vehiclebodymake\r\n",
							"    ,vb.vehiclecolour as md_vehicle_vehiclecolour\r\n",
							"    ,vb.vehiclemake as md_vehicle_vehiclemake\r\n",
							"    ,vb.vehiclemodel as md_vehicle_vehiclemodel\r\n",
							"    ,vb.vehicleseries as md_vehicle_vehicleseries\r\n",
							"    ,vb.year as md_vehicle_year\r\n",
							"    ,vb.vehicleownerid as md_vehicle_vehicleownerid\r\n",
							"    ,vb.vehicleowner as md_vehicle_vehicleowner\r\n",
							"    ,vb.staffvehicleflag as md_vehicle_staffvehicleflag\r\n",
							"    ,vb.vehicletrailersize as md_vehicle_vehicletrailersize\r\n",
							"    ,vb.vehicletrailertype as md_vehicle_vehicletrailertype\r\n",
							"    ,vb.vehicleetollclass as md_vehicle_vehicleetollclass\r\n",
							"    ,vb.brn as md_vehicle_brn\r\n",
							"    ,vb.brnnumber as md_vehicle_brnnumber\r\n",
							"    ,vb.vehicleregisterno as md_vehicle_vehicleregisterno\r\n",
							"    ,to_timestamp(vb.insdate) as md_vehicle_vehiclebasicinsdate\r\n",
							"    ,vb.insby as md_vehicle_insby\r\n",
							"    ,vb.islinehaulvehicle as md_vehicle_islinehaulvehicle\r\n",
							"    ,vb.internalheight as md_vehicle_internalheight\r\n",
							"    ,vb.internallength as md_vehicle_internallength\r\n",
							"    ,vb.internalvolume as md_vehicle_internalvolume\r\n",
							"    ,vb.internalwidth as md_vehicle_internalwidth\r\n",
							"    ,to_timestamp(vb.moddate) as md_vehicle_vehiclebasicmoddate\r\n",
							"    ,vb.trailerflag as md_vehicle_trailerflag\r\n",
							"    ,vb.workflowstatus as md_vehicle_workflowstatus\r\n",
							"    ,to_timestamp(vb.approveddate) as md_vehicle_vehicleapproveddate\r\n",
							"    ,vb.mmcode as md_vehicle_mmcode\r\n",
							"    ,vb.tmplicenseexpiery as md_vehicle_tmplicenseexpiery\r\n",
							"    ,va.currentadvertising as md_vehicle_currentadvertising\r\n",
							"    ,va.fuelcardnumber as md_vehicle_fuelcardnumber\r\n",
							"    ,to_timestamp(va.fuelcardreceivedate) as md_vehicle_fuelcardreceivedate\r\n",
							"    ,to_timestamp(va.licenseexpirydate) as md_vehicle_licenseexpirydate\r\n",
							"    ,va.licensefee as md_vehicle_licensefee\r\n",
							"    ,va.vehicleadvertisingindicator as md_vehicle_vehicleadvertisingindicator\r\n",
							"    ,va.vehiclespeedgoverningdevice as md_vehicle_vehiclespeedgoverningdevice\r\n",
							"    ,va.oldregistrationnumber as md_vehicle_oldregistrationnumber\r\n",
							"    ,va.oldchassisnumber as md_vehicle_oldchassisnumber\r\n",
							"    ,va.oldenginenumber as md_vehicle_oldenginenumber\r\n",
							"    ,va.logbookrecord as md_vehicle_logbookrecord\r\n",
							"    ,va.status as md_vehicle_adminstatus\r\n",
							"    ,to_timestamp(va.completebydate) as md_vehicle_vehicleadmincompletebydate\r\n",
							"    ,vo.backdoorheight as md_vehicle_backdoorheight\r\n",
							"    ,vo.backdoorwidth as md_vehicle_backdoorwidth\r\n",
							"    ,vo.externalheight as md_vehicle_externalheight\r\n",
							"    ,vo.externallength as md_vehicle_externallength\r\n",
							"    ,vo.externalwidth as md_vehicle_externalwidth\r\n",
							"    ,vo.fleximounts as md_vehicle_fleximounts\r\n",
							"    ,vo.fuelcapacity as md_vehicle_fuelcapacity\r\n",
							"    ,vo.grossvehiclemass as md_vehicle_grossvehiclemass\r\n",
							"    ,vo.numberoftyres as md_vehicle_numberoftyres\r\n",
							"    ,vo.numberoftyresrear as md_vehicle_numberoftyresrear\r\n",
							"    ,vo.numberoftyressteering as md_vehicle_numberoftyressteering\r\n",
							"    ,vo.payload as md_vehicle_payload\r\n",
							"    ,vo.peoplecarrier as md_vehicle_peoplecarrier\r\n",
							"    ,vo.tareweight as md_vehicle_tareweight\r\n",
							"    ,vo.tyresize as md_vehicle_tyresize\r\n",
							"    ,vo.tyresizealternative as md_vehicle_tyresizealternative\r\n",
							"    ,vo.targetfuelconsumption as md_vehicle_targetfuelconsumption\r\n",
							"    ,vo.tankrange as md_vehicle_tankrange\r\n",
							"    ,vo.status as md_vehicle_operationsstatus\r\n",
							"    ,to_timestamp(vo.completebydate) as md_vehicle_vehiclevehicleoperationscompletebydate\r\n",
							"    ,vf.assetnumber as md_vehicle_assetnumber\r\n",
							"    ,to_timestamp(vf.contractenddate) as md_vehicle_contractenddate\r\n",
							"    ,vf.contractnumber as md_vehicle_contractnumber\r\n",
							"    ,to_timestamp(vf.contractstartdate) as md_vehicle_contractstartdate\r\n",
							"    ,vf.vehiclefinancecompany as md_vehicle_vehiclefinancecompany\r\n",
							"    ,vf.budgetrmcpk as md_vehicle_budgetrmcpk\r\n",
							"    ,vf.budgettyrecpk as md_vehicle_budgettyrecpk\r\n",
							"    ,vf.retailvalue as md_vehicle_retailvalue\r\n",
							"    ,vf.tradeinvalue as md_vehicle_tradeinvalue\r\n",
							"    ,vf.loadbodyreplacementcost as md_vehicle_loadbodyreplacementcost\r\n",
							"    ,vf.status as md_vehicle_financestatus\r\n",
							"    ,to_timestamp(vf.completebydate) as md_vehicle_vehiclefinancecompletebydate\r\n",
							"    ,vt.status as md_vehicle_trackingstatus\r\n",
							"    ,to_timestamp(vt.completebydate) as md_vehicle_vehicletrackingcompletebydate\r\n",
							"    ,to_timestamp(vt.moddate) as md_vehicle_vehicletrackingmoddate\r\n",
							"    ,vt.assetid as md_vehicle_assetid\r\n",
							"    ,vt.providerid as md_vehicle_providerid\r\n",
							"    ,vt.providername as md_vehicle_providername\r\n",
							"    ,vt.serial as md_vehicle_serial\r\n",
							"    ,to_timestamp(vt.insdate) as md_vehicle_vehicletrackinginsdate\r\n",
							"    ,to_timestamp(vt.linkdate) as md_vehicle_linkdate\r\n",
							"    ,vt.linkby as md_vehicle_linkby\r\n",
							"    ,vt.linkbyname as md_vehicle_linkbyname\r\n",
							"    ,vl.id as md_vehicle_lmsid\r\n",
							"    ,vorv.id as md_vehicle_orvid\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"FROM dbovehiclebasic vb \r\n",
							"LEFT JOIN dbovehicleadmin va on va.vid = vb.id\r\n",
							"LEFT JOIN dbovehicleoperations vo on vo.vid = vb.id\r\n",
							"LEFT JOIN dbovehiclefinance vf on vf.vid = vb.id\r\n",
							"LEFT JOIN dbovehicletracking vt on vt.vid = vb.id\r\n",
							"LEFT JOIN dbovehicle vl on vl.uid = vb.uid\r\n",
							"LEFT JOIN publicvehicle vorv on vorv.lmsid = vl.id\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stmdvehicle = spark.sql(\"SELECT * FROM stmdvehicle\")"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# stmdvehicle.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/MD/stmdvehicle.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"SUCCESS\")"
						],
						"outputs": [],
						"execution_count": 12
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/STMD_RunOrder')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/MasterData"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "TESTSparkPoolL",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "112g",
					"driverCores": 16,
					"executorMemory": "112g",
					"executorCores": 16,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "248181b0-4b28-4747-a3be-1dee4e5371de"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse_Prod/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace-prod/bigDataPools/TESTSparkPoolL",
						"name": "TESTSparkPoolL",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/TESTSparkPoolL",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 16,
						"memory": 112
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"if 'prod' in Environment:\r\n",
							"    StorageAccount = 'citylogisticsstorageprod'\r\n",
							"    StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"    StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"    StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"else:\r\n",
							"    StorageAccount = 'citylogisticsstorage'\r\n",
							"    StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"    StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"    StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the dbopeoplebasic MD Table\r\n",
							"dbopeoplebasic = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/CLMasterData/dbopeoplebasic.parquet', format='parquet')\r\n",
							"dbopeoplebasic.createOrReplaceTempView(\"dbopeoplebasic\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbopeopledriver MD Table\r\n",
							"dbopeopledriver = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/CLMasterData/dbopeopledriver.parquet', format='parquet')\r\n",
							"dbopeopledriver.createOrReplaceTempView(\"dbopeopledriver\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbopeoplepersonal MD Table\r\n",
							"dbopeoplepersonal = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/CLMasterData/dbopeoplepersonal.parquet', format='parquet')\r\n",
							"dbopeoplepersonal.createOrReplaceTempView(\"dbopeoplepersonal\")"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/MasterData/STMDPeople"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stmdpeople.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/MD/stmdpeople.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the dbovehiclebasic MD Table\r\n",
							"dbovehiclebasic = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/CLMasterData/dbovehiclebasic.parquet', format='parquet')\r\n",
							"dbovehiclebasic.createOrReplaceTempView(\"dbovehiclebasic\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbovehicleadmin MD Table\r\n",
							"dbovehicleadmin = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/CLMasterData/dbovehicleadmin.parquet', format='parquet')\r\n",
							"dbovehicleadmin.createOrReplaceTempView(\"dbovehicleadmin\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbovehicleoperations MD Table\r\n",
							"dbovehicleoperations = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/CLMasterData/dbovehicleoperations.parquet', format='parquet')\r\n",
							"dbovehicleoperations.createOrReplaceTempView(\"dbovehicleoperations\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbovehiclefinance MD Table\r\n",
							"dbovehiclefinance = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/CLMasterData/dbovehiclefinance.parquet', format='parquet')\r\n",
							"dbovehiclefinance.createOrReplaceTempView(\"dbovehiclefinance\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbovehicletracking MD Table\r\n",
							"dbovehicletracking = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/CLMasterData/dbovehicletracking.parquet', format='parquet')\r\n",
							"dbovehicletracking.createOrReplaceTempView(\"dbovehicletracking\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbovehicle MD Table\r\n",
							"dbovehicle = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbovehicle.parquet', format='parquet')\r\n",
							"dbovehicle.createOrReplaceTempView(\"dbovehicle\")\r\n",
							"\r\n",
							"#Create DataFrame for the publicvehicle MD Table\r\n",
							"publicvehicle = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicvehicle.parquet', format='parquet')\r\n",
							"publicvehicle.createOrReplaceTempView(\"publicvehicle\")"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/MasterData/STMDVehicle"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stmdvehicle.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/MD/stmdvehicle.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the dbodepot MD Table\r\n",
							"dbodepot = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/CLMasterData/dbodepot.parquet', format='parquet')\r\n",
							"dbodepot.createOrReplaceTempView(\"dbodepot\")"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/MasterData/STMDDepot"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stmddepot.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/MD/stmddepot.parquet', mode = \"overwrite\")\r\n",
							"stmdsapdepot.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/FINANCE/stmdsapdepot.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 10
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/STOPS_RunOrderDistribution')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Ops/Distribution"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "TESTSparkPoolL",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "112g",
					"driverCores": 16,
					"executorMemory": "112g",
					"executorCores": 16,
					"numExecutors": 4,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "4",
						"spark.dynamicAllocation.maxExecutors": "6",
						"spark.autotune.trackingId": "9d345d36-5ec5-4692-8acf-46d52e0ec703"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse_Prod/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace-prod/bigDataPools/TESTSparkPoolL",
						"name": "TESTSparkPoolL",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/TESTSparkPoolL",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 16,
						"memory": 112
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"if 'prod' in Environment:\r\n",
							"    StorageAccount = 'citylogisticsstorageprod'\r\n",
							"    StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"    StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"    StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"else:\r\n",
							"    StorageAccount = 'citylogisticsstorage'\r\n",
							"    StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"    StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"    StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 76
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import *\r\n",
							"import pandas as pd\r\n",
							"from pyspark.sql import Window   \r\n",
							"import os\r\n",
							"from pyspark.sql.types import StructType, StructField, DoubleType, StringType, DecimalType, IntegerType, DateType\r\n",
							""
						],
						"outputs": [],
						"execution_count": 77
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#create dataframe for the publicdispatchsegment orv table\r\n",
							"publicdispatchsegment = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicdispatchsegment.parquet', format='parquet')\r\n",
							"publicdispatchsegment.createOrReplaceTempView(\"publicdispatchsegment\")\r\n",
							"\r\n",
							"#create dataframe for the publicorvuser orv table\r\n",
							"publicorvuser = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicorvuser.parquet', format='parquet')\r\n",
							"publicorvuser.createOrReplaceTempView(\"publicorvuser\")\r\n",
							"\r\n",
							"#create dataframe for the publicvehicle orv table\r\n",
							"publicvehicle = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicvehicle.parquet', format='parquet')\r\n",
							"publicvehicle.createOrReplaceTempView(\"publicvehicle\")\r\n",
							"\r\n",
							"#create dataframe for the publicdepot orv table\r\n",
							"publicdepot = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicdepo.parquet', format='parquet')\r\n",
							"publicdepot.createOrReplaceTempView(\"publicdepot\")\r\n",
							"\r\n",
							"#create dataframe for the publiccustomer orv table\r\n",
							"publiccustomer = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publiccustomer.parquet', format='parquet')\r\n",
							"publiccustomer.createOrReplaceTempView(\"publiccustomer\")\r\n",
							"\r\n",
							"#create dataframe for the dbocustomer lms table\r\n",
							"dbocustomer = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbocustomer.parquet', format='parquet')\r\n",
							"dbocustomer.createOrReplaceTempView(\"dbocustomer\")\r\n",
							"\r\n",
							"#create dataframe for the dbozone lms table\r\n",
							"dbozone = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbozone.parquet', format='parquet')\r\n",
							"dbozone.createOrReplaceTempView(\"dbozone\")\r\n",
							"\r\n",
							"#create dataframe for the dbosroute lms table\r\n",
							"dbosroute = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbosroute.parquet', format='parquet')\r\n",
							"dbosroute.createOrReplaceTempView(\"dbosroute\")\r\n",
							"\r\n",
							"#create dataframe for the dbobillzoneroute lms table\r\n",
							"dbobillzoneroute = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbobill_zoneroute.parquet', format='parquet')\r\n",
							"dbobillzoneroute.createOrReplaceTempView(\"dbobillzoneroute\")\r\n",
							"\r\n",
							"#create dataframe for the dbobillzone lms table\r\n",
							"dbobillzone = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbobill_zone.parquet', format='parquet')\r\n",
							"dbobillzone.createOrReplaceTempView(\"dbobillzone\")\r\n",
							"\r\n",
							"#create dataframe for the dbobillbillcustomersr  lms table\r\n",
							"dbobillbillcustomersr = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbobill_billcustomersr.parquet', format='parquet')\r\n",
							"dbobillbillcustomersr.createOrReplaceTempView(\"dbobillbillcustomersr\")\r\n",
							"\r\n",
							"#create dataframe for the dbobillcustomergroups  lms table\r\n",
							"dbobillcustomergroups  = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbobill_customergroups.parquet', format='parquet')\r\n",
							"dbobillcustomergroups.createOrReplaceTempView(\"dbobillcustomergroups\")\r\n",
							"\r\n",
							"#create dataframe for the dbobillcustomergrouptypes  lms table\r\n",
							"dbobillcustomergrouptypes  = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbobill_customergrouptypes.parquet', format='parquet')\r\n",
							"dbobillcustomergrouptypes.createOrReplaceTempView(\"dbobillcustomergrouptypes\")\r\n",
							"\r\n",
							"#create dataframe for the dbolocation  lms table\r\n",
							"dbolocation  = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbolocation.parquet', format='parquet')\r\n",
							"dbolocation.createOrReplaceTempView(\"dbolocation\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbodepot LMS Table\r\n",
							"dbodepot = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/CLMasterData/dbodepot.parquet', format='parquet')\r\n",
							"dbodepot.createOrReplaceTempView(\"dbodepot\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbovehicle LMS Table\r\n",
							"dbovehicle = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbovehicle.parquet', format='parquet')\r\n",
							"dbovehicle.createOrReplaceTempView(\"dbovehicle\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbodb_reasongroup LMS Table\r\n",
							"dbodb_reasongroup = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodb_reasongroup.parquet', format='parquet')\r\n",
							"dbodb_reasongroup.createOrReplaceTempView(\"dbodb_reasongroup\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbodb_inservicereason LMS Table\r\n",
							"dbodb_inservicereason = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodb_inservicereason.parquet', format='parquet')\r\n",
							"dbodb_inservicereason.createOrReplaceTempView(\"dbodb_inservicereason\")\r\n",
							"\r\n",
							"#create dataframe for the publicpod orv table\r\n",
							"publicpod = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicpod.parquet', format='parquet')\r\n",
							"publicpod.createOrReplaceTempView(\"publicpod\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbodb_reasondetail LMS Table\r\n",
							"dbodb_reasondetail = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodb_reasondetail.parquet', format='parquet')\r\n",
							"dbodb_reasondetail.createOrReplaceTempView(\"dbodb_reasondetail\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbodb_delayreason LMS Table\r\n",
							"dbodb_delayreason = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodb_delayreason.parquet', format='parquet')\r\n",
							"dbodb_delayreason.createOrReplaceTempView(\"dbodb_delayreason\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbodb_notdbreason LMS Table\r\n",
							"dbodb_notdbreason = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodb_notdbreason.parquet', format='parquet')\r\n",
							"dbodb_notdbreason.createOrReplaceTempView(\"dbodb_notdbreason\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbodb_deliverystatus LMS Table\r\n",
							"dbodb_deliverystatus = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodb_deliverystatus.parquet', format='parquet')\r\n",
							"dbodb_deliverystatus.createOrReplaceTempView(\"dbodb_deliverystatus\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbocustacc LMS Table\r\n",
							"dbocustacc = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbocustacc.parquet', format='parquet')\r\n",
							"dbocustacc.createOrReplaceTempView(\"dbocustacc\")\r\n",
							"\r\n",
							"#Create DataFrame for the dborouteratetype LMS Table\r\n",
							"dborouteratetype = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dborouteratetype.parquet', format='parquet')\r\n",
							"dborouteratetype.createOrReplaceTempView(\"dborouteratetype\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbodriver LMS Table\r\n",
							"dbodriver = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodriver.parquet', format='parquet')\r\n",
							"dbodriver.createOrReplaceTempView(\"dbodriver\")\r\n",
							"\r\n",
							"#Create DataFrame for the publicorvuser LMS Table\r\n",
							"publicorvuser = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicorvuser.parquet', format='parquet')\r\n",
							"publicorvuser.createOrReplaceTempView(\"publicorvuser\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbodamagedparcelauditfluteprofiles LMS Table\r\n",
							"dbodamagedparcelauditfluteprofiles = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodamagedparcelauditfluteprofiles.parquet', format='parquet')\r\n",
							"dbodamagedparcelauditfluteprofiles.createOrReplaceTempView(\"dbodamagedparcelauditfluteprofiles\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbodamagedparcelaudittapetype LMS Table\r\n",
							"dbodamagedparcelaudittapetype = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodamagedparcelaudittapetype.parquet', format='parquet')\r\n",
							"dbodamagedparcelaudittapetype.createOrReplaceTempView(\"dbodamagedparcelaudittapetype\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbodamagedparcelaudittype LMS Table\r\n",
							"dbodamagedparcelaudittype = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodamagedparcelaudittype.parquet', format='parquet')\r\n",
							"dbodamagedparcelaudittype.createOrReplaceTempView(\"dbodamagedparcelaudittype\")"
						],
						"outputs": [],
						"execution_count": 78
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Ops/Distribution/01_DIST_STDimensions"
						],
						"outputs": [],
						"execution_count": 79
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stcustacc.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stcustacc.parquet', mode = \"overwrite\")\r\n",
							"stpod.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stpod.parquet', mode = \"overwrite\")\r\n",
							"stinservicereasons.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stinservicereasons.parquet', mode = \"overwrite\")\r\n",
							"stdispatchsegment.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stdispatchsegment.parquet', mode = \"overwrite\")\r\n",
							"storvuser.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/storvuser.parquet', mode = \"overwrite\")\r\n",
							"stvehicle.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stvehicle.parquet', mode = \"overwrite\")\r\n",
							"stdepotorv.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stdepotorv.parquet', mode = \"overwrite\")\r\n",
							"stdepot.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stdepot.parquet', mode = \"overwrite\")\r\n",
							"stbillcustomer.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stbillcustomer.parquet', mode = \"overwrite\")\r\n",
							"stdeliverypickupcustomer.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stdeliverypickupcustomer.parquet', mode = \"overwrite\")\r\n",
							"stzone.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stzone.parquet', mode = \"overwrite\")\r\n",
							"stsroute.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stsroute.parquet', mode = \"overwrite\")\r\n",
							"stbillzoneroute.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stbillzoneroute.parquet', mode = \"overwrite\")\r\n",
							"stbillzone.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stbillzone.parquet', mode = \"overwrite\")\r\n",
							"stlocation.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stlocation.parquet', mode = \"overwrite\")\r\n",
							"stendorsmentreason.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stendorsmentreason.parquet', mode = \"overwrite\")\r\n",
							"stwaybilldelayreasons.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stwaybilldelayreasons.parquet', mode = \"overwrite\")\r\n",
							"stwaybillnotdbreasons.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stwaybillnotdbreasons.parquet', mode = \"overwrite\")\r\n",
							"stwaybilldeliverystatusreasons.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stwaybilldeliverystatusreasons.parquet', mode = \"overwrite\")\r\n",
							"stwaybillreasondetails.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stwaybillreasondetails.parquet', mode = \"overwrite\")\r\n",
							"stwaybillinservicereasons.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stwaybillinservicereasons.parquet', mode = \"overwrite\")\r\n",
							"strouteratetype.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/strouteratetype.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 80
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the ssttrackunion LMS Table\r\n",
							"ssttrackunion = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Semi Structured Data/OPS/ssttrackunion.parquet', format='parquet')\r\n",
							"ssttrackunion.createOrReplaceTempView(\"ssttrackunion\")\r\n",
							"\r\n",
							"#Create DataFrame for the sstwaybillsperparcel LMS Table\r\n",
							"sstwaybillsperparcel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Semi Structured Data/OPS/sstwaybillsperparcel.parquet', format='parquet')\r\n",
							"sstwaybillsperparcel.createOrReplaceTempView(\"sstwaybillsperparcel\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbolocation LMS Table\r\n",
							"dbolocation = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbolocation.parquet', format='parquet')\r\n",
							"dbolocation.createOrReplaceTempView(\"dbolocation\")\r\n",
							"\r\n",
							"#Create DataFrame for the dboloads LMS Table\r\n",
							"dboloads = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboloads.parquet', format='parquet')\r\n",
							"dboloads.createOrReplaceTempView(\"dboloads\")\r\n",
							"\r\n",
							"#Create DataFrame for the sstparcelunion LMS Table\r\n",
							"sstparcelunion = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Semi Structured Data/OPS/sstparcelunion.parquet', format='parquet')\r\n",
							"sstparcelunion.createOrReplaceTempView(\"sstparcelunion\")"
						],
						"outputs": [],
						"execution_count": 81
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Ops/Distribution/02_DIST_STLMSTrack"
						],
						"outputs": [],
						"execution_count": 82
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stlmstrack.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stlmstrack.parquet', mode = \"overwrite\")\r\n",
							"stvsmparcel.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stvsmparcel.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 83
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the sstparcelunion LMS Table\r\n",
							"sstparcelunion = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Semi Structured Data/OPS/sstparcelunion.parquet', format='parquet')\r\n",
							"sstparcelunion.createOrReplaceTempView(\"sstparcelunion\")\r\n",
							"\r\n",
							"#Create DataFrame for the dboorder LMS Table\r\n",
							"dboorder = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboorder.parquet', format='parquet')\r\n",
							"dboorder.createOrReplaceTempView(\"dboorder\")\r\n",
							"\r\n",
							"#Create DataFrame for the sstconsignmentunion LMS Table\r\n",
							"sstconsignmentunion = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Semi Structured Data/OPS/sstconsignmentunion.parquet', format='parquet')\r\n",
							"sstconsignmentunion.createOrReplaceTempView(\"sstconsignmentunion\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbowaybill LMS Table\r\n",
							"dbowaybill = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbowaybill.parquet', format='parquet')\r\n",
							"dbowaybill.createOrReplaceTempView(\"dbowaybill\")\r\n",
							"\r\n",
							"#Create DataFrame for the dboparceldetail LMS Table\r\n",
							"dboparceldetail = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboparceldetail.parquet', format='parquet')\r\n",
							"dboparceldetail.createOrReplaceTempView(\"dboparceldetail\")\r\n",
							"\r\n",
							"#Create DataFrame for the publicparcel LMS Table\r\n",
							"publicparcel = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicparcel.parquet', format='parquet')\r\n",
							"publicparcel.createOrReplaceTempView(\"publicparcel\")\r\n",
							"\r\n",
							"#Create DataFrame for the publicdelivery ORV Table\r\n",
							"publicdelivery = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicdelivery.parquet', format='parquet')\r\n",
							"publicdelivery.createOrReplaceTempView(\"publicdelivery\")\r\n",
							"\r\n",
							"#Create DataFrame for the stvsmparcel LMS Table\r\n",
							"stvsmparcel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stvsmparcel.parquet', format='parquet')\r\n",
							"stvsmparcel.createOrReplaceTempView(\"stvsmparcel\")\r\n",
							"\r\n",
							"#Create DataFrame for the dboweekendholiday LMS Table\r\n",
							"dboweekendholiday = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboweekendholiday.parquet', format='parquet')\r\n",
							"dboweekendholiday.createOrReplaceTempView(\"dboweekendholiday\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbocustacc LMS Table\r\n",
							"dbocustacc = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbocustacc.parquet', format='parquet')\r\n",
							"dbocustacc.createOrReplaceTempView(\"dbocustacc\")\r\n",
							"\r\n",
							"#Create DataFrame for the stdeliverypickupcustomer LMS Table\r\n",
							"stdeliverypickupcustomer = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Structured Data/OPS/stdeliverypickupcustomer.parquet', format='parquet')\r\n",
							"stdeliverypickupcustomer.createOrReplaceTempView(\"stdeliverypickupcustomer\")\r\n",
							"\r\n",
							"#Create DataFrame for the stsroute LMS Table\r\n",
							"stsroute = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Structured Data/OPS/stsroute.parquet', format='parquet')\r\n",
							"stsroute.createOrReplaceTempView(\"stsroute\")\r\n",
							"\r\n",
							"#Create DataFrame for the stzone LMS Table\r\n",
							"stzone = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Structured Data/OPS/stzone.parquet', format='parquet')\r\n",
							"stzone.createOrReplaceTempView(\"stzone\")\r\n",
							"\r\n",
							"#Create DataFrame for the stlocation LMS Table\r\n",
							"stlocation = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Structured Data/OPS/stlocation.parquet', format='parquet')\r\n",
							"stlocation.createOrReplaceTempView(\"stlocation\")"
						],
						"outputs": [],
						"execution_count": 84
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Ops/Distribution/03_DIST_STParcel"
						],
						"outputs": [],
						"execution_count": 85
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stparcel.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stparcel.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the dboorder LMS Table\r\n",
							"dboorder = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboorder.parquet', format='parquet')\r\n",
							"dboorder.createOrReplaceTempView(\"dboorder\")\r\n",
							"\r\n",
							"#Create DataFrame for the sstparcelbyorder LMS Table\r\n",
							"sstparcelbyorder = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Semi Structured Data/OPS/sstparcelbyorder.parquet', format='parquet')\r\n",
							"sstparcelbyorder.createOrReplaceTempView(\"sstparcelbyorder\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbowaybill LMS Table\r\n",
							"dbowaybill = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbowaybill.parquet', format='parquet')\r\n",
							"dbowaybill.createOrReplaceTempView(\"dbowaybill\")\r\n",
							"\r\n",
							"#Create DataFrame for the sstparcelunion LMS Table\r\n",
							"sstparcelunion = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Semi Structured Data/OPS/sstparcelunion.parquet', format='parquet')\r\n",
							"sstparcelunion.createOrReplaceTempView(\"sstparcelunion\")\r\n",
							"\r\n",
							"#Create DataFrame for the sstconsignmentunion LMS Table\r\n",
							"sstconsignmentunion = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Semi Structured Data/OPS/sstconsignmentunion.parquet', format='parquet')\r\n",
							"sstconsignmentunion.createOrReplaceTempView(\"sstconsignmentunion\")"
						],
						"outputs": [],
						"execution_count": 86
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Ops/Distribution/04_DIST_STOrder"
						],
						"outputs": [],
						"execution_count": 87
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"storder.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/storder.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#create dataframe for the sstconsignemntunion lms table\r\n",
							"sstconsignemntunion = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Semi Structured Data/OPS/sstconsignmentunion.parquet/', format='parquet')\r\n",
							"sstconsignemntunion.createOrReplaceTempView(\"sstconsignemntunion\")\r\n",
							"\r\n",
							"#create dataframe for the sstparcelbyconsignment lms table\r\n",
							"sstparcelbyconsignment = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Semi Structured Data/OPS/sstparcelbyconsignment.parquet', format='parquet')\r\n",
							"sstparcelbyconsignment.createOrReplaceTempView(\"sstparcelbyconsignment\")\r\n",
							"\r\n",
							"#create dataframe for the dbobill_consignmentr lms table\r\n",
							"dbobill_consignmentr = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbobill_consignmentr.parquet', format='parquet')\r\n",
							"dbobill_consignmentr.createOrReplaceTempView(\"dbobill_consignmentr\")\r\n",
							"\r\n",
							"#create dataframe for the dbobill_billcustomersr lms table\r\n",
							"dbobill_billcustomersr = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbobill_billcustomersr.parquet', format='parquet')\r\n",
							"dbobill_billcustomersr.createOrReplaceTempView(\"dbobill_billcustomersr\")\r\n",
							"\r\n",
							"#create dataframe for the stparcel lms table\r\n",
							"stparcel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stparcel.parquet', format='parquet')\r\n",
							"stparcel.createOrReplaceTempView(\"stparcel\")\r\n",
							"\r\n",
							"#create dataframe for the dbowaybill lms table\r\n",
							"dbowaybill = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbowaybill.parquet', format='parquet')\r\n",
							"dbowaybill.createOrReplaceTempView(\"dbowaybill\")\r\n",
							"\r\n",
							"#Create DataFrame for the dboweekendholiday LMS Table\r\n",
							"dboweekendholiday = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboweekendholiday.parquet', format='parquet')\r\n",
							"dboweekendholiday.createOrReplaceTempView(\"dboweekendholiday\")\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 88
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Ops/Distribution/05_DIST_STConsignment"
						],
						"outputs": [],
						"execution_count": 89
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stconsignment.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stconsignment.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the dbowaybill LMS Table\r\n",
							"dbowaybill = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbowaybill.parquet', format='parquet')\r\n",
							"dbowaybill.createOrReplaceTempView(\"dbowaybill\")\r\n",
							"\r\n",
							"#Create DataFrame for the sstparcelbywaybill LMS Table\r\n",
							"sstparcelbywaybill = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Semi Structured Data/OPS/sstparcelbywaybill.parquet', format='parquet')\r\n",
							"sstparcelbywaybill.createOrReplaceTempView(\"sstparcelbywaybill\")\r\n",
							"\r\n",
							"#Create DataFrame for the publicdelivery ORV Table\r\n",
							"publicdelivery = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicdelivery.parquet', format='parquet')\r\n",
							"publicdelivery.createOrReplaceTempView(\"publicdelivery\")\r\n",
							"\r\n",
							"#Create DataFrame for the publicdispatch ORV Table\r\n",
							"publicdispatch = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicdispatch.parquet', format='parquet')\r\n",
							"publicdispatch.createOrReplaceTempView(\"publicdispatch\")\r\n",
							"\r\n",
							"#create dataframe for the publicpod orv table\r\n",
							"publicpod = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicpod.parquet', format='parquet')\r\n",
							"publicpod.createOrReplaceTempView(\"publicpod\")\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 90
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Ops/Distribution/06_DIST_STWaybill"
						],
						"outputs": [],
						"execution_count": 91
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stwaybill.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stwaybill.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the dboloads LMS Table\r\n",
							"dboloads = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboloads.parquet', format='parquet')\r\n",
							"dboloads.createOrReplaceTempView(\"dboloads\")\r\n",
							"\r\n",
							"#Create DataFrame for the sstparcelbyload LMS Table\r\n",
							"sstparcelbyload = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Semi Structured Data/OPS/sstparcelbyload.parquet', format='parquet')\r\n",
							"sstparcelbyload.createOrReplaceTempView(\"sstparcelbyload\")"
						],
						"outputs": [],
						"execution_count": 92
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Ops/Distribution/07_DIST_STLoadChild"
						],
						"outputs": [],
						"execution_count": 93
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stloadchild.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stloadchild.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the stloadchild LMS Table\r\n",
							"stloadchild = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stloadchild.parquet', format='parquet')\r\n",
							"stloadchild.createOrReplaceTempView(\"stloadchild\")\r\n",
							"\r\n",
							"#Create DataFrame for the stlmstrack LMS Table\r\n",
							"stlmstrack = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stlmstrack.parquet', format='parquet')\r\n",
							"stlmstrack.createOrReplaceTempView(\"stlmstrack\")\r\n",
							"\r\n",
							"#Create DataFrame for the stwaybill LMS Table\r\n",
							"stwaybill = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stwaybill.parquet', format='parquet')\r\n",
							"stwaybill.createOrReplaceTempView(\"stwaybill\")\r\n",
							"\r\n",
							"#Create DataFrame for the stdeliverypickupcustomer LMS Table\r\n",
							"stdeliverypickupcustomer = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stdeliverypickupcustomer.parquet', format='parquet')\r\n",
							"stdeliverypickupcustomer.createOrReplaceTempView(\"stdeliverypickupcustomer\")\r\n",
							"\r\n",
							"#Create DataFrame for the stsroute LMS Table\r\n",
							"stsroute = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stsroute.parquet', format='parquet')\r\n",
							"stsroute.createOrReplaceTempView(\"stsroute\")\r\n",
							"\r\n",
							"#Create DataFrame for the publicdispatch ORV Table\r\n",
							"publicdispatch = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicdispatch.parquet', format='parquet')\r\n",
							"publicdispatch.createOrReplaceTempView(\"publicdispatch\")\r\n",
							"\r\n",
							"#Create DataFrame for the publicreportdistotmgntdispatchdt ORV Table\r\n",
							"publicreportdistotmgntdispatchdt = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicreportdistotmgntdispatchdt.parquet', format='parquet')\r\n",
							"publicreportdistotmgntdispatchdt.createOrReplaceTempView(\"publicreportdistotmgntdispatchdt\")\r\n",
							"\r\n",
							"#Create DataFrame for the stmdvehicle ORV Table\r\n",
							"stmdvehicle = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Structured Data/MD/stmdvehicle.parquet', format='parquet')\r\n",
							"stmdvehicle.createOrReplaceTempView(\"stmdvehicle\")\r\n",
							"\r\n",
							"#Create DataFrame for the publiccourier ORV Table\r\n",
							"publiccourier = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publiccourier.parquet', format='parquet')\r\n",
							"publiccourier.createOrReplaceTempView(\"publiccourier\")"
						],
						"outputs": [],
						"execution_count": 94
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Ops/Distribution/08_DIST_STLoadParent"
						],
						"outputs": [],
						"execution_count": 95
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stloadparent.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stloadparent.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the dbodc_transfer LMS Table\r\n",
							"dbodc_transfer = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodc_transfer.parquet', format='parquet')\r\n",
							"dbodc_transfer.createOrReplaceTempView(\"dbodc_transfer\")"
						],
						"outputs": [],
						"execution_count": 96
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Ops/Distribution/09_DIST_STDCTransfer"
						],
						"outputs": [],
						"execution_count": 97
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stdctransfer.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stdctransfer.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 29
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#create dataframe for the publicclaim orv table\r\n",
							"publicclaim = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicclaim.parquet', format='parquet')\r\n",
							"publicclaim.createOrReplaceTempView(\"publicclaim\")\r\n",
							"\r\n",
							"#create dataframe for the publicdelivery orv table\r\n",
							"publicdelivery = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicdelivery.parquet', format='parquet')\r\n",
							"publicdelivery.createOrReplaceTempView(\"publicdelivery\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbowbendorsement LMS Table\r\n",
							"dbowbendorsement = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbowbendorsement.parquet', format='parquet')\r\n",
							"dbowbendorsement.createOrReplaceTempView(\"dbowbendorsement\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbodb_endorsement LMS Table\r\n",
							"dbodb_endorsement = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodb_endorsement.parquet', format='parquet')\r\n",
							"dbodb_endorsement.createOrReplaceTempView(\"dbodb_endorsement\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 98
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Ops/Distribution/10_DIST_STEndorsments"
						],
						"outputs": [],
						"execution_count": 99
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stendorsments.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stendorsments.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 32
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the dbocollect LMS Table\r\n",
							"dbocollect = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbocollect.parquet', format='parquet')\r\n",
							"dbocollect.createOrReplaceTempView(\"dbocollect\")\r\n",
							"\r\n",
							"#Create DataFrame for the publicdraftcollection ORV Table\r\n",
							"publicdelivery = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicdelivery.parquet', format='parquet')\r\n",
							"publicdelivery.createOrReplaceTempView(\"publicdelivery\")\r\n",
							"\r\n",
							"#Create DataFrame for the publicdraftcollection ORV Table\r\n",
							"publicdraftcollection = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicdraftcollection.parquet', format='parquet')\r\n",
							"publicdraftcollection.createOrReplaceTempView(\"publicdraftcollection\")\r\n",
							"\r\n",
							"#Create DataFrame for the publicdispatch ORV Table\r\n",
							"publicdispatch = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicdispatch.parquet', format='parquet')\r\n",
							"publicdispatch.createOrReplaceTempView(\"publicdispatch\")\r\n",
							"\r\n",
							"#create dataframe for the publicpod orv table\r\n",
							"publicpod = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicpod.parquet', format='parquet')\r\n",
							"publicpod.createOrReplaceTempView(\"publicpod\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 100
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Ops/Distribution/11_DIST_STCollection"
						],
						"outputs": [],
						"execution_count": 101
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stcollection.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stcollection.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 35
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#create dataframe for the stparcel lms table\r\n",
							"stparcel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stparcel.parquet/', format='parquet')\r\n",
							"stparcel.createOrReplaceTempView(\"stparcel\")\r\n",
							"\r\n",
							"#Create DataFrame for the stlmstrack LMS Table\r\n",
							"stlmstrack = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stlmstrack.parquet', format='parquet')\r\n",
							"stlmstrack.createOrReplaceTempView(\"stlmstrack\")\r\n",
							"\r\n",
							"#create dataframe for the stconsignment lms table\r\n",
							"stconsignment = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stconsignment.parquet/', format='parquet')\r\n",
							"stconsignment.createOrReplaceTempView(\"stconsignment\")\r\n",
							"\r\n",
							"#create dataframe for the stwaybill lms table\r\n",
							"stwaybill = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stwaybill.parquet/', format='parquet')\r\n",
							"stwaybill.createOrReplaceTempView(\"stwaybill\")\r\n",
							"\r\n",
							"#create dataframe for the storder lms table\r\n",
							"storder = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/storder.parquet/', format='parquet')\r\n",
							"storder.createOrReplaceTempView(\"storder\")\r\n",
							"\r\n",
							"#Create DataFrame for the stloadparent LMS Table\r\n",
							"stloadparent = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stloadparent.parquet', format='parquet')\r\n",
							"stloadparent.createOrReplaceTempView(\"stloadparent\")\r\n",
							"\r\n",
							"#Create DataFrame for the stloadchild LMS Table\r\n",
							"stloadchild = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stloadchild.parquet', format='parquet')\r\n",
							"stloadchild.createOrReplaceTempView(\"stloadchild\")\r\n",
							"\r\n",
							"#Create DataFrame for the stcollection LMS Table\r\n",
							"stcollection = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stcollection.parquet', format='parquet')\r\n",
							"stcollection.createOrReplaceTempView(\"stcollection\")\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 102
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the dbodamagedparcelaudit LMS Table\r\n",
							"dbodamagedparcelaudit = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodamagedparcelaudit.parquet', format='parquet')\r\n",
							"dbodamagedparcelaudit.createOrReplaceTempView(\"dbodamagedparcelaudit\")\r\n",
							"\r\n",
							"#Create DataFrame for the stdamagedparcelauditfluteprofiles LMS Table\r\n",
							"stdamagedparcelauditfluteprofiles = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stdamagedparcelauditfluteprofiles.parquet', format='parquet')\r\n",
							"stdamagedparcelauditfluteprofiles.createOrReplaceTempView(\"stdamagedparcelauditfluteprofiles\")\r\n",
							"\r\n",
							"#Create DataFrame for the stdamagedparcelaudittapetype LMS Table\r\n",
							"stdamagedparcelaudittapetype = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stdamagedparcelaudittapetype.parquet', format='parquet')\r\n",
							"stdamagedparcelaudittapetype.createOrReplaceTempView(\"stdamagedparcelaudittapetype\")\r\n",
							"\r\n",
							"#Create DataFrame for the stdamagedparcelaudittype LMS Table\r\n",
							"stdamagedparcelaudittype = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stdamagedparcelaudittype.parquet', format='parquet')\r\n",
							"stdamagedparcelaudittype.createOrReplaceTempView(\"stdamagedparcelaudittype\")"
						],
						"outputs": [],
						"execution_count": 110
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Ops/Distribution/16_DIST_STParcelDamagedAudit"
						],
						"outputs": [],
						"execution_count": 111
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stdamagedparcelaudit.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stdamagedparcelaudit.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 114
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql import Window\r\n",
							"from pyspark.sql import functions as F\r\n",
							"my_window = (Window.orderBy('daymonth').rowsBetween(Window.unboundedPreceding, 0))"
						],
						"outputs": [],
						"execution_count": 112
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Ops/Distribution/12_DIST_ST3MonthRoling"
						],
						"outputs": [],
						"execution_count": 117
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stparcel3myoy.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stparcel3myoy.parquet', mode = \"overwrite\")\r\n",
							"stconsignment3myoy.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stconsignment3myoy.parquet', mode = \"overwrite\")\r\n",
							"stwaybill3myoy.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stwaybill3myoy.parquet', mode = \"overwrite\")\r\n",
							"storder3myoy.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/storder3myoy.parquet', mode = \"overwrite\")\r\n",
							"stlmstrack3myoy.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stlmstrack3myoy.parquet', mode = \"overwrite\")\r\n",
							"stloadchild3myoy.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stloadchild3myoy.parquet', mode = \"overwrite\")\r\n",
							"stloadparent3myoy.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stloadparent3myoy.parquet', mode = \"overwrite\")\r\n",
							"stcollection3myoy.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stcollection3myoy.parquet', mode = \"overwrite\")\r\n",
							"staggrigatemeasures.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/staggrigatemeasures.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 118
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the sttransactionsdetail SAP Table\r\n",
							"sttransactionsdetail = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/FINANCE/sttransactionsdetail.parquet', format='parquet')\r\n",
							"sttransactionsdetail.createOrReplaceTempView(\"sttransactionsdetail\")\r\n",
							"\r\n",
							"#Create DataFrame for the stchartofaccounts SAP Table\r\n",
							"stchartofaccounts = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/FINANCE/stchartofaccounts.parquet', format='parquet')\r\n",
							"stchartofaccounts.createOrReplaceTempView(\"stchartofaccounts\")\r\n",
							"\r\n",
							"#Create DataFrame for the stmdsapdepot SAP Table\r\n",
							"stmdsapdepot = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/FINANCE/stmdsapdepot.parquet', format='parquet')\r\n",
							"stmdsapdepot.createOrReplaceTempView(\"stmdsapdepot\")\r\n",
							"\r\n",
							"#Create DataFrame for the stbusinnesspartner SAP Table\r\n",
							"stbusinnesspartner = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/FINANCE/stbusinnesspartner.parquet', format='parquet')\r\n",
							"stbusinnesspartner.createOrReplaceTempView(\"stbusinnesspartner\")\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 119
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Ops/Distribution/13_DIST_STOpsFinTransactions"
						],
						"outputs": [],
						"execution_count": 120
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stopsfintransactions.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stopsfintransactions.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 121
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Variables\r\n",
							"\r\n",
							"var_File_Path = \"abfss://synapse@\"+ StorageAccountRead2 +\".dfs.core.windows.net/Unstructured Data/MRP\"\r\n",
							"# Processing\r\n",
							"\r\n",
							"mrp_csv_files = spark.read.option(\"header\", True).csv(var_File_Path)"
						],
						"outputs": [],
						"execution_count": 122
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Ops/Distribution/14_DIST_STReadMRPFiles"
						],
						"outputs": [],
						"execution_count": 123
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stmrpcsvfiles.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stmrpcsvfiles.parquet', mode = \"overwrite\")\r\n",
							"stparcelmrprecon.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stparcelmrprecon.parquet', mode = \"overwrite\")\r\n",
							"sttransfers.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/sttransfers.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 124
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the publicretroanalysis LMS Table\r\n",
							"publicretroanalysis = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicreportdispatchretroanalysis.parquet', format='parquet')\r\n",
							"publicretroanalysis.createOrReplaceTempView(\"publicretroanalysis\")"
						],
						"outputs": [],
						"execution_count": 125
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Ops/Distribution/15_DIST_STRetroDispatch"
						],
						"outputs": [],
						"execution_count": 126
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stretroanalysis.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stretroanalysis.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 127
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"SUCCESS\")"
						],
						"outputs": [],
						"execution_count": 49
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/STOPS_RunOrderLinehaul')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Ops/Linehaul"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "e58da4e3-8f16-434c-a8d3-17008d060d12"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse_Prod/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace-prod/bigDataPools/PRDSparkPool",
						"name": "PRDSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/PRDSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"if 'prod' in Environment:\r\n",
							"    StorageAccount = 'citylogisticsstorageprod'\r\n",
							"    StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"    StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"    StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"else:\r\n",
							"    StorageAccount = 'citylogisticsstorage'\r\n",
							"    StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"    StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"    StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import *\r\n",
							"from pyspark.sql.window import Window\r\n",
							"from pyspark.sql.types import IntegralType"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the publicbooking TMS Table\r\n",
							"publicbooking = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publicbooking.parquet', format='parquet')\r\n",
							"publicbooking.createOrReplaceTempView(\"publicbooking\")\r\n",
							"\r\n",
							"#Create DataFrame for the publicnonbooking TMS Table\r\n",
							"publicnonbooking = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publicnonbooking.parquet', format='parquet')\r\n",
							"publicnonbooking.createOrReplaceTempView(\"publicnonbooking\")\r\n",
							"\r\n",
							"#Create DataFrame for the publictrip TMS Table\r\n",
							"publictrip = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publictrip.parquet', format='parquet')\r\n",
							"publictrip.createOrReplaceTempView(\"publictrip\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbolhbooking TMS Table\r\n",
							"dboLHBooking = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbolh_booking.parquet', format='parquet')\r\n",
							"dboLHBooking.createOrReplaceTempView(\"dboLHBooking\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbolhroutes TMS Table\r\n",
							"dbolhroutes = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbolh_routes.parquet', format='parquet')\r\n",
							"dbolhroutes.createOrReplaceTempView(\"dbolhroutes\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbolhbookingspertrip TMS Table\r\n",
							"dbolhbookingspertrip = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbolh_bookingspertrip.parquet', format='parquet')\r\n",
							"dbolhbookingspertrip.createOrReplaceTempView(\"dbolhbookingspertrip\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Ops/Linehaul/01_LH_STBooking"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stbooking.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stbooking.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the publicdriverpayitem TMS Table\r\n",
							"publicdriverpayitem = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publicdriverpayitem.parquet', format='parquet')\r\n",
							"publicdriverpayitem.createOrReplaceTempView(\"publicdriverpayitem\")"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Ops/Linehaul/02_LH_STDriverPayItem"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stdriverpayitem.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stdriverpayitem.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the publicfinancedata TMS Table\r\n",
							"publicfinancedata = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publicfinancedata.parquet', format='parquet')\r\n",
							"publicfinancedata.createOrReplaceTempView(\"publicfinancedata\")"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Ops/Linehaul/03_LH_STFinanceData"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stfinancedata.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stfinancedata.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the publicInstruction TMS Table\r\n",
							"publicInstruction = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publicinstruction.parquet', format='parquet')\r\n",
							"publicInstruction.createOrReplaceTempView(\"publicInstruction\")\r\n",
							"\r\n",
							"#Create DataFrame for the dboLHInstruction LMS Table\r\n",
							"dboLHInstruction = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbolh_instruction.parquet', format='parquet')\r\n",
							"dboLHInstruction.createOrReplaceTempView(\"dboLHInstruction\")\r\n",
							"\r\n",
							"#Create DataFrame for the publicLHInstruction LMS Table\r\n",
							"publicLHInstruction = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publiclhdelivery.parquet', format='parquet')\r\n",
							"publicLHInstruction.createOrReplaceTempView(\"publicLHInstruction\")\r\n",
							"\r\n",
							"#Create DataFrame for the stbooking TMSLMS combine Table\r\n",
							"stbooking = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stbooking.parquet', format='parquet')\r\n",
							"stbooking.createOrReplaceTempView(\"stbooking\")"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Ops/Linehaul/04_LH_STInstruction"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stinstruction.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stinstruction.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the publicinvoice TMS Table\r\n",
							"publicinvoice = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publicinvoice.parquet', format='parquet')\r\n",
							"publicinvoice.createOrReplaceTempView(\"publicinvoice\")\r\n",
							"\r\n",
							"#Create DataFrame for the publicinvoiceitem TMS Table\r\n",
							"publicinvoiceitem = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publicinvoiceitem.parquet', format='parquet')\r\n",
							"publicinvoiceitem.createOrReplaceTempView(\"publicinvoiceitem\")"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Ops/Linehaul/05_LH_STInvoice"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stinvoice.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stinvoice.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the publiccustomer TMS Table\r\n",
							"publiccustomer = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publiccustomer.parquet', format='parquet')\r\n",
							"publiccustomer.createOrReplaceTempView(\"publiccustomer\")\r\n",
							"\r\n",
							"#Create DataFrame for the publicthirdparty TMS Table\r\n",
							"publicthirdparty = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publicthirdparty.parquet', format='parquet')\r\n",
							"publicthirdparty.createOrReplaceTempView(\"publicthirdparty\")\r\n",
							"\r\n",
							"#Create DataFrame for the publicroute TMS Table\r\n",
							"publicroute = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publicroute.parquet', format='parquet')\r\n",
							"publicroute.createOrReplaceTempView(\"publicroute\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbolh_routes TMS Table\r\n",
							"dbolh_routes = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbolh_routes.parquet', format='parquet')\r\n",
							"dbolh_routes.createOrReplaceTempView(\"dbolh_routes\")\r\n",
							"\r\n",
							"#Create DataFrame for the publicaddress TMS Table\r\n",
							"publicaddress = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publicaddress.parquet', format='parquet')\r\n",
							"publicaddress.createOrReplaceTempView(\"publicaddress\")\r\n",
							"\r\n",
							"#Create DataFrame for the publicfuelcard TMS Table\r\n",
							"publicfuelcard = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publicfuelcard.parquet', format='parquet')\r\n",
							"publicfuelcard.createOrReplaceTempView(\"publicfuelcard\")\r\n",
							"\r\n",
							"#Create DataFrame for the publicfuelzone TMS Table\r\n",
							"publicfuelzone = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publicfuelzone.parquet', format='parquet')\r\n",
							"publicfuelzone.createOrReplaceTempView(\"publicfuelzone\")\r\n",
							"\r\n",
							"#Create DataFrame for the publicfuelzonehistory TMS Table\r\n",
							"publicfuelzonehistory = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publicfuelzonehistory.parquet', format='parquet')\r\n",
							"publicfuelzonehistory.createOrReplaceTempView(\"publicfuelzonehistory\")\r\n",
							"\r\n",
							"#Create DataFrame for the publicratecomponent TMS Table\r\n",
							"publicratecomponent = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publicratecomponent.parquet', format='parquet')\r\n",
							"publicratecomponent.createOrReplaceTempView(\"publicratecomponent\")\r\n",
							"\r\n",
							"#Create DataFrame for the publicrateprofile TMS Table\r\n",
							"publicrateprofile = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publicrateprofile.parquet', format='parquet')\r\n",
							"publicrateprofile.createOrReplaceTempView(\"publicrateprofile\")\r\n",
							"\r\n",
							"#Create DataFrame for the publicselist TMS Table\r\n",
							"publicselist = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publicselist.parquet', format='parquet')\r\n",
							"publicselist.createOrReplaceTempView(\"publicselist\")\r\n",
							"\r\n",
							"#Create DataFrame for the publicselistitem TMS Table\r\n",
							"publicselistitem = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publicselistitem.parquet', format='parquet')\r\n",
							"publicselistitem.createOrReplaceTempView(\"publicselistitem\")\r\n",
							"\r\n",
							"#Create DataFrame for the publicdriverpay TMS Table\r\n",
							"publicdriverpay = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publicdriverpay.parquet', format='parquet')\r\n",
							"publicdriverpay.createOrReplaceTempView(\"publicdriverpay\")\r\n",
							"\r\n",
							"#Create DataFrame for the ssttrackunion LMS Table\r\n",
							"ssttrackunion = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Semi Structured Data/OPS/ssttrackunion.parquet', format='parquet')\r\n",
							"ssttrackunion.createOrReplaceTempView(\"ssttrackunion\")\r\n",
							"\r\n",
							"#Create DataFrame for the dboloads LMS Table\r\n",
							"dboloads = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dboloads.parquet', format='parquet')\r\n",
							"dboloads.createOrReplaceTempView(\"dboloads\")\r\n",
							"\r\n",
							"#Create DataFrame for the publicdispatchactionlog LMS Table\r\n",
							"publicdispatchactionlog = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicdispatchactionlog.parquet', format='parquet')\r\n",
							"publicdispatchactionlog.createOrReplaceTempView(\"publicdispatchactionlog\")\r\n",
							"\r\n",
							"#Create DataFrame for the publiclmsdata LMS Table\r\n",
							"publiclmsdata = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publiclmsdata.parquet', format='parquet')\r\n",
							"publiclmsdata.createOrReplaceTempView(\"publiclmsdata\")\r\n",
							"\r\n",
							"#create dataframe for the dbocustomer lms table\r\n",
							"dbocustomer = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbocustomer.parquet', format='parquet')\r\n",
							"dbocustomer.createOrReplaceTempView(\"dbocustomer\")\r\n",
							"\r\n",
							"#create dataframe for the publicbooking lms table\r\n",
							"publicbooking = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publicbooking.parquet', format='parquet')\r\n",
							"publicbooking.createOrReplaceTempView(\"publicbooking\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Ops/Linehaul/06_LH_STLinehaulDimensions"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stlmstrackscantime.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stlmstrackscantime.parquet', mode = \"overwrite\")\r\n",
							"stdriverpay.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stdriverpay.parquet', mode = \"overwrite\")\r\n",
							"stfuelcard.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stfuelcard.parquet', mode = \"overwrite\")\r\n",
							"stfuelzone.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stfuelzone.parquet', mode = \"overwrite\")\r\n",
							"stfuelzonehistory.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stfuelzonehistory.parquet', mode = \"overwrite\")\r\n",
							"stratecomponent.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stratecomponent.parquet', mode = \"overwrite\")\r\n",
							"strateprofile.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/strateprofile.parquet', mode = \"overwrite\")\r\n",
							"sttmslist.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/sttmslist.parquet', mode = \"overwrite\")\r\n",
							"stprimarycustomer.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stprimarycustomer.parquet', mode = \"overwrite\")\r\n",
							"stsecondarycustomer.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stsecondarycustomer.parquet', mode = \"overwrite\")\r\n",
							"stthirdparty.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stthirdparty.parquet', mode = \"overwrite\")\r\n",
							"stparentroute.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stparentroute.parquet', mode = \"overwrite\")\r\n",
							"stopsroute.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stopsroute.parquet', mode = \"overwrite\")\r\n",
							"stdispatchactionlog.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stdispatchactionlog.parquet', mode = \"overwrite\")\r\n",
							"stlmsparcelweights.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stlmsparcelweights.parquet', mode = \"overwrite\")\r\n",
							"stlmsbookingweights.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/stlmsbookingweights.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the publictrack TMS Table\r\n",
							"publictrack = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publictrack.parquet', format='parquet')\r\n",
							"publictrack.createOrReplaceTempView(\"publictrack\")"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Ops/Linehaul/07_LH_STTMSTrack"
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sttmstrack.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/sttmstrack.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the publictrip TMS Table\r\n",
							"publictrip = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publictrip.parquet', format='parquet')\r\n",
							"publictrip.createOrReplaceTempView(\"publictrip\")\r\n",
							"\r\n",
							"#Create DataFrame for the publicdispatch ORV Table\r\n",
							"publicdispatch = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/Onroute/publicdispatch.parquet', format='parquet')\r\n",
							"publicdispatch.createOrReplaceTempView(\"publicdispatch\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbolhtrip LMS Table\r\n",
							"dbolhtrip = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbolh_trip.parquet', format='parquet')\r\n",
							"dbolhtrip.createOrReplaceTempView(\"dbolhtrip\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbolhtrackstatus LMS Table\r\n",
							"dbolhtrackstatus = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbolh_trackstatus.parquet', format='parquet')\r\n",
							"dbolhtrackstatus.createOrReplaceTempView(\"dbolhtrackstatus\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbolhbookingspertrip TMS Table\r\n",
							"dbolhbookingspertrip = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbolh_bookingspertrip.parquet', format='parquet')\r\n",
							"dbolhbookingspertrip.createOrReplaceTempView(\"dbolhbookingspertrip\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbolhbooking TMS Table\r\n",
							"dbolhbooking = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbolh_booking.parquet', format='parquet')\r\n",
							"dbolhbooking.createOrReplaceTempView(\"dbolhbooking\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbolhtrailersize TMS Table\r\n",
							"dbolhtrailersize = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbolh_trailersize.parquet', format='parquet')\r\n",
							"dbolhtrailersize.createOrReplaceTempView(\"dbolhtrailersize\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbolh_3pl TMS Table\r\n",
							"dbolh_3pl = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbolh_3pl.parquet', format='parquet')\r\n",
							"dbolh_3pl.createOrReplaceTempView(\"dbolh_3pl\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbovehicle TMS Table\r\n",
							"dbovehicle = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbovehicle.parquet', format='parquet')\r\n",
							"dbovehicle.createOrReplaceTempView(\"dbovehicle\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbodriver TMS Table\r\n",
							"dbodriver = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbodriver.parquet', format='parquet')\r\n",
							"dbodriver.createOrReplaceTempView(\"dbodriver\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbovehiclebasic MD Table\r\n",
							"dbovehiclebasic = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/CLMasterData/dbovehiclebasic.parquet', format='parquet')\r\n",
							"dbovehiclebasic.createOrReplaceTempView(\"dbovehiclebasic\")\r\n",
							"\r\n",
							"#Create DataFrame for the dbopeoplebasic MD Table\r\n",
							"dbopeoplebasic = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/CLMasterData/dbopeoplebasic.parquet', format='parquet')\r\n",
							"dbopeoplebasic.createOrReplaceTempView(\"dbopeoplebasic\")\r\n",
							"\r\n",
							"#Create DataFrame for the stbooking TMS Table\r\n",
							"stbooking = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Structured Data/OPS/stbooking.parquet', format='parquet')\r\n",
							"stbooking.createOrReplaceTempView(\"stbooking\")\r\n",
							"\r\n",
							"#Create DataFrame for the publictrack TMS Table\r\n",
							"publictrack = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publictrack.parquet', format='parquet')\r\n",
							"publictrack.createOrReplaceTempView(\"publictrack\")"
						],
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Ops/Linehaul/08_LH_STTrip"
						],
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sttrip.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/sttrip.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 27
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create DataFrame for the publictriptrackinghistory TMS Table\r\n",
							"publictriptrackinghistory = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publictriptrackinghistory.parquet', format='parquet')\r\n",
							"publictriptrackinghistory.createOrReplaceTempView(\"publictriptrackinghistory\")"
						],
						"outputs": [],
						"execution_count": 28
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%run TRANSFORM/02 STRUCTURED/Ops/Linehaul/09_LH_STTripTrackingHistory"
						],
						"outputs": [],
						"execution_count": 29
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sttriptrackinghistory.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/OPS/sttriptrackinghistory.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 30
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"SUCCESS\")"
						],
						"outputs": [],
						"execution_count": 31
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/UnionSparkJob')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "183fca7b-3cbc-4d59-862a-3013dc038f2d"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse_Prod/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace-prod/bigDataPools/TESTSparkPool",
						"name": "TESTSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/TESTSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"cw_FolderName = \"\"\r\n",
							"cw_FileName = \"\"\r\n",
							"cw_IncFolderName = \"\"\r\n",
							"cw_IdField = \"\"\r\n",
							"ContainerName = \"\"\r\n",
							"DataLakeDF = \"\""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"##Load the Left File\r\n",
							"path = 'abfss://' + ContainerName + '@' + DataLakeDF + '/' + cw_FolderName + '/' + cw_FileName\r\n",
							"LeftFile = spark.read.load(path, format='parquet')"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"##Load the Right File\r\n",
							"path = 'abfss://' + ContainerName + '@' + DataLakeDF + '/' + cw_IncFolderName + '/' + cw_FileName\r\n",
							"RightFile = spark.read.load(path, format='parquet')"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"JoinFields = cw_IdField.split()"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"JoinFields"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"df_with_rows_deleted = LeftFile.join(RightFile, on = JoinFields, how = 'left_anti')"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"unioned = df_with_rows_deleted.unionByName(RightFile, allowMissingColumns=True)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"if unioned.count() < (0.5 * LeftFile.count()):\r\n",
							"    raise Exception (\"Error when Unioning table \" + cw_FileName + \" the Left side had \" + str(LeftFile.count()) + \" rows and the Unioned table has \" + str(unioned.count()))\r\n",
							"else: print(\"Union completed Succesfully\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"test = unioned.select(JoinFields)\r\n",
							"UnionDistinctCount = test.distinct().count()\r\n",
							"UnionCount = unioned.count()"
						],
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"if UnionDistinctCount != UnionCount :\r\n",
							"        raise Exception (\"Duplicates when Unioning table \" + cw_FileName + \" the Unioned table has \" + str(UnionCount) + \" rows and the Distinct Unioned table has \" + str(UnionDistinctCount))\r\n",
							"else: print(\"Union completed Succesfully\")"
						],
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"unioned.write.parquet('abfss://' + ContainerName + '@' + DataLakeDF + cw_IncFolderName + '/T1/' + cw_FileName, mode='overwrite')"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/UnionSparkJob_DateLoad')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "TESTSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "486d2ffa-2a90-422b-b67e-be3c2efe736c"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse_Prod/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace-prod/bigDataPools/TESTSparkPool",
						"name": "TESTSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/TESTSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"cw_FolderName = \"\"\r\n",
							"cw_FileName = \"\"\r\n",
							"cw_IncFolderName = \"\"\r\n",
							"cw_IdField = \"\"\r\n",
							"ContainerName = \"\"\r\n",
							"DataLakeDF = \"\""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import broadcast"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"##Load the Left File\r\n",
							"path = 'abfss://' + ContainerName + '@' + DataLakeDF + '/' + cw_FolderName + '/' + cw_FileName\r\n",
							"LeftFile = spark.read.load(path, format='parquet')"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"##Load the Right File\r\n",
							"path = 'abfss://' + ContainerName + '@' + DataLakeDF + '/' + cw_IncFolderName + '/' + cw_FileName\r\n",
							"RightFile = spark.read.load(path, format='parquet')"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"JoinFields = cw_IdField.split()"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"JoinFields"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"df_with_rows_deleted = LeftFile.join(broadcast(RightFile), on = JoinFields, how = 'left_anti')"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"unioned = df_with_rows_deleted.unionByName(RightFile, allowMissingColumns=True)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"if unioned.count() < (0.75 * LeftFile.count()):\r\n",
							"    raise Exception (\"Error when Unioning table \" + cw_FileName + \" the Left side had \" + str(LeftFile.count()) + \" rows and the Unioned table has \" + str(unioned.count()))\r\n",
							"else: print(\"Union completed Succesfully\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"test = unioned.select(JoinFields)\r\n",
							"UnionDistinctCount = test.distinct().count()\r\n",
							"UnionCount = unioned.count()"
						],
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"if UnionDistinctCount != UnionCount :\r\n",
							"        raise Exception (\"Duplicates when Unioning table \" + cw_FileName + \" the Unioned table has \" + str(UnionCount) + \" rows and the Distinct Unioned table has \" + str(UnionDistinctCount))\r\n",
							"else: print(\"Union completed Succesfully\")"
						],
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"unioned.write.parquet('abfss://' + ContainerName + '@' + DataLakeDF + '/' + cw_IncFolderName + '/T1/' + cw_FileName, mode='overwrite')"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"##Load the Final File\r\n",
							"path = 'abfss://' + ContainerName + '@' + DataLakeDF + '/' + cw_IncFolderName + '/T1/' + cw_FileName\r\n",
							"FinalDF = spark.read.load(path, format='parquet')"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Finalpath = 'abfss://' + ContainerName + '@' + DataLakeDF + '/' + cw_FolderName + '/' + cw_FileName"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"FinalDF.write.parquet(Finalpath, mode='overwrite')"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/UnionSparkJob_DateLoad_NoDupeCheck')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "TESTSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "75ca497f-d461-4fa2-8f90-dd00b56a2b93"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse_Prod/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace-prod/bigDataPools/TESTSparkPool",
						"name": "TESTSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/TESTSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"cw_FolderName = \"\"\r\n",
							"cw_FileName = \"\"\r\n",
							"cw_IncFolderName = \"\"\r\n",
							"cw_IdField = \"\"\r\n",
							"ContainerName = \"\"\r\n",
							"DataLakeDF = \"\""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import broadcast"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"##Load the Left File\r\n",
							"path = 'abfss://' + ContainerName + '@' + DataLakeDF + '/' + cw_FolderName + '/' + cw_FileName\r\n",
							"LeftFile = spark.read.load(path, format='parquet')"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"##Load the Right File\r\n",
							"path = 'abfss://' + ContainerName + '@' + DataLakeDF + '/' + cw_IncFolderName + '/' + cw_FileName\r\n",
							"RightFile = spark.read.load(path, format='parquet')"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"JoinFields = cw_IdField.split()"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"JoinFields"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"df_with_rows_deleted = LeftFile.join(broadcast(RightFile), on = JoinFields, how = 'left_anti')"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"unioned = df_with_rows_deleted.unionByName(RightFile, allowMissingColumns=True)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"if unioned.count() < (0.75 * LeftFile.count()):\r\n",
							"    raise Exception (\"Error when Unioning table \" + cw_FileName + \" the Left side had \" + str(LeftFile.count()) + \" rows and the Unioned table has \" + str(unioned.count()))\r\n",
							"else: print(\"Union completed Succesfully\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#test = unioned.select(JoinFields)\r\n",
							"#UnionDistinctCount = test.distinct().count()\r\n",
							"#UnionCount = unioned.count()"
						],
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#if UnionDistinctCount != UnionCount :\r\n",
							"#        raise Exception (\"Duplicates when Unioning table \" + cw_FileName + \" the Unioned table has \" + str(UnionCount) + \" rows and the Distinct Unioned table has \" + str(UnionDistinctCount))\r\n",
							"#else: print(\"Union completed Succesfully\")"
						],
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"unioned.write.parquet('abfss://' + ContainerName + '@' + DataLakeDF + '/'  + cw_IncFolderName + '/T1/' + cw_FileName, mode='overwrite')"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"##Load the Final File\r\n",
							"path = 'abfss://' + ContainerName + '@' + DataLakeDF + '/'  + cw_IncFolderName + '/T1/' + cw_FileName\r\n",
							"FinalDF = spark.read.load(path, format='parquet')"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Finalpath = 'abfss://' + ContainerName + '@' + DataLakeDF + '/' + cw_FolderName + '/' + cw_FileName"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"FinalDF.write.parquet(Finalpath, mode='overwrite')"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/UnionSparkJob_IDLoad')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "TESTSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "b60d1a5d-7651-4dc1-837b-a9dcc8849f79"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse_Prod/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace-prod/bigDataPools/TESTSparkPool",
						"name": "TESTSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/TESTSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"cw_FolderName = \"\"\r\n",
							"cw_FileName = \"\"\r\n",
							"cw_IncFolderName = \"\"\r\n",
							"cw_IdField = \"\"\r\n",
							"ContainerName = \"\"\r\n",
							"DataLakeDF = \"\""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"##Load the Left File\r\n",
							"path = 'abfss://' + ContainerName + '@' + DataLakeDF + '/' + cw_FolderName + '/' + cw_FileName\r\n",
							"LeftFile = spark.read.load(path, format='parquet')"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"##Load the Right File\r\n",
							"path = 'abfss://' + ContainerName + '@' + DataLakeDF + '/' + cw_IncFolderName + '/' + cw_FileName\r\n",
							"RightFile = spark.read.load(path, format='parquet')"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"JoinFields = cw_IdField.split()"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"JoinFields"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"unioned = LeftFile.unionByName(RightFile, allowMissingColumns=True)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"if unioned.count() < (0.75 * LeftFile.count()):\r\n",
							"    raise Exception (\"Error when Unioning table \" + cw_FileName + \" the Left side had \" + str(LeftFile.count()) + \" rows and the Unioned table has \" + str(unioned.count()))\r\n",
							"else: print(\"Union completed Succesfully\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"test = unioned.select(JoinFields)\r\n",
							"UnionDistinctCount = test.distinct().count()\r\n",
							"UnionCount = unioned.count()"
						],
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"if UnionDistinctCount != UnionCount :\r\n",
							"        raise Exception (\"Duplicates when Unioning table \" + cw_FileName + \" the Unioned table has \" + str(UnionCount) + \" rows and the Distinct Unioned table has \" + str(UnionDistinctCount))\r\n",
							"else: print(\"Union completed Succesfully\")"
						],
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"unioned.write.parquet('abfss://' + ContainerName + '@' + DataLakeDF + cw_IncFolderName + '/T1/' + cw_FileName, mode='overwrite')"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"##Load the Final File\r\n",
							"path = 'abfss://' + ContainerName + '@' + DataLakeDF + cw_IncFolderName + '/T1/' + cw_FileName\r\n",
							"FinalDF = spark.read.load(path, format='parquet')"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Finalpath = 'abfss://' + ContainerName + '@' + DataLakeDF + '/' + cw_FolderName + '/' + cw_FileName"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"FinalDF.write.parquet(Finalpath, mode='overwrite')"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/UnionSparkJob_NoDupeCheck')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "87ac01e0-f020-408d-9801-f27cf6d0d2f0"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse_Prod/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace-prod/bigDataPools/TESTSparkPool",
						"name": "TESTSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/TESTSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"cw_FolderName = \"\"\r\n",
							"cw_FileName = \"\"\r\n",
							"cw_IncFolderName = \"\"\r\n",
							"cw_IdField = \"\"\r\n",
							"ContainerName = \"\"\r\n",
							"DataLakeDF = \"\""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"##Load the Left File\r\n",
							"path = 'abfss://' + ContainerName + '@' + DataLakeDF + '/' + cw_FolderName + '/' + cw_FileName\r\n",
							"LeftFile = spark.read.load(path, format='parquet')"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"##Load the Right File\r\n",
							"path = 'abfss://' + ContainerName + '@' + DataLakeDF + '/' + cw_IncFolderName + '/' + cw_FileName\r\n",
							"RightFile = spark.read.load(path, format='parquet')"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"JoinFields = cw_IdField.split()"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"JoinFields"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"df_with_rows_deleted = LeftFile.join(RightFile, on = JoinFields, how = 'left_anti')"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"unioned = df_with_rows_deleted.unionByName(RightFile, allowMissingColumns=True)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"if unioned.count() < (0.5 * LeftFile.count()):\r\n",
							"    raise Exception (\"Error when Unioning table \" + cw_FileName + \" the Left side had \" + str(LeftFile.count()) + \" rows and the Unioned table has \" + str(unioned.count()))\r\n",
							"else: print(\"Union completed Succesfully\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#test = unioned.select(JoinFields)\r\n",
							"#UnionDistinctCount = test.distinct().count()\r\n",
							"#UnionCount = unioned.count()"
						],
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#if UnionDistinctCount != UnionCount :\r\n",
							"#        raise Exception (\"Duplicates when Unioning table \" + cw_FileName + \" the Unioned table has \" + str(UnionCount) + \" rows and the Distinct Unioned table has \" + str(UnionDistinctCount))\r\n",
							"#else: print(\"Union completed Succesfully\")"
						],
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"unioned.write.parquet('abfss://' + ContainerName + '@' + DataLakeDF + cw_IncFolderName + '/T1/' + cw_FileName, mode='overwrite')"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/_01RptTripLevel')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Reports/TMS"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "867f5a74-a824-46f6-9514-af54de99008e"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace/bigDataPools/DevSparkPool",
						"name": "DevSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/DevSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 34
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# #Create DataFrame for the sttrip LMS Table\r\n",
							"# sttrip = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/sttrip.parquet', format='parquet')\r\n",
							"# sttrip.createOrReplaceTempView(\"sttrip\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stlmsbookingweights LMS Table\r\n",
							"# stlmsbookingweights = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stlmsbookingweights.parquet', format='parquet')\r\n",
							"# stlmsbookingweights.createOrReplaceTempView(\"stlmsbookingweights\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stloadchild LMS Table\r\n",
							"# stloadchild = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stloadchild.parquet', format='parquet')\r\n",
							"# stloadchild.createOrReplaceTempView(\"stloadchild\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stloadparent LMS Table\r\n",
							"# stloadparent = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stloadparent.parquet', format='parquet')\r\n",
							"# stloadparent.createOrReplaceTempView(\"stloadparent\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stthirdparty LMS Table\r\n",
							"# stthirdparty = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stthirdparty.parquet', format='parquet')\r\n",
							"# stthirdparty.createOrReplaceTempView(\"stthirdparty\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 35
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"rpttriplevel\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"\r\n",
							" tms_trip_id tripid\r\n",
							",tms_trip_thirdpartyid thirdpartyid\r\n",
							",coalesce(tms_thirdparty_name,'CITY LINEHAUL') thirdpartyname\r\n",
							",concat_ws(',',tms_lmsbookingweights_bookingid) bookingids\r\n",
							",concat_ws(',',tms_lmsbookingweights_loadid) loadids\r\n",
							"\r\n",
							",sum(tms_lmsbookingweights_noofparcels) noofparcels\r\n",
							",sum(tms_lmsbookingweights_weight) weight\r\n",
							",sum(tms_lmsbookingweights_chargeweight) chargeweight\r\n",
							",sum(tms_lmsbookingweights_volweight) volweight\r\n",
							",sum(tms_lmsbookingweights_volumiserweight) volumiserweight\r\n",
							",max(orv_dispatch_startdate) dispatchstartdate\r\n",
							",max(lms_loadchild_ldate) loadclosedate\r\n",
							",max(coalesce(orv_dispatch_tsstartdepoexitapp,orv_dispatch_tsstartdepoexit)) exitdepotgeofence\r\n",
							",sum(lms_loadchild_effectiveloaddurationminutes) effectiveloaddurationminutes\r\n",
							",sum(lms_loadchild_effectiveoffloaddurationminutes) effectiveoffloaddurationminutes\r\n",
							"FROM sttrip t\r\n",
							"LEFT JOIN stthirdparty on tms_trip_thirdpartyid = tms_thirdparty_id\r\n",
							"LEFT JOIN stlmsbookingweights l on l.tms_lmsbookingweights_tripid = t.tms_trip_id\r\n",
							"LEFT JOIN stloadchild lc on lc.lms_loadchild_id = l.tms_lmsbookingweights_loadid\r\n",
							"\r\n",
							"WHERE tms_trip_startdate >= to_timestamp(year(current_date())-1||'-'||(month(current_date()))||'-'||'01')\r\n",
							"\r\n",
							"\r\n",
							"GROUP BY\r\n",
							"     tms_trip_id\r\n",
							"    ,tms_trip_thirdpartyid\r\n",
							"    ,tms_thirdparty_name\r\n",
							"    ,concat_ws(',',tms_lmsbookingweights_bookingid)\r\n",
							"    ,concat_ws(',',tms_lmsbookingweights_loadid)\r\n",
							"    ,orv_dispatch_startdate\r\n",
							"    ,coalesce(orv_dispatch_tsstartdepoexitapp,orv_dispatch_tsstartdepoexit)\r\n",
							"\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 36
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"rpttriplevel = spark.sql(\"SELECT * FROM rpttriplevel\")"
						],
						"outputs": [],
						"execution_count": 37
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# rpttriplevel.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/RPT/rpttriplevel.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 38
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/_01_RptParcelLevel')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Reports/LMS"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "e3ac5ec8-a485-43da-acaf-a3082707af0a"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace/bigDataPools/ApacheSparkPool",
						"name": "ApacheSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/ApacheSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# #Create DataFrame for the stparcel LMS Table\r\n",
							"# stparcel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stparcel.parquet', format='parquet')\r\n",
							"# stparcel.createOrReplaceTempView(\"stparcel\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stdeliverypickupcustomer LMS Table\r\n",
							"# stdeliverypickupcustomer = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stdeliverypickupcustomer.parquet', format='parquet')\r\n",
							"# stdeliverypickupcustomer.createOrReplaceTempView(\"stdeliverypickupcustomer\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stbillcustomer LMS Table\r\n",
							"# stbillcustomer = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stbillcustomer.parquet', format='parquet')\r\n",
							"# stbillcustomer.createOrReplaceTempView(\"stbillcustomer\")\r\n",
							"\r\n",
							"# #create dataframe for the stsroute lms table\r\n",
							"# stsroute = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stsroute.parquet', format='parquet')\r\n",
							"# stsroute.createOrReplaceTempView(\"stsroute\")\r\n",
							"\r\n",
							"# #create dataframe for the storder lms table\r\n",
							"# storder = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/storder.parquet', format='parquet')\r\n",
							"# storder.createOrReplaceTempView(\"storder\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"rptparcellevel\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"\r\n",
							"     p.*\r\n",
							"    ,bc.lms_customer_name as lms_parcel_orderbillcustname\r\n",
							"    \r\n",
							"    ,dc.lms_customer_name as lms_parcel_orderdelivercustname\r\n",
							"    ,dc.lms_customer_cref as lms_parcel_orderdelivercustcref\r\n",
							"    ,dc.lms_customer_srouteid as lms_parcel_orderdelivercustsrouteid\r\n",
							"    ,ds.lms_sroute_code as lms_parcel_sroutecodedelivercust\r\n",
							"    ,ds.lms_sroute_description as lms_parcel_sroutedescriptiondelivercust\r\n",
							"\r\n",
							"    ,pc.lms_customer_name as lms_parcel_orderpickupcustname\r\n",
							"    ,pc.lms_customer_cref as lms_parcel_orderpickupcustcref\r\n",
							"    ,pc.lms_customer_srouteid as lms_parcel_orderpickupcustsrouteid\r\n",
							"    ,ps.lms_sroute_code as lms_parcel_sroutecodepickupcust\r\n",
							"    ,ps.lms_sroute_description as lms_parcel_sroutedescriptionpickupcust\r\n",
							"   \r\n",
							"  \r\n",
							"FROM stparcel p\r\n",
							"LEFT JOIN stbillcustomer bc on bc.lms_customer_id =  p.lms_parcel_orderbillcustid\r\n",
							"LEFT JOIN stdeliverypickupcustomer dc on dc.lms_customer_id =  p.lms_parcel_orderdelivercustid\r\n",
							"LEFT JOIN stdeliverypickupcustomer pc on pc.lms_customer_id =  p.lms_parcel_orderpickupcustid\r\n",
							"LEFT JOIN stsroute ds on ds.lms_sroute_id = dc.lms_customer_srouteid\r\n",
							"LEFT JOIN stsroute ps on ps.lms_sroute_id = pc.lms_customer_srouteid\r\n",
							"LEFT JOIN storder o on o.lms_order_id = p.lms_parcel_orderid"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"rptparcellevel = spark.sql(\"SELECT * FROM rptparcellevel\")"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"rptparcellevellimitload\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"\r\n",
							"    * \r\n",
							"FROM rptparcellevel\r\n",
							"\r\n",
							"WHERE lms_parcel_pdate >= to_timestamp(year(current_date())-1||'-'||(month(current_date()))||'-'||'01')"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"rptparcellevellimitload = spark.sql(\"SELECT * FROM rptparcellevellimitload\")"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# rptparcellevel.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/RPT/rptparcellevel.parquet', mode = \"overwrite\")\r\n",
							"# rptparcellevellimitload.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/RPT/rptparcellevellimitload.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 7
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/_01_RptTripLevel')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Reports/TMS"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "571f485f-b44f-4c8f-8ab5-cb0443b2afcb"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace/bigDataPools/DevSparkPool",
						"name": "DevSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/DevSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 53
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# #Create DataFrame for the sttrip LMS Table\r\n",
							"# sttrip = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/sttrip.parquet', format='parquet')\r\n",
							"# sttrip.createOrReplaceTempView(\"sttrip\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stbooking LMS Table\r\n",
							"# stbooking = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stbooking.parquet', format='parquet')\r\n",
							"# stbooking.createOrReplaceTempView(\"stbooking\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stlmsbookingweights LMS Table\r\n",
							"# stlmsbookingweights = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stlmsbookingweights.parquet', format='parquet')\r\n",
							"# stlmsbookingweights.createOrReplaceTempView(\"stlmsbookingweights\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stloadchild LMS Table\r\n",
							"# stloadchild = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stloadchild.parquet', format='parquet')\r\n",
							"# stloadchild.createOrReplaceTempView(\"stloadchild\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stloadparent LMS Table\r\n",
							"# stloadparent = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stloadparent.parquet', format='parquet')\r\n",
							"# stloadparent.createOrReplaceTempView(\"stloadparent\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stthirdparty LMS Table\r\n",
							"# stthirdparty = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stthirdparty.parquet', format='parquet')\r\n",
							"# stthirdparty.createOrReplaceTempView(\"stthirdparty\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stlmstrack LMS Table\r\n",
							"# stlmstrack = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stlmstrack.parquet', format='parquet')\r\n",
							"# stlmstrack.createOrReplaceTempView(\"stlmstrack\")\r\n",
							"\r\n",
							"# #Create DataFrame for the publiclmsdata LMS Table\r\n",
							"# publiclmsdata = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/TMS/publiclmsdata.parquet', format='parquet')\r\n",
							"# publiclmsdata.createOrReplaceTempView(\"publiclmsdata\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 54
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"lmsdata_tmp\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"     l.bookingid\r\n",
							"    ,l.loadid\r\n",
							"    ,b.tms_booking_tripid as tripid\r\n",
							"FROM publiclmsdata l\r\n",
							"LEFT JOIN stbooking b on b. tms_booking_id = l.bookingid"
						],
						"outputs": [],
						"execution_count": 55
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"lmsdata_tmp = spark.sql(\"SELECT * FROM lmsdata_tmp\")"
						],
						"outputs": [],
						"execution_count": 56
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stlmstrack_tmp\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"     lms_track_loadid as loadid\r\n",
							"    ,max(lms_track_opendt) as maxopendt\r\n",
							"    ,max(lms_track_closedt) as maxclosedt\r\n",
							"FROM stlmstrack\r\n",
							"GROUP BY lms_track_loadid \r\n",
							""
						],
						"outputs": [],
						"execution_count": 57
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stlmstrack_tmp = spark.sql(\"SELECT * FROM stlmstrack_tmp\")"
						],
						"outputs": [],
						"execution_count": 58
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"lmsdata_tmp2\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"     l.tripid\r\n",
							"    ,max(maxopendt) as maxopendt\r\n",
							"    ,max(maxclosedt) as maxclosedt\r\n",
							"FROM lmsdata_tmp l\r\n",
							"LEFT JOIN stlmstrack_tmp t on t.loadid = l.loadid\r\n",
							"GROUP BY l.tripid"
						],
						"outputs": [],
						"execution_count": 59
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"lmsdata_tmp2 = spark.sql(\"SELECT * FROM lmsdata_tmp2\")"
						],
						"outputs": [],
						"execution_count": 60
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import pyspark.sql.functions"
						],
						"outputs": [],
						"execution_count": 61
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"rpttriplevel\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"\r\n",
							" tms_trip_id tripid\r\n",
							",tms_trip_startdate startdate\r\n",
							",tms_trip_thirdpartyid thirdpartyid\r\n",
							",coalesce(tms_thirdparty_name,'CITY LINEHAUL') thirdpartyname\r\n",
							",collect_set(tms_lmsbookingweights_bookingid) bookingids\r\n",
							",collect_set(tms_lmsbookingweights_loadid) loadids\r\n",
							",maxopendt lastonscan \r\n",
							",maxclosedt lastoffscan\r\n",
							",sum(tms_lmsbookingweights_noofparcels) noofparcels\r\n",
							",sum(tms_lmsbookingweights_weight) weight\r\n",
							",sum(tms_lmsbookingweights_chargeweight) chargeweight\r\n",
							",sum(tms_lmsbookingweights_volweight) volweight\r\n",
							",sum(tms_lmsbookingweights_volumiserweight) volumiserweight\r\n",
							",max(orv_dispatch_startdate) dispatchstartdate\r\n",
							",max(lms_loadchild_ldate) loadclosedate\r\n",
							",max(coalesce(orv_dispatch_tsstartdepoexitapp,orv_dispatch_tsstartdepoexit)) exitdepotgeofence\r\n",
							",sum(lms_loadchild_effectiveloaddurationminutes) effectiveloaddurationminutes\r\n",
							",sum(lms_loadchild_effectiveoffloaddurationminutes) effectiveoffloaddurationminutes\r\n",
							"FROM sttrip t\r\n",
							"LEFT JOIN stthirdparty on tms_trip_thirdpartyid = tms_thirdparty_id\r\n",
							"LEFT JOIN stlmsbookingweights l on l.tms_lmsbookingweights_tripid = t.tms_trip_id\r\n",
							"LEFT JOIN stloadchild lc on lc.lms_loadchild_id = l.tms_lmsbookingweights_loadid\r\n",
							"LEFT JOIN lmsdata_tmp2 l2 on l2.tripid = t.tms_trip_id\r\n",
							"\r\n",
							"WHERE tms_trip_startdate >= to_timestamp(year(current_date())-1||'-'||(month(current_date()))||'-'||'01')\r\n",
							"\r\n",
							"GROUP BY\r\n",
							"     tms_trip_id\r\n",
							"    ,tms_trip_startdate \r\n",
							"    ,tms_trip_thirdpartyid\r\n",
							"    ,tms_thirdparty_name\r\n",
							"    ,orv_dispatch_startdate\r\n",
							"    ,coalesce(orv_dispatch_tsstartdepoexitapp,orv_dispatch_tsstartdepoexit)\r\n",
							"    ,maxopendt\r\n",
							"    ,maxclosedt\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 62
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"rpttriplevel = spark.sql(\"SELECT * FROM rpttriplevel\")"
						],
						"outputs": [],
						"execution_count": 63
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# rpttriplevel.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/RPT/rpttriplevel.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 64
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/_02_RptTrackLevel')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Reports/LMS"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "25c6c47e-f72e-4706-b21f-a0f0ca43f0ee"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace/bigDataPools/ApacheSparkPool",
						"name": "ApacheSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/ApacheSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# #Create DataFrame for the stlmstrack LMS Table\r\n",
							"# stlmstrack = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stlmstrack.parquet', format='parquet')\r\n",
							"# stlmstrack.createOrReplaceTempView(\"stlmstrack\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stparcel LMS Table\r\n",
							"# stparcel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stparcel.parquet', format='parquet')\r\n",
							"# stparcel.createOrReplaceTempView(\"stparcel\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stbillcustomer LMS Table\r\n",
							"# stbillcustomer = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stbillcustomer.parquet', format='parquet')\r\n",
							"# stbillcustomer.createOrReplaceTempView(\"stbillcustomer\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stdeliverypickupcustomer LMS Table\r\n",
							"# stdeliverypickupcustomer = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stdeliverypickupcustomer.parquet', format='parquet')\r\n",
							"# stdeliverypickupcustomer.createOrReplaceTempView(\"stdeliverypickupcustomer\")\r\n",
							"\r\n",
							"# #create dataframe for the stsroute lms table\r\n",
							"# stsroute = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stsroute.parquet', format='parquet')\r\n",
							"# stsroute.createOrReplaceTempView(\"stsroute\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stzone LMS Table\r\n",
							"# stzone = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stzone.parquet', format='parquet')\r\n",
							"# stzone.createOrReplaceTempView(\"stzone\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stlocation LMS Table\r\n",
							"# stlocation = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stlocation.parquet', format='parquet')\r\n",
							"# stlocation.createOrReplaceTempView(\"stlocation\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"rpttracklevel\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"\r\n",
							"    t.*\r\n",
							"    ,lt.lms_location_id as locationid\r\n",
							"    ,lt.lms_location_description as location\r\n",
							"    ,bc.lms_customer_id as  billingcustomerid\r\n",
							"    ,bc.lms_customer_name as  billingcustomer\r\n",
							"    ,dc.lms_customer_id as  deliverycustomerid\r\n",
							"    ,dc.lms_customer_name as  deliverycustomer\r\n",
							"    ,dc.lms_customer_cref as deliverycustomercref\r\n",
							"    ,ds.lms_sroute_code as deliverysroutecode\r\n",
							"    ,dz.lms_zone_description as deliverycustcustomerzonedesc\r\n",
							"    ,ld.lms_location_id as deliverycustomerzonelocationid\r\n",
							"    ,ld.lms_location_description as deliverycustomerzonelocationdescription\r\n",
							"\r\n",
							"FROM stlmstrack t\r\n",
							"LEFT JOIN stparcel p on p.lms_parcel_id = t.lms_track_parcelid\r\n",
							"LEFT JOIN stlocation lt on lt.lms_location_id = t.lms_track_tolocid\r\n",
							"LEFT JOIN stbillcustomer bc on bc.lms_customer_id =  t.lms_track_orderbillcustid\r\n",
							"LEFT JOIN stdeliverypickupcustomer dc on dc.lms_customer_id =  t.lms_track_orderdelivercustid\r\n",
							"LEFT JOIN stsroute ds on ds.lms_sroute_id = dc.lms_customer_srouteid\r\n",
							"LEFT JOIN stzone dz on dz.lms_zone_id = ds.lms_sroute_zoneid\r\n",
							"LEFT JOIN stlocation ld on ld.lms_location_id = dz.lms_zone_locid\r\n",
							"\r\n",
							"WHERE \r\n",
							"p.lms_parcel_pdate >= (to_timestamp(year(current_date())-1||'-'||(month(current_date()))||'-'||'01'))\r\n",
							""
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"rpttracklevel = spark.sql(\"SELECT * FROM rpttracklevel\")"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# rpttracklevel.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/RPT/rpttracklevel.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 8
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/_03_RptLoadLevel')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Reports/LMS"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "e1c76344-9c15-46c6-9a3e-daa0e93f8513"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace/bigDataPools/DevSparkPool",
						"name": "DevSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/DevSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 49
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# #Create DataFrame for the stloadchild LMS Table\r\n",
							"# stloadchild = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stloadchild.parquet', format='parquet')\r\n",
							"# stloadchild.createOrReplaceTempView(\"stloadchild\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stloadparent LMS Table\r\n",
							"# stloadparent = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stloadparent.parquet', format='parquet')\r\n",
							"# stloadparent.createOrReplaceTempView(\"stloadparent\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbolocation LMS Table\r\n",
							"# dbolocation = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbolocation.parquet', format='parquet')\r\n",
							"# dbolocation.createOrReplaceTempView(\"dbolocation\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stmdvehicle LMS Table\r\n",
							"# stmdvehicle = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/MD/stmdvehicle.parquet', format='parquet')\r\n",
							"# stmdvehicle.createOrReplaceTempView(\"stmdvehicle\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stwaybill LMS Table\r\n",
							"# stwaybill = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stwaybill.parquet', format='parquet')\r\n",
							"# stwaybill.createOrReplaceTempView(\"stwaybill\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stbillcustomer LMS Table\r\n",
							"# stbillcustomer = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stbillcustomer.parquet', format='parquet')\r\n",
							"# stbillcustomer.createOrReplaceTempView(\"stbillcustomer\")"
						],
						"outputs": [],
						"execution_count": 50
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stwaybill_tmp\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    lms_waybill_loadid\r\n",
							"    ,count(distinct lms_waybill_delivercustid) as lms_loadparent_nodeliverycustomers\r\n",
							"    ,sum(CASE \r\n",
							"        WHEN lms_customer_billcustgrouptypedescription = 'Express' Then 1\r\n",
							"        ELSE 0\r\n",
							"    END) as lms_loadparent_expressdelivercustomers\r\n",
							"    ,sum(CASE \r\n",
							"        WHEN lms_customer_billcustgrouptypedescription <> 'Express' Then 1\r\n",
							"        ELSE 0\r\n",
							"    END) as lms_loadparent_retailelivercustomers\r\n",
							"\r\n",
							"\r\n",
							"FROM stwaybill w\r\n",
							"LEFT JOIN stbillcustomer bc on bc.lms_customer_id =w.lms_waybill_billcust\r\n",
							"GROUP BY lms_waybill_loadid\r\n",
							""
						],
						"outputs": [],
						"execution_count": 51
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"rptloadparentlevel\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    lp.lms_loadparent_parentloadid\r\n",
							"    ,lp.lms_loadparent_ldate\r\n",
							"    ,lp.orv_dispatch_estdepdate as lms_loadparent_estdepdate\r\n",
							"    ,lp.orv_dispatch_originalstartdate as lms_loadparent_originalstartdate\r\n",
							"    ,concat_ws(\",\",collect_list(lc.lms_loadchild_id)) as lms_loadparent_childloadids\r\n",
							"    -- ,explode(collect_set(lc.lms_loadchild_id)) as lms_loadparent_loadchildids\r\n",
							"    ,fl.id as lms_loadparent_locationfromid\r\n",
							"    ,fl.description as lms_loadparent_locationfromdescription\r\n",
							"    ,tl.id as lms_loadparent_locationtoid\r\n",
							"    ,tl.description as lms_loadparent_locationtodescription\r\n",
							"    ,v.md_vehicle_id as lms_loadparent_vehcileid\r\n",
							"    ,v.md_vehicle_uid as lms_loadparent_vehcileuid\r\n",
							"    ,v.md_vehicle_fleetcode as lms_loadparent_vehcilefleetcode\r\n",
							"    ,v.md_vehicle_registrationnumber as lms_loadparent_vehicleregistrationnumber\r\n",
							"    ,v.md_vehicle_internalvolume as lms_loadparent_vehicleinternalvolume\r\n",
							"    ,lp.lms_loadparent_vehodo as lms_loadparent_lmsvehodo\r\n",
							"    ,lp.lms_loadparent_vehodo2 as lms_loadparent_lmsvehodo2\r\n",
							"    ,(lp.lms_loadparent_vehodo2 - lp.lms_loadparent_vehodo) as lms_loadparent_lmsodokms\r\n",
							"    ,orv_dispatch_startodo as lms_loadparent_orvstartodo\r\n",
							"    ,orv_dispatch_stopodo as lms_loadparent_orvstopodo\r\n",
							"    ,(orv_dispatch_stopodo - orv_dispatch_startodo) as lms_loadparent_orvodokms\r\n",
							"    ,orv_dispatch_distance as lms_loadparent_orvdistance\r\n",
							"    ,orv_dispatch_duration  as lms_loadparent_orvduration\r\n",
							"    ,w.lms_loadparent_nodeliverycustomers \r\n",
							"    ,w.lms_loadparent_expressdelivercustomers\r\n",
							"    ,w.lms_loadparent_retailelivercustomers\r\n",
							"    ,sum(lms_loadparent_aggrnoparcels) as lms_loadparent_noparcels\r\n",
							"    ,sum(lms_loadparent_aggrweight )as lms_loadparent_weight\r\n",
							"    ,sum(lms_loadparent_aggrchargeweight) as lms_loadparent_chargeweight\r\n",
							"    ,count(lc.lms_loadchild_id) as lms_loadparent_nochildloads\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"FROM stloadparent lp\r\n",
							"LEFT JOIN stloadchild lc on lc.lms_loadchild_parentloadid = lp.lms_loadparent_parentloadid\r\n",
							"LEFT JOIN dbolocation fl on fl.id = lms_loadchild_fromlocid and lc.lms_loadchild_parentloadid = lp.lms_loadparent_parentloadid\r\n",
							"LEFT JOIN dbolocation tl on tl.id = lms_loadchild_tolocid and lc.lms_loadchild_parentloadid = lp.lms_loadparent_parentloadid\r\n",
							"LEFT JOIN stmdvehicle v on v.md_vehicle_orvid = orv_dispatch_vid\r\n",
							"LEFT JOIN stwaybill_tmp w on w.lms_waybill_loadid = lc.lms_loadchild_id\r\n",
							"\r\n",
							"\r\n",
							"WHERE lms_loadparent_ldate >= (to_timestamp(year(current_date())-1||'-'||(month(current_date()))||'-'||'01'))\r\n",
							"\r\n",
							"GROUP BY \r\n",
							"    lp.lms_loadparent_parentloadid\r\n",
							"    ,lp.lms_loadparent_ldate\r\n",
							"    ,lp.orv_dispatch_estdepdate\r\n",
							"    ,lp.orv_dispatch_originalstartdate \r\n",
							"    ,fl.id\r\n",
							"    ,fl.description\r\n",
							"    ,tl.id\r\n",
							"    ,tl.description\r\n",
							"    ,v.md_vehicle_id\r\n",
							"    ,v.md_vehicle_uid\r\n",
							"    ,v.md_vehicle_fleetcode\r\n",
							"    ,v.md_vehicle_registrationnumber\r\n",
							"    ,v.md_vehicle_internalvolume\r\n",
							"    ,lp.lms_loadparent_vehodo\r\n",
							"    ,lp.lms_loadparent_vehodo2\r\n",
							"    ,orv_dispatch_startodo\r\n",
							"    ,orv_dispatch_stopodo\r\n",
							"    ,orv_dispatch_distance\r\n",
							"    ,orv_dispatch_duration\r\n",
							"    ,w.lms_loadparent_nodeliverycustomers \r\n",
							"    ,w.lms_loadparent_expressdelivercustomers\r\n",
							"    ,w.lms_loadparent_retailelivercustomers\r\n",
							"\r\n",
							"    \r\n",
							"order by lms_loadparent_parentloadid desc\r\n",
							""
						],
						"outputs": [],
						"execution_count": 52
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"rptloadparentlevel = spark.sql(\"SELECT * FROM rptloadparentlevel\")"
						],
						"outputs": [],
						"execution_count": 53
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# rptloadparentlevel.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/RPT/rptloadparentlevel.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 54
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/_04_RptConsignmentLevel')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Reports/LMS"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "49b55784-01c6-4f77-aa1c-800b9cc23dd7"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace/bigDataPools/DevSparkPool",
						"name": "DevSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/DevSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 38
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# #Create DataFrame for the stparcel LMS Table\r\n",
							"# stparcel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stparcel.parquet', format='parquet')\r\n",
							"# stparcel.createOrReplaceTempView(\"stparcel\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stconsignment LMS Table\r\n",
							"# stconsignment = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stconsignment.parquet', format='parquet')\r\n",
							"# stconsignment.createOrReplaceTempView(\"stconsignment\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stbillcustomer LMS Table\r\n",
							"# stbillcustomer = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stbillcustomer.parquet', format='parquet')\r\n",
							"# stbillcustomer.createOrReplaceTempView(\"stbillcustomer\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stdeliverypickupcustomer LMS Table\r\n",
							"# stdeliverypickupcustomer = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stdeliverypickupcustomer.parquet', format='parquet')\r\n",
							"# stdeliverypickupcustomer.createOrReplaceTempView(\"stdeliverypickupcustomer\")\r\n",
							"\r\n",
							"# #create dataframe for the stsroute lms table\r\n",
							"# stsroute = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stsroute.parquet', format='parquet')\r\n",
							"# stsroute.createOrReplaceTempView(\"stsroute\")\r\n",
							"\r\n",
							"# #create dataframe for the stzone lms table\r\n",
							"# stzone = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stzone.parquet', format='parquet')\r\n",
							"# stzone.createOrReplaceTempView(\"stzone\")\r\n",
							"\r\n",
							"# #create dataframe for the stlocation lms table\r\n",
							"# stlocation = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stlocation.parquet', format='parquet')\r\n",
							"# stlocation.createOrReplaceTempView(\"stlocation\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbobillconsignmentr LMS Table\r\n",
							"# dbobillconsignmentr = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbobill_consignmentr.parquet', format='parquet')\r\n",
							"# dbobillconsignmentr.createOrReplaceTempView(\"dbobillconsignmentr\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbobillexceptionr LMS Table\r\n",
							"# dbobillexceptionr = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbobill_exceptionr.parquet', format='parquet')\r\n",
							"# dbobillexceptionr.createOrReplaceTempView(\"dbobillexceptionr\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbobillzoneroute LMS Table\r\n",
							"# dbobillzoneroute = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbobill_zoneroute.parquet', format='parquet')\r\n",
							"# dbobillzoneroute.createOrReplaceTempView(\"dbobillzoneroute\")"
						],
						"outputs": [],
						"execution_count": 39
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"rptconsignmentlevel\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"       c.*\r\n",
							"      ,bc.lms_customer_name as billingcustomername\r\n",
							"      ,dc.lms_customer_name as deliverycustomername\r\n",
							"      ,ds.lms_sroute_description as deliverycustomersroutedescription\r\n",
							"      ,dz.lms_zone_description as deliverycustomerzonedescription\r\n",
							"      ,l.lms_location_description as deliverycustomerzonelocationdescription\r\n",
							"      \r\n",
							"FROM stconsignment c\r\n",
							"LEFT JOIN stbillcustomer bc on bc.lms_customer_id = c.lms_consignment_billcust\r\n",
							"LEFT JOIN stdeliverypickupcustomer dc on dc.lms_customer_id = c.lms_consignment_delivercustid\r\n",
							"LEFT JOIN stsroute ds on ds.lms_sroute_id = dc.lms_customer_srouteid\r\n",
							"LEFT JOIN stzone dz on dz.lms_zone_id =  ds.lms_sroute_zoneid\r\n",
							"LEFT JOIN stlocation l on l.lms_location_id =  dz.lms_zone_locid \r\n",
							"\r\n",
							"\r\n",
							"WHERE lms_consignment_cdate >= to_timestamp(year(current_date())-1||'-'||(month(current_date()))||'-'||'01')"
						],
						"outputs": [],
						"execution_count": 40
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"rptconsignmentlevel = spark.sql(\"SELECT * FROM rptconsignmentlevel\")"
						],
						"outputs": [],
						"execution_count": 41
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# rptconsignmentlevel.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/RPT/rptconsignmentlevel.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 42
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stparceldistinctconsignment_tmp\r\n",
							"AS\r\n",
							"SELECT DISTINCT\r\n",
							"     lms_parcel_consignid\r\n",
							"    ,lms_parcel_orderdelivercustid\r\n",
							"    ,lms_parcel_orderpickupcustid\r\n",
							"FROM stparcel"
						],
						"outputs": [],
						"execution_count": 43
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stparceldistinctconsignment_tmp = spark.sql(\"SELECT * FROM stparceldistinctconsignment_tmp\")"
						],
						"outputs": [],
						"execution_count": 44
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"rptbillconsignmentlevel\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"     bcr.*\r\n",
							"    ,p.lms_parcel_orderpickupcustid as pickupcustid\r\n",
							"    ,p.lms_parcel_orderdelivercustid as deliverycustid\r\n",
							"    ,bc.lms_customer_name as billingcustomername\r\n",
							"    ,pc.lms_customer_name as pickupcustomername\r\n",
							"    ,dc.lms_customer_name as deliverycustomername\r\n",
							"    ,pzr.description as pickupbillzonedescription\r\n",
							"    ,dzr.description as deliverybillzonedescription\r\n",
							"\r\n",
							"    ,ps.lms_sroute_id as pickupsrouteid\r\n",
							"    ,ps.lms_sroute_description as pickupsroutedescription\r\n",
							"    ,ds.lms_sroute_id as deliverysrouteid\r\n",
							"    ,ds.lms_sroute_description as deliverysroutedescription\r\n",
							"\r\n",
							"    ,pz.lms_zone_id as pickupzoneid\r\n",
							"    ,pz.lms_zone_description as pickupzonedescription\r\n",
							"    ,dz.lms_zone_id as deliveryzoneid\r\n",
							"    ,dz.lms_zone_description as deliveryzonedescription\r\n",
							"\r\n",
							"    ,pl.lms_location_id as pickuplocationid\r\n",
							"    ,pl.lms_location_description as pickuplocationdescription\r\n",
							"    ,dl.lms_location_id as deliverylocationid\r\n",
							"    ,dl.lms_location_description as deliverylocationdescription\r\n",
							"\r\n",
							"\r\n",
							"FROM dbobillconsignmentr bcr \r\n",
							"LEFT JOIN stparceldistinctconsignment_tmp p on p.lms_parcel_consignid = bcr.consignid\r\n",
							"LEFT JOIN stbillcustomer bc on bc.lms_customer_id = bcr.BillCust\r\n",
							"LEFT JOIN stdeliverypickupcustomer pc on pc.lms_customer_id = p.lms_parcel_orderpickupcustid\r\n",
							"LEFT JOIN stdeliverypickupcustomer dc on dc.lms_customer_id = p.lms_parcel_orderdelivercustid\r\n",
							"LEFT JOIN dbobillzoneroute pzr  on pzr.ID = bcr.PickUpRouteID\r\n",
							"LEFT JOIN dbobillzoneroute dzr  on dzr.ID = bcr.DeliverRouteID\r\n",
							"LEFT JOIN stsroute ps on ps.lms_sroute_id = pc.lms_customer_srouteid\r\n",
							"LEFT JOIN stzone pz on pz.lms_zone_id =  ps.lms_sroute_zoneid\r\n",
							"LEFT JOIN stlocation pl on pl.lms_location_id =  pz.lms_zone_locid \r\n",
							"LEFT JOIN stsroute ds on ds.lms_sroute_id = dc.lms_customer_srouteid\r\n",
							"LEFT JOIN stzone dz on dz.lms_zone_id =  ds.lms_sroute_zoneid\r\n",
							"LEFT JOIN stlocation dl on dl.lms_location_id =  dz.lms_zone_locid \r\n",
							"\r\n",
							"WHERE bcr.cdate1 >= (to_timestamp(year(current_date())-1||'-'||(month(current_date()))||'-'||'01'))\r\n",
							"AND bcr.chargeweight > 0\r\n",
							"AND coalesce(bcr.RouteMissingInd,0) = 0\r\n",
							""
						],
						"outputs": [],
						"execution_count": 45
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"rptbillconsignmentlevel = spark.sql(\"SELECT * FROM rptbillconsignmentlevel\")"
						],
						"outputs": [],
						"execution_count": 46
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# rptbillconsignmentlevel.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/RPT/rptbillconsignmentlevel.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 47
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"rptbillconsignmentexceptionlevel\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"     bcr.*\r\n",
							"    ,p.lms_parcel_orderpickupcustid as pickupcustid\r\n",
							"    ,p.lms_parcel_orderdelivercustid as deliverycustid\r\n",
							"    ,bc.lms_customer_name as billingcustomername\r\n",
							"    ,pc.lms_customer_name as pickupcustomername\r\n",
							"    ,dc.lms_customer_name as deliverycustomername\r\n",
							"    ,pzr.description as pickupbillzonedescription\r\n",
							"    ,dzr.description as deliverybillzonedescription\r\n",
							"\r\n",
							"    ,ps.lms_sroute_id as pickupsrouteid\r\n",
							"    ,ps.lms_sroute_description as pickupsroutedescription\r\n",
							"    ,ds.lms_sroute_id as deliverysrouteid\r\n",
							"    ,ds.lms_sroute_description as deliverysroutedescription\r\n",
							"\r\n",
							"    ,pz.lms_zone_id as pickupzoneid\r\n",
							"    ,pz.lms_zone_description as pickupzonedescription\r\n",
							"    ,dz.lms_zone_id as deliveryzoneid\r\n",
							"    ,dz.lms_zone_description as deliveryzonedescription\r\n",
							"\r\n",
							"    ,pl.lms_location_id as pickuplocationid\r\n",
							"    ,pl.lms_location_description as pickuplocationdescription\r\n",
							"    ,dl.lms_location_id as deliverylocationid\r\n",
							"    ,dl.lms_location_description as deliverylocationdescription\r\n",
							"\r\n",
							"\r\n",
							"FROM dbobillexceptionr bcr\r\n",
							"LEFT JOIN stparceldistinctconsignment_tmp p on p.lms_parcel_consignid = bcr.consignid\r\n",
							"LEFT JOIN stbillcustomer bc on bc.lms_customer_id = bcr.BillCust\r\n",
							"LEFT JOIN stdeliverypickupcustomer pc on pc.lms_customer_id = p.lms_parcel_orderpickupcustid\r\n",
							"LEFT JOIN stdeliverypickupcustomer dc on dc.lms_customer_id = p.lms_parcel_orderdelivercustid\r\n",
							"LEFT JOIN dbobillzoneroute pzr  on pzr.ID = bcr.PickUpRouteID\r\n",
							"LEFT JOIN dbobillzoneroute dzr  on dzr.ID = bcr.DeliverRouteID\r\n",
							"LEFT JOIN stsroute ps on ps.lms_sroute_id = pc.lms_customer_srouteid\r\n",
							"LEFT JOIN stzone pz on pz.lms_zone_id =  ps.lms_sroute_zoneid\r\n",
							"LEFT JOIN stlocation pl on pl.lms_location_id =  pz.lms_zone_locid \r\n",
							"LEFT JOIN stsroute ds on ds.lms_sroute_id = dc.lms_customer_srouteid\r\n",
							"LEFT JOIN stzone dz on dz.lms_zone_id =  ds.lms_sroute_zoneid\r\n",
							"LEFT JOIN stlocation dl on dl.lms_location_id =  dz.lms_zone_locid \r\n",
							"\r\n",
							"WHERE bcr.cdate1 >= (to_timestamp(year(current_date())-1||'-'||(month(current_date()))||'-'||'01'))\r\n",
							"AND bcr.chargeweight > 0\r\n",
							"AND coalesce(bcr.RouteMissingInd,0) = 0\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 48
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"rptbillconsignmentexceptionlevel = spark.sql(\"SELECT * FROM rptbillconsignmentexceptionlevel\")"
						],
						"outputs": [],
						"execution_count": 49
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# rptbillconsignmentexceptionlevel.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/RPT/rptbillconsignmentexceptionlevel.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 51
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/_05_RptWaybillLevel')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TRANSFORM/02 STRUCTURED/Reports/LMS"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "PRDSparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "418b0eb9-130d-4f6d-831f-a0fbb8888f8f"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace/bigDataPools/DevSparkPool",
						"name": "DevSparkPool",
						"type": "Spark",
						"endpoint": "https://citylogistics-synapseanalytics-workspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/DevSparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Environment = mssparkutils.env.getWorkspaceName()\r\n",
							"# if 'prod' in Environment:\r\n",
							"#     StorageAccount = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorageprod'\r\n",
							"# else:\r\n",
							"#     StorageAccount = 'citylogisticsstorage'\r\n",
							"#     StorageAccountRead = 'citylogisticsstorageprod'\r\n",
							"#     StorageAccountRead2 = 'citylogisticsstorage'\r\n",
							"#     StorageAccountWrite = 'citylogisticsstorage'\r\n",
							"\r\n",
							"# # ' + StorageAccount + '"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# #Create DataFrame for the stwaybill LMS Table\r\n",
							"# stwaybill = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stwaybill.parquet', format='parquet')\r\n",
							"# stwaybill.createOrReplaceTempView(\"stwaybill\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stparcel LMS Table\r\n",
							"# stparcel = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stparcel.parquet', format='parquet')\r\n",
							"# stparcel.createOrReplaceTempView(\"stparcel\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stbillcustomer LMS Table\r\n",
							"# stbillcustomer = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stbillcustomer.parquet', format='parquet')\r\n",
							"# stbillcustomer.createOrReplaceTempView(\"stbillcustomer\")\r\n",
							"\r\n",
							"# #Create DataFrame for the stdeliverypickupcustomer LMS Table\r\n",
							"# stdeliverypickupcustomer = spark.read.load('abfss://synapse@' + StorageAccountRead2 + '.dfs.core.windows.net/Structured Data/OPS/stdeliverypickupcustomer.parquet', format='parquet')\r\n",
							"# stdeliverypickupcustomer.createOrReplaceTempView(\"stdeliverypickupcustomer\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dborouteratetype LMS Table\r\n",
							"# dborouteratetype = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dborouteratetype.parquet', format='parquet')\r\n",
							"# dborouteratetype.createOrReplaceTempView(\"dborouteratetype\")\r\n",
							"\r\n",
							"# #Create DataFrame for the dbocustacc LMS Table\r\n",
							"# dbocustacc = spark.read.load('abfss://synapse@' + StorageAccountRead + '.dfs.core.windows.net/Unstructured Data/LMS/dbocustacc.parquet', format='parquet')\r\n",
							"# dbocustacc.createOrReplaceTempView(\"dbocustacc\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"stparcel_tmp\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"       p.lms_parcel_waybillid\r\n",
							"      ,max(p.lms_parcel_pdate) as lms_parcel_maxpdate \r\n",
							"      ,min(p.lms_parcel_pdate) as lms_parcel_minpdate \r\n",
							"\r\n",
							"FROM stparcel p\r\n",
							"GROUP BY p.lms_parcel_waybillid"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stparcel_tmp = spark.sql(\"SELECT * FROM stparcel_tmp\")"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE TEMP VIEW\r\n",
							"rptwaybilllevel\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"       w.*\r\n",
							"      ,p.lms_parcel_maxpdate \r\n",
							"      ,p.lms_parcel_minpdate \r\n",
							"      ,bc.lms_customer_name as billingcustomername\r\n",
							"      ,dc.lms_customer_name as deliverycustomername\r\n",
							"      ,pc.lms_customer_name as pickupcustomername\r\n",
							"      ,rrt.id as delcustbrouterouteratetypeid\r\n",
							"      ,rrt.description as delcustbrouterouteratetypedescription\r\n",
							"      ,ca.id as custaccid\r\n",
							"      ,ca.description as custaccdescription\r\n",
							"      ,ca.cref as custacccref\r\n",
							"      \r\n",
							"FROM stwaybill w\r\n",
							"LEFT JOIN stparcel_tmp p on p.lms_parcel_waybillid =  w.lms_waybill_id\r\n",
							"LEFT JOIN stbillcustomer bc on bc.lms_customer_id = w.lms_waybill_billcust\r\n",
							"LEFT JOIN stdeliverypickupcustomer dc on dc.lms_customer_id = w.lms_waybill_delivercustid\r\n",
							"LEFT JOIN stdeliverypickupcustomer pc on pc.lms_customer_id = w.lms_waybill_pickupcustid\r\n",
							"LEFT JOIN dborouteratetype rrt on rrt.id = dc.lms_customer_brouteid\r\n",
							"LEFT JOIN dbocustacc ca on ca.id = w.lms_waybill_custaccid\r\n",
							"\r\n",
							"WHERE lms_waybill_date >= to_timestamp(year(current_date())-1||'-'||(month(current_date()))||'-'||'01')\r\n",
							"\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"rptwaybilllevel = spark.sql(\"SELECT * FROM rptwaybilllevel\")"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# rptwaybilllevel.write.parquet('abfss://synapse@' + StorageAccountWrite + '.dfs.core.windows.net/Structured Data/RPT/rptwaybilllevel.parquet', mode = \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 18
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SynapseApacheSparkConfigv1')]",
			"type": "Microsoft.Synapse/workspaces/sparkConfigurations",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"configs": {
					"spark.sql.parquet.datetimeRebaseModeInWrite": "CORRECTED",
					"spark.sql.parquet.datetimeRebaseModeInRead": "CORRECTED",
					"spark.sql.parquet.int96RebaseModeInWrite": "CORRECTED",
					"spark.sql.parquet.int96RebaseModeInRead": "CORRECTED"
				},
				"created": "2023-08-02T11:40:47.0450000+02:00",
				"createdBy": "christoff.dupreez@synapse-junction.com",
				"annotations": [],
				"configMergeRule": {
					"artifact.currentOperation.spark.sql.parquet.datetimeRebaseModeInWrite": "replace",
					"artifact.currentOperation.spark.sql.parquet.datetimeRebaseModeInRead": "replace",
					"artifact.currentOperation.spark.sql.parquet.int96RebaseModeInWrite": "replace",
					"artifact.currentOperation.spark.sql.parquet.int96RebaseModeInRead": "replace"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/PRDSparkPool')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 10,
					"minNodeCount": 3
				},
				"nodeCount": 10,
				"nodeSize": "Medium",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.3",
				"isComputeIsolationEnabled": false,
				"sparkConfigProperties": {
					"configurationType": "Artifact",
					"filename": "SynapseApacheSparkConfigv1",
					"content": "{\"name\":\"SynapseApacheSparkConfigv1\",\"properties\":{\"configs\":{\"spark.sql.parquet.datetimeRebaseModeInWrite\":\"CORRECTED\",\"spark.sql.parquet.datetimeRebaseModeInRead\":\"CORRECTED\",\"spark.sql.parquet.int96RebaseModeInWrite\":\"CORRECTED\",\"spark.sql.parquet.int96RebaseModeInRead\":\"CORRECTED\"},\"annotations\":[],\"type\":\"Microsoft.Synapse/workspaces/sparkconfigurations\",\"description\":\"\",\"notes\":\"\",\"created\":\"2023-08-02T11:40:47.0450000+02:00\",\"createdBy\":\"christoff.dupreez@synapse-junction.com\",\"configMergeRule\":{\"admin.currentOperation.spark.sql.parquet.datetimeRebaseModeInWrite\":\"replace\",\"admin.currentOperation.spark.sql.parquet.datetimeRebaseModeInRead\":\"replace\",\"admin.currentOperation.spark.sql.parquet.int96RebaseModeInWrite\":\"replace\",\"admin.currentOperation.spark.sql.parquet.int96RebaseModeInRead\":\"replace\"}}}",
					"time": "2023-08-02T09:40:35.6540805Z"
				},
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "southafricanorth"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/TESTSparkPool')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 10,
					"minNodeCount": 3
				},
				"nodeCount": 10,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.3",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "southafricanorth"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/TESTSparkPoolL')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 10,
					"minNodeCount": 3
				},
				"nodeCount": 10,
				"nodeSize": "Large",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.3",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "southafricanorth"
		}
	]
}