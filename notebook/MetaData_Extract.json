{
	"name": "MetaData_Extract",
	"properties": {
		"folder": {
			"name": "DEV/MetaDataExtraction"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "TESTSparkPool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "0f8aa5bf-5cda-42b9-b7bb-452698b9459f"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse_Prod/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace-prod/bigDataPools/TESTSparkPool",
				"name": "TESTSparkPool",
				"type": "Spark",
				"endpoint": "https://citylogistics-synapseanalytics-workspace-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/TESTSparkPool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import functions as F\r\n",
					"from pyspark.sql.types import StructType, StructField, StringType, TimestampType\r\n",
					"from datetime import datetime\r\n",
					"import os\r\n",
					"import py4j\r\n",
					"from pyspark.sql import Row\r\n",
					"from pyspark.sql.types import StructType, StructField, StringType\r\n",
					"from pyspark.sql import functions as F\r\n",
					"import json\r\n",
					"from pyspark.sql.functions import col,when, lit, expr, explode_outer,explode,size\r\n",
					"from pyspark.sql.types import StringType\r\n",
					"import re\r\n",
					"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df = \"\""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.types import ArrayType, StructType\r\n",
					"def flatten_json(df):\r\n",
					"    \"\"\"\r\n",
					"    Flattens a DataFrame with complex nested fields (Arrays and Structs) by converting them into individual columns.\r\n",
					"\r\n",
					"    Parameters:\r\n",
					"    - df: The input DataFrame with complex nested fields\r\n",
					"\r\n",
					"    Returns:\r\n",
					"    - The flattened DataFrame with all complex fields expanded into separate columns.\r\n",
					"    \"\"\"\r\n",
					"    # compute Complex Fields (Lists and Structs) in Schema   \r\n",
					"    complex_fields = dict([(field.name, field.dataType)\r\n",
					"    for field in df.schema.fields\r\n",
					"    if type(field.dataType) == ArrayType or  type(field.dataType) == StructType])\r\n",
					"    print(df.schema)\r\n",
					"    print(\"\")\r\n",
					"    while len(complex_fields)!=0:\r\n",
					"        col_name=list(complex_fields.keys())[0]\r\n",
					"        print (\"Processing :\"+col_name+\" Type : \"+str(type(complex_fields[col_name])))\r\n",
					"\r\n",
					"        # if StructType then convert all sub element to columns.\r\n",
					"        # i.e. flatten structs\r\n",
					"        if (type(complex_fields[col_name]) == StructType):\r\n",
					"            expanded = [col(col_name+'.'+k).alias(col_name+'_'+k) for k in [ n.name for n in  complex_fields[col_name]]]\r\n",
					"            df=df.select(\"*\", *expanded).drop(col_name)\r\n",
					"\r\n",
					"        # if ArrayType then add the Array Elements as Rows using the explode function\r\n",
					"        # i.e. explode Arrays\r\n",
					"        elif (type(complex_fields[col_name]) == ArrayType):    \r\n",
					"            df=df.withColumn(col_name,explode_outer(col_name))\r\n",
					"\r\n",
					"        # recompute remaining Complex Fields in Schema       \r\n",
					"        complex_fields = dict([(field.name, field.dataType)\r\n",
					"        for field in df.schema.fields\r\n",
					"            if type(field.dataType) == ArrayType or  type(field.dataType) == StructType])\r\n",
					"    \r\n",
					"    return df"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"final_df = flatten_json(df)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"flatten_df.write.parquet(\"\",mode='overwrite')"
				],
				"execution_count": null
			}
		]
	}
}