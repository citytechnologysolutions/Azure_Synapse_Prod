{
	"name": "LMS_Files_dTypes_V2",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "TESTSparkPool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "8363701a-a36d-42ed-9383-30c62d5df4f3"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse_Prod/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace-prod/bigDataPools/TESTSparkPool",
				"name": "TESTSparkPool",
				"type": "Spark",
				"endpoint": "https://citylogistics-synapseanalytics-workspace-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/TESTSparkPool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import functions as F\r\n",
					"from pyspark.sql.types import StructType, StructField, StringType, TimestampType,ArrayType,LongType,BooleanType\r\n",
					"from datetime import datetime\r\n",
					"import os\r\n",
					"import py4j\r\n",
					"from pyspark.sql import Row\r\n",
					"from pyspark.sql import functions as F\r\n",
					"import json\r\n",
					"from pyspark.sql.functions import col, explode_outer\r\n",
					"from io import StringIO\r\n",
					"import csv"
				],
				"execution_count": 2
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## LMSv1"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import os\r\n",
					"from pyspark.sql import SparkSession\r\n",
					"from pyspark.sql import Row\r\n",
					"\r\n",
					"# Initialize Spark session\r\n",
					"spark = SparkSession.builder \\\r\n",
					"    .appName(\"Get Data Types\") \\\r\n",
					"    .getOrCreate()\r\n",
					"\r\n",
					"# Define the path (keep spaces as is)\r\n",
					"path_with_spaces = \"abfss://synapse@citylogisticsstorageprod.dfs.core.windows.net/Unstructured Data/LMS/\"\r\n",
					"\r\n",
					"# List to hold file paths\r\n",
					"file_list = []\r\n",
					"\r\n",
					"# Get the file system\r\n",
					"URI = spark._jvm.java.net.URI\r\n",
					"Path = spark._jvm.org.apache.hadoop.fs.Path\r\n",
					"FileSystem = spark._jvm.org.apache.hadoop.fs.FileSystem\r\n",
					"\r\n",
					"try:\r\n",
					"    # Create a Path object with the given path\r\n",
					"    hadoop_path = Path(path_with_spaces)\r\n",
					"    \r\n",
					"    # Get the file system\r\n",
					"    fs = FileSystem.get(hadoop_path.toUri(), spark._jsc.hadoopConfiguration())\r\n",
					"    \r\n",
					"    # Check if the path exists\r\n",
					"    if not fs.exists(hadoop_path):\r\n",
					"        raise Exception(f\"The specified path does not exist: {path_with_spaces}\")\r\n",
					"    \r\n",
					"    # List status of the path\r\n",
					"    status = fs.listStatus(hadoop_path)\r\n",
					"\r\n",
					"    for file_status in status:\r\n",
					"        file_path = file_status.getPath().toString()\r\n",
					"        file_list.append(file_path)\r\n",
					"\r\n",
					"    #print(\"Files found:\")\r\n",
					"    #for file in file_list:\r\n",
					"    #    print(file)\r\n",
					"    \r\n",
					"    # List to hold schema information as rows\r\n",
					"    schema_info = []\r\n",
					"\r\n",
					"    # Read each file and get its schema\r\n",
					"    for file_path in file_list:\r\n",
					"        try:\r\n",
					"            # Read the Parquet file into a DataFrame\r\n",
					"            df = spark.read.parquet(file_path)\r\n",
					"            \r\n",
					"            # Collect the schema\r\n",
					"            schema = df.dtypes\r\n",
					"            for column_name, data_type in schema:\r\n",
					"                file_name = os.path.basename(file_path)  # Extract file name from path\r\n",
					"                schema_info.append(Row(file_name=file_name, column_name=column_name, data_type=data_type))\r\n",
					"        \r\n",
					"        except Exception as e:\r\n",
					"            print(f\"Error reading {file_path}: {e}\")\r\n",
					"\r\n",
					"    # Create a DataFrame from the schema information\r\n",
					"    schema_df = spark.createDataFrame(schema_info)\r\n",
					"\r\n",
					"except py4j.protocol.Py4JJavaError as e:\r\n",
					"    print(\"Error occurred while listing files: \", e)\r\n",
					"except Exception as e:\r\n",
					"    print(f\"Error: {e}\")\r\n",
					""
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import pandas as pd\r\n",
					"\r\n",
					"date_columns = schema_df.toPandas()\r\n",
					"date_columns = date_columns[date_columns.data_type == 'timestamp']\r\n",
					"date_columns = date_columns['column_name']\r\n",
					"date_columns = date_columns.drop_duplicates()\r\n",
					"date_columns = date_columns.to_frame()\r\n",
					"print(date_columns)"
				],
				"execution_count": 30
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"date_columns[date_columns['column_name'] == 'moddate']"
				],
				"execution_count": 32
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Print the DataFrame\r\n",
					"def print_dataframe(df, num_rows=10):\r\n",
					"    \"\"\"Print the DataFrame with a specified number of rows.\"\"\"\r\n",
					"    df.show(n=num_rows, truncate=False)\r\n",
					"\r\n",
					"# Call the print function\r\n",
					"print_dataframe(schema_df)"
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Print the DataFrame with timestamp data types\r\n",
					"def print_filtered_dataframe(df, num_rows=10):\r\n",
					"    \"\"\"Print the DataFrame where data_type is 'timestamp' with a specified number of rows.\"\"\"\r\n",
					"    filtered_df = df.filter(df.data_type == 'timestamp')\r\n",
					"    filtered_df.show(n=num_rows, truncate=False)\r\n",
					"    \r\n",
					"\r\n",
					"# Call the print function\r\n",
					"print_filtered_dataframe(schema_df)\r\n",
					""
				],
				"execution_count": 6
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## LMSv2"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Initialize Spark session\r\n",
					"#spark = SparkSession.builder \\\r\n",
					"#    .appName(\"Get Data Types\") \\\r\n",
					"#    .getOrCreate()\r\n",
					"\r\n",
					"# Define the path (keep spaces as is)\r\n",
					"path_with_spaces = \"abfss://synapse@citylogisticsstorageprod.dfs.core.windows.net/Unstructured Data/CLMasterData/LMSV2/\"\r\n",
					"\r\n",
					"# List to hold file paths\r\n",
					"file_list = []\r\n",
					"\r\n",
					"# Get the file system\r\n",
					"URI = spark._jvm.java.net.URI\r\n",
					"Path = spark._jvm.org.apache.hadoop.fs.Path\r\n",
					"FileSystem = spark._jvm.org.apache.hadoop.fs.FileSystem\r\n",
					"\r\n",
					"try:\r\n",
					"    # Create a Path object with the given path\r\n",
					"    hadoop_path = Path(path_with_spaces)\r\n",
					"    \r\n",
					"    # Get the file system\r\n",
					"    fs = FileSystem.get(hadoop_path.toUri(), spark._jsc.hadoopConfiguration())\r\n",
					"    \r\n",
					"    # Check if the path exists\r\n",
					"    if not fs.exists(hadoop_path):\r\n",
					"        raise Exception(f\"The specified path does not exist: {path_with_spaces}\")\r\n",
					"    \r\n",
					"    # List status of the path\r\n",
					"    status = fs.listStatus(hadoop_path)\r\n",
					"\r\n",
					"    for file_status in status:\r\n",
					"        file_path = file_status.getPath().toString()\r\n",
					"        file_list.append(file_path)\r\n",
					"\r\n",
					"    #print(\"Files found:\")\r\n",
					"    #for file in file_list:\r\n",
					"    #    print(file)\r\n",
					"    \r\n",
					"    # List to hold schema information as rows\r\n",
					"    schema_info_LMSv2 = []\r\n",
					"\r\n",
					"    # Read each file and get its schema\r\n",
					"    for file_path in file_list:\r\n",
					"        try:\r\n",
					"            # Read the Parquet file into a DataFrame\r\n",
					"            df = spark.read.parquet(file_path)\r\n",
					"            \r\n",
					"            # Collect the schema\r\n",
					"            schema = df.dtypes\r\n",
					"            for column_name, data_type in schema:\r\n",
					"                file_name = os.path.basename(file_path)  # Extract file name from path\r\n",
					"                schema_info_LMSv2.append(Row(file_name=file_name, column_name=column_name, data_type=data_type))\r\n",
					"        \r\n",
					"        except Exception as e:\r\n",
					"            print(f\"Error reading {file_path}: {e}\")\r\n",
					"\r\n",
					"    # Create a DataFrame from the schema information\r\n",
					"    schema_df_LMSv2 = spark.createDataFrame(schema_info_LMSv2)\r\n",
					"\r\n",
					"except py4j.protocol.Py4JJavaError as e:\r\n",
					"    print(\"Error occurred while listing files: \", e)\r\n",
					"except Exception as e:\r\n",
					"    print(f\"Error: {e}\")\r\n",
					""
				],
				"execution_count": 7
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Print the DataFrame\r\n",
					"def print_dataframe(df, num_rows=10):\r\n",
					"    \"\"\"Print the DataFrame with a specified number of rows.\"\"\"\r\n",
					"    df.show(n=num_rows, truncate=False)\r\n",
					"\r\n",
					"# Call the print function\r\n",
					"print_dataframe(schema_df_LMSv2)"
				],
				"execution_count": 8
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#Filter where column name is in Date list\r\n",
					"print_dataframe(schema_df_LMSv2)"
				],
				"execution_count": 9
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df_LMSv2 = schema_df_LMSv2.toPandas()"
				],
				"execution_count": 26
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df_LMSv2['Date_col'] = df_LMSv2['column_name'].apply(lambda x: 1 if 'date' in x else 0)"
				],
				"execution_count": 44
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df_LMSv2[df_LMSv2['Date_col'] ==1]"
				],
				"execution_count": 45
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#check if column is in date_columns from LMSv2\r\n",
					"df_LMSv2 = schema_df_LMSv2.toPandas()\r\n",
					"\r\n",
					"for row in df_LMSv2.iterrows():\r\n",
					"    print(row)\r\n",
					"    if row['column_name'] in date_columns:\r\n",
					"        df_LMSv2['indi'] = 1\r\n",
					"print(df_LMSv2)\r\n",
					""
				]
			}
		]
	}
}