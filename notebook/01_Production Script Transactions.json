{
	"name": "01_Production Script Transactions",
	"properties": {
		"folder": {
			"name": "ARCHIVE/PROD/Transformation Scripts - Running Notebooks"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "PRDSparkPool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "56g",
			"driverCores": 8,
			"executorMemory": "56g",
			"executorCores": 8,
			"numExecutors": 1,
			"runAsWorkspaceSystemIdentity": true,
			"conf": {
				"spark.dynamicAllocation.enabled": "true",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "4",
				"spark.autotune.trackingId": "d52b7b3f-ac55-4694-9f0f-8f02c7275311"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/99b4fa8b-7705-4d41-a2fb-4f8f9ee006c7/resourceGroups/AZ_Resource_DataWarehouse/providers/Microsoft.Synapse/workspaces/citylogistics-synapseanalytics-workspace/bigDataPools/ApacheSparkPool",
				"name": "ApacheSparkPool",
				"type": "Spark",
				"endpoint": "https://citylogistics-synapseanalytics-workspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/ApacheSparkPool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.1",
				"nodeCount": 10,
				"cores": 8,
				"memory": 56
			},
			"sessionKeepAliveTimeout": 3
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Start of Single Notebook Run Automation for Transactions Scripts"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##Set the Parameter for Dev or Production\r\n",
					"Environment = 'DataLake'  #FOR DEV USE: 'DEV DataLake'"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Execute Script 1- Finance_Data_Preparation_Dates"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Load the Source Files for first part of the scripting\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"dboHLD1 = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboHLD1.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"dboHLD1.createOrReplaceTempView(\"dboHLD1\")"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run PROD/Transactions Notebooks PROD/1- Finance_Data_Preparation_Dates_PROD"
				],
				"execution_count": 4
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Write Structured Data back to BlobStorage"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"DIM_DATES.write.parquet('/' + Environment + '/Structured/Dimensions/DIM_DATES.parquet', mode='overwrite')\r\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## End of Part1 scripting"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Execute Script 2- Transactions-Document_Categories"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Load the Source Files for the second part of the scripting\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"dboOPOR = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboOPOR.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"dboOPOR.createOrReplaceTempView(\"dboOPOR\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"dboPOR1 = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboPOR1.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"dboPOR1.createOrReplaceTempView(\"dboPOR1\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"dboOINV = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboOINV.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"dboOINV.createOrReplaceTempView(\"dboOINV\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"dboINV1 = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboINV1.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"dboINV1.createOrReplaceTempView(\"dboINV1\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"dboORIN = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboORIN.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"dboORIN.createOrReplaceTempView(\"dboORIN\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"dboRIN1 = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboRIN1.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"dboRIN1.createOrReplaceTempView(\"dboRIN1\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"dboOPCH = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboOPCH.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"dboOPCH.createOrReplaceTempView(\"dboOPCH\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"dboPCH1 = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboPCH1.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"dboPCH1.createOrReplaceTempView(\"dboPCH1\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"dboORPC = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboORPC.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"dboORPC.createOrReplaceTempView(\"dboORPC\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"dboRPC1 = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboRPC1.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"dboRPC1.createOrReplaceTempView(\"dboRPC1\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"dboOPDN = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboOPDN.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"dboOPDN.createOrReplaceTempView(\"dboOPDN\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"dboPDN1 = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboPDN1.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"dboPDN1.createOrReplaceTempView(\"dboPDN1\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"dboORPD = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboORPD.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"dboORPD.createOrReplaceTempView(\"dboORPD\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"dboRPD1 = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboRPD1.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"dboRPD1.createOrReplaceTempView(\"dboRPD1\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"dboOMRV = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboOMRV.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"dboOMRV.createOrReplaceTempView(\"dboOMRV\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"dboMRV1 = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboMRV1.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"dboMRV1.createOrReplaceTempView(\"dboMRV1\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"dboMRV2 = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboMRV2.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"dboMRV2.createOrReplaceTempView(\"dboMRV2\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"dboOINM = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/OINM.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"dboOINM.createOrReplaceTempView(\"dboOINM\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"dboOJDT = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboOJDT.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"dboOJDT.createOrReplaceTempView(\"dboOJDT\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"dboJDT1 = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboJDT1.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"dboJDT1.createOrReplaceTempView(\"dboJDT1\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"dboOACT = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboOACT.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"dboOACT.createOrReplaceTempView(\"dboOACT\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"dboOITM = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboOITM.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"dboOITM.createOrReplaceTempView(\"dboOITM\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"dboMFRHistoryDaily = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboMFRHistoryDaily.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"dboMFRHistoryDaily.createOrReplaceTempView(\"dboMFRHistoryDaily\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"ListDepots = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/CCMasterData/ListDepots.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"ListDepots.createOrReplaceTempView(\"ListDepots\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"dboVEHICLE_H = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/@VEHICLE_H.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"dboVEHICLE_H.createOrReplaceTempView(\"dboVEHICLE_H\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"dboOASC = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboOASC.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"dboOASC.createOrReplaceTempView(\"dboOASC\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"dboOIGE = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboOIGE.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"dboOIGE.createOrReplaceTempView(\"dboOIGE\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"dboIGE1 = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboIGE1.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"dboIGE1.createOrReplaceTempView(\"dboIGE1\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"Dim_ITEM = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/DataLake/Structured/Dimensions/Dim_ITEM.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"Dim_ITEM.createOrReplaceTempView(\"Dim_ITEM\")\r\n",
					"\r\n",
					"#Duplicated loaded (Removed for Optimization)\r\n",
					"#dboHLD1 = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboHLD1.parquet', format='parquet')\r\n",
					"#dboHLD1.createOrReplaceTempView(\"dboHLD1\")"
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run PROD/Transactions Notebooks PROD/2- Transactions-Document_Categories_PROD"
				],
				"execution_count": 6
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Write structured TRANSACTIONS_HEADE&DETAIL back to datalake"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Purchase_Order.write.parquet('/' + Environment + '/Structured/Facts/F_Purchase_Order.parquet', mode='overwrite')\r\n",
					"#Purchase_Order.write.mode(\"overwrite\").saveAsTable(\"mdw_uat_Transactions.F_Purchase_Order\")\r\n",
					"\r\n",
					"Sales_Invoice.write.parquet('/' + Environment + '/Structured/Facts/F_Sales_Invoice.parquet', mode='overwrite')\r\n",
					"#Sales_Invoice.write.mode(\"overwrite\").saveAsTable(\"mdw_uat_Transactions.F_Sales_Invoice\")\r\n",
					"\r\n",
					"Sales_Credit_Note.write.parquet('/' + Environment + '/Structured/Facts/F_Sales_Credit_Note.parquet', mode='overwrite')\r\n",
					"#Sales_Credit_Note.write.mode(\"overwrite\").saveAsTable(\"mdw_uat_Transactions.F_Sales_Credit_Note\")\r\n",
					"\r\n",
					"Purchase_Invoice.write.parquet('/' + Environment + '/Structured/Facts/F_Purchase_Invoice.parquet', mode='overwrite')\r\n",
					"#Purchase_Invoice.write.mode(\"overwrite\").saveAsTable(\"mdw_uat_Transactions.F_Purchase_Invoice\")\r\n",
					"\r\n",
					"Purchase_Credit_Note.write.parquet('/' + Environment + '/Structured/Facts/F_Purchase_Credit_Note.parquet', mode='overwrite')\r\n",
					"#Purchase_Credit_Note.write.mode(\"overwrite\").saveAsTable(\"mdw_uat_Transactions.F_Purchase_Credit_Note\")\r\n",
					"\r\n",
					"Goods_Receipt_Notes.write.parquet('/' + Environment + '/Structured/Facts/F_Goods_Receipt_Notes.parquet', mode='overwrite')\r\n",
					"#Goods_Receipt_Notes.write.mode(\"overwrite\").saveAsTable(\"mdw_uat_Transactions.F_Goods_Receipt_Notes\")\r\n",
					"\r\n",
					"Goods_Returns.write.parquet('/' + Environment + '/Structured/Facts/F_Goods_Returns.parquet', mode='overwrite')\r\n",
					"#Goods_Returns.write.mode(\"overwrite\").saveAsTable(\"mdw_uat_Transactions.F_Goods_Returns\")\r\n",
					"\r\n",
					"Stock_Revaluation.write.parquet('/' + Environment + '/Structured/Facts/F_Stock_Revaluation.parquet', mode='overwrite')\r\n",
					"#Stock_Revaluation.write.mode(\"overwrite\").saveAsTable(\"mdw_uat_Transactions.F_Stock_Revaluation\")\r\n",
					"\r\n",
					"STOCK_TRANSACTIONS.write.parquet('/' + Environment + '/Structured/Facts/F_STOCK_TRANSACTIONS.parquet', mode='overwrite')\r\n",
					"#STOCK_TRANSACTIONS.write.mode(\"overwrite\").saveAsTable(\"mdw_uat_Transactions.F_STOCK_TRANSACTIONS\")\r\n",
					"\r\n",
					"GENERAL_JOURNAL.write.parquet('/' + Environment + '/Structured/Facts/F_GENERAL_JOURNAL.parquet', mode='overwrite')\r\n",
					"#GENERAL_JOURNAL.write.mode(\"overwrite\").saveAsTable(\"mdw_uat_Transactions.F_GENERAL_JOURNAL\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## End of Part2 scripting"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Execute Script 3- Finance_SAP_Budget"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Load the Source Files for the third part of the scripting\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"dboOBGS = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboOBGS.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"dboOBGS.createOrReplaceTempView(\"dboOBGS\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"dboOBGT = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboOBGT.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"dboOBGT.createOrReplaceTempView(\"dboOBGT\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"dboBGT1 = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboBGT1.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"dboBGT1.createOrReplaceTempView(\"dboBGT1\")"
				],
				"execution_count": 7
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run PROD/Transactions Notebooks PROD/3- Finance_SAP_Budget_PROD"
				],
				"execution_count": 8
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Write Structured Data back to BlobStorage"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"F_OBGS.write.parquet('/' + Environment + '/Structured/Facts/F_OBGS.parquet', mode='overwrite')\r\n",
					"\r\n",
					"F_OBGT.write.parquet('/' + Environment + '/Structured/Facts/F_OBGT.parquet', mode='overwrite')\r\n",
					"\r\n",
					"F_BGT1.write.parquet('/' + Environment + '/Structured/Facts/F_BGT1.parquet', mode='overwrite')\r\n",
					"\r\n",
					"F_Budget.write.parquet('/' + Environment + '/Structured/Facts/F_Budget.parquet', mode='overwrite')\r\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## End of Part3 scripting"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Execute Script 4- Finance_SAP_Dimensions"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Load the Source Files for the fourth part of the scripting\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"dboOITB = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboOITB.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"dboOITB.createOrReplaceTempView(\"dboOITB\")\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#dboOITM = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboOITM.parquet', format='parquet')\r\n",
					"#dboOITM.createOrReplaceTempView(\"dboOITM\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run /PROD/Transactions Notebooks PROD/4- Finance_SAP_Dimensions_PROD"
				],
				"execution_count": 10
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Write Structured Data back to BlobStorage"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"ITEM_Expose.write.parquet('/' + Environment + '/Structured/Dimensions/DIM_ITEM_Expose.parquet', mode='overwrite')\r\n",
					"\r\n",
					"ITEM.write.parquet('/' + Environment + '/Structured/Dimensions/Dim_ITEM.parquet', mode='overwrite')"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## End of Part4 scripting"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Execute Script 5- Finance_SAP_Groups"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Load the Source Files for the fith part of the scripting\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#ListDepots = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/CCMasterData/ListDepots.parquet', format='parquet')\r\n",
					"#ListDepots.createOrReplaceTempView(\"ListDepots\")"
				],
				"execution_count": 11
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run PROD/Transactions Notebooks PROD/5- Finance_SAP_Groups_PROD"
				],
				"execution_count": 12
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Write Structured Data back to BlobStorage"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"GROUP_Expose.write.parquet('/' + Environment + '/Structured/Dimensions/DIM_GROUP_Expose.parquet', mode='overwrite')\r\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## End of Part5 scripting"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Execute Script 6- Transactions_Journal_Creation"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Load the Source Files for the sixth part of the scripting\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#dboOJDT = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboOJDT.parquet', format='parquet')\r\n",
					"#dboOJDT.createOrReplaceTempView(\"dboOJDT\")\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#dboJDT1 = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboJDT1.parquet', format='parquet')\r\n",
					"#dboJDT1.createOrReplaceTempView(\"dboJDT1\")\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#dboOACT = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboOACT.parquet', format='parquet')\r\n",
					"#dboOACT.createOrReplaceTempView(\"dboOACT\")"
				],
				"execution_count": 13
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run PROD/Transactions Notebooks PROD/6- Transactions_Journal_Creation_PROD"
				],
				"execution_count": 14
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Write structured TRANSACTIONS_HEADE&DETAIL back to datalake"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Manual_Journal.write.parquet('/' + Environment + '/Structured/Facts/F_MANUAL_JOURNAL.parquet', mode='overwrite')\r\n",
					"\r\n",
					"GENERAL_Journal.write.parquet('/' + Environment + '/Structured/Facts/F_GENERAL_JOURNAL.parquet', mode='overwrite')\r\n",
					"\r\n",
					"MANUAL_JOURNAL_TMP.write.parquet('/' + Environment + '/Structured/Facts/F_MANUAL_JOURNAL_TMP.parquet', mode='overwrite')\r\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## End of Part6 scripting"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Execute Script 7- Finance_Data_Preparation_Sales"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Load the Source Files for the seventh part of the scripting\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"F_Purchase_Order = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net//DataLake/Structured/Facts/F_Purchase_Order.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"F_Purchase_Order.createOrReplaceTempView(\"F_Purchase_Order\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"F_Sales_Invoice = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net//DataLake/Structured/Facts/F_Sales_Invoice.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"F_Sales_Invoice.createOrReplaceTempView(\"F_Sales_Invoice\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"F_Sales_Credit_Note = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net//DataLake/Structured/Facts/F_Sales_Credit_Note.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"F_Sales_Credit_Note.createOrReplaceTempView(\"F_Sales_Credit_Note\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"F_Purchase_Invoice = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net//DataLake/Structured/Facts/F_Purchase_Invoice.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"F_Purchase_Invoice.createOrReplaceTempView(\"F_Purchase_Invoice\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"F_Purchase_Credit_Note = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net//DataLake/Structured/Facts/F_Purchase_Credit_Note.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"F_Purchase_Credit_Note.createOrReplaceTempView(\"F_Purchase_Credit_Note\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"F_Goods_Receipt_Notes = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net//DataLake/Structured/Facts/F_Goods_Receipt_Notes.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"F_Goods_Receipt_Notes.createOrReplaceTempView(\"F_Goods_Receipt_Notes\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"F_Goods_Returns = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net//DataLake/Structured/Facts/F_Goods_Returns.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"F_Goods_Returns.createOrReplaceTempView(\"F_Goods_Returns\")\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#dboOINV = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboOINV.parquet', format='parquet')\r\n",
					"#dboOINV.createOrReplaceTempView(\"dboOINV\")\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#dboORIN = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboORIN.parquet', format='parquet')\r\n",
					"#dboORIN.createOrReplaceTempView(\"dboORIN\")\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#dboOPCH = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboOPCH.parquet', format='parquet')\r\n",
					"#dboOPCH.createOrReplaceTempView(\"dboOPCH\")\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#dboORPC = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboORPC.parquet', format='parquet')\r\n",
					"#dboORPC.createOrReplaceTempView(\"dboORPC\")\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#dboOPDN = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboOPDN.parquet', format='parquet')\r\n",
					"#dboOPDN.createOrReplaceTempView(\"dboOPDN\")\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#dboOIGE = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboOIGE.parquet', format='parquet')\r\n",
					"#dboOIGE.createOrReplaceTempView(\"dboOIGE\")\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#dboORPD = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboORPD.parquet', format='parquet')\r\n",
					"#dboORPD.createOrReplaceTempView(\"dboORPD\")\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#dboINV1 = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboINV1.parquet', format='parquet')\r\n",
					"#dboINV1.createOrReplaceTempView(\"dboINV1\")\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#dboRIN1 = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboRIN1.parquet', format='parquet')\r\n",
					"#dboRIN1.createOrReplaceTempView(\"dboRIN1\")\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#dboRPC1 = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboRPC1.parquet', format='parquet')\r\n",
					"#dboRPC1.createOrReplaceTempView(\"dboRPC1\")\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#dboPDN1 = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboPDN1.parquet', format='parquet')\r\n",
					"#dboPDN1.createOrReplaceTempView(\"dboPDN1\")\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#dboIGE1 = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboIGE1.parquet', format='parquet')\r\n",
					"#dboIGE1.createOrReplaceTempView(\"dboIGE1\")"
				],
				"execution_count": 15
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run PROD/Transactions Notebooks PROD/7- Finance_Data_Preparation_Sales_PROD"
				],
				"execution_count": 16
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Write structured Fact Tables back to datalake"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"TMPDOCDISC.write.parquet('/' + Environment + '/Structured/Facts/F_TMPDOCDISC.parquet', mode='overwrite')\r\n",
					"\r\n",
					"TMPLINES.write.parquet('/' + Environment + '/Structured/Facts/F_TMPLINES.parquet', mode='overwrite')\r\n",
					"\r\n",
					"TMPDOCDISC2.write.parquet('/' + Environment + '/Structured/Facts/F_TMPDOCDISC2.parquet', mode='overwrite')\r\n",
					"\r\n",
					"TMPDOCDISC3.write.parquet('/' + Environment + '/Structured/Facts/F_TMPDOCDISC3.parquet', mode='overwrite')\r\n",
					"\r\n",
					"Goods_Issues.write.parquet('/' + Environment + '/Structured/Facts/F_Goods_Issues.parquet', mode='overwrite')"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## End of Part7 scripting"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Execute Script 8- Concat_TransDocCategories"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Load the Source Files for the eight part of the scripting\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"F_Purchase_Order = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/DataLake/Structured/Facts/F_Purchase_Order.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"F_Purchase_Order.createOrReplaceTempView(\"F_Purchase_Order\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"F_Sales_Invoice = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/DataLake/Structured/Facts/F_Sales_Invoice.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"F_Sales_Invoice.createOrReplaceTempView(\"F_Sales_Invoice\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"F_Sales_Credit_Note = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/DataLake/Structured/Facts/F_Sales_Credit_Note.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"F_Sales_Credit_Note.createOrReplaceTempView(\"F_Sales_Credit_Note\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"F_Purchase_Invoice = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/DataLake/Structured/Facts/F_Purchase_Invoice.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"F_Purchase_Invoice.createOrReplaceTempView(\"F_Purchase_Invoice\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"F_Purchase_Credit_Note = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/DataLake/Structured/Facts/F_Purchase_Credit_Note.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"F_Purchase_Credit_Note.createOrReplaceTempView(\"F_Purchase_Credit_Note\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"F_Goods_Receipt_Notes = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/DataLake/Structured/Facts/F_Goods_Receipt_Notes.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"F_Goods_Receipt_Notes.createOrReplaceTempView(\"F_Goods_Receipt_Notes\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"F_Goods_Returns = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/DataLake/Structured/Facts/F_Goods_Returns.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"F_Goods_Returns.createOrReplaceTempView(\"F_Goods_Returns\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"F_Stock_Revaluation = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/DataLake/Structured/Facts/F_Stock_Revaluation.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"F_Stock_Revaluation.createOrReplaceTempView(\"F_Stock_Revaluation\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"F_STOCK_TRANSACTIONS = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/DataLake/Structured/Facts/F_STOCK_TRANSACTIONS.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"F_STOCK_TRANSACTIONS.createOrReplaceTempView(\"F_STOCK_TRANSACTIONS\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"F_GENERAL_JOURNAL = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/DataLake/Structured/Facts/F_GENERAL_JOURNAL.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"F_GENERAL_JOURNAL.createOrReplaceTempView(\"F_GENERAL_JOURNAL\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"F_MANUAL_JOURNAL = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/DataLake/Structured/Facts/F_MANUAL_JOURNAL.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"F_MANUAL_JOURNAL.createOrReplaceTempView(\"F_MANUAL_JOURNAL\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"F_TMPDOCDISC3 = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/DataLake/Structured/Facts/F_TMPDOCDISC3.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"F_TMPDOCDISC3.createOrReplaceTempView(\"F_TMPDOCDISC3\")\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#dboOIGE = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboOIGE.parquet', format='parquet')\r\n",
					"#dboOIGE.createOrReplaceTempView(\"dboOIGE\")\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#dboIGE1 = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboIGE1.parquet', format='parquet')\r\n",
					"#dboIGE1.createOrReplaceTempView(\"dboIGE1\")\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#dboOINV = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboOINV.parquet', format='parquet')\r\n",
					"#dboOINV.createOrReplaceTempView(\"dboOINV\")\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#dboORIN = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboORIN.parquet', format='parquet')\r\n",
					"#dboORIN.createOrReplaceTempView(\"dboORIN\")\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#dboOPCH = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboOPCH.parquet', format='parquet')\r\n",
					"#dboOPCH.createOrReplaceTempView(\"dboOPCH\")\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#dboORPC = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboORPC.parquet', format='parquet')\r\n",
					"#dboORPC.createOrReplaceTempView(\"dboORPC\")\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#dboOPDN = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboOPDN.parquet', format='parquet')\r\n",
					"#dboOPDN.createOrReplaceTempView(\"dboOPDN\")\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#dboORPD = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboORPD.parquet', format='parquet')\r\n",
					"#dboORPD.createOrReplaceTempView(\"dboORPD\")\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#dboRIN1 = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboRIN1.parquet', format='parquet')\r\n",
					"#dboRIN1.createOrReplaceTempView(\"dboRIN1\")\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#dboOJDT = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboOJDT.parquet', format='parquet')\r\n",
					"#dboOJDT.createOrReplaceTempView(\"dboOJDT\")\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#dboJDT1 = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboJDT1.parquet', format='parquet')\r\n",
					"#dboJDT1.createOrReplaceTempView(\"dboJDT1\")\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#dboOACT = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboOACT.parquet', format='parquet')\r\n",
					"#dboOACT.createOrReplaceTempView(\"dboOACT\")\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#dboOITM = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboOITM.parquet', format='parquet')\r\n",
					"#dboOITM.createOrReplaceTempView(\"dboOITM\")\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#dboPDN1 = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboPDN1.parquet', format='parquet')\r\n",
					"#dboPDN1.createOrReplaceTempView(\"dboPDN1\")\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#dboINV1 = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboINV1.parquet', format='parquet')\r\n",
					"#dboINV1.createOrReplaceTempView(\"dboINV1\")\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#dboRPC1 = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboRPC1.parquet', format='parquet')\r\n",
					"#dboRPC1.createOrReplaceTempView(\"dboRPC1\")"
				],
				"execution_count": 17
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"%run PROD/Transactions Notebooks PROD/8- Concat_TransDocCategories_PROD"
				],
				"execution_count": 18
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##  Write Fact tables back to Blob Storage"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"TRANSACTIONS.write.parquet('/' + Environment + '/Structured/Facts/F_TRANSACTIONS.parquet', mode='overwrite')\r\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## End of Part8 scripting"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Execute Script 9- Transactions Transformations"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Load the Source Files for the night part of the scripting\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"MANUAL_JOURNAL_TMP = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/DataLake/Structured/Facts/F_MANUAL_JOURNAL_TMP.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"MANUAL_JOURNAL_TMP.createOrReplaceTempView(\"MANUAL_JOURNAL_TMP\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"Sales_Invoice = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/DataLake/Structured/Facts/F_Sales_Invoice.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"Sales_Invoice.createOrReplaceTempView(\"Sales_Invoice\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"Dim_Item = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net//DataLake/Structured/Dimensions/Dim_ITEM.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"Dim_Item.createOrReplaceTempView(\"Dim_Item\")\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#dboOINM = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/OINM.parquet', format='parquet')\r\n",
					"#dboOINM.createOrReplaceTempView(\"dboOINM\")\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#dboOINV = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboOINV.parquet', format='parquet')\r\n",
					"#dboOINV.createOrReplaceTempView(\"dboOINV\")\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#dboORIN = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboORIN.parquet', format='parquet')\r\n",
					"#dboORIN.createOrReplaceTempView(\"dboORIN\")\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#dboOPCH = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboOPCH.parquet', format='parquet')\r\n",
					"#dboOPCH.createOrReplaceTempView(\"dboOPCH\")\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#dboORPC = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboORPC.parquet', format='parquet')\r\n",
					"#dboORPC.createOrReplaceTempView(\"dboORPC\")\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#dboOPDN = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboOPDN.parquet', format='parquet')\r\n",
					"#dboOPDN.createOrReplaceTempView(\"dboOPDN\")\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#dboOIGE = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboOIGE.parquet', format='parquet')\r\n",
					"#dboOIGE.createOrReplaceTempView(\"dboOIGE\")\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#dboOPDN = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboOPDN.parquet', format='parquet')\r\n",
					"#dboOPDN.createOrReplaceTempView(\"dboOPDN\")\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#dboORPD = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboORPD.parquet', format='parquet')\r\n",
					"#dboORPD.createOrReplaceTempView(\"dboORPD\")\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#dboOJDT = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboOJDT.parquet', format='parquet')\r\n",
					"#dboOJDT.createOrReplaceTempView(\"dboOJDT\")\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#dboJDT1 = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboJDT1.parquet', format='parquet')\r\n",
					"#dboJDT1.createOrReplaceTempView(\"dboJDT1\")\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#dboOACT = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboOACT.parquet', format='parquet')\r\n",
					"#dboOACT.createOrReplaceTempView(\"dboOACT\")"
				],
				"execution_count": 19
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run PROD/Transactions Notebooks PROD/9- Transactions Transformations_PROD"
				],
				"execution_count": 20
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Write Fact Tables back to Blobstorage"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"SALES_INVOICES_INVENTORY_ITEMS_TMP.write.parquet('/' + Environment + '/Structured/Facts/F_SALES_INVOICES_INVENTORY_ITEMS_TMP.parquet', mode='overwrite')\r\n",
					"#SALES_INVOICES_INVENTORY_ITEMS_TMP.write.mode(\"overwrite\").saveAsTable(\"mdw_uat_Transactions.F_SALES_INVOICES_INVENTORY_ITEMS_TMP\")\r\n",
					"\r\n",
					"PURCHASE_PRICE_VARIANCE.write.parquet('/' + Environment + '/Structured/Facts/F_PURCHASE_PRICE_VARIANCE.parquet', mode='overwrite')\r\n",
					"#PURCHASE_PRICE_VARIANCE.write.mode(\"overwrite\").saveAsTable(\"mdw_uat_Transactions.F_PURCHASE_PRICE_VARIANCE\")\r\n",
					"\r\n",
					"COGS.write.parquet('/' + Environment + '/Structured/Facts/F_COGS.parquet', mode='overwrite')\r\n",
					"#COGS.write.mode(\"overwrite\").saveAsTable(\"mdw_uat_Transactions.F_COGS\")\r\n",
					"\r\n",
					"TMPDOCDISC.write.parquet('/' + Environment + '/Structured/Facts/F_TMPDOCDISC.parquet', mode='overwrite')\r\n",
					"#TMPDOCDISC.write.mode(\"overwrite\").saveAsTable(\"mdw_uat_Transactions.F_TMPDOCDISC\")\r\n",
					"\r\n",
					"Goods_Receipts_HACK.write.parquet('/' + Environment + '/Structured/Facts/F_Goods_Receipts_HACK.parquet', mode='overwrite')\r\n",
					"#Goods_Receipts_HACK.write.mode(\"overwrite\").saveAsTable(\"mdw_uat_Transactions.F_Goods_Receipts_HACK\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## End of Part9 scripting"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Execute Script 10- Finance_Data_Preparation_Accounts"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Load the Source Files for the night part of the scripting\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#dboOACT = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboOACT.parquet', format='parquet')\r\n",
					"#dboOACT.createOrReplaceTempView(\"dboOACT\")"
				],
				"execution_count": 21
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run PROD/Transactions Notebooks PROD/10- Finance_Data_Preparation_Accounts_PROD"
				],
				"execution_count": 22
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Write Fact Tables back to Blobstorage"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"TMPACCOUNTS.write.parquet('/' + Environment + '/Structured/Dimensions/DIM_TMPACCOUNTS.parquet', mode='overwrite')\r\n",
					"\r\n",
					"Account_Expose.write.parquet('/' + Environment + '/Structured/Dimensions/DIM_Account_Expose.parquet', mode='overwrite')\r\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## End of Part10 scripting"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Execute Script 11- Transactions_Continued"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Load the Source Files for the night part of the scripting\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"TRANSACTIONS = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net//DataLake/Structured/Facts/F_TRANSACTIONS.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"TRANSACTIONS.createOrReplaceTempView(\"TRANSACTIONS\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"Dim_ITEM = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/DataLake/Structured/Dimensions/Dim_ITEM.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"Dim_ITEM.createOrReplaceTempView(\"Dim_ITEM\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"Goods_Issues = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/DataLake/Structured/Facts/F_Goods_Issues.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"Goods_Issues.createOrReplaceTempView(\"Goods_Issues\")\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#dboOACT = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboOACT.parquet', format='parquet')\r\n",
					"#dboOACT.createOrReplaceTempView(\"dboOACT\")"
				],
				"execution_count": 23
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run PROD/Transactions Notebooks PROD/11- Transactions_Continued_PROD"
				],
				"execution_count": 24
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Write Fact table to Blobstorage"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"TRANSACTIONS.write.parquet('/' + Environment + '/Structured/Facts/F_TRANSACTIONS_CONTINUED.parquet', mode='overwrite')\r\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## End of Part11 scripting"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Execute Script 12- Finance_Data_Preparation_BudgetPreFY18"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Load the Source Files for the night part of the scripting\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"F_Budget = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/DataLake/Structured/Facts/F_Budget.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"F_Budget.createOrReplaceTempView(\"F_Budget\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"Dim_DATE = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/DataLake/Structured/Dimensions/DIM_DATES.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"Dim_DATE.createOrReplaceTempView(\"Dim_DATE\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"F_TRANSACTIONS = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/DataLake/Structured/Facts/F_TRANSACTIONS.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"F_TRANSACTIONS.createOrReplaceTempView(\"F_TRANSACTIONS\")"
				],
				"execution_count": 25
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"%run PROD/Transactions Notebooks PROD/12- Finance_Data_Preparation_BudgetPreFY18_PROD"
				],
				"execution_count": 26
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## End of Part12 scripting"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Execute Script 13- Finance_Data_Preparation_Account"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Load the Source Files for the night part of the scripting\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"TMPACCOUNTS = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/DataLake/Structured/Dimensions/DIM_TMPACCOUNTS.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"TMPACCOUNTS.createOrReplaceTempView(\"TMPACCOUNTS\")\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#dboOACT = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboOACT.parquet', format='parquet')\r\n",
					"#dboOACT.createOrReplaceTempView(\"dboOACT\")\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#dboOASC = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboOASC.parquet', format='parquet')\r\n",
					"#dboOASC.createOrReplaceTempView(\"dboOASC\")"
				],
				"execution_count": 27
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run PROD/Transactions Notebooks PROD/13- Finance_Data_Preparation_Account_PROD"
				],
				"execution_count": 28
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Write Structured table to Blobstorage"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"ACCOUNT.write.parquet('/' + Environment + '/Structured/Dimensions/DIM_ACCOUNT.parquet', mode='overwrite')\r\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## End of Part13 scripting"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Execute Script 14- Finance_Data_Preparation_Depreciation"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Load the Source Files for the night part of the scripting\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"F_FIXED_ASSETS_CURRENT_VALUE = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/DataLake/Structured/Facts/F_FIXED_ASSETS_CURRENT_VALUE.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"F_FIXED_ASSETS_CURRENT_VALUE.createOrReplaceTempView(\"F_FIXED_ASSETS_CURRENT_VALUE\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"F_FIXED_ASSETS_DEPRECIATION = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/DataLake/Structured/Facts/F_FIXED_ASSETS_DEPRECIATION.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"F_FIXED_ASSETS_DEPRECIATION.createOrReplaceTempView(\"F_FIXED_ASSETS_DEPRECIATION\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"TRANSACTIONS = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/DataLake/Structured/Facts/F_TRANSACTIONS_CONTINUED.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"TRANSACTIONS.createOrReplaceTempView(\"TRANSACTIONS\")"
				],
				"execution_count": 29
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run PROD/Transactions Notebooks PROD/14- Finance_Data_Preparation_Depreciation_PROD"
				],
				"execution_count": 30
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Write Structured table to Blobstorage"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"DEPRECIATION.write.parquet('/' + Environment + '/Structured/Facts/F_DEPRECIATION.parquet', mode='overwrite')\r\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## End of Part14 scripting"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Execute Script 15- Finance_Data_Preparation_Key"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Load the Source Files for the night part of the scripting\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"TRANSACTIONS = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net//DataLake/Structured/Facts/F_TRANSACTIONS_CONTINUED.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"TRANSACTIONS.createOrReplaceTempView(\"TRANSACTIONS\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"DEPRECIATION = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net//DataLake/Structured/Facts/F_DEPRECIATION.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"DEPRECIATION.createOrReplaceTempView(\"DEPRECIATION\")"
				],
				"execution_count": 31
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run PROD/Transactions Notebooks PROD/15- Finance_Data_Preparation_Key_PROD"
				],
				"execution_count": 32
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Write Structured table to Blobstorage"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"KEY.write.parquet('/' + Environment + '/Structured/Facts/F_KEY.parquet', mode='overwrite')\r\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## End of Part15 scripting"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Execute Script 16- Finance_Data_Preparation_Customer"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Load the Source Files for the night part of the scripting\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"KEY = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/DataLake/Structured/Facts/F_KEY.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"KEY.createOrReplaceTempView(\"F_KEY\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"dboOCRD = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboOCRD.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"dboOCRD.createOrReplaceTempView(\"dboOCRD\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"dboOCTG = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboOCTG.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"dboOCTG.createOrReplaceTempView(\"dboOCTG\")"
				],
				"execution_count": 33
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run PROD/Transactions Notebooks PROD/16- Finance_Data_Preparation_Customer_PROD"
				],
				"execution_count": 34
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Write Structured table to Blobstorage"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"CUSTOMER_Expose.write.parquet('/' + Environment + '/Structured/Dimensions/DIM_CUSTOMER_Expose.parquet', mode='overwrite')\r\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## End of Part16 scripting"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Execute Script 17- Finance_Data_Preparation_Supplier"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Load the Source Files for the night part of the scripting\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"KEY = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/DataLake/Structured/Facts/F_KEY.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"KEY.createOrReplaceTempView(\"F_KEY\")\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#dboOCRD = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboOCRD.parquet', format='parquet')\r\n",
					"#dboOCRD.createOrReplaceTempView(\"dboOCRD\")\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#dboOCTG = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboOCTG.parquet', format='parquet')\r\n",
					"#dboOCTG.createOrReplaceTempView(\"dboOCTG\")"
				],
				"execution_count": 35
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run PROD/Transactions Notebooks PROD/17- Finance_Data_Preparation_Supplier_PROD"
				],
				"execution_count": 36
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Write Structured table to Blobstorage"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"SUPPLIER_Expose.write.parquet('/' + Environment + '/Structured/Dimensions/DIM_SUPPLIER_Expose.parquet', mode='overwrite')\r\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## End of Part17 scripting"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Execute Script 18- Finance_Data_Preparation_Transform"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Load the Source Files for the night part of the scripting\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"TRANSACTIONS = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/DataLake/Structured/Facts/F_TRANSACTIONS_CONTINUED.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"TRANSACTIONS.createOrReplaceTempView(\"TRANSACTIONS\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"TMPACCOUNTS = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/DataLake/Structured/Dimensions/DIM_TMPACCOUNTS.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"TMPACCOUNTS.createOrReplaceTempView(\"TMPACCOUNTS\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"ACCOUNT = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/DataLake/Structured/Dimensions/DIM_ACCOUNT.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"ACCOUNT.createOrReplaceTempView(\"DIM_ACCOUNT\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"F_Goods_Receipt_Notes = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/DataLake/Structured/Facts/F_Goods_Receipt_Notes.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"F_Goods_Receipt_Notes.createOrReplaceTempView(\"F_Goods_Receipt_Notes\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"KEY = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/DataLake/Structured/Facts/F_KEY.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"KEY.createOrReplaceTempView(\"F_KEY\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"F_MFRHISTORY = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/DataLake/Structured/Facts/F_MFRHISTORY.parquet', mode='overwrite')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"F_MFRHISTORY.createOrReplaceTempView(\"F_MFRHISTORY\")\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#ListDepots = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/CCMasterData/ListDepots.parquet', format='parquet')\r\n",
					"#ListDepots.createOrReplaceTempView(\"ListDepots\")"
				],
				"execution_count": 37
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run PROD/Transactions Notebooks PROD/18- Finance_Data_Preparation_Transform_PROD"
				],
				"execution_count": 38
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Write Structured table to Blobstorage"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"TRANSACTIONS.write.parquet('/' + Environment + '/Structured/Facts/F_TRANSACTIONS_Transformed.parquet', mode='overwrite')\r\n",
					"\r\n",
					"DIM_PO_LOOKUP.write.parquet('/' + Environment + '/Structured/Dimensions/DIM_PO_LOOKUP.parquet', mode='overwrite')\r\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## End of Part18 scripting"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Execute Script 19- Finance_SAP_DIM_MFR"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Load the Source Files for the night part of the scripting\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"VEHICLE_H = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/@VEHICLE_H.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"VEHICLE_H.createOrReplaceTempView(\"VEHICLE_H\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"dboMFRHistoryDaily = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboMFRHistoryDaily.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"dboMFRHistoryDaily.createOrReplaceTempView(\"dboMFRHistoryDaily\")"
				],
				"execution_count": 39
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run PROD/Transactions Notebooks PROD/19- Finance_SAP_DIM_MFR_PROD"
				],
				"execution_count": 40
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Write Structured table to Blobstorage"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"HISTORY_JOIN.write.parquet('/' + Environment + '/Structured/Facts/F_MFRHISTORY.parquet', mode='overwrite')\r\n",
					"\r\n",
					"SAP_MFR.write.parquet('/' + Environment + '/Structured/Dimensions/DIM_MFR.parquet', mode='overwrite')\r\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## End of Part19 scripting"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Execute Script 20 - Finace_SAP_DIM_Assets"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Load the Source Files for the night part of the scripting\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"dboXTSD_XA_ASR = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboXTSD_XA_ASR.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"dboXTSD_XA_ASR.createOrReplaceTempView(\"dboXTSD_XA_ASR\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"dboXTSD_XA_DEP = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/SAP/dboXTSD_XA_DEP.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"dboXTSD_XA_DEP.createOrReplaceTempView(\"dboXTSD_XA_DEP\")"
				],
				"execution_count": 41
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run PROD/Transactions Notebooks PROD/20 - Finace_SAP_DIM_Assets_PROD"
				],
				"execution_count": 42
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Write Structured table to Blobstorage"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"F_FIXED_ASSETS_CURRENT_VALUE.write.parquet('/' + Environment + '/Structured/Facts/F_FIXED_ASSETS_CURRENT_VALUE.parquet', mode='overwrite')\r\n",
					"\r\n",
					"F_FIXED_ASSETS_DEPRECIATION.write.parquet('/' + Environment + '/Structured/Facts/F_FIXED_ASSETS_DEPRECIATION.parquet', mode='overwrite')\r\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## End if Part20 scripting"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Execute Script 21- Finance_SAP_DIM_PO LOOKUP"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Load the Source Files for the night part of the scripting\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"F_Goods_Receipt_Notes = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/DataLake/Structured/Facts/F_Goods_Receipt_Notes.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"F_Goods_Receipt_Notes.createOrReplaceTempView(\"F_Goods_Receipt_Notes\")\r\n",
					"\r\n",
					"## Read .parquet file from Blobstorage enviroment\r\n",
					"F_TRANSACTIONS_FINAL = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/DataLake/Structured/Facts/F_TRANSACTIONS_CONTINUED.parquet', format='parquet')\r\n",
					"## Expose the .parquet file as a temp SQL view via pyspark (This is done so that SQL scripting can take place on the files data %note that the schema is predefined in .parquet files.%)\r\n",
					"F_TRANSACTIONS_FINAL.createOrReplaceTempView(\"F_TRANSACTIONS_FINAL\")\r\n",
					"\r\n",
					"# .parquet File is already loaded (Removed in this script for improved automatization)\r\n",
					"#ListDepots = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net/Unstructured Data/CCMasterData/ListDepots.parquet', format='parquet')\r\n",
					"#ListDepots.createOrReplaceTempView(\"ListDepots\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run PROD/Transactions Notebooks PROD/21- Finance_SAP_DIM_PO LOOKUP_PROD"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Write Structured table to Blobstorage"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"DIM_PO_LOOKUP.write.parquet('/' + Environment + '/Structured/Dimensions/DIM_PO_LOOKUP.parquet', mode='overwrite')\r\n",
					"\r\n",
					"GROUP.write.parquet('/' + Environment + '/Structured/Facts/GROUP.parquet', mode='overwrite')\r\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## End of Part21 scripting"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## End of Automation Scripting for Transactions"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Load the Source Files for the night part of the scripting\r\n",
					"\r\n",
					"TRANSACTIONS = spark.read.load('abfss://synapse@citylogisticsstorage.dfs.core.windows.net//DataLake/Structured/Facts/F_TRANSACTIONS_Transformed.parquet', format='parquet')\r\n",
					"TRANSACTIONS.createOrReplaceTempView(\"TRANSACTIONS\")"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run /DEV/Transactions Notebooks/15- 02d_Finance_Data_Preparation_26_TransactionFinal"
				]
			}
		]
	}
}